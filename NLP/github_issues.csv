html_url,title,comments,body
https://github.com/huggingface/datasets/issues/2945,Protect master branch,"['Cool, I think we can do both :)'
 '@lhoestq now the 2 are implemented.\r\n\r\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).']","After accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:
- 00cc036fea7c7745cfe722360036ed306796a3f2
- 13ae8c98602bbad8197de3b9b425f4c78f582af1
- ...

I propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:
- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch
  - Currently, simple merge commits are already disabled
  - I propose to disable rebase merging as well
- ~~Protect the master branch from direct pushes (to avoid accidentally pushing of merge commits)~~
  - ~~This protection would reject direct pushes to master branch~~
  - ~~If so, for each release (when we need to commit directly to the master branch), we should previously disable the protection and re-enable it again after the release~~
- [x] Protect the master branch only from direct pushing of **merge commits**
  - GitHub offers the possibility to protect the master branch only from merge commits (which are the ones that introduce all the commits from the feature branch into the master branch).
  - No need to disable/re-enable this protection on each release 

This purpose of this Issue is to open a discussion about this problem and to agree in a solution."
https://github.com/huggingface/datasets/issues/2943,Backwards compatibility broken for cached datasets that use `.filter()`,"[""Hi ! I guess the caching mechanism should have considered the new `filter` to be different from the old one, and don't use cached results from the old `filter`.\r\nTo avoid other users from having this issue we could make the caching differentiate the two, what do you think ?""
 ""If it's easy enough to implement, then yes please 😄  But this issue can be low-priority, since I've only encountered it in a couple of `transformers` CI tests.""
 ""Well it can cause issue with anyone that updates `datasets` and re-run some code that uses filter, so I'm creating a PR""
 ""I just merged a fix, let me know if you're still having this kind of issues :)\r\n\r\nWe'll do a release soon to make this fix available""
 'Definitely works on several manual cases with our dummy datasets, thank you @lhoestq !'
 'Fixed by #2947.']","## Describe the bug
After upgrading to datasets `1.12.0`, some cached `.filter()` steps from `1.11.0` started failing with 
`ValueError: Keys mismatch: between {'indices': Value(dtype='uint64', id=None)} and {'file': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'speaker_id': Value(dtype='int64', id=None), 'chapter_id': Value(dtype='int64', id=None), 'id': Value(dtype='string', id=None)}`

Related feature: https://github.com/huggingface/datasets/pull/2836

:question:  This is probably a `wontfix` bug, since it can be solved by simply cleaning the related cache dirs, but the workaround could be useful for someone googling the error :) 

## Workaround
Remove the cache for the given dataset, e.g. `rm -rf ~/.cache/huggingface/datasets/librispeech_asr`.

## Steps to reproduce the bug
1. Delete `~/.cache/huggingface/datasets/librispeech_asr` if it exists.

2. `pip install datasets==1.11.0` and run the following snippet:

```python
from datasets import load_dataset

ids = [""1272-141231-0000""]
ds = load_dataset(""patrickvonplaten/librispeech_asr_dummy"", ""clean"", split=""validation"")
ds = ds.filter(lambda x: x[""id""] in ids)
```
3. `pip install datasets==1.12.1` and re-run the code again

## Expected results
Same result as with the previous `datasets` version.

## Actual results
```bash
Reusing dataset librispeech_asr (./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1)
Loading cached processed dataset at ./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1/cache-cd1c29844fdbc87a.arrow
Traceback (most recent call last):
  File ""./repos/transformers/src/transformers/models/wav2vec2/try_dataset.py"", line 5, in <module>
    ds = ds.filter(lambda x: x[""id""] in ids)
  File ""./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 185, in wrapper
    out: Union[""Dataset"", ""DatasetDict""] = func(self, *args, **kwargs)
  File ""./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py"", line 398, in wrapper
    out = func(self, *args, **kwargs)
  File ""./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 2169, in filter
    indices = self.map(
  File ""./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 1686, in map
    return self._map_single(
  File ""./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 185, in wrapper
    out: Union[""Dataset"", ""DatasetDict""] = func(self, *args, **kwargs)
  File ""./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py"", line 398, in wrapper
    out = func(self, *args, **kwargs)
  File ""./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 1896, in _map_single
    return Dataset.from_file(cache_file_name, info=info, split=self.split)
  File ""./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 343, in from_file
    return cls(
  File ""./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 282, in __init__
    self.info.features = self.info.features.reorder_fields_as(inferred_features)
  File ""./envs/transformers/lib/python3.8/site-packages/datasets/features.py"", line 1151, in reorder_fields_as
    return Features(recursive_reorder(self, other))
  File ""./envs/transformers/lib/python3.8/site-packages/datasets/features.py"", line 1140, in recursive_reorder
    raise ValueError(f""Keys mismatch: between {source} and {target}"" + stack_position)
ValueError: Keys mismatch: between {'indices': Value(dtype='uint64', id=None)} and {'file': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'speaker_id': Value(dtype='int64', id=None), 'chapter_id': Value(dtype='int64', id=None), 'id': Value(dtype='string', id=None)}

Process finished with exit code 1

```

## Environment info
- `datasets` version: 1.12.1
- Platform: Linux-5.11.0-34-generic-x86_64-with-glibc2.17
- Python version: 3.8.10
- PyArrow version: 5.0.0
"
https://github.com/huggingface/datasets/issues/2941,OSCAR unshuffled_original_ko: NonMatchingSplitsSizesError,['I tried `unshuffled_original_da` and it is also not working'],"## Describe the bug

Cannot download OSCAR `unshuffled_original_ko` due to `NonMatchingSplitsSizesError`.

## Steps to reproduce the bug

```python
>>> dataset = datasets.load_dataset('oscar', 'unshuffled_original_ko')
NonMatchingSplitsSizesError: [{'expected': SplitInfo(name='train', num_bytes=25292102197, num_examples=7345075, dataset_name='oscar'), 'recorded': SplitInfo(name='train', num_bytes=25284578514, num_examples=7344907, dataset_name='oscar')}]
```

## Expected results

Loading is successful.

## Actual results

Loading throws above error.

## Environment info

- `datasets` version: 1.12.1
- Platform: Linux-5.4.0-81-generic-x86_64-with-glibc2.29
- Python version: 3.8.10
- PyArrow version: 5.0.0
"
https://github.com/huggingface/datasets/issues/2937,load_dataset using default cache on Windows causes PermissionError: [WinError 5] Access is denied,"[""Hi @daqieq, thanks for reporting.\r\n\r\nUnfortunately, I was not able to reproduce this bug:\r\n```ipython\r\nIn [1]: from datasets import load_dataset\r\n   ...: ds = load_dataset('wiki_bio')\r\nDownloading: 7.58kB [00:00, 26.3kB/s]\r\nDownloading: 2.71kB [00:00, ?B/s]\r\nUsing custom data configuration default\r\nDownloading and preparing dataset wiki_bio/default (download: 318.53 MiB, generated: 736.94 MiB, post-processed: Unknown size, total: 1.03 GiB) to C:\\Users\\username\\.cache\\huggingface\\datasets\\wiki_bio\\default\\\r\n1.1.0\\5293ce565954ba965dada626f1e79684e98172d950371d266bf3caaf87e911c9...\r\nDownloading: 334MB [01:17, 4.32MB/s]\r\nDataset wiki_bio downloaded and prepared to C:\\Users\\username\\.cache\\huggingface\\datasets\\wiki_bio\\default\\1.1.0\\5293ce565954ba965dada626f1e79684e98172d950371d266bf3caaf87e911c9. Subsequent calls will reuse thi\r\ns data.\r\n```\r\n\r\nThis kind of error messages usually happen because:\r\n- Your running Python script hasn't write access to that directory\r\n- You have another program (the File Explorer?) already browsing inside that directory""
 ""Thanks @albertvillanova for looking at it! I tried on my personal Windows machine and it downloaded just fine.\r\n\r\nRunning on my work machine and on a colleague's machine it is consistently hitting this error. It's not a write access issue because the `.incomplete` directory is written just fine. It just won't rename and then it deletes the directory in the `finally` step. Also the zip file is written and extracted fine in the downloads directory.\r\n\r\nThat leaves another program that might be interfering, and there are plenty of those in my work machine ... (full antivirus, data loss prevention, etc.). So the question remains, why not extend the `try` block to allow catching the error and circle back to the rename after the unknown program is finished doing its 'stuff'. This is the approach that I read about in the linked repo (see my comments above).\r\n\r\nIf it's not high priority, that's fine. However, if someone were to write an PR that solved this issue in our environment in an `except` clause, would it be reviewed for inclusion in a future release? Just wondering whether I should spend any more time on this issue.""]","## Describe the bug
Standard process to download and load the wiki_bio dataset causes PermissionError in Windows 10 and 11.

## Steps to reproduce the bug
```python
from datasets import load_dataset
ds = load_dataset('wiki_bio')
```

## Expected results
It is expected that the dataset downloads without any errors.

## Actual results
PermissionError see trace below:
```
Using custom data configuration default
Downloading and preparing dataset wiki_bio/default (download: 318.53 MiB, generated: 736.94 MiB, post-processed: Unknown size, total: 1.03 GiB) to C:\Users\username\.cache\huggingface\datasets\wiki_bio\default\1.1.0\5293ce565954ba965dada626f1e79684e98172d950371d266bf3caaf87e911c9...
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\username\.conda\envs\hf\lib\site-packages\datasets\load.py"", line 1112, in load_dataset
    builder_instance.download_and_prepare(
  File ""C:\Users\username\.conda\envs\hf\lib\site-packages\datasets\builder.py"", line 644, in download_and_prepare
    self._save_info()
  File ""C:\Users\username\.conda\envs\hf\lib\contextlib.py"", line 120, in __exit__
    next(self.gen)
  File ""C:\Users\username\.conda\envs\hf\lib\site-packages\datasets\builder.py"", line 598, in incomplete_dir
    os.rename(tmp_dir, dirname)
PermissionError: [WinError 5] Access is denied: 'C:\\Users\\username\\.cache\\huggingface\\datasets\\wiki_bio\\default\\1.1.0\\5293ce565954ba965dada626f1e79684e98172d950371d266bf3caaf87e911c9.incomplete' -> 'C:\\Users\\username\\.cache\\huggingface\\datasets\\wiki_bio\\default\\1.1.0\\5293ce565954ba965dada626f1e79684e98172d950371d266bf3caaf87e911c9'
```
By commenting out the os.rename() [L604](https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L604) and the shutil.rmtree() [L607](https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L607) lines, in my virtual environment, I was able to get the load process to complete, rename the directory manually and then rerun the `load_dataset('wiki_bio')` to get what I needed.

It seems that os.rename() in the `incomplete_dir` content manager is the culprit. Here's another project [Conan](https://github.com/conan-io/conan/issues/6560) with similar issue with os.rename() if it helps debug this issue.

## Environment info
- `datasets` version: 1.12.1
- Platform: Windows-10-10.0.22449-SP0
- Python version: 3.8.12
- PyArrow version: 5.0.0
"
https://github.com/huggingface/datasets/issues/2934,"to_tf_dataset keeps a reference to the open data somewhere, causing issues on windows","[""I did some investigation and, as it seems, the bug stems from [this line](https://github.com/huggingface/datasets/blob/8004d7c3e1d74b29c3e5b0d1660331cd26758363/src/datasets/arrow_dataset.py#L325). The lifecycle of the dataset from the linked line is bound to one of the returned `tf.data.Dataset`. So my (hacky) solution involves wrapping the linked dataset with `weakref.proxy` and adding a custom `__del__` to `tf.python.data.ops.dataset_ops.TensorSliceDataset` (this is the type of a dataset that is returned by `tf.data.Dataset.from_tensor_slices`; this works for TF 2.x, but I'm not sure `tf.python.data.ops.dataset_ops` is a valid path for TF 1.x) that deletes the linked dataset, which is assigned to the dataset object as a property. Will open a draft PR soon!""
 'Thanks a lot for investigating !']","To reproduce:
```python
import datasets as ds
import weakref
import gc

d = ds.load_dataset(""mnist"", split=""train"")
ref = weakref.ref(d._data.table)
tfd = d.to_tf_dataset(""image"", batch_size=1, shuffle=False, label_cols=""label"")
del tfd, d
gc.collect()
assert ref() is None, ""Error: there is at least one reference left""
```

This causes issues because the table holds a reference to an open arrow file that should be closed. So on windows it's not possible to delete or move the arrow file afterwards.

Moreover the CI test of the `to_tf_dataset` method isn't able to clean up the temporary arrow files because of this.

cc @Rocketknight1 "
https://github.com/huggingface/datasets/issues/2932,Conda build fails,"['Why 1.9 ?\r\n\r\nhttps://anaconda.org/HuggingFace/datasets currently says 1.11'
 'Alright I added 1.12.0 and 1.12.1 and fixed the conda build #2952 ']","## Describe the bug
Current `datasets` version in conda is 1.9 instead of 1.12.

The build of the conda package fails.
"
https://github.com/huggingface/datasets/issues/2930,Mutable columns argument breaks set_format,['Pushed a fix to my branch #2731 '],"## Describe the bug
If you pass a mutable list to the `columns` argument of `set_format` and then change the list afterwards, the returned columns also change.

## Steps to reproduce the bug
```python
from datasets import load_dataset
dataset = load_dataset(""glue"", ""cola"")

column_list = [""idx"", ""label""]
dataset.set_format(""python"", columns=column_list)
column_list[1] = ""foo""  # Change the list after we call `set_format`
dataset['train'][:4].keys()
```

## Expected results
```python
dict_keys(['idx', 'label'])
```

## Actual results
```python
dict_keys(['idx'])
```"
https://github.com/huggingface/datasets/issues/2927,Datasets 1.12 dataset.filter TypeError: get_indices_from_mask_function() got an unexpected keyword argument,"[""Thanks for reporting, I'm looking into it :)"" 'Fixed by #2950.']","## Describe the bug
Upgrading to 1.12 caused `dataset.filter` call to fail with 

> get_indices_from_mask_function() got an unexpected keyword argument valid_rel_labels


## Steps to reproduce the bug
```pythondef 

filter_good_rows(
    ex: Dict,
    valid_rel_labels: Set[str],
    valid_ner_labels: Set[str],
    tokenizer: PreTrainedTokenizerFast,
) -> bool:
    """"""Get the good rows""""""
    encoding = get_encoding_for_text(text=ex[""text""], tokenizer=tokenizer)
    ex[""encoding""] = encoding
    for relation in ex[""relations""]:
        if not is_valid_relation(relation, valid_rel_labels):
            return False
    for span in ex[""spans""]:
        if not is_valid_span(span, valid_ner_labels, encoding):
            return False
    return True
    
def get_dataset():    
    loader_path = str(Path(__file__).parent / ""prodigy_dataset_builder.py"")
    ds = load_dataset(
        loader_path,
        name=""prodigy-dataset"",
        data_files=sorted(file_paths),
        cache_dir=cache_dir,
    )[""train""]

    valid_ner_labels = set(vocab.ner_category)
    valid_relations = set(vocab.relation_types.keys())
    ds = ds.filter(
        filter_good_rows,
        fn_kwargs=dict(
            valid_rel_labels=valid_relations,
            valid_ner_labels=valid_ner_labels,
            tokenizer=vocab.tokenizer,
        ),
        keep_in_memory=True,
        num_proc=num_proc,
    )

```

`ds` is a `DatasetDict` produced by a jsonl dataset.
This runs fine on 1.11 but fails on 1.12

**Stack Trace**



## Expected results

I expect 1.12 datasets filter to filter the dataset without raising as it does on 1.11

## Actual results
```
tf_ner_rel_lib/dataset.py:695: in load_prodigy_arrow_datasets_from_jsonl
    ds = ds.filter(
../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:185: in wrapper
    out: Union[""Dataset"", ""DatasetDict""] = func(self, *args, **kwargs)
../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/fingerprint.py:398: in wrapper
    out = func(self, *args, **kwargs)
../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:2169: in filter
    indices = self.map(
../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:1686: in map
    return self._map_single(
../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:185: in wrapper
    out: Union[""Dataset"", ""DatasetDict""] = func(self, *args, **kwargs)
../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/fingerprint.py:398: in wrapper
    out = func(self, *args, **kwargs)
../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:2048: in _map_single
    batch = apply_function_on_filtered_inputs(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = {'_input_hash': [2108817714, 1477695082, -1021597032, 2130671338, -1260483858, -1203431639, ...], '_task_hash': [18070...ons', 'relations', 'relations', ...], 'answer': ['accept', 'accept', 'accept', 'accept', 'accept', 'accept', ...], ...}
indices = [0, 1, 2, 3, 4, 5, ...], check_same_num_examples = False, offset = 0

    def apply_function_on_filtered_inputs(inputs, indices, check_same_num_examples=False, offset=0):
        """"""Utility to apply the function on a selection of columns.""""""
        nonlocal update_data
        fn_args = [inputs] if input_columns is None else [inputs[col] for col in input_columns]
        if offset == 0:
            effective_indices = indices
        else:
            effective_indices = [i + offset for i in indices] if isinstance(indices, list) else indices + offset
        processed_inputs = (
>           function(*fn_args, effective_indices, **fn_kwargs) if with_indices else function(*fn_args, **fn_kwargs)
        )
E       TypeError: get_indices_from_mask_function() got an unexpected keyword argument 'valid_rel_labels'

../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:1939: TypeError
```

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: 1.12.1
- Platform: Mac
- Python version: 3.8.9
- PyArrow version: pyarrow==5.0.0

"
https://github.com/huggingface/datasets/issues/2924,"""File name too long"" error for file locks","['Hi, the filename here is less than 255\r\n```python\r\n>>> len(""_home_garrett_.cache_huggingface_datasets_csv_test-7c856aea083a7043_0.0.0_9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff.incomplete.lock"")\r\n154\r\n```\r\nso not sure why it\'s considered too long for your filesystem.\r\n(also note that the lock files we use always have smaller filenames than 255)\r\n\r\nhttps://github.com/huggingface/datasets/blob/5d1a9f1e3c6c495dc0610b459e39d2eb8893f152/src/datasets/utils/filelock.py#L135-L135'
 ""Yes, you're right! I need to get you more info here. Either there's something going with the name itself that the file system doesn't like (an encoding that blows up the name length??) or perhaps there's something with the path that's causing the entire string to  be used as a name. I haven't seen this on any system before and the Internet's not forthcoming with any info.""]","## Describe the bug

Getting the following error when calling `load_dataset(""gar1t/test"")`:

```
OSError: [Errno 36] File name too long: '<user>/.cache/huggingface/datasets/_home_garrett_.cache_huggingface_datasets_csv_test-7c856aea083a7043_0.0.0_9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff.incomplete.lock'
```

## Steps to reproduce the bug

Where the user cache dir (e.g. `~/.cache`) is on a file system that limits filenames to 255 chars (e.g. ext4):

```python
from datasets import load_dataset
load_dataset(""gar1t/test"")
```

## Expected results

Expect the function to return without an error.

## Actual results

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<python_venv>/lib/python3.9/site-packages/datasets/load.py"", line 1112, in load_dataset
    builder_instance.download_and_prepare(
  File ""<python_venv>/lib/python3.9/site-packages/datasets/builder.py"", line 644, in download_and_prepare
    self._save_info()
  File ""<python_venv>/lib/python3.9/site-packages/datasets/builder.py"", line 765, in _save_info
    with FileLock(lock_path):
  File ""<python_venv>/lib/python3.9/site-packages/datasets/utils/filelock.py"", line 323, in __enter__
    self.acquire()
  File ""<python_venv>/lib/python3.9/site-packages/datasets/utils/filelock.py"", line 272, in acquire
    self._acquire()
  File ""<python_venv>/lib/python3.9/site-packages/datasets/utils/filelock.py"", line 403, in _acquire
    fd = os.open(self._lock_file, open_mode)
OSError: [Errno 36] File name too long: '<user>/.cache/huggingface/datasets/_home_garrett_.cache_huggingface_datasets_csv_test-7c856aea083a7043_0.0.0_9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff.incomplete.lock'
```

## Environment info

- `datasets` version: 1.12.1
- Platform: Linux-5.11.0-27-generic-x86_64-with-glibc2.31
- Python version: 3.9.7
- PyArrow version: 5.0.0
"
https://github.com/huggingface/datasets/issues/2919,Unwanted progress bars when accessing examples,['doing a patch release now :)'],"When accessing examples from a dataset formatted for pytorch, some progress bars appear when accessing examples:
```python
In [1]: import datasets as ds                                        

In [2]: d = ds.Dataset.from_dict({""a"": [0, 1, 2]}).with_format(""torch"")                                                           

In [3]: d[0]                                                         
100%|████████████████████████████████| 1/1 [00:00<00:00, 3172.70it/s]
Out[3]: {'a': tensor(0)}
```

This is because the pytorch formatter calls `map_nested` that uses progress bars

cc @sgugger "
https://github.com/huggingface/datasets/issues/2918,`Can not decode content-encoding: gzip` when loading `scitldr` dataset with streaming,"['Hi @SBrandeis, thanks for reporting! ^^\r\n\r\nI think this is an issue with `fsspec`: https://github.com/intake/filesystem_spec/issues/389\r\n\r\nI will ask them if they are planning to fix it...'
 'Code to reproduce the bug: `ClientPayloadError: 400, message=\'Can not decode content-encoding: gzip\'`\r\n```python\r\nIn [1]: import fsspec\r\n\r\nIn [2]: import json\r\n\r\nIn [3]: with fsspec.open(\'https://raw.githubusercontent.com/allenai/scitldr/master/SciTLDR-Data/SciTLDR-FullText/test.jsonl\', encoding=""utf-8"") as f:\r\n   ...:     for row in f:\r\n   ...:         data = json.loads(row)\r\n   ...:\r\n---------------------------------------------------------------------------\r\nClientPayloadError                        Traceback (most recent call last)\r\n```'
 'Thanks for investigating @albertvillanova ! 🤗 ']","## Describe the bug

Trying to load the `""FullText""` config of the `""scitldr""` dataset with `streaming=True` raises an error from `aiohttp`:
```python
ClientPayloadError: 400, message='Can not decode content-encoding: gzip'
```

cc @lhoestq 

## Steps to reproduce the bug
```python
from datasets import load_dataset

iter_dset = iter(
    load_dataset(""scitldr"", name=""FullText"", split=""test"", streaming=True)
)

next(iter_dset)
```

## Expected results
Returns the first sample of the dataset

## Actual results
Calling `__next__` crashes with the following Traceback:

```python
----> 1 next(dset_iter)

~\miniconda3\envs\datasets\lib\site-packages\datasets\iterable_dataset.py in __iter__(self)
    339
    340     def __iter__(self):
--> 341         for key, example in self._iter():
    342             if self.features:
    343                 # we encode the example for ClassLabel feature types for example

~\miniconda3\envs\datasets\lib\site-packages\datasets\iterable_dataset.py in _iter(self)
    336         else:
    337             ex_iterable = self._ex_iterable
--> 338         yield from ex_iterable
    339
    340     def __iter__(self):

~\miniconda3\envs\datasets\lib\site-packages\datasets\iterable_dataset.py in __iter__(self)
     76
     77     def __iter__(self):
---> 78         for key, example in self.generate_examples_fn(**self.kwargs):
     79             yield key, example
     80

~\.cache\huggingface\modules\datasets_modules\datasets\scitldr\72d6e2195786c57e1d343066fb2cc4f93ea39c5e381e53e6ae7c44bbfd1f05ef\scitldr.py in _generate_examples(self, filepath, split)
    162
    163         with open(filepath, encoding=""utf-8"") as f:
--> 164             for id_, row in enumerate(f):
    165                 data = json.loads(row)
    166                 if self.config.name == ""AIC"":

~\miniconda3\envs\datasets\lib\site-packages\fsspec\implementations\http.py in read(self, length)
    496         else:
    497             length = min(self.size - self.loc, length)
--> 498         return super().read(length)
    499
    500     async def async_fetch_all(self):

~\miniconda3\envs\datasets\lib\site-packages\fsspec\spec.py in read(self, length)
   1481             # don't even bother calling fetch
   1482             return b""""
-> 1483         out = self.cache._fetch(self.loc, self.loc + length)
   1484         self.loc += len(out)
   1485         return out

~\miniconda3\envs\datasets\lib\site-packages\fsspec\caching.py in _fetch(self, start, end)
    378         elif start < self.start:
    379             if self.end - end > self.blocksize:
--> 380                 self.cache = self.fetcher(start, bend)
    381                 self.start = start
    382             else:

~\miniconda3\envs\datasets\lib\site-packages\fsspec\asyn.py in wrapper(*args, **kwargs)
     86     def wrapper(*args, **kwargs):
     87         self = obj or args[0]
---> 88         return sync(self.loop, func, *args, **kwargs)
     89
     90     return wrapper

~\miniconda3\envs\datasets\lib\site-packages\fsspec\asyn.py in sync(loop, func, timeout, *args, **kwargs)
     67         raise FSTimeoutError
     68     if isinstance(result[0], BaseException):
---> 69         raise result[0]
     70     return result[0]
     71

~\miniconda3\envs\datasets\lib\site-packages\fsspec\asyn.py in _runner(event, coro, result, timeout)
     23         coro = asyncio.wait_for(coro, timeout=timeout)
     24     try:
---> 25         result[0] = await coro
     26     except Exception as ex:
     27         result[0] = ex

~\miniconda3\envs\datasets\lib\site-packages\fsspec\implementations\http.py in async_fetch_range(self, start, end)
    538             if r.status == 206:
    539                 # partial content, as expected
--> 540                 out = await r.read()
    541             elif ""Content-Length"" in r.headers:
    542                 cl = int(r.headers[""Content-Length""])

~\miniconda3\envs\datasets\lib\site-packages\aiohttp\client_reqrep.py in read(self)
   1030         if self._body is None:
   1031             try:
-> 1032                 self._body = await self.content.read()
   1033                 for trace in self._traces:
   1034                     await trace.send_response_chunk_received(

~\miniconda3\envs\datasets\lib\site-packages\aiohttp\streams.py in read(self, n)
    342     async def read(self, n: int = -1) -> bytes:
    343         if self._exception is not None:
--> 344             raise self._exception
    345
    346         # migration problem; with DataQueue you have to catch

ClientPayloadError: 400, message='Can not decode content-encoding: gzip'
```

## Environment info

- `datasets` version: 1.12.0
- Platform: Windows-10-10.0.19041-SP0
- Python version: 3.8.5
- PyArrow version: 2.0.0
- aiohttp version: 3.7.4.post0
"
https://github.com/huggingface/datasets/issues/2917,windows download abnormal,"[""Hi ! Is there some kind of proxy that is configured in your browser that gives you access to internet ? If it's the case it could explain why it doesn't work in the code, since the proxy wouldn't be used""
 'It is indeed an agency problem, thank you very, very much'
 'Let me know if you have other questions :)\r\n\r\nClosing this issue now']","## Describe the bug
The script clearly exists (accessible from the browser), but the script download fails on windows. Then I tried it again and it can be downloaded normally on linux. why??
## Steps to reproduce the bug
```python3.7 + windows
![image](https://user-images.githubusercontent.com/52347799/133436174-4303f847-55d5-434f-a749-08da3bb9b654.png)


# Sample code to reproduce the bug
```

## Expected results
It can be downloaded normally.

## Actual results
it cann't

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version:1.11.0
- Platform:windows
- Python version:3.7
- PyArrow version:
"
https://github.com/huggingface/datasets/issues/2914,Having a dependency defining fsspec entrypoint raises an AttributeError when importing datasets,['Closed by #2915.'],"## Describe the bug
In one of my project, I defined a custom fsspec filesystem with an entrypoint.
My guess is that by doing so, a variable named `spec` is created in the module `fsspec` (created by entering a for loop as there are entrypoints defined, see the loop in question [here](https://github.com/intake/filesystem_spec/blob/0589358d8a029ed6b60d031018f52be2eb721291/fsspec/__init__.py#L55)).
So that `fsspec.spec`, that was previously referring to the `spec` submodule, is now referring to that `spec` variable.
This make the import of datasets failing as it is using that `fsspec.spec`.

## Steps to reproduce the bug
I could reproduce the bug with a dummy poetry project.

Here is the pyproject.toml:
```toml
[tool.poetry]
name = ""debug-datasets""
version = ""0.1.0""
description = """"
authors = [""Pierre Godard""]

[tool.poetry.dependencies]
python = ""^3.8""
datasets = ""^1.11.0""

[tool.poetry.dev-dependencies]

[build-system]
requires = [""poetry-core>=1.0.0""]
build-backend = ""poetry.core.masonry.api""

[tool.poetry.plugins.""fsspec.specs""]
""file2"" = ""fsspec.implementations.local.LocalFileSystem""
```

The only other file being a `debug_datasets/__init__.py` empty file.

The overall structure of the project is as follows:
```
.
├── pyproject.toml
└── debug_datasets
    └── __init__.py
```

Then, within the project folder run:

```
poetry install
poetry run python
```

And in the python interpreter, try to import `datasets`:

```
import datasets
```

## Expected results
The import should run successfully.

## Actual results

Here is the trace of the error I get:

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/godarpi/.cache/pypoetry/virtualenvs/debug-datasets-JuFzTKL--py3.8/lib/python3.8/site-packages/datasets/__init__.py"", line 33, in <module>
    from .arrow_dataset import Dataset, concatenate_datasets
  File ""/home/godarpi/.cache/pypoetry/virtualenvs/debug-datasets-JuFzTKL--py3.8/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 48, in <module>
    from .filesystems import extract_path_from_uri, is_remote_filesystem
  File ""/home/godarpi/.cache/pypoetry/virtualenvs/debug-datasets-JuFzTKL--py3.8/lib/python3.8/site-packages/datasets/filesystems/__init__.py"", line 30, in <module>
    def is_remote_filesystem(fs: fsspec.spec.AbstractFileSystem) -> bool:
AttributeError: 'EntryPoint' object has no attribute 'AbstractFileSystem'
```

## Suggested fix

`datasets/filesystems/__init__.py`, line 30, replace:
```
    def is_remote_filesystem(fs: fsspec.spec.AbstractFileSystem) -> bool:
```
by:
```
    def is_remote_filesystem(fs: fsspec.AbstractFileSystem) -> bool:
```

I will come up with a PR soon if this effectively solves the issue.

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: 1.11.0
- Platform: WSL2 (Ubuntu 20.04.1 LTS)
- Python version: 3.8.5
- PyArrow version: 5.0.0
- `fsspec` version: 2021.8.1
"
https://github.com/huggingface/datasets/issues/2913,timit_asr dataset only includes one text phrase,"['Hi @margotwagner, \r\nThis bug was fixed in #1995. Upgrading the datasets should work (min v1.8.0 ideally)'
 'Hi @margotwagner,\r\n\r\nYes, as @bhavitvyamalik has commented, this bug was fixed in `datasets` version 1.5.0. You need to update it, as your current version is 1.4.1:\r\n> Environment info\r\n> - `datasets` version: 1.4.1']","## Describe the bug
The dataset 'timit_asr' only includes one text phrase. It only includes the transcription ""Would such an act of refusal be useful?"" multiple times rather than different phrases.

## Steps to reproduce the bug
Note: I am following the tutorial https://huggingface.co/blog/fine-tune-wav2vec2-english

1. Install the dataset and other packages
```python
!pip install datasets>=1.5.0
!pip install transformers==4.4.0
!pip install soundfile
!pip install jiwer
```
2. Load the dataset
```python
from datasets import load_dataset, load_metric

timit = load_dataset(""timit_asr"")
```
3. Remove columns that we don't want
```python
timit = timit.remove_columns([""phonetic_detail"", ""word_detail"", ""dialect_region"", ""id"", ""sentence_type"", ""speaker_id""])
```
4. Write a short function to display some random samples of the dataset.
```python
from datasets import ClassLabel
import random
import pandas as pd
from IPython.display import display, HTML

def show_random_elements(dataset, num_examples=10):
    assert num_examples <= len(dataset), ""Can't pick more elements than there are in the dataset.""
    picks = []
    for _ in range(num_examples):
        pick = random.randint(0, len(dataset)-1)
        while pick in picks:
            pick = random.randint(0, len(dataset)-1)
        picks.append(pick)
    
    df = pd.DataFrame(dataset[picks])
    display(HTML(df.to_html()))

show_random_elements(timit[""train""].remove_columns([""file""]))
```

## Expected results
10 random different transcription phrases.

## Actual results
10 of the same transcription phrase ""Would such an act of refusal be useful?""

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: 1.4.1
- Platform: macOS-10.15.7-x86_64-i386-64bit
- Python version: 3.8.5
- PyArrow version: not listed
"
https://github.com/huggingface/datasets/issues/2904,FORCE_REDOWNLOAD does not work,"[""Hi ! Thanks for reporting. The error seems to happen only if you use compressed files.\r\n\r\nThe second dataset is prepared in another dataset cache directory than the first - which is normal, since the source file is different. However, it doesn't uncompress the new data file because it finds the old uncompressed data in the extraction cache directory.\r\n\r\nIf we fix the extraction cache mechanism to uncompress a local file if it changed then it should fix the issue.\r\nCurrently the extraction cache mechanism only takes into account the path of the compressed file, which is an issue.""]","## Describe the bug
With GenerateMode.FORCE_REDOWNLOAD, the documentation says 
    +------------------------------------+-----------+---------+
    |                                    | Downloads | Dataset |
    +====================================+===========+=========+
    | `REUSE_DATASET_IF_EXISTS` (default)| Reuse     | Reuse   |
    +------------------------------------+-----------+---------+
    | `REUSE_CACHE_IF_EXISTS`            | Reuse     | Fresh   |
    +------------------------------------+-----------+---------+
    | `FORCE_REDOWNLOAD`                 | Fresh     | Fresh   |
    +------------------------------------+-----------+---------+

However, the old dataset is loaded even when FORCE_REDOWNLOAD is chosen.

## Steps to reproduce the bug
```python

import pandas as pd
from datasets import load_dataset, GenerateMode
pd.DataFrame(range(5), columns=['numbers']).to_csv('/tmp/test.tsv.gz', index=False)
ee = load_dataset('csv', data_files=['/tmp/test.tsv.gz'], delimiter='\t', split='train', download_mode=GenerateMode.FORCE_REDOWNLOAD)
print(ee)
pd.DataFrame(range(10), columns=['numerals']).to_csv('/tmp/test.tsv.gz', index=False)
ee = load_dataset('csv', data_files=['/tmp/test.tsv.gz'], delimiter='\t', split='train', download_mode=GenerateMode.FORCE_REDOWNLOAD)
print(ee)

```

## Expected results
Dataset({
    features: ['numbers'],
    num_rows: 5
})
Dataset({
    features: ['numerals'],
    num_rows: 10
})

## Actual results
Dataset({
    features: ['numbers'],
    num_rows: 5
})
Dataset({
    features: ['numbers'],
    num_rows: 5
})


## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: 1.8.0
- Platform: Linux-4.14.181-108.257.amzn1.x86_64-x86_64-with-glibc2.10
- Python version: 3.7.10
- PyArrow version: 3.0.0
"
https://github.com/huggingface/datasets/issues/2902,Add WIT Dataset,"['@hassiahk is working on it  #2810 '
 'WikiMedia is now hosting the pixel values directly which should make it a lot easier!\r\nThe files can be found here:\r\nhttps://techblog.wikimedia.org/2021/09/09/the-wikipedia-image-caption-matching-challenge-and-a-huge-release-of-image-data-for-research/\r\nhttps://analytics.wikimedia.org/published/datasets/one-off/caption_competition/training/image_pixels/'
 '> @hassiahk is working on it #2810\r\n\r\nThank you @bhavitvyamalik! Added this issue so we could track progress 😄 . Just linked the PR as well for visibility. ']","## Adding a Dataset
- **Name:** *WIT*
- **Description:** *Wikipedia-based Image Text Dataset*
- **Paper:** *[WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning
](https://arxiv.org/abs/2103.01913)*
- **Data:** *https://github.com/google-research-datasets/wit*
- **Motivation:**  (excerpt from their Github README.md)

> - The largest multimodal dataset (publicly available at the time of this writing) by the number of image-text examples.
> - A massively multilingual dataset (first of its kind) with coverage for over 100+ languages.
> - A collection of diverse set of concepts and real world entities.
> - Brings forth challenging real-world test sets.

Instructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).
"
https://github.com/huggingface/datasets/issues/2901,Incompatibility with pytest,"[""Sorry, my bad... When implementing `xpathopen`, I just considered the use case in the COUNTER dataset... I'm fixing it!""]","## Describe the bug

pytest complains about xpathopen / path.open(""w"")

## Steps to reproduce the bug

Create a test file, `test.py`:

```python
import datasets as ds
def load_dataset():
    ds.load_dataset(""counter"", split=""train"", streaming=True)
```

And launch it with pytest:

```bash
python -m pytest test.py
```

## Expected results

It should give something like:

```
collected 1 item

test.py .                                                                                                                                                                                                                                             [100%]

======= 1 passed in 3.15s =======
```

## Actual results

```
============================================================================================================================= test session starts ==============================================================================================================================
platform linux -- Python 3.8.11, pytest-6.2.5, py-1.10.0, pluggy-1.0.0
rootdir: /home/slesage/hf/datasets-preview-backend, configfile: pyproject.toml
plugins: anyio-3.3.1
collected 1 item

tests/queries/test_rows.py .                                                                                                                                                                                                                                             [100%]Traceback (most recent call last):
  File ""/home/slesage/.pyenv/versions/3.8.11/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/home/slesage/.pyenv/versions/3.8.11/lib/python3.8/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pytest/__main__.py"", line 5, in <module>
    raise SystemExit(pytest.console_main())
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/config/__init__.py"", line 185, in console_main
    code = main()
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/config/__init__.py"", line 162, in main
    ret: Union[ExitCode, int] = config.hook.pytest_cmdline_main(
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_hooks.py"", line 265, in __call__
    return self._hookexec(self.name, self.get_hookimpls(), kwargs, firstresult)
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_manager.py"", line 80, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_callers.py"", line 60, in _multicall
    return outcome.get_result()
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_result.py"", line 60, in get_result
    raise ex[1].with_traceback(ex[2])
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_callers.py"", line 39, in _multicall
    res = hook_impl.function(*args)
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/main.py"", line 316, in pytest_cmdline_main
    return wrap_session(config, _main)
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/main.py"", line 304, in wrap_session
    config.hook.pytest_sessionfinish(
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_hooks.py"", line 265, in __call__
    return self._hookexec(self.name, self.get_hookimpls(), kwargs, firstresult)
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_manager.py"", line 80, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_callers.py"", line 55, in _multicall
    gen.send(outcome)
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/terminal.py"", line 803, in pytest_sessionfinish
    outcome.get_result()
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_result.py"", line 60, in get_result
    raise ex[1].with_traceback(ex[2])
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_callers.py"", line 39, in _multicall
    res = hook_impl.function(*args)
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/cacheprovider.py"", line 428, in pytest_sessionfinish
    config.cache.set(""cache/nodeids"", sorted(self.cached_nodeids))
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/cacheprovider.py"", line 188, in set
    f = path.open(""w"")
TypeError: xpathopen() takes 1 positional argument but 2 were given
```

## Environment info

- `datasets` version: 1.12.0
- Platform: Linux-5.11.0-1017-aws-x86_64-with-glibc2.29
- Python version: 3.8.11
- PyArrow version: 4.0.1
"
https://github.com/huggingface/datasets/issues/2892,Error when encoding a dataset with None objects with a Sequence feature,"[""This has been fixed by https://github.com/huggingface/datasets/pull/2900\r\nWe're doing a new release 1.12 today to make the fix available :)""]","There is an error when encoding a dataset with None objects with a Sequence feature

To reproduce:
```python
from datasets import Dataset, Features, Value, Sequence
data = {""a"": [[0], None]}
features = Features({""a"": Sequence(Value(""int32""))})
dataset = Dataset.from_dict(data, features=features)
```
raises

```python
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-24-40add67f8751> in <module>
      2 data = {""a"": [[0], None]}
      3 features = Features({""a"": Sequence(Value(""int32""))})
----> 4 dataset = Dataset.from_dict(data, features=features)
[...]
~/datasets/features.py in encode_nested_example(schema, obj)
    888         if isinstance(obj, str):  # don't interpret a string as a list
    889             raise ValueError(""Got a string but expected a list instead: '{}'"".format(obj))
--> 890         return [encode_nested_example(schema.feature, o) for o in obj]
    891     # Object with special encoding:
    892     # ClassLabel will convert from string to int, TranslationVariableLanguages does some checks

TypeError: 'NoneType' object is not iterable
```

Instead, if should run without error, as if the `features` were not passed"
https://github.com/huggingface/datasets/issues/2888,v1.11.1 release date,"['Hi ! Probably 1.12 on monday :)\r\n'
 '@albertvillanova i think this issue is still valid and should not be closed till `>1.11.0` is published :)']","Hello, i need to use latest features in one of my packages but there have been no new datasets release since 2 months ago.

When do you plan to publush v1.11.1 release?"
https://github.com/huggingface/datasets/issues/2885,Adding an Elastic Search index to a Dataset,"[""Hi, is this bug deterministic in your poetry env ? I mean, does it always stop at 90% or is it random ?\r\n\r\nAlso, can you try using another version of Elasticsearch ? Maybe there's an issue with the one of you poetry env""]","## Describe the bug
When trying to index documents from the squad dataset, the connection to ElasticSearch seems to break:

Reusing dataset squad (/Users/andreasmotz/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)
 90%|████████████████████████████████████████████▉     | 9501/10570 [00:01<00:00, 6335.61docs/s]

No error is thrown, but the indexing breaks ~90%.

## Steps to reproduce the bug
```python
# Sample code to reproduce the bug
from datasets import load_dataset
from elasticsearch import Elasticsearch
es = Elasticsearch()
squad = load_dataset('squad', split='validation')
index_name = ""corpus""
es_config = {
    ""settings"": {
        ""number_of_shards"": 1,
        ""analysis"": {""analyzer"": {""stop_standard"": {""type"": ""standard"", "" stopwords"": ""_english_""}}},
    },
    ""mappings"": {
        ""properties"": {
            ""idx"" : {""type"" : ""keyword""},
            ""title"" : {""type"" : ""keyword""},
            ""text"": {
                ""type"": ""text"",
                ""analyzer"": ""standard"",
                ""similarity"": ""BM25""
            },
        }
    },
}
class IndexBuilder:
    """"""
    Elastic search indexing of a corpus
    """"""
    def __init__(
        self,
        *args,
        #corpus : None,
        dataset : squad,
        index_name = str,
        query = str,
        config = dict,
        **kwargs,
    ):
        #instantiate HuggingFace dataset
        self.dataset = dataset
        #instantiate ElasticSearch config
        self.config = config
        self.es = Elasticsearch()
        self.index_name = index_name
        self.query = query
    def elastic_index(self):
        print(self.es.info)
        self.es.indices.delete(index=self.index_name, ignore=[400, 404])
        search_index = self.dataset.add_elasticsearch_index(column='context', host='localhost', port='9200', es_index_name=self.index_name, es_index_config=self.config)
        return search_index
    def exact_match_method(self, index):
        scores, retrieved_examples = index.get_nearest_examples('context', query=self.query, k=1)
        return scores, retrieved_examples
if __name__ == ""__main__"":
    print(type(squad))
    Index = IndexBuilder(dataset=squad, index_name='corpus_index', query='Where was Chopin born?', config=es_config)
    search_index = Index.elastic_index()
    scores, examples = Index.exact_match_method(search_index)
    print(scores, examples)
    for name in squad.column_names:
        print(type(squad[name]))
```

## Environment info
We run the code in Poetry. This might be the issue, since the script runs successfully in our local environment.

Poetry:
- Python version: 3.8
- PyArrow: 4.0.1
- Elasticsearch: 7.13.4
- datasets: 1.10.2

Local:
- Python version: 3.8
- PyArrow: 3.0.0
- Elasticsearch: 7.7.1
- datasets: 1.7.0
"
https://github.com/huggingface/datasets/issues/2882,`load_dataset('docred')` results in a `NonMatchingChecksumError` ,"[""Hi @tmpr, thanks for reporting.\r\n\r\nTwo weeks ago (23th Aug), the host of the source `docred` dataset updated one of the files (`dev.json`): you can see it [here](https://drive.google.com/drive/folders/1c5-0YwnoJx8NS6CV2f-NoTHR__BdkNqw).\r\n\r\nTherefore, the checksum needs to be updated.\r\n\r\nNormally, in the meantime, you could avoid the error by passing `ignore_verifications=True` to `load_dataset`. However, as the old link points to a non-existing file, the link must be updated too.\r\n\r\nI'm fixing all this.\r\n\r\n""]","## Describe the bug
I get consistent `NonMatchingChecksumError: Checksums didn't match for dataset source files` errors when trying to execute `datasets.load_dataset('docred')`.

## Steps to reproduce the bug
It is quasi only this code:
```python
import datasets
data = datasets.load_dataset('docred')
```

## Expected results
The DocRED dataset should be loaded without any problems.

## Actual results
```
NonMatchingChecksumError                  Traceback (most recent call last)
<ipython-input-4-b1b83f25a16c> in <module>
----> 1 d = datasets.load_dataset('docred')

~/anaconda3/lib/python3.8/site-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, script_version, use_auth_token, task, streaming, **config_kwargs)
    845 
    846     # Download and prepare data
--> 847     builder_instance.download_and_prepare(
    848         download_config=download_config,
    849         download_mode=download_mode,

~/anaconda3/lib/python3.8/site-packages/datasets/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)
    613                             logger.warning(""HF google storage unreachable. Downloading and preparing it from source"")
    614                     if not downloaded_from_gcs:
--> 615                         self._download_and_prepare(
    616                             dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
    617                         )

~/anaconda3/lib/python3.8/site-packages/datasets/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)
    673         # Checksums verification
    674         if verify_infos:
--> 675             verify_checksums(
    676                 self.info.download_checksums, dl_manager.get_recorded_sizes_checksums(), ""dataset source files""
    677             )

~/anaconda3/lib/python3.8/site-packages/datasets/utils/info_utils.py in verify_checksums(expected_checksums, recorded_checksums, verification_name)
     38     if len(bad_urls) > 0:
     39         error_msg = ""Checksums didn't match"" + for_verification_name + "":\n""
---> 40         raise NonMatchingChecksumError(error_msg + str(bad_urls))
     41     logger.info(""All the checksums matched successfully"" + for_verification_name)
     42 

NonMatchingChecksumError: Checksums didn't match for dataset source files:
['https://drive.google.com/uc?export=download&id=1fDmfUUo5G7gfaoqWWvK81u08m71TK2g7']
```

## Environment info
- `datasets` version: 1.11.0
- Platform: Linux-5.11.0-7633-generic-x86_64-with-glibc2.10
- Python version: 3.8.5
- PyArrow version: 5.0.0

This error also happened on my Windows-partition, after freshly installing python 3.9 and `datasets`.

## Remarks

- I have already called `rm -rf /home/<user>/.cache/huggingface`, i.e., I have tried clearing the cache.
- The problem does not exist for other datasets, i.e., it seems to be DocRED-specific."
https://github.com/huggingface/datasets/issues/2879,"In v1.4.1, all TIMIT train transcripts are ""Would such an act of refusal be useful?""","['Hi @rcgale, thanks for reporting.\r\n\r\nPlease note that this bug was fixed on `datasets` version 1.5.0: https://github.com/huggingface/datasets/commit/a23c73e526e1c30263834164f16f1fdf76722c8c#diff-f12a7a42d4673bb6c2ca5a40c92c29eb4fe3475908c84fd4ce4fad5dc2514878\r\n\r\nIf you update `datasets` version, that should work.\r\n\r\nOn the other hand, would it be possible for @patrickvonplaten to update the [blog post](https://huggingface.co/blog/fine-tune-wav2vec2-english) with the correct version of `datasets`?'
 'I just proposed a change in the blog post.\r\n\r\nI had assumed there was a data format change that broke a previous version of the code, since presumably @patrickvonplaten tested the tutorial with the version they explicitly referenced. But that fix you linked suggests a problem in the code, which surprised me.\r\n\r\nI still wonder, though, is there a way for downloads to be invalidated server-side? If the client can announce its version during a download request, perhaps the server could reject known incompatibilities? It would save much valuable time if `datasets` raised an informative error on a known problem (""Error: the requested data set requires `datasets>=1.5.0`.""). This kind of API versioning is a prudent move anyhow, as there will surely come a time when you\'ll need to make a breaking change to data.'
 'Also, thank you for a quick and helpful reply!']","## Describe the bug
Using version 1.4.1 of `datasets`, TIMIT transcripts are all the same.

## Steps to reproduce the bug
I was following this tutorial
- https://huggingface.co/blog/fine-tune-wav2vec2-english

But here's a distilled repro:
```python
!pip install datasets==1.4.1
from datasets import load_dataset
timit = load_dataset(""timit_asr"", cache_dir=""./temp"")
unique_transcripts = set(timit[""train""][""text""])
print(unique_transcripts)
assert len(unique_transcripts) > 1
```
## Expected results
Expected the correct TIMIT data. Or an error saying that this version of `datasets` can't produce it.

## Actual results
Every train transcript was ""Would such an act of refusal be useful?"" Every test transcript was ""The bungalow was pleasantly situated near the shore.""

## Environment info
- `datasets` version: 1.4.1
- Platform: Darwin-18.7.0-x86_64-i386-64bit
- Python version: 3.7.9
- PyTorch version (GPU?): 1.9.0 (False)
- Tensorflow version (GPU?): not installed (NA)
- Using GPU in script?: tried both
- Using distributed or parallel set-up in script?: no
- 

"
https://github.com/huggingface/datasets/issues/2871,datasets.config.PYARROW_VERSION has no attribute 'major',"['I have changed line 288 to `if int(datasets.config.PYARROW_VERSION.split(""."")[0]) < 3:` just to get around it.'
 ""Hi @bwang482,\r\n\r\nI'm sorry but I'm not able to reproduce your bug.\r\n\r\nPlease note that in our current master branch, we made a commit (d03223d4d64b89e76b48b00602aba5aa2f817f1e) that simultaneously modified:\r\n- test_dataset_common.py: https://github.com/huggingface/datasets/commit/d03223d4d64b89e76b48b00602aba5aa2f817f1e#diff-a1bc225bd9a5bade373d1f140e24d09cbbdc97971c2f73bb627daaa803ada002L289 that introduces the usage of `datasets.config.PYARROW_VERSION.major`\r\n- but also changed config.py: https://github.com/huggingface/datasets/commit/d03223d4d64b89e76b48b00602aba5aa2f817f1e#diff-e021fcfc41811fb970fab889b8d245e68382bca8208e63eaafc9a396a336f8f2L40, so that `datasets.config.PYARROW_VERSION.major` exists\r\n""
 'Sorted. Thanks!'
 'Reopening this. Although the `test_dataset_common.py` script works fine now.\r\n\r\nHas this got something to do with my pull request not passing `ci/circleci: run_dataset_script_tests_pyarrow` tests?\r\n\r\nhttps://github.com/huggingface/datasets/pull/2873'
 'Hi @bwang482,\r\n\r\nIf you click on `Details` (on the right of your non passing CI test names: `ci/circleci: run_dataset_script_tests_pyarrow`), you can have more information about the non-passing tests.\r\n\r\nFor example, for [""ci/circleci: run_dataset_script_tests_pyarrow_1"" details](https://circleci.com/gh/huggingface/datasets/46324?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link), you can see that the only non-passing test has to do with the dataset card (missing information in the `README.md` file): `test_changed_dataset_card`\r\n```\r\n=========================== short test summary info ============================\r\nFAILED tests/test_dataset_cards.py::test_changed_dataset_card[swedish_medical_ner]\r\n= 1 failed, 3214 passed, 2874 skipped, 2 xfailed, 1 xpassed, 15 warnings in 175.59s (0:02:55) =\r\n```\r\n\r\nTherefore, your PR non-passing test has nothing to do with this issue.']","In the test_dataset_common.py script, line 288-289

```
if datasets.config.PYARROW_VERSION.major < 3:
   packaged_datasets = [pd for pd in packaged_datasets if pd[""dataset_name""] != ""parquet""]
```

which throws the error below. `datasets.config.PYARROW_VERSION` itself return the string '4.0.1'. I have tested this on both datasets.__version_=='1.11.0' and '1.9.0'. I am using Mac OS.

```
import datasets
datasets.config.PYARROW_VERSION.major
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
/var/folders/1f/0wqmlgp90qjd5mpj53fnjq440000gn/T/ipykernel_73361/2547517336.py in <module>
      1 import datasets
----> 2 datasets.config.PYARROW_VERSION.major

AttributeError: 'str' object has no attribute 'major'
```

## Environment info
- `datasets` version: 1.11.0
- Platform: Darwin-20.6.0-x86_64-i386-64bit
- Python version: 3.7.11
- PyArrow version: 4.0.1
"
https://github.com/huggingface/datasets/issues/2869,TypeError: 'NoneType' object is not callable,"['Hi, @Chenfei-Kang.\r\n\r\nI\'m sorry, but I\'m not able to reproduce your bug:\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset(""glue"", \'cola\')\r\nds\r\n```\r\n```\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: [\'sentence\', \'label\', \'idx\'],\r\n        num_rows: 8551\r\n    })\r\n    validation: Dataset({\r\n        features: [\'sentence\', \'label\', \'idx\'],\r\n        num_rows: 1043\r\n    })\r\n    test: Dataset({\r\n        features: [\'sentence\', \'label\', \'idx\'],\r\n        num_rows: 1063\r\n    })\r\n})\r\n```\r\n\r\nCould you please give more details and environment info (platform, PyArrow version)?'
 '> Hi, @Chenfei-Kang.\r\n> \r\n> I\'m sorry, but I\'m not able to reproduce your bug:\r\n> \r\n> ```python\r\n> from datasets import load_dataset\r\n> \r\n> ds = load_dataset(""glue"", \'cola\')\r\n> ds\r\n> ```\r\n> \r\n> ```\r\n> DatasetDict({\r\n>     train: Dataset({\r\n>         features: [\'sentence\', \'label\', \'idx\'],\r\n>         num_rows: 8551\r\n>     })\r\n>     validation: Dataset({\r\n>         features: [\'sentence\', \'label\', \'idx\'],\r\n>         num_rows: 1043\r\n>     })\r\n>     test: Dataset({\r\n>         features: [\'sentence\', \'label\', \'idx\'],\r\n>         num_rows: 1063\r\n>     })\r\n> })\r\n> ```\r\n> \r\n> Could you please give more details and environment info (platform, PyArrow version)?\r\n\r\nSorry to reply you so late.\r\nplatform: pycharm 2021 + anaconda with python 3.7\r\nPyArrow version: 5.0.0\r\nhuggingface-hub: 0.0.16\r\ndatasets: 1.9.0\r\n'
 ""- For the platform, we need to know the operating system of your machine. Could you please run the command `datasets-cli env` and copy-and-paste its output below?\r\n- In relation with the error, you just gave us the error type and message (`TypeError: 'NoneType' object is not callable`). Could you please copy-paste the complete stack trace, so that we know exactly which part of the code threw the error?""
 '> * For the platform, we need to know the operating system of your machine. Could you please run the command `datasets-cli env` and copy-and-paste its output below?\r\n> * In relation with the error, you just gave us the error type and message (`TypeError: \'NoneType\' object is not callable`). Could you please copy-paste the complete stack trace, so that we know exactly which part of the code threw the error?\r\n\r\n1. For the platform, here are the output:\r\n        - datasets` version: 1.11.0\r\n        - Platform: Windows-10-10.0.19041-SP0\r\n        - Python version: 3.7.10\r\n        - PyArrow version: 5.0.0\r\n2. For the code and error：\r\n     ```python\r\n     from datasets import load_dataset, load_metric\r\n     dataset = load_dataset(""glue"", ""cola"")\r\n    ```\r\n    ```python\r\n    Traceback (most recent call last):\r\n    ....\r\n    ....\r\n    File ""my_file.py"", line 2, in <module>\r\n    dataset = load_dataset(""glue"", ""cola"")\r\n    File ""My environments\\lib\\site-packages\\datasets\\load.py"", line 830, in load_dataset\r\n    **config_kwargs,\r\n     File ""My environments\\lib\\site-packages\\datasets\\load.py"", line 710, in load_dataset_builder\r\n    **config_kwargs,\r\n    TypeError: \'NoneType\' object is not callable\r\n    ```\r\n   Thank you!'
 ""For that environment, I am sorry but I can't reproduce the bug: I can load the dataset without any problem.""
 'One naive question: do you have internet access from the machine where you execute the code?'
 ""> For that environment, I am sorry but I can't reproduce the bug: I can load the dataset without any problem.\r\n\r\nBut I can download other task dataset such as `dataset = load_dataset('squad')`. I don't know what went wrong.  Thank you so much!""]","## Describe the bug

TypeError: 'NoneType' object is not callable
## Steps to reproduce the bug
```python
from datasets import load_dataset, load_metric
dataset = datasets.load_dataset(""glue"", 'cola')
```

## Expected results
A clear and concise description of the expected results.

## Actual results
Specify the actual results or traceback.

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: 1.11.0
- Platform:
- Python version: 3.7
- PyArrow version:
"
https://github.com/huggingface/datasets/issues/2866,"""counter"" dataset raises an error in normal mode, but not in streaming mode","['Hi @severo, thanks for reporting.\r\n\r\nJust note that currently not all canonical datasets support streaming mode: this is one case!\r\n\r\nAll datasets that use `pathlib` joins (using `/`) instead of `os.path.join` (as in this dataset) do not support streaming mode yet.'
 ""OK. Do you think it's possible to detect this, and raise an exception (maybe `NotImplementedError`, or a specific `StreamingError`)?""
 'We should definitely support datasets using `pathlib` in streaming mode...\r\n\r\nFor non-supported datasets in streaming mode, we have already a request of raising an error/warning: see #2654.'
 'Hi @severo, please note that ""counter"" dataset will be streamable (at least until it arrives at the missing file, error already in normal mode) once these PRs are merged:\r\n- #2874\r\n- #2876\r\n- #2880\r\n\r\nI have tested it. 😉 '
 'Now (on master), we get:\r\n\r\n```\r\nimport datasets as ds\r\nds.load_dataset(\'counter\', split=""train"", streaming=False)\r\n```\r\n\r\n```\r\nUsing custom data configuration default\r\nDownloading and preparing dataset counter/default (download: 1.29 MiB, generated: 2.48 MiB, post-processed: Unknown size, total: 3.77 MiB) to /home/slesage/.cache/huggingface/datasets/counter/default/1.0.0/9f84962fa0f35bec5a34fe0bdff8681838d497008c457f7856c48654476ec0e9...\r\nTraceback (most recent call last):\r\n  File ""/home/slesage/hf/datasets/src/datasets/builder.py"", line 726, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File ""/home/slesage/hf/datasets/src/datasets/builder.py"", line 1124, in _prepare_split\r\n    for key, record in utils.tqdm(\r\n  File ""/home/slesage/hf/datasets/.venv/lib/python3.8/site-packages/tqdm/std.py"", line 1185, in __iter__\r\n    for obj in iterable:\r\n  File ""/home/slesage/.cache/huggingface/modules/datasets_modules/datasets/counter/9f84962fa0f35bec5a34fe0bdff8681838d497008c457f7856c48654476ec0e9/counter.py"", line 161, in _generate_examples\r\n    with derived_file.open(encoding=""utf-8"") as f:\r\n  File ""/home/slesage/.pyenv/versions/3.8.11/lib/python3.8/pathlib.py"", line 1222, in open\r\n    return io.open(self, mode, buffering, encoding, errors, newline,\r\n  File ""/home/slesage/.pyenv/versions/3.8.11/lib/python3.8/pathlib.py"", line 1078, in _opener\r\n    return self._accessor.open(self, flags, mode)\r\nFileNotFoundError: [Errno 2] No such file or directory: \'/home/slesage/.cache/huggingface/datasets/downloads/extracted/b57aa6db5601a738e57b95c1fd8cced54ff28fc540efcdaf0f6c4f1bb5dfe211/COUNTER/0032p.xml\'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/slesage/hf/datasets/src/datasets/load.py"", line 1112, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File ""/home/slesage/hf/datasets/src/datasets/builder.py"", line 636, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File ""/home/slesage/hf/datasets/src/datasets/builder.py"", line 728, in _download_and_prepare\r\n    raise OSError(\r\nOSError: Cannot find data file.\r\nOriginal error:\r\n[Errno 2] No such file or directory: \'/home/slesage/.cache/huggingface/datasets/downloads/extracted/b57aa6db5601a738e57b95c1fd8cced54ff28fc540efcdaf0f6c4f1bb5dfe211/COUNTER/0032p.xml\'\r\n```\r\n\r\nThe error is now the same with or without streaming. I close the issue, thanks @albertvillanova and @lhoestq!\r\n'
 'Note that we might want to open an issue to fix the ""counter"" dataset by itself, but I let it up to you.'
 'Fixed here: https://github.com/huggingface/datasets/pull/2894. Thanks @albertvillanova ']","## Describe the bug

`counter` dataset raises an error on `load_dataset()`, but simply returns an empty iterator in streaming mode.

## Steps to reproduce the bug

```python
>>> import datasets as ds
>>> a = ds.load_dataset('counter', split=""train"", streaming=False)
Using custom data configuration default
Downloading and preparing dataset counter/default (download: 1.29 MiB, generated: 2.48 MiB, post-processed: Unknown size, total: 3.77 MiB) to /home/slesage/.cache/huggingface/datasets/counter/default/1.0.0/9f84962fa0f35bec5a34fe0bdff8681838d497008c457f7856c48654476ec0e9...
Traceback (most recent call last):
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/builder.py"", line 726, in _download_and_prepare
    self._prepare_split(split_generator, **prepare_split_kwargs)
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/builder.py"", line 1124, in _prepare_split
    for key, record in utils.tqdm(
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/tqdm/std.py"", line 1185, in __iter__
    for obj in iterable:
  File ""/home/slesage/.cache/huggingface/modules/datasets_modules/datasets/counter/9f84962fa0f35bec5a34fe0bdff8681838d497008c457f7856c48654476ec0e9/counter.py"", line 161, in _generate_examples
    with derived_file.open(encoding=""utf-8"") as f:
  File ""/home/slesage/.pyenv/versions/3.8.11/lib/python3.8/pathlib.py"", line 1222, in open
    return io.open(self, mode, buffering, encoding, errors, newline,
  File ""/home/slesage/.pyenv/versions/3.8.11/lib/python3.8/pathlib.py"", line 1078, in _opener
    return self._accessor.open(self, flags, mode)
FileNotFoundError: [Errno 2] No such file or directory: '/home/slesage/.cache/huggingface/datasets/downloads/extracted/b57aa6db5601a738e57b95c1fd8cced54ff28fc540efcdaf0f6c4f1bb5dfe211/COUNTER/0032p.xml'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/load.py"", line 1112, in load_dataset
    builder_instance.download_and_prepare(
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/builder.py"", line 636, in download_and_prepare
    self._download_and_prepare(
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/builder.py"", line 728, in _download_and_prepare
    raise OSError(
OSError: Cannot find data file.
Original error:
[Errno 2] No such file or directory: '/home/slesage/.cache/huggingface/datasets/downloads/extracted/b57aa6db5601a738e57b95c1fd8cced54ff28fc540efcdaf0f6c4f1bb5dfe211/COUNTER/0032p.xml'
```

```python
>>> import datasets as ds
>>> b = ds.load_dataset('counter', split=""train"", streaming=True)
Using custom data configuration default
>>> list(b)
[]
```

## Expected results

An exception should be raised in streaming mode

## Actual results

No exception is raised in streaming mode: there is no way to tell if something has broken or if the dataset is simply empty.

## Environment info

- `datasets` version: 1.11.1.dev0
- Platform: Linux-5.11.0-1016-aws-x86_64-with-glibc2.29
- Python version: 3.8.11
- PyArrow version: 4.0.1
"
https://github.com/huggingface/datasets/issues/2860,Cannot download TOTTO dataset,"[""Hola @mrm8488, thanks for reporting.\r\n\r\nApparently, the data source host changed their URL one week ago: https://github.com/google-research-datasets/ToTTo/commit/cebeb430ec2a97747e704d16a9354f7d9073ff8f\r\n\r\nI'm fixing it.""]","Error: Couldn't find file at https://storage.googleapis.com/totto/totto_data.zip

`datasets version: 1.11.0`
# How to reproduce:

```py
from datasets import load_dataset
dataset = load_dataset('totto')
```


"
https://github.com/huggingface/datasets/issues/2859,Loading allenai/c4 in streaming mode does too many HEAD requests,['https://github.com/huggingface/datasets/blob/6c766f9115d686182d76b1b937cb27e099c45d68/src/datasets/builder.py#L179-L186'],"This does 60,000+ HEAD requests to get all the ETags of all the data files:
```python
from datasets import load_dataset
load_dataset(""allenai/c4"", streaming=True)
```
It makes loading the dataset completely impractical.

The ETags are used to compute the config id (it must depend on the data files being used).
Instead of using the ETags, we could simply use the commit hash of the dataset repository on the hub, as well and the glob pattern used to resolve the files (here it's `*` by default, to load all the files of the repository)"
https://github.com/huggingface/datasets/issues/2945,Protect master branch,"['Cool, I think we can do both :)'
 '@lhoestq now the 2 are implemented.\r\n\r\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).']","After accidental merge commit (91c55355b634d0dc73350a7ddee1a6776dbbdd69) into `datasets` master branch, all commits present in the feature branch were permanently added to `datasets` master branch history, as e.g.:
- 00cc036fea7c7745cfe722360036ed306796a3f2
- 13ae8c98602bbad8197de3b9b425f4c78f582af1
- ...

I propose to protect our master branch, so that we avoid we can accidentally make this kind of mistakes in the future:
- [x] For Pull Requests using GitHub, allow only squash merging, so that only a single commit per Pull Request is merged into the master branch
  - Currently, simple merge commits are already disabled
  - I propose to disable rebase merging as well
- ~~Protect the master branch from direct pushes (to avoid accidentally pushing of merge commits)~~
  - ~~This protection would reject direct pushes to master branch~~
  - ~~If so, for each release (when we need to commit directly to the master branch), we should previously disable the protection and re-enable it again after the release~~
- [x] Protect the master branch only from direct pushing of **merge commits**
  - GitHub offers the possibility to protect the master branch only from merge commits (which are the ones that introduce all the commits from the feature branch into the master branch).
  - No need to disable/re-enable this protection on each release 

This purpose of this Issue is to open a discussion about this problem and to agree in a solution."
https://github.com/huggingface/datasets/issues/2943,Backwards compatibility broken for cached datasets that use `.filter()`,"[""Hi ! I guess the caching mechanism should have considered the new `filter` to be different from the old one, and don't use cached results from the old `filter`.\r\nTo avoid other users from having this issue we could make the caching differentiate the two, what do you think ?""
 ""If it's easy enough to implement, then yes please 😄  But this issue can be low-priority, since I've only encountered it in a couple of `transformers` CI tests.""
 ""Well it can cause issue with anyone that updates `datasets` and re-run some code that uses filter, so I'm creating a PR""
 ""I just merged a fix, let me know if you're still having this kind of issues :)\r\n\r\nWe'll do a release soon to make this fix available""
 'Definitely works on several manual cases with our dummy datasets, thank you @lhoestq !'
 'Fixed by #2947.']","## Describe the bug
After upgrading to datasets `1.12.0`, some cached `.filter()` steps from `1.11.0` started failing with 
`ValueError: Keys mismatch: between {'indices': Value(dtype='uint64', id=None)} and {'file': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'speaker_id': Value(dtype='int64', id=None), 'chapter_id': Value(dtype='int64', id=None), 'id': Value(dtype='string', id=None)}`

Related feature: https://github.com/huggingface/datasets/pull/2836

:question:  This is probably a `wontfix` bug, since it can be solved by simply cleaning the related cache dirs, but the workaround could be useful for someone googling the error :) 

## Workaround
Remove the cache for the given dataset, e.g. `rm -rf ~/.cache/huggingface/datasets/librispeech_asr`.

## Steps to reproduce the bug
1. Delete `~/.cache/huggingface/datasets/librispeech_asr` if it exists.

2. `pip install datasets==1.11.0` and run the following snippet:

```python
from datasets import load_dataset

ids = [""1272-141231-0000""]
ds = load_dataset(""patrickvonplaten/librispeech_asr_dummy"", ""clean"", split=""validation"")
ds = ds.filter(lambda x: x[""id""] in ids)
```
3. `pip install datasets==1.12.1` and re-run the code again

## Expected results
Same result as with the previous `datasets` version.

## Actual results
```bash
Reusing dataset librispeech_asr (./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1)
Loading cached processed dataset at ./.cache/huggingface/datasets/librispeech_asr/clean/2.1.0/468ec03677f46a8714ac6b5b64dba02d246a228d92cbbad7f3dc190fa039eab1/cache-cd1c29844fdbc87a.arrow
Traceback (most recent call last):
  File ""./repos/transformers/src/transformers/models/wav2vec2/try_dataset.py"", line 5, in <module>
    ds = ds.filter(lambda x: x[""id""] in ids)
  File ""./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 185, in wrapper
    out: Union[""Dataset"", ""DatasetDict""] = func(self, *args, **kwargs)
  File ""./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py"", line 398, in wrapper
    out = func(self, *args, **kwargs)
  File ""./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 2169, in filter
    indices = self.map(
  File ""./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 1686, in map
    return self._map_single(
  File ""./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 185, in wrapper
    out: Union[""Dataset"", ""DatasetDict""] = func(self, *args, **kwargs)
  File ""./envs/transformers/lib/python3.8/site-packages/datasets/fingerprint.py"", line 398, in wrapper
    out = func(self, *args, **kwargs)
  File ""./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 1896, in _map_single
    return Dataset.from_file(cache_file_name, info=info, split=self.split)
  File ""./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 343, in from_file
    return cls(
  File ""./envs/transformers/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 282, in __init__
    self.info.features = self.info.features.reorder_fields_as(inferred_features)
  File ""./envs/transformers/lib/python3.8/site-packages/datasets/features.py"", line 1151, in reorder_fields_as
    return Features(recursive_reorder(self, other))
  File ""./envs/transformers/lib/python3.8/site-packages/datasets/features.py"", line 1140, in recursive_reorder
    raise ValueError(f""Keys mismatch: between {source} and {target}"" + stack_position)
ValueError: Keys mismatch: between {'indices': Value(dtype='uint64', id=None)} and {'file': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'speaker_id': Value(dtype='int64', id=None), 'chapter_id': Value(dtype='int64', id=None), 'id': Value(dtype='string', id=None)}

Process finished with exit code 1

```

## Environment info
- `datasets` version: 1.12.1
- Platform: Linux-5.11.0-34-generic-x86_64-with-glibc2.17
- Python version: 3.8.10
- PyArrow version: 5.0.0
"
https://github.com/huggingface/datasets/issues/2941,OSCAR unshuffled_original_ko: NonMatchingSplitsSizesError,['I tried `unshuffled_original_da` and it is also not working'],"## Describe the bug

Cannot download OSCAR `unshuffled_original_ko` due to `NonMatchingSplitsSizesError`.

## Steps to reproduce the bug

```python
>>> dataset = datasets.load_dataset('oscar', 'unshuffled_original_ko')
NonMatchingSplitsSizesError: [{'expected': SplitInfo(name='train', num_bytes=25292102197, num_examples=7345075, dataset_name='oscar'), 'recorded': SplitInfo(name='train', num_bytes=25284578514, num_examples=7344907, dataset_name='oscar')}]
```

## Expected results

Loading is successful.

## Actual results

Loading throws above error.

## Environment info

- `datasets` version: 1.12.1
- Platform: Linux-5.4.0-81-generic-x86_64-with-glibc2.29
- Python version: 3.8.10
- PyArrow version: 5.0.0
"
https://github.com/huggingface/datasets/issues/2937,load_dataset using default cache on Windows causes PermissionError: [WinError 5] Access is denied,"[""Hi @daqieq, thanks for reporting.\r\n\r\nUnfortunately, I was not able to reproduce this bug:\r\n```ipython\r\nIn [1]: from datasets import load_dataset\r\n   ...: ds = load_dataset('wiki_bio')\r\nDownloading: 7.58kB [00:00, 26.3kB/s]\r\nDownloading: 2.71kB [00:00, ?B/s]\r\nUsing custom data configuration default\r\nDownloading and preparing dataset wiki_bio/default (download: 318.53 MiB, generated: 736.94 MiB, post-processed: Unknown size, total: 1.03 GiB) to C:\\Users\\username\\.cache\\huggingface\\datasets\\wiki_bio\\default\\\r\n1.1.0\\5293ce565954ba965dada626f1e79684e98172d950371d266bf3caaf87e911c9...\r\nDownloading: 334MB [01:17, 4.32MB/s]\r\nDataset wiki_bio downloaded and prepared to C:\\Users\\username\\.cache\\huggingface\\datasets\\wiki_bio\\default\\1.1.0\\5293ce565954ba965dada626f1e79684e98172d950371d266bf3caaf87e911c9. Subsequent calls will reuse thi\r\ns data.\r\n```\r\n\r\nThis kind of error messages usually happen because:\r\n- Your running Python script hasn't write access to that directory\r\n- You have another program (the File Explorer?) already browsing inside that directory""
 ""Thanks @albertvillanova for looking at it! I tried on my personal Windows machine and it downloaded just fine.\r\n\r\nRunning on my work machine and on a colleague's machine it is consistently hitting this error. It's not a write access issue because the `.incomplete` directory is written just fine. It just won't rename and then it deletes the directory in the `finally` step. Also the zip file is written and extracted fine in the downloads directory.\r\n\r\nThat leaves another program that might be interfering, and there are plenty of those in my work machine ... (full antivirus, data loss prevention, etc.). So the question remains, why not extend the `try` block to allow catching the error and circle back to the rename after the unknown program is finished doing its 'stuff'. This is the approach that I read about in the linked repo (see my comments above).\r\n\r\nIf it's not high priority, that's fine. However, if someone were to write an PR that solved this issue in our environment in an `except` clause, would it be reviewed for inclusion in a future release? Just wondering whether I should spend any more time on this issue.""]","## Describe the bug
Standard process to download and load the wiki_bio dataset causes PermissionError in Windows 10 and 11.

## Steps to reproduce the bug
```python
from datasets import load_dataset
ds = load_dataset('wiki_bio')
```

## Expected results
It is expected that the dataset downloads without any errors.

## Actual results
PermissionError see trace below:
```
Using custom data configuration default
Downloading and preparing dataset wiki_bio/default (download: 318.53 MiB, generated: 736.94 MiB, post-processed: Unknown size, total: 1.03 GiB) to C:\Users\username\.cache\huggingface\datasets\wiki_bio\default\1.1.0\5293ce565954ba965dada626f1e79684e98172d950371d266bf3caaf87e911c9...
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\username\.conda\envs\hf\lib\site-packages\datasets\load.py"", line 1112, in load_dataset
    builder_instance.download_and_prepare(
  File ""C:\Users\username\.conda\envs\hf\lib\site-packages\datasets\builder.py"", line 644, in download_and_prepare
    self._save_info()
  File ""C:\Users\username\.conda\envs\hf\lib\contextlib.py"", line 120, in __exit__
    next(self.gen)
  File ""C:\Users\username\.conda\envs\hf\lib\site-packages\datasets\builder.py"", line 598, in incomplete_dir
    os.rename(tmp_dir, dirname)
PermissionError: [WinError 5] Access is denied: 'C:\\Users\\username\\.cache\\huggingface\\datasets\\wiki_bio\\default\\1.1.0\\5293ce565954ba965dada626f1e79684e98172d950371d266bf3caaf87e911c9.incomplete' -> 'C:\\Users\\username\\.cache\\huggingface\\datasets\\wiki_bio\\default\\1.1.0\\5293ce565954ba965dada626f1e79684e98172d950371d266bf3caaf87e911c9'
```
By commenting out the os.rename() [L604](https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L604) and the shutil.rmtree() [L607](https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L607) lines, in my virtual environment, I was able to get the load process to complete, rename the directory manually and then rerun the `load_dataset('wiki_bio')` to get what I needed.

It seems that os.rename() in the `incomplete_dir` content manager is the culprit. Here's another project [Conan](https://github.com/conan-io/conan/issues/6560) with similar issue with os.rename() if it helps debug this issue.

## Environment info
- `datasets` version: 1.12.1
- Platform: Windows-10-10.0.22449-SP0
- Python version: 3.8.12
- PyArrow version: 5.0.0
"
https://github.com/huggingface/datasets/issues/2934,"to_tf_dataset keeps a reference to the open data somewhere, causing issues on windows","[""I did some investigation and, as it seems, the bug stems from [this line](https://github.com/huggingface/datasets/blob/8004d7c3e1d74b29c3e5b0d1660331cd26758363/src/datasets/arrow_dataset.py#L325). The lifecycle of the dataset from the linked line is bound to one of the returned `tf.data.Dataset`. So my (hacky) solution involves wrapping the linked dataset with `weakref.proxy` and adding a custom `__del__` to `tf.python.data.ops.dataset_ops.TensorSliceDataset` (this is the type of a dataset that is returned by `tf.data.Dataset.from_tensor_slices`; this works for TF 2.x, but I'm not sure `tf.python.data.ops.dataset_ops` is a valid path for TF 1.x) that deletes the linked dataset, which is assigned to the dataset object as a property. Will open a draft PR soon!""
 'Thanks a lot for investigating !']","To reproduce:
```python
import datasets as ds
import weakref
import gc

d = ds.load_dataset(""mnist"", split=""train"")
ref = weakref.ref(d._data.table)
tfd = d.to_tf_dataset(""image"", batch_size=1, shuffle=False, label_cols=""label"")
del tfd, d
gc.collect()
assert ref() is None, ""Error: there is at least one reference left""
```

This causes issues because the table holds a reference to an open arrow file that should be closed. So on windows it's not possible to delete or move the arrow file afterwards.

Moreover the CI test of the `to_tf_dataset` method isn't able to clean up the temporary arrow files because of this.

cc @Rocketknight1 "
https://github.com/huggingface/datasets/issues/2932,Conda build fails,"['Why 1.9 ?\r\n\r\nhttps://anaconda.org/HuggingFace/datasets currently says 1.11'
 'Alright I added 1.12.0 and 1.12.1 and fixed the conda build #2952 ']","## Describe the bug
Current `datasets` version in conda is 1.9 instead of 1.12.

The build of the conda package fails.
"
https://github.com/huggingface/datasets/issues/2930,Mutable columns argument breaks set_format,['Pushed a fix to my branch #2731 '],"## Describe the bug
If you pass a mutable list to the `columns` argument of `set_format` and then change the list afterwards, the returned columns also change.

## Steps to reproduce the bug
```python
from datasets import load_dataset
dataset = load_dataset(""glue"", ""cola"")

column_list = [""idx"", ""label""]
dataset.set_format(""python"", columns=column_list)
column_list[1] = ""foo""  # Change the list after we call `set_format`
dataset['train'][:4].keys()
```

## Expected results
```python
dict_keys(['idx', 'label'])
```

## Actual results
```python
dict_keys(['idx'])
```"
https://github.com/huggingface/datasets/issues/2927,Datasets 1.12 dataset.filter TypeError: get_indices_from_mask_function() got an unexpected keyword argument,"[""Thanks for reporting, I'm looking into it :)"" 'Fixed by #2950.']","## Describe the bug
Upgrading to 1.12 caused `dataset.filter` call to fail with 

> get_indices_from_mask_function() got an unexpected keyword argument valid_rel_labels


## Steps to reproduce the bug
```pythondef 

filter_good_rows(
    ex: Dict,
    valid_rel_labels: Set[str],
    valid_ner_labels: Set[str],
    tokenizer: PreTrainedTokenizerFast,
) -> bool:
    """"""Get the good rows""""""
    encoding = get_encoding_for_text(text=ex[""text""], tokenizer=tokenizer)
    ex[""encoding""] = encoding
    for relation in ex[""relations""]:
        if not is_valid_relation(relation, valid_rel_labels):
            return False
    for span in ex[""spans""]:
        if not is_valid_span(span, valid_ner_labels, encoding):
            return False
    return True
    
def get_dataset():    
    loader_path = str(Path(__file__).parent / ""prodigy_dataset_builder.py"")
    ds = load_dataset(
        loader_path,
        name=""prodigy-dataset"",
        data_files=sorted(file_paths),
        cache_dir=cache_dir,
    )[""train""]

    valid_ner_labels = set(vocab.ner_category)
    valid_relations = set(vocab.relation_types.keys())
    ds = ds.filter(
        filter_good_rows,
        fn_kwargs=dict(
            valid_rel_labels=valid_relations,
            valid_ner_labels=valid_ner_labels,
            tokenizer=vocab.tokenizer,
        ),
        keep_in_memory=True,
        num_proc=num_proc,
    )

```

`ds` is a `DatasetDict` produced by a jsonl dataset.
This runs fine on 1.11 but fails on 1.12

**Stack Trace**



## Expected results

I expect 1.12 datasets filter to filter the dataset without raising as it does on 1.11

## Actual results
```
tf_ner_rel_lib/dataset.py:695: in load_prodigy_arrow_datasets_from_jsonl
    ds = ds.filter(
../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:185: in wrapper
    out: Union[""Dataset"", ""DatasetDict""] = func(self, *args, **kwargs)
../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/fingerprint.py:398: in wrapper
    out = func(self, *args, **kwargs)
../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:2169: in filter
    indices = self.map(
../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:1686: in map
    return self._map_single(
../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:185: in wrapper
    out: Union[""Dataset"", ""DatasetDict""] = func(self, *args, **kwargs)
../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/fingerprint.py:398: in wrapper
    out = func(self, *args, **kwargs)
../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:2048: in _map_single
    batch = apply_function_on_filtered_inputs(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = {'_input_hash': [2108817714, 1477695082, -1021597032, 2130671338, -1260483858, -1203431639, ...], '_task_hash': [18070...ons', 'relations', 'relations', ...], 'answer': ['accept', 'accept', 'accept', 'accept', 'accept', 'accept', ...], ...}
indices = [0, 1, 2, 3, 4, 5, ...], check_same_num_examples = False, offset = 0

    def apply_function_on_filtered_inputs(inputs, indices, check_same_num_examples=False, offset=0):
        """"""Utility to apply the function on a selection of columns.""""""
        nonlocal update_data
        fn_args = [inputs] if input_columns is None else [inputs[col] for col in input_columns]
        if offset == 0:
            effective_indices = indices
        else:
            effective_indices = [i + offset for i in indices] if isinstance(indices, list) else indices + offset
        processed_inputs = (
>           function(*fn_args, effective_indices, **fn_kwargs) if with_indices else function(*fn_args, **fn_kwargs)
        )
E       TypeError: get_indices_from_mask_function() got an unexpected keyword argument 'valid_rel_labels'

../../../../.pyenv/versions/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:1939: TypeError
```

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: 1.12.1
- Platform: Mac
- Python version: 3.8.9
- PyArrow version: pyarrow==5.0.0

"
https://github.com/huggingface/datasets/issues/2924,"""File name too long"" error for file locks","['Hi, the filename here is less than 255\r\n```python\r\n>>> len(""_home_garrett_.cache_huggingface_datasets_csv_test-7c856aea083a7043_0.0.0_9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff.incomplete.lock"")\r\n154\r\n```\r\nso not sure why it\'s considered too long for your filesystem.\r\n(also note that the lock files we use always have smaller filenames than 255)\r\n\r\nhttps://github.com/huggingface/datasets/blob/5d1a9f1e3c6c495dc0610b459e39d2eb8893f152/src/datasets/utils/filelock.py#L135-L135'
 ""Yes, you're right! I need to get you more info here. Either there's something going with the name itself that the file system doesn't like (an encoding that blows up the name length??) or perhaps there's something with the path that's causing the entire string to  be used as a name. I haven't seen this on any system before and the Internet's not forthcoming with any info.""]","## Describe the bug

Getting the following error when calling `load_dataset(""gar1t/test"")`:

```
OSError: [Errno 36] File name too long: '<user>/.cache/huggingface/datasets/_home_garrett_.cache_huggingface_datasets_csv_test-7c856aea083a7043_0.0.0_9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff.incomplete.lock'
```

## Steps to reproduce the bug

Where the user cache dir (e.g. `~/.cache`) is on a file system that limits filenames to 255 chars (e.g. ext4):

```python
from datasets import load_dataset
load_dataset(""gar1t/test"")
```

## Expected results

Expect the function to return without an error.

## Actual results

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<python_venv>/lib/python3.9/site-packages/datasets/load.py"", line 1112, in load_dataset
    builder_instance.download_and_prepare(
  File ""<python_venv>/lib/python3.9/site-packages/datasets/builder.py"", line 644, in download_and_prepare
    self._save_info()
  File ""<python_venv>/lib/python3.9/site-packages/datasets/builder.py"", line 765, in _save_info
    with FileLock(lock_path):
  File ""<python_venv>/lib/python3.9/site-packages/datasets/utils/filelock.py"", line 323, in __enter__
    self.acquire()
  File ""<python_venv>/lib/python3.9/site-packages/datasets/utils/filelock.py"", line 272, in acquire
    self._acquire()
  File ""<python_venv>/lib/python3.9/site-packages/datasets/utils/filelock.py"", line 403, in _acquire
    fd = os.open(self._lock_file, open_mode)
OSError: [Errno 36] File name too long: '<user>/.cache/huggingface/datasets/_home_garrett_.cache_huggingface_datasets_csv_test-7c856aea083a7043_0.0.0_9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff.incomplete.lock'
```

## Environment info

- `datasets` version: 1.12.1
- Platform: Linux-5.11.0-27-generic-x86_64-with-glibc2.31
- Python version: 3.9.7
- PyArrow version: 5.0.0
"
https://github.com/huggingface/datasets/issues/2919,Unwanted progress bars when accessing examples,['doing a patch release now :)'],"When accessing examples from a dataset formatted for pytorch, some progress bars appear when accessing examples:
```python
In [1]: import datasets as ds                                        

In [2]: d = ds.Dataset.from_dict({""a"": [0, 1, 2]}).with_format(""torch"")                                                           

In [3]: d[0]                                                         
100%|████████████████████████████████| 1/1 [00:00<00:00, 3172.70it/s]
Out[3]: {'a': tensor(0)}
```

This is because the pytorch formatter calls `map_nested` that uses progress bars

cc @sgugger "
https://github.com/huggingface/datasets/issues/2918,`Can not decode content-encoding: gzip` when loading `scitldr` dataset with streaming,"['Hi @SBrandeis, thanks for reporting! ^^\r\n\r\nI think this is an issue with `fsspec`: https://github.com/intake/filesystem_spec/issues/389\r\n\r\nI will ask them if they are planning to fix it...'
 'Code to reproduce the bug: `ClientPayloadError: 400, message=\'Can not decode content-encoding: gzip\'`\r\n```python\r\nIn [1]: import fsspec\r\n\r\nIn [2]: import json\r\n\r\nIn [3]: with fsspec.open(\'https://raw.githubusercontent.com/allenai/scitldr/master/SciTLDR-Data/SciTLDR-FullText/test.jsonl\', encoding=""utf-8"") as f:\r\n   ...:     for row in f:\r\n   ...:         data = json.loads(row)\r\n   ...:\r\n---------------------------------------------------------------------------\r\nClientPayloadError                        Traceback (most recent call last)\r\n```'
 'Thanks for investigating @albertvillanova ! 🤗 ']","## Describe the bug

Trying to load the `""FullText""` config of the `""scitldr""` dataset with `streaming=True` raises an error from `aiohttp`:
```python
ClientPayloadError: 400, message='Can not decode content-encoding: gzip'
```

cc @lhoestq 

## Steps to reproduce the bug
```python
from datasets import load_dataset

iter_dset = iter(
    load_dataset(""scitldr"", name=""FullText"", split=""test"", streaming=True)
)

next(iter_dset)
```

## Expected results
Returns the first sample of the dataset

## Actual results
Calling `__next__` crashes with the following Traceback:

```python
----> 1 next(dset_iter)

~\miniconda3\envs\datasets\lib\site-packages\datasets\iterable_dataset.py in __iter__(self)
    339
    340     def __iter__(self):
--> 341         for key, example in self._iter():
    342             if self.features:
    343                 # we encode the example for ClassLabel feature types for example

~\miniconda3\envs\datasets\lib\site-packages\datasets\iterable_dataset.py in _iter(self)
    336         else:
    337             ex_iterable = self._ex_iterable
--> 338         yield from ex_iterable
    339
    340     def __iter__(self):

~\miniconda3\envs\datasets\lib\site-packages\datasets\iterable_dataset.py in __iter__(self)
     76
     77     def __iter__(self):
---> 78         for key, example in self.generate_examples_fn(**self.kwargs):
     79             yield key, example
     80

~\.cache\huggingface\modules\datasets_modules\datasets\scitldr\72d6e2195786c57e1d343066fb2cc4f93ea39c5e381e53e6ae7c44bbfd1f05ef\scitldr.py in _generate_examples(self, filepath, split)
    162
    163         with open(filepath, encoding=""utf-8"") as f:
--> 164             for id_, row in enumerate(f):
    165                 data = json.loads(row)
    166                 if self.config.name == ""AIC"":

~\miniconda3\envs\datasets\lib\site-packages\fsspec\implementations\http.py in read(self, length)
    496         else:
    497             length = min(self.size - self.loc, length)
--> 498         return super().read(length)
    499
    500     async def async_fetch_all(self):

~\miniconda3\envs\datasets\lib\site-packages\fsspec\spec.py in read(self, length)
   1481             # don't even bother calling fetch
   1482             return b""""
-> 1483         out = self.cache._fetch(self.loc, self.loc + length)
   1484         self.loc += len(out)
   1485         return out

~\miniconda3\envs\datasets\lib\site-packages\fsspec\caching.py in _fetch(self, start, end)
    378         elif start < self.start:
    379             if self.end - end > self.blocksize:
--> 380                 self.cache = self.fetcher(start, bend)
    381                 self.start = start
    382             else:

~\miniconda3\envs\datasets\lib\site-packages\fsspec\asyn.py in wrapper(*args, **kwargs)
     86     def wrapper(*args, **kwargs):
     87         self = obj or args[0]
---> 88         return sync(self.loop, func, *args, **kwargs)
     89
     90     return wrapper

~\miniconda3\envs\datasets\lib\site-packages\fsspec\asyn.py in sync(loop, func, timeout, *args, **kwargs)
     67         raise FSTimeoutError
     68     if isinstance(result[0], BaseException):
---> 69         raise result[0]
     70     return result[0]
     71

~\miniconda3\envs\datasets\lib\site-packages\fsspec\asyn.py in _runner(event, coro, result, timeout)
     23         coro = asyncio.wait_for(coro, timeout=timeout)
     24     try:
---> 25         result[0] = await coro
     26     except Exception as ex:
     27         result[0] = ex

~\miniconda3\envs\datasets\lib\site-packages\fsspec\implementations\http.py in async_fetch_range(self, start, end)
    538             if r.status == 206:
    539                 # partial content, as expected
--> 540                 out = await r.read()
    541             elif ""Content-Length"" in r.headers:
    542                 cl = int(r.headers[""Content-Length""])

~\miniconda3\envs\datasets\lib\site-packages\aiohttp\client_reqrep.py in read(self)
   1030         if self._body is None:
   1031             try:
-> 1032                 self._body = await self.content.read()
   1033                 for trace in self._traces:
   1034                     await trace.send_response_chunk_received(

~\miniconda3\envs\datasets\lib\site-packages\aiohttp\streams.py in read(self, n)
    342     async def read(self, n: int = -1) -> bytes:
    343         if self._exception is not None:
--> 344             raise self._exception
    345
    346         # migration problem; with DataQueue you have to catch

ClientPayloadError: 400, message='Can not decode content-encoding: gzip'
```

## Environment info

- `datasets` version: 1.12.0
- Platform: Windows-10-10.0.19041-SP0
- Python version: 3.8.5
- PyArrow version: 2.0.0
- aiohttp version: 3.7.4.post0
"
https://github.com/huggingface/datasets/issues/2917,windows download abnormal,"[""Hi ! Is there some kind of proxy that is configured in your browser that gives you access to internet ? If it's the case it could explain why it doesn't work in the code, since the proxy wouldn't be used""
 'It is indeed an agency problem, thank you very, very much'
 'Let me know if you have other questions :)\r\n\r\nClosing this issue now']","## Describe the bug
The script clearly exists (accessible from the browser), but the script download fails on windows. Then I tried it again and it can be downloaded normally on linux. why??
## Steps to reproduce the bug
```python3.7 + windows
![image](https://user-images.githubusercontent.com/52347799/133436174-4303f847-55d5-434f-a749-08da3bb9b654.png)


# Sample code to reproduce the bug
```

## Expected results
It can be downloaded normally.

## Actual results
it cann't

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version:1.11.0
- Platform:windows
- Python version:3.7
- PyArrow version:
"
https://github.com/huggingface/datasets/issues/2914,Having a dependency defining fsspec entrypoint raises an AttributeError when importing datasets,['Closed by #2915.'],"## Describe the bug
In one of my project, I defined a custom fsspec filesystem with an entrypoint.
My guess is that by doing so, a variable named `spec` is created in the module `fsspec` (created by entering a for loop as there are entrypoints defined, see the loop in question [here](https://github.com/intake/filesystem_spec/blob/0589358d8a029ed6b60d031018f52be2eb721291/fsspec/__init__.py#L55)).
So that `fsspec.spec`, that was previously referring to the `spec` submodule, is now referring to that `spec` variable.
This make the import of datasets failing as it is using that `fsspec.spec`.

## Steps to reproduce the bug
I could reproduce the bug with a dummy poetry project.

Here is the pyproject.toml:
```toml
[tool.poetry]
name = ""debug-datasets""
version = ""0.1.0""
description = """"
authors = [""Pierre Godard""]

[tool.poetry.dependencies]
python = ""^3.8""
datasets = ""^1.11.0""

[tool.poetry.dev-dependencies]

[build-system]
requires = [""poetry-core>=1.0.0""]
build-backend = ""poetry.core.masonry.api""

[tool.poetry.plugins.""fsspec.specs""]
""file2"" = ""fsspec.implementations.local.LocalFileSystem""
```

The only other file being a `debug_datasets/__init__.py` empty file.

The overall structure of the project is as follows:
```
.
├── pyproject.toml
└── debug_datasets
    └── __init__.py
```

Then, within the project folder run:

```
poetry install
poetry run python
```

And in the python interpreter, try to import `datasets`:

```
import datasets
```

## Expected results
The import should run successfully.

## Actual results

Here is the trace of the error I get:

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/godarpi/.cache/pypoetry/virtualenvs/debug-datasets-JuFzTKL--py3.8/lib/python3.8/site-packages/datasets/__init__.py"", line 33, in <module>
    from .arrow_dataset import Dataset, concatenate_datasets
  File ""/home/godarpi/.cache/pypoetry/virtualenvs/debug-datasets-JuFzTKL--py3.8/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 48, in <module>
    from .filesystems import extract_path_from_uri, is_remote_filesystem
  File ""/home/godarpi/.cache/pypoetry/virtualenvs/debug-datasets-JuFzTKL--py3.8/lib/python3.8/site-packages/datasets/filesystems/__init__.py"", line 30, in <module>
    def is_remote_filesystem(fs: fsspec.spec.AbstractFileSystem) -> bool:
AttributeError: 'EntryPoint' object has no attribute 'AbstractFileSystem'
```

## Suggested fix

`datasets/filesystems/__init__.py`, line 30, replace:
```
    def is_remote_filesystem(fs: fsspec.spec.AbstractFileSystem) -> bool:
```
by:
```
    def is_remote_filesystem(fs: fsspec.AbstractFileSystem) -> bool:
```

I will come up with a PR soon if this effectively solves the issue.

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: 1.11.0
- Platform: WSL2 (Ubuntu 20.04.1 LTS)
- Python version: 3.8.5
- PyArrow version: 5.0.0
- `fsspec` version: 2021.8.1
"
https://github.com/huggingface/datasets/issues/2913,timit_asr dataset only includes one text phrase,"['Hi @margotwagner, \r\nThis bug was fixed in #1995. Upgrading the datasets should work (min v1.8.0 ideally)'
 'Hi @margotwagner,\r\n\r\nYes, as @bhavitvyamalik has commented, this bug was fixed in `datasets` version 1.5.0. You need to update it, as your current version is 1.4.1:\r\n> Environment info\r\n> - `datasets` version: 1.4.1']","## Describe the bug
The dataset 'timit_asr' only includes one text phrase. It only includes the transcription ""Would such an act of refusal be useful?"" multiple times rather than different phrases.

## Steps to reproduce the bug
Note: I am following the tutorial https://huggingface.co/blog/fine-tune-wav2vec2-english

1. Install the dataset and other packages
```python
!pip install datasets>=1.5.0
!pip install transformers==4.4.0
!pip install soundfile
!pip install jiwer
```
2. Load the dataset
```python
from datasets import load_dataset, load_metric

timit = load_dataset(""timit_asr"")
```
3. Remove columns that we don't want
```python
timit = timit.remove_columns([""phonetic_detail"", ""word_detail"", ""dialect_region"", ""id"", ""sentence_type"", ""speaker_id""])
```
4. Write a short function to display some random samples of the dataset.
```python
from datasets import ClassLabel
import random
import pandas as pd
from IPython.display import display, HTML

def show_random_elements(dataset, num_examples=10):
    assert num_examples <= len(dataset), ""Can't pick more elements than there are in the dataset.""
    picks = []
    for _ in range(num_examples):
        pick = random.randint(0, len(dataset)-1)
        while pick in picks:
            pick = random.randint(0, len(dataset)-1)
        picks.append(pick)
    
    df = pd.DataFrame(dataset[picks])
    display(HTML(df.to_html()))

show_random_elements(timit[""train""].remove_columns([""file""]))
```

## Expected results
10 random different transcription phrases.

## Actual results
10 of the same transcription phrase ""Would such an act of refusal be useful?""

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: 1.4.1
- Platform: macOS-10.15.7-x86_64-i386-64bit
- Python version: 3.8.5
- PyArrow version: not listed
"
https://github.com/huggingface/datasets/issues/2904,FORCE_REDOWNLOAD does not work,"[""Hi ! Thanks for reporting. The error seems to happen only if you use compressed files.\r\n\r\nThe second dataset is prepared in another dataset cache directory than the first - which is normal, since the source file is different. However, it doesn't uncompress the new data file because it finds the old uncompressed data in the extraction cache directory.\r\n\r\nIf we fix the extraction cache mechanism to uncompress a local file if it changed then it should fix the issue.\r\nCurrently the extraction cache mechanism only takes into account the path of the compressed file, which is an issue.""]","## Describe the bug
With GenerateMode.FORCE_REDOWNLOAD, the documentation says 
    +------------------------------------+-----------+---------+
    |                                    | Downloads | Dataset |
    +====================================+===========+=========+
    | `REUSE_DATASET_IF_EXISTS` (default)| Reuse     | Reuse   |
    +------------------------------------+-----------+---------+
    | `REUSE_CACHE_IF_EXISTS`            | Reuse     | Fresh   |
    +------------------------------------+-----------+---------+
    | `FORCE_REDOWNLOAD`                 | Fresh     | Fresh   |
    +------------------------------------+-----------+---------+

However, the old dataset is loaded even when FORCE_REDOWNLOAD is chosen.

## Steps to reproduce the bug
```python

import pandas as pd
from datasets import load_dataset, GenerateMode
pd.DataFrame(range(5), columns=['numbers']).to_csv('/tmp/test.tsv.gz', index=False)
ee = load_dataset('csv', data_files=['/tmp/test.tsv.gz'], delimiter='\t', split='train', download_mode=GenerateMode.FORCE_REDOWNLOAD)
print(ee)
pd.DataFrame(range(10), columns=['numerals']).to_csv('/tmp/test.tsv.gz', index=False)
ee = load_dataset('csv', data_files=['/tmp/test.tsv.gz'], delimiter='\t', split='train', download_mode=GenerateMode.FORCE_REDOWNLOAD)
print(ee)

```

## Expected results
Dataset({
    features: ['numbers'],
    num_rows: 5
})
Dataset({
    features: ['numerals'],
    num_rows: 10
})

## Actual results
Dataset({
    features: ['numbers'],
    num_rows: 5
})
Dataset({
    features: ['numbers'],
    num_rows: 5
})


## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: 1.8.0
- Platform: Linux-4.14.181-108.257.amzn1.x86_64-x86_64-with-glibc2.10
- Python version: 3.7.10
- PyArrow version: 3.0.0
"
https://github.com/huggingface/datasets/issues/2902,Add WIT Dataset,"['@hassiahk is working on it  #2810 '
 'WikiMedia is now hosting the pixel values directly which should make it a lot easier!\r\nThe files can be found here:\r\nhttps://techblog.wikimedia.org/2021/09/09/the-wikipedia-image-caption-matching-challenge-and-a-huge-release-of-image-data-for-research/\r\nhttps://analytics.wikimedia.org/published/datasets/one-off/caption_competition/training/image_pixels/'
 '> @hassiahk is working on it #2810\r\n\r\nThank you @bhavitvyamalik! Added this issue so we could track progress 😄 . Just linked the PR as well for visibility. ']","## Adding a Dataset
- **Name:** *WIT*
- **Description:** *Wikipedia-based Image Text Dataset*
- **Paper:** *[WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning
](https://arxiv.org/abs/2103.01913)*
- **Data:** *https://github.com/google-research-datasets/wit*
- **Motivation:**  (excerpt from their Github README.md)

> - The largest multimodal dataset (publicly available at the time of this writing) by the number of image-text examples.
> - A massively multilingual dataset (first of its kind) with coverage for over 100+ languages.
> - A collection of diverse set of concepts and real world entities.
> - Brings forth challenging real-world test sets.

Instructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).
"
https://github.com/huggingface/datasets/issues/2901,Incompatibility with pytest,"[""Sorry, my bad... When implementing `xpathopen`, I just considered the use case in the COUNTER dataset... I'm fixing it!""]","## Describe the bug

pytest complains about xpathopen / path.open(""w"")

## Steps to reproduce the bug

Create a test file, `test.py`:

```python
import datasets as ds
def load_dataset():
    ds.load_dataset(""counter"", split=""train"", streaming=True)
```

And launch it with pytest:

```bash
python -m pytest test.py
```

## Expected results

It should give something like:

```
collected 1 item

test.py .                                                                                                                                                                                                                                             [100%]

======= 1 passed in 3.15s =======
```

## Actual results

```
============================================================================================================================= test session starts ==============================================================================================================================
platform linux -- Python 3.8.11, pytest-6.2.5, py-1.10.0, pluggy-1.0.0
rootdir: /home/slesage/hf/datasets-preview-backend, configfile: pyproject.toml
plugins: anyio-3.3.1
collected 1 item

tests/queries/test_rows.py .                                                                                                                                                                                                                                             [100%]Traceback (most recent call last):
  File ""/home/slesage/.pyenv/versions/3.8.11/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/home/slesage/.pyenv/versions/3.8.11/lib/python3.8/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pytest/__main__.py"", line 5, in <module>
    raise SystemExit(pytest.console_main())
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/config/__init__.py"", line 185, in console_main
    code = main()
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/config/__init__.py"", line 162, in main
    ret: Union[ExitCode, int] = config.hook.pytest_cmdline_main(
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_hooks.py"", line 265, in __call__
    return self._hookexec(self.name, self.get_hookimpls(), kwargs, firstresult)
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_manager.py"", line 80, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_callers.py"", line 60, in _multicall
    return outcome.get_result()
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_result.py"", line 60, in get_result
    raise ex[1].with_traceback(ex[2])
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_callers.py"", line 39, in _multicall
    res = hook_impl.function(*args)
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/main.py"", line 316, in pytest_cmdline_main
    return wrap_session(config, _main)
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/main.py"", line 304, in wrap_session
    config.hook.pytest_sessionfinish(
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_hooks.py"", line 265, in __call__
    return self._hookexec(self.name, self.get_hookimpls(), kwargs, firstresult)
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_manager.py"", line 80, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_callers.py"", line 55, in _multicall
    gen.send(outcome)
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/terminal.py"", line 803, in pytest_sessionfinish
    outcome.get_result()
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_result.py"", line 60, in get_result
    raise ex[1].with_traceback(ex[2])
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/pluggy/_callers.py"", line 39, in _multicall
    res = hook_impl.function(*args)
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/cacheprovider.py"", line 428, in pytest_sessionfinish
    config.cache.set(""cache/nodeids"", sorted(self.cached_nodeids))
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/_pytest/cacheprovider.py"", line 188, in set
    f = path.open(""w"")
TypeError: xpathopen() takes 1 positional argument but 2 were given
```

## Environment info

- `datasets` version: 1.12.0
- Platform: Linux-5.11.0-1017-aws-x86_64-with-glibc2.29
- Python version: 3.8.11
- PyArrow version: 4.0.1
"
https://github.com/huggingface/datasets/issues/2892,Error when encoding a dataset with None objects with a Sequence feature,"[""This has been fixed by https://github.com/huggingface/datasets/pull/2900\r\nWe're doing a new release 1.12 today to make the fix available :)""]","There is an error when encoding a dataset with None objects with a Sequence feature

To reproduce:
```python
from datasets import Dataset, Features, Value, Sequence
data = {""a"": [[0], None]}
features = Features({""a"": Sequence(Value(""int32""))})
dataset = Dataset.from_dict(data, features=features)
```
raises

```python
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-24-40add67f8751> in <module>
      2 data = {""a"": [[0], None]}
      3 features = Features({""a"": Sequence(Value(""int32""))})
----> 4 dataset = Dataset.from_dict(data, features=features)
[...]
~/datasets/features.py in encode_nested_example(schema, obj)
    888         if isinstance(obj, str):  # don't interpret a string as a list
    889             raise ValueError(""Got a string but expected a list instead: '{}'"".format(obj))
--> 890         return [encode_nested_example(schema.feature, o) for o in obj]
    891     # Object with special encoding:
    892     # ClassLabel will convert from string to int, TranslationVariableLanguages does some checks

TypeError: 'NoneType' object is not iterable
```

Instead, if should run without error, as if the `features` were not passed"
https://github.com/huggingface/datasets/issues/2888,v1.11.1 release date,"['Hi ! Probably 1.12 on monday :)\r\n'
 '@albertvillanova i think this issue is still valid and should not be closed till `>1.11.0` is published :)']","Hello, i need to use latest features in one of my packages but there have been no new datasets release since 2 months ago.

When do you plan to publush v1.11.1 release?"
https://github.com/huggingface/datasets/issues/2885,Adding an Elastic Search index to a Dataset,"[""Hi, is this bug deterministic in your poetry env ? I mean, does it always stop at 90% or is it random ?\r\n\r\nAlso, can you try using another version of Elasticsearch ? Maybe there's an issue with the one of you poetry env""]","## Describe the bug
When trying to index documents from the squad dataset, the connection to ElasticSearch seems to break:

Reusing dataset squad (/Users/andreasmotz/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)
 90%|████████████████████████████████████████████▉     | 9501/10570 [00:01<00:00, 6335.61docs/s]

No error is thrown, but the indexing breaks ~90%.

## Steps to reproduce the bug
```python
# Sample code to reproduce the bug
from datasets import load_dataset
from elasticsearch import Elasticsearch
es = Elasticsearch()
squad = load_dataset('squad', split='validation')
index_name = ""corpus""
es_config = {
    ""settings"": {
        ""number_of_shards"": 1,
        ""analysis"": {""analyzer"": {""stop_standard"": {""type"": ""standard"", "" stopwords"": ""_english_""}}},
    },
    ""mappings"": {
        ""properties"": {
            ""idx"" : {""type"" : ""keyword""},
            ""title"" : {""type"" : ""keyword""},
            ""text"": {
                ""type"": ""text"",
                ""analyzer"": ""standard"",
                ""similarity"": ""BM25""
            },
        }
    },
}
class IndexBuilder:
    """"""
    Elastic search indexing of a corpus
    """"""
    def __init__(
        self,
        *args,
        #corpus : None,
        dataset : squad,
        index_name = str,
        query = str,
        config = dict,
        **kwargs,
    ):
        #instantiate HuggingFace dataset
        self.dataset = dataset
        #instantiate ElasticSearch config
        self.config = config
        self.es = Elasticsearch()
        self.index_name = index_name
        self.query = query
    def elastic_index(self):
        print(self.es.info)
        self.es.indices.delete(index=self.index_name, ignore=[400, 404])
        search_index = self.dataset.add_elasticsearch_index(column='context', host='localhost', port='9200', es_index_name=self.index_name, es_index_config=self.config)
        return search_index
    def exact_match_method(self, index):
        scores, retrieved_examples = index.get_nearest_examples('context', query=self.query, k=1)
        return scores, retrieved_examples
if __name__ == ""__main__"":
    print(type(squad))
    Index = IndexBuilder(dataset=squad, index_name='corpus_index', query='Where was Chopin born?', config=es_config)
    search_index = Index.elastic_index()
    scores, examples = Index.exact_match_method(search_index)
    print(scores, examples)
    for name in squad.column_names:
        print(type(squad[name]))
```

## Environment info
We run the code in Poetry. This might be the issue, since the script runs successfully in our local environment.

Poetry:
- Python version: 3.8
- PyArrow: 4.0.1
- Elasticsearch: 7.13.4
- datasets: 1.10.2

Local:
- Python version: 3.8
- PyArrow: 3.0.0
- Elasticsearch: 7.7.1
- datasets: 1.7.0
"
https://github.com/huggingface/datasets/issues/2882,`load_dataset('docred')` results in a `NonMatchingChecksumError` ,"[""Hi @tmpr, thanks for reporting.\r\n\r\nTwo weeks ago (23th Aug), the host of the source `docred` dataset updated one of the files (`dev.json`): you can see it [here](https://drive.google.com/drive/folders/1c5-0YwnoJx8NS6CV2f-NoTHR__BdkNqw).\r\n\r\nTherefore, the checksum needs to be updated.\r\n\r\nNormally, in the meantime, you could avoid the error by passing `ignore_verifications=True` to `load_dataset`. However, as the old link points to a non-existing file, the link must be updated too.\r\n\r\nI'm fixing all this.\r\n\r\n""]","## Describe the bug
I get consistent `NonMatchingChecksumError: Checksums didn't match for dataset source files` errors when trying to execute `datasets.load_dataset('docred')`.

## Steps to reproduce the bug
It is quasi only this code:
```python
import datasets
data = datasets.load_dataset('docred')
```

## Expected results
The DocRED dataset should be loaded without any problems.

## Actual results
```
NonMatchingChecksumError                  Traceback (most recent call last)
<ipython-input-4-b1b83f25a16c> in <module>
----> 1 d = datasets.load_dataset('docred')

~/anaconda3/lib/python3.8/site-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, script_version, use_auth_token, task, streaming, **config_kwargs)
    845 
    846     # Download and prepare data
--> 847     builder_instance.download_and_prepare(
    848         download_config=download_config,
    849         download_mode=download_mode,

~/anaconda3/lib/python3.8/site-packages/datasets/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)
    613                             logger.warning(""HF google storage unreachable. Downloading and preparing it from source"")
    614                     if not downloaded_from_gcs:
--> 615                         self._download_and_prepare(
    616                             dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
    617                         )

~/anaconda3/lib/python3.8/site-packages/datasets/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)
    673         # Checksums verification
    674         if verify_infos:
--> 675             verify_checksums(
    676                 self.info.download_checksums, dl_manager.get_recorded_sizes_checksums(), ""dataset source files""
    677             )

~/anaconda3/lib/python3.8/site-packages/datasets/utils/info_utils.py in verify_checksums(expected_checksums, recorded_checksums, verification_name)
     38     if len(bad_urls) > 0:
     39         error_msg = ""Checksums didn't match"" + for_verification_name + "":\n""
---> 40         raise NonMatchingChecksumError(error_msg + str(bad_urls))
     41     logger.info(""All the checksums matched successfully"" + for_verification_name)
     42 

NonMatchingChecksumError: Checksums didn't match for dataset source files:
['https://drive.google.com/uc?export=download&id=1fDmfUUo5G7gfaoqWWvK81u08m71TK2g7']
```

## Environment info
- `datasets` version: 1.11.0
- Platform: Linux-5.11.0-7633-generic-x86_64-with-glibc2.10
- Python version: 3.8.5
- PyArrow version: 5.0.0

This error also happened on my Windows-partition, after freshly installing python 3.9 and `datasets`.

## Remarks

- I have already called `rm -rf /home/<user>/.cache/huggingface`, i.e., I have tried clearing the cache.
- The problem does not exist for other datasets, i.e., it seems to be DocRED-specific."
https://github.com/huggingface/datasets/issues/2879,"In v1.4.1, all TIMIT train transcripts are ""Would such an act of refusal be useful?""","['Hi @rcgale, thanks for reporting.\r\n\r\nPlease note that this bug was fixed on `datasets` version 1.5.0: https://github.com/huggingface/datasets/commit/a23c73e526e1c30263834164f16f1fdf76722c8c#diff-f12a7a42d4673bb6c2ca5a40c92c29eb4fe3475908c84fd4ce4fad5dc2514878\r\n\r\nIf you update `datasets` version, that should work.\r\n\r\nOn the other hand, would it be possible for @patrickvonplaten to update the [blog post](https://huggingface.co/blog/fine-tune-wav2vec2-english) with the correct version of `datasets`?'
 'I just proposed a change in the blog post.\r\n\r\nI had assumed there was a data format change that broke a previous version of the code, since presumably @patrickvonplaten tested the tutorial with the version they explicitly referenced. But that fix you linked suggests a problem in the code, which surprised me.\r\n\r\nI still wonder, though, is there a way for downloads to be invalidated server-side? If the client can announce its version during a download request, perhaps the server could reject known incompatibilities? It would save much valuable time if `datasets` raised an informative error on a known problem (""Error: the requested data set requires `datasets>=1.5.0`.""). This kind of API versioning is a prudent move anyhow, as there will surely come a time when you\'ll need to make a breaking change to data.'
 'Also, thank you for a quick and helpful reply!']","## Describe the bug
Using version 1.4.1 of `datasets`, TIMIT transcripts are all the same.

## Steps to reproduce the bug
I was following this tutorial
- https://huggingface.co/blog/fine-tune-wav2vec2-english

But here's a distilled repro:
```python
!pip install datasets==1.4.1
from datasets import load_dataset
timit = load_dataset(""timit_asr"", cache_dir=""./temp"")
unique_transcripts = set(timit[""train""][""text""])
print(unique_transcripts)
assert len(unique_transcripts) > 1
```
## Expected results
Expected the correct TIMIT data. Or an error saying that this version of `datasets` can't produce it.

## Actual results
Every train transcript was ""Would such an act of refusal be useful?"" Every test transcript was ""The bungalow was pleasantly situated near the shore.""

## Environment info
- `datasets` version: 1.4.1
- Platform: Darwin-18.7.0-x86_64-i386-64bit
- Python version: 3.7.9
- PyTorch version (GPU?): 1.9.0 (False)
- Tensorflow version (GPU?): not installed (NA)
- Using GPU in script?: tried both
- Using distributed or parallel set-up in script?: no
- 

"
https://github.com/huggingface/datasets/issues/2871,datasets.config.PYARROW_VERSION has no attribute 'major',"['I have changed line 288 to `if int(datasets.config.PYARROW_VERSION.split(""."")[0]) < 3:` just to get around it.'
 ""Hi @bwang482,\r\n\r\nI'm sorry but I'm not able to reproduce your bug.\r\n\r\nPlease note that in our current master branch, we made a commit (d03223d4d64b89e76b48b00602aba5aa2f817f1e) that simultaneously modified:\r\n- test_dataset_common.py: https://github.com/huggingface/datasets/commit/d03223d4d64b89e76b48b00602aba5aa2f817f1e#diff-a1bc225bd9a5bade373d1f140e24d09cbbdc97971c2f73bb627daaa803ada002L289 that introduces the usage of `datasets.config.PYARROW_VERSION.major`\r\n- but also changed config.py: https://github.com/huggingface/datasets/commit/d03223d4d64b89e76b48b00602aba5aa2f817f1e#diff-e021fcfc41811fb970fab889b8d245e68382bca8208e63eaafc9a396a336f8f2L40, so that `datasets.config.PYARROW_VERSION.major` exists\r\n""
 'Sorted. Thanks!'
 'Reopening this. Although the `test_dataset_common.py` script works fine now.\r\n\r\nHas this got something to do with my pull request not passing `ci/circleci: run_dataset_script_tests_pyarrow` tests?\r\n\r\nhttps://github.com/huggingface/datasets/pull/2873'
 'Hi @bwang482,\r\n\r\nIf you click on `Details` (on the right of your non passing CI test names: `ci/circleci: run_dataset_script_tests_pyarrow`), you can have more information about the non-passing tests.\r\n\r\nFor example, for [""ci/circleci: run_dataset_script_tests_pyarrow_1"" details](https://circleci.com/gh/huggingface/datasets/46324?utm_campaign=vcs-integration-link&utm_medium=referral&utm_source=github-build-link), you can see that the only non-passing test has to do with the dataset card (missing information in the `README.md` file): `test_changed_dataset_card`\r\n```\r\n=========================== short test summary info ============================\r\nFAILED tests/test_dataset_cards.py::test_changed_dataset_card[swedish_medical_ner]\r\n= 1 failed, 3214 passed, 2874 skipped, 2 xfailed, 1 xpassed, 15 warnings in 175.59s (0:02:55) =\r\n```\r\n\r\nTherefore, your PR non-passing test has nothing to do with this issue.']","In the test_dataset_common.py script, line 288-289

```
if datasets.config.PYARROW_VERSION.major < 3:
   packaged_datasets = [pd for pd in packaged_datasets if pd[""dataset_name""] != ""parquet""]
```

which throws the error below. `datasets.config.PYARROW_VERSION` itself return the string '4.0.1'. I have tested this on both datasets.__version_=='1.11.0' and '1.9.0'. I am using Mac OS.

```
import datasets
datasets.config.PYARROW_VERSION.major
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
/var/folders/1f/0wqmlgp90qjd5mpj53fnjq440000gn/T/ipykernel_73361/2547517336.py in <module>
      1 import datasets
----> 2 datasets.config.PYARROW_VERSION.major

AttributeError: 'str' object has no attribute 'major'
```

## Environment info
- `datasets` version: 1.11.0
- Platform: Darwin-20.6.0-x86_64-i386-64bit
- Python version: 3.7.11
- PyArrow version: 4.0.1
"
https://github.com/huggingface/datasets/issues/2869,TypeError: 'NoneType' object is not callable,"['Hi, @Chenfei-Kang.\r\n\r\nI\'m sorry, but I\'m not able to reproduce your bug:\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset(""glue"", \'cola\')\r\nds\r\n```\r\n```\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: [\'sentence\', \'label\', \'idx\'],\r\n        num_rows: 8551\r\n    })\r\n    validation: Dataset({\r\n        features: [\'sentence\', \'label\', \'idx\'],\r\n        num_rows: 1043\r\n    })\r\n    test: Dataset({\r\n        features: [\'sentence\', \'label\', \'idx\'],\r\n        num_rows: 1063\r\n    })\r\n})\r\n```\r\n\r\nCould you please give more details and environment info (platform, PyArrow version)?'
 '> Hi, @Chenfei-Kang.\r\n> \r\n> I\'m sorry, but I\'m not able to reproduce your bug:\r\n> \r\n> ```python\r\n> from datasets import load_dataset\r\n> \r\n> ds = load_dataset(""glue"", \'cola\')\r\n> ds\r\n> ```\r\n> \r\n> ```\r\n> DatasetDict({\r\n>     train: Dataset({\r\n>         features: [\'sentence\', \'label\', \'idx\'],\r\n>         num_rows: 8551\r\n>     })\r\n>     validation: Dataset({\r\n>         features: [\'sentence\', \'label\', \'idx\'],\r\n>         num_rows: 1043\r\n>     })\r\n>     test: Dataset({\r\n>         features: [\'sentence\', \'label\', \'idx\'],\r\n>         num_rows: 1063\r\n>     })\r\n> })\r\n> ```\r\n> \r\n> Could you please give more details and environment info (platform, PyArrow version)?\r\n\r\nSorry to reply you so late.\r\nplatform: pycharm 2021 + anaconda with python 3.7\r\nPyArrow version: 5.0.0\r\nhuggingface-hub: 0.0.16\r\ndatasets: 1.9.0\r\n'
 ""- For the platform, we need to know the operating system of your machine. Could you please run the command `datasets-cli env` and copy-and-paste its output below?\r\n- In relation with the error, you just gave us the error type and message (`TypeError: 'NoneType' object is not callable`). Could you please copy-paste the complete stack trace, so that we know exactly which part of the code threw the error?""
 '> * For the platform, we need to know the operating system of your machine. Could you please run the command `datasets-cli env` and copy-and-paste its output below?\r\n> * In relation with the error, you just gave us the error type and message (`TypeError: \'NoneType\' object is not callable`). Could you please copy-paste the complete stack trace, so that we know exactly which part of the code threw the error?\r\n\r\n1. For the platform, here are the output:\r\n        - datasets` version: 1.11.0\r\n        - Platform: Windows-10-10.0.19041-SP0\r\n        - Python version: 3.7.10\r\n        - PyArrow version: 5.0.0\r\n2. For the code and error：\r\n     ```python\r\n     from datasets import load_dataset, load_metric\r\n     dataset = load_dataset(""glue"", ""cola"")\r\n    ```\r\n    ```python\r\n    Traceback (most recent call last):\r\n    ....\r\n    ....\r\n    File ""my_file.py"", line 2, in <module>\r\n    dataset = load_dataset(""glue"", ""cola"")\r\n    File ""My environments\\lib\\site-packages\\datasets\\load.py"", line 830, in load_dataset\r\n    **config_kwargs,\r\n     File ""My environments\\lib\\site-packages\\datasets\\load.py"", line 710, in load_dataset_builder\r\n    **config_kwargs,\r\n    TypeError: \'NoneType\' object is not callable\r\n    ```\r\n   Thank you!'
 ""For that environment, I am sorry but I can't reproduce the bug: I can load the dataset without any problem.""
 'One naive question: do you have internet access from the machine where you execute the code?'
 ""> For that environment, I am sorry but I can't reproduce the bug: I can load the dataset without any problem.\r\n\r\nBut I can download other task dataset such as `dataset = load_dataset('squad')`. I don't know what went wrong.  Thank you so much!""]","## Describe the bug

TypeError: 'NoneType' object is not callable
## Steps to reproduce the bug
```python
from datasets import load_dataset, load_metric
dataset = datasets.load_dataset(""glue"", 'cola')
```

## Expected results
A clear and concise description of the expected results.

## Actual results
Specify the actual results or traceback.

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: 1.11.0
- Platform:
- Python version: 3.7
- PyArrow version:
"
https://github.com/huggingface/datasets/issues/2866,"""counter"" dataset raises an error in normal mode, but not in streaming mode","['Hi @severo, thanks for reporting.\r\n\r\nJust note that currently not all canonical datasets support streaming mode: this is one case!\r\n\r\nAll datasets that use `pathlib` joins (using `/`) instead of `os.path.join` (as in this dataset) do not support streaming mode yet.'
 ""OK. Do you think it's possible to detect this, and raise an exception (maybe `NotImplementedError`, or a specific `StreamingError`)?""
 'We should definitely support datasets using `pathlib` in streaming mode...\r\n\r\nFor non-supported datasets in streaming mode, we have already a request of raising an error/warning: see #2654.'
 'Hi @severo, please note that ""counter"" dataset will be streamable (at least until it arrives at the missing file, error already in normal mode) once these PRs are merged:\r\n- #2874\r\n- #2876\r\n- #2880\r\n\r\nI have tested it. 😉 '
 'Now (on master), we get:\r\n\r\n```\r\nimport datasets as ds\r\nds.load_dataset(\'counter\', split=""train"", streaming=False)\r\n```\r\n\r\n```\r\nUsing custom data configuration default\r\nDownloading and preparing dataset counter/default (download: 1.29 MiB, generated: 2.48 MiB, post-processed: Unknown size, total: 3.77 MiB) to /home/slesage/.cache/huggingface/datasets/counter/default/1.0.0/9f84962fa0f35bec5a34fe0bdff8681838d497008c457f7856c48654476ec0e9...\r\nTraceback (most recent call last):\r\n  File ""/home/slesage/hf/datasets/src/datasets/builder.py"", line 726, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File ""/home/slesage/hf/datasets/src/datasets/builder.py"", line 1124, in _prepare_split\r\n    for key, record in utils.tqdm(\r\n  File ""/home/slesage/hf/datasets/.venv/lib/python3.8/site-packages/tqdm/std.py"", line 1185, in __iter__\r\n    for obj in iterable:\r\n  File ""/home/slesage/.cache/huggingface/modules/datasets_modules/datasets/counter/9f84962fa0f35bec5a34fe0bdff8681838d497008c457f7856c48654476ec0e9/counter.py"", line 161, in _generate_examples\r\n    with derived_file.open(encoding=""utf-8"") as f:\r\n  File ""/home/slesage/.pyenv/versions/3.8.11/lib/python3.8/pathlib.py"", line 1222, in open\r\n    return io.open(self, mode, buffering, encoding, errors, newline,\r\n  File ""/home/slesage/.pyenv/versions/3.8.11/lib/python3.8/pathlib.py"", line 1078, in _opener\r\n    return self._accessor.open(self, flags, mode)\r\nFileNotFoundError: [Errno 2] No such file or directory: \'/home/slesage/.cache/huggingface/datasets/downloads/extracted/b57aa6db5601a738e57b95c1fd8cced54ff28fc540efcdaf0f6c4f1bb5dfe211/COUNTER/0032p.xml\'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/slesage/hf/datasets/src/datasets/load.py"", line 1112, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File ""/home/slesage/hf/datasets/src/datasets/builder.py"", line 636, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File ""/home/slesage/hf/datasets/src/datasets/builder.py"", line 728, in _download_and_prepare\r\n    raise OSError(\r\nOSError: Cannot find data file.\r\nOriginal error:\r\n[Errno 2] No such file or directory: \'/home/slesage/.cache/huggingface/datasets/downloads/extracted/b57aa6db5601a738e57b95c1fd8cced54ff28fc540efcdaf0f6c4f1bb5dfe211/COUNTER/0032p.xml\'\r\n```\r\n\r\nThe error is now the same with or without streaming. I close the issue, thanks @albertvillanova and @lhoestq!\r\n'
 'Note that we might want to open an issue to fix the ""counter"" dataset by itself, but I let it up to you.'
 'Fixed here: https://github.com/huggingface/datasets/pull/2894. Thanks @albertvillanova ']","## Describe the bug

`counter` dataset raises an error on `load_dataset()`, but simply returns an empty iterator in streaming mode.

## Steps to reproduce the bug

```python
>>> import datasets as ds
>>> a = ds.load_dataset('counter', split=""train"", streaming=False)
Using custom data configuration default
Downloading and preparing dataset counter/default (download: 1.29 MiB, generated: 2.48 MiB, post-processed: Unknown size, total: 3.77 MiB) to /home/slesage/.cache/huggingface/datasets/counter/default/1.0.0/9f84962fa0f35bec5a34fe0bdff8681838d497008c457f7856c48654476ec0e9...
Traceback (most recent call last):
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/builder.py"", line 726, in _download_and_prepare
    self._prepare_split(split_generator, **prepare_split_kwargs)
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/builder.py"", line 1124, in _prepare_split
    for key, record in utils.tqdm(
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/tqdm/std.py"", line 1185, in __iter__
    for obj in iterable:
  File ""/home/slesage/.cache/huggingface/modules/datasets_modules/datasets/counter/9f84962fa0f35bec5a34fe0bdff8681838d497008c457f7856c48654476ec0e9/counter.py"", line 161, in _generate_examples
    with derived_file.open(encoding=""utf-8"") as f:
  File ""/home/slesage/.pyenv/versions/3.8.11/lib/python3.8/pathlib.py"", line 1222, in open
    return io.open(self, mode, buffering, encoding, errors, newline,
  File ""/home/slesage/.pyenv/versions/3.8.11/lib/python3.8/pathlib.py"", line 1078, in _opener
    return self._accessor.open(self, flags, mode)
FileNotFoundError: [Errno 2] No such file or directory: '/home/slesage/.cache/huggingface/datasets/downloads/extracted/b57aa6db5601a738e57b95c1fd8cced54ff28fc540efcdaf0f6c4f1bb5dfe211/COUNTER/0032p.xml'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/load.py"", line 1112, in load_dataset
    builder_instance.download_and_prepare(
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/builder.py"", line 636, in download_and_prepare
    self._download_and_prepare(
  File ""/home/slesage/hf/datasets-preview-backend/.venv/lib/python3.8/site-packages/datasets/builder.py"", line 728, in _download_and_prepare
    raise OSError(
OSError: Cannot find data file.
Original error:
[Errno 2] No such file or directory: '/home/slesage/.cache/huggingface/datasets/downloads/extracted/b57aa6db5601a738e57b95c1fd8cced54ff28fc540efcdaf0f6c4f1bb5dfe211/COUNTER/0032p.xml'
```

```python
>>> import datasets as ds
>>> b = ds.load_dataset('counter', split=""train"", streaming=True)
Using custom data configuration default
>>> list(b)
[]
```

## Expected results

An exception should be raised in streaming mode

## Actual results

No exception is raised in streaming mode: there is no way to tell if something has broken or if the dataset is simply empty.

## Environment info

- `datasets` version: 1.11.1.dev0
- Platform: Linux-5.11.0-1016-aws-x86_64-with-glibc2.29
- Python version: 3.8.11
- PyArrow version: 4.0.1
"
https://github.com/huggingface/datasets/issues/2860,Cannot download TOTTO dataset,"[""Hola @mrm8488, thanks for reporting.\r\n\r\nApparently, the data source host changed their URL one week ago: https://github.com/google-research-datasets/ToTTo/commit/cebeb430ec2a97747e704d16a9354f7d9073ff8f\r\n\r\nI'm fixing it.""]","Error: Couldn't find file at https://storage.googleapis.com/totto/totto_data.zip

`datasets version: 1.11.0`
# How to reproduce:

```py
from datasets import load_dataset
dataset = load_dataset('totto')
```


"
https://github.com/huggingface/datasets/issues/2859,Loading allenai/c4 in streaming mode does too many HEAD requests,['https://github.com/huggingface/datasets/blob/6c766f9115d686182d76b1b937cb27e099c45d68/src/datasets/builder.py#L179-L186'],"This does 60,000+ HEAD requests to get all the ETags of all the data files:
```python
from datasets import load_dataset
load_dataset(""allenai/c4"", streaming=True)
```
It makes loading the dataset completely impractical.

The ETags are used to compute the config id (it must depend on the data files being used).
Instead of using the ETags, we could simply use the commit hash of the dataset repository on the hub, as well and the glob pattern used to resolve the files (here it's `*` by default, to load all the files of the repository)"
https://github.com/huggingface/datasets/issues/2846,Negative timezone,['Fixed by #2847.'],"## Describe the bug
The load_dataset method do not accept a parquet file with a negative timezone, as it has the following regex:
```
""^(s|ms|us|ns),\s*tz=([a-zA-Z0-9/_+:]*)$""
```
So a valid timestap ```timestamp[us, tz=-03:00]``` returns an error when loading parquet files.

## Steps to reproduce the bug
```python
# Where the timestamp column has a tz of -03:00
datasets = load_dataset('parquet', data_files={'train': train_files, 'validation': validation_files,
                                        'test': test_files}, cache_dir=""./cache_teste/"")
```

## Expected results
The -03:00 is a valid tz so the regex should accept this without raising an error.

## Actual results
As this regex disaproves a valid tz it raises the following error:
```python
raise ValueError(
                f""{datasets_dtype} is not a validly formatted string representation of a pyarrow timestamp.""
                f""Examples include timestamp[us] or timestamp[us, tz=America/New_York]""
                f""See: https://arrow.apache.org/docs/python/generated/pyarrow.timestamp.html#pyarrow.timestamp""
            )
```

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: 1.11.0
- Platform: Ubuntu 20.04
- Python version: 3.8
- PyArrow version: 5.0.0
"
https://github.com/huggingface/datasets/issues/2842,always requiring the username in the dataset name when there is one,"['From what I can understand, you want the saved arrow file directory to have username as well instead of just dataset name if it was downloaded with the user prefix?'
 'I don\'t think the user cares of how this is done, but the 2nd command should fail, IMHO, as its dataset name is invalid:\r\n```\r\n# first run\r\npython -c ""from datasets import load_dataset; load_dataset(\'stas/openwebtext-10k\')""\r\n# now run immediately\r\npython -c ""from datasets import load_dataset; load_dataset(\'openwebtext-10k\')""\r\n# the second command should fail, but it doesn\'t fail now.\r\n```\r\n\r\nMoreover, if someone were to create `openwebtext-10k` w/o the prefix, they will now get the wrong dataset, if they previously downloaded `stas/openwebtext-10k`.\r\n\r\nAnd if there are 2 users with the same dataset name `foo/ds` and `bar/ds` - currently this won\'t work to get the correct dataset.\r\n\r\nSo really there 3 unrelated issues hiding in the current behavior.']","Me and now another person have been bitten by the `datasets`'s non-strictness on requiring a dataset creator's username when it's due.

So both of us started with `stas/openwebtext-10k`, somewhere along the lines lost `stas/` and continued using `openwebtext-10k` and it all was good until we published the software and things broke, since there is no `openwebtext-10k`

So this feature request is asking to tighten the checking and not allow dataset loading if it was downloaded with the user prefix, but then attempted to be used w/o it.

The same in code:

```
# first run
python -c ""from datasets import load_dataset; load_dataset('stas/openwebtext-10k')""
# now run immediately
python -c ""from datasets import load_dataset; load_dataset('openwebtext-10k')""
# the second command should fail, but it doesn't fail now.
```

Please let me know if I explained myself clearly.

Thank you!"
https://github.com/huggingface/datasets/issues/2839,OpenWebText: NonMatchingSplitsSizesError,"[""Thanks for reporting, I'm updating the verifications metadata""
 ""I just regenerated the verifications metadata and noticed that nothing changed: the data file is fine (the checksum didn't change), and the number of examples is still 8013769. Not sure how you managed to get 7982430 examples.\r\n\r\nCan you try to delete your cache ( by default at `~/.cache/huggingface/datasets`) and try again please ?\r\nAlso, on which platform are you (linux/macos/windows) ?""
 'I\'ll try without deleting the whole cache (we have large datasets already stored). I was under the impression that `download_mode=""force_redownload""` would bypass cache.\r\nSorry plateform should be linux (Redhat version 8.1)'
 'Hi @thomasw21 , are you still having this issue after clearing your cache ?'
 ""Sorry I haven't had time to work on this. I'll close and re-open if I can't figure out why I'm having this issue. Thanks for taking a look !""]","## Describe the bug

When downloading `openwebtext`, I'm getting:
```
datasets.utils.info_utils.NonMatchingSplitsSizesError: [{'expected': SplitInfo(name='train', num_bytes=39769494896, num_examples=8013769, dataset_name='openwebtext'), 'recorded': SplitInfo(name='train', num_bytes=39611023912, num_examples=7982430, dataset_name='openwebtext')}]
```

I suspect that the file we download from has changed since the size doesn't look like to match with documentation

`Downloading:   0%|          | 0.00/12.9G [00:00<?, ?B/s]` This suggest the total size is 12.9GB, whereas the one documented mentions `Size of downloaded dataset files: 12283.35 MB`.

## Steps to reproduce the bug
```python
from datasets import load_dataset

load_dataset(""openwebtext"", download_mode=""force_redownload"")
```

## Expected results

Loading is successful

## Actual results

Loading throws above error.

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: 1.10.2
- Platform: linux (Redhat version 8.1)
- Python version: 3.8
- PyArrow version: 4.0.1
"
https://github.com/huggingface/datasets/issues/2837,prepare_module issue when loading from read-only fs,"['Hello, I opened #2887 to fix this.']","## Describe the bug

When we use prepare_module from a readonly file system, we create a FileLock using the `local_path`.
This path is not necessarily writable.

`lock_path = local_path + "".lock""`


## Steps to reproduce the bug

Run `load_dataset` on a readonly python loader file.
```python
ds = load_dataset(
        python_loader, data_files={""train"": train_path, ""test"": test_path}
    )
```

where `python_loader` is a path to a file located in a readonly folder.

## Expected results
This should work I think?

## Actual results

```python
    return load_dataset(
  File ""/usr/local/lib/python3.8/dist-packages/datasets/load.py"", line 711, in load_dataset
    module_path, hash, resolved_file_path = prepare_module(
  File ""/usr/local/lib/python3.8/dist-packages/datasets/load.py"", line 465, in prepare_module
    with FileLock(lock_path):
  File ""/usr/local/lib/python3.8/dist-packages/datasets/utils/filelock.py"", line 314, in __enter__
    self.acquire()
  File ""/usr/local/lib/python3.8/dist-packages/datasets/utils/filelock.py"", line 263, in acquire
    self._acquire()
  File ""/usr/local/lib/python3.8/dist-packages/datasets/utils/filelock.py"", line 378, in _acquire
    fd = os.open(self._lock_file, open_mode)
OSError: [Errno 30] Read-only file system: 'YOUR_FILE.py.lock'
```

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: 1.7.0
- Platform: macOS-10.15.7-x86_64-i386-64bit
- Python version: 3.8.8
- PyArrow version: 3.0.0
"
https://github.com/huggingface/datasets/issues/2831,ArrowInvalid when mapping dataset with missing values,"['Hi ! It fails because of the feature type inference.\r\n\r\nBecause the first 1000 examples all have null values in the ""match"" field, then it infers that the type for this field is `null` type before writing the data on disk. But as soon as it tries to map an example with a non-null ""match"" field, then it fails.\r\n\r\nTo fix that you can either:\r\n- increase the writer_batch_size to >2000 (default is 1000) so that some non-null values will be in the first batch written to disk\r\n```python\r\ndatasets = datasets.map(lambda e: {\'labels\': e[\'match\']}, remove_columns=[\'id\'], writer_batch_size=2000)\r\n```\r\n- OR force the feature type with:\r\n```python\r\nfrom datasets import Features, Value\r\n\r\nfeatures = Features({\r\n    \'conflict\': Value(\'int64\'),\r\n    \'date\': Value(\'string\'),\r\n    \'headline\': Value(\'string\'),\r\n    \'match\': Value(\'float64\'),\r\n    \'label\': Value(\'float64\')\r\n})\r\ndatasets = datasets.map(lambda e: {\'labels\': e[\'match\']}, remove_columns=[\'id\'], features=features)\r\n```']","## Describe the bug
I encountered an `ArrowInvalid` when mapping dataset with missing values. 
Here are the files for a minimal example. The exception is only thrown when the first line in the csv has a missing value (if you move the last line to the top it isn't thrown).
[data_small.csv](https://github.com/huggingface/datasets/files/7037838/data_small.csv)
[data.csv](https://github.com/huggingface/datasets/files/7037842/data.csv)

## Steps to reproduce the bug
```python
from datasets import load_dataset

datasets = load_dataset(""csv"", data_files=['data_small.csv'])

datasets = datasets.map(lambda e: {'labels': e['match']},
                        remove_columns=['id'])
```

## Expected results
No error

## Actual results
```
File ""pyarrow/error.pxi"", line 84, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: Invalid null value
```

## Environment info
- `datasets` version: 1.5.0
- Platform: Linux-5.11.0-25-generic-x86_64-with-glibc2.29
- Python version: 3.8.10
- PyTorch version (GPU?): 1.7.1+cpu (False)
- Tensorflow version (GPU?): 2.4.1 (False)
- Using GPU in script?: no
- Using distributed or parallel set-up in script?: no
"
https://github.com/huggingface/datasets/issues/2826,Add a Text Classification dataset: KanHope,"[""Hi ! In your script it looks like you're trying to load the dataset `bn_hate_speech,`, not KanHope.\r\n\r\nMoreover the error `KeyError: ' '` means that you have a feature of type ClassLabel, but for a certain example of the dataset, it looks like the label is empty (it's just a string with a space). Can you make sure that the data don't have missing labels, and that your dataset script parses the labels correctly ?""]","## Adding a Dataset
- **Name:** *KanHope*
- **Description:** *A code-mixed English-Kannada dataset for Hope speech detection*
- **Paper:** *https://arxiv.org/abs/2108.04616* (I am the author of the paper}
- **Author:** *[AdeepH](https://github.com/adeepH)*
- **Data:** *https://github.com/adeepH/KanHope/tree/main/dataset*
- **Motivation:** *The dataset is amongst the very few resources available for code-mixed Dravidian languages*

- I tried following the steps as per the instructions. However, could not resolve an error. Any help would be appreciated.

- The dataset card and the scripts for the dataset *https://github.com/adeepH/datasets/tree/multilingual-hope-speech/datasets/mhs_eval*

```
Using custom data configuration default
Downloading and preparing dataset bn_hate_speech/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/bn_hate_speech/default/0.0.0/5f417ddc89777278abd29988f909f39495f0ec802090f7d8fa63b5bffb121762...
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-114-4a9cdb519e4c> in <module>()
      1 from datasets import load_dataset
      2 
----> 3 data = load_dataset('/content/bn')

9 frames
/usr/local/lib/python3.7/dist-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, script_version, use_auth_token, task, streaming, **config_kwargs)
    850         ignore_verifications=ignore_verifications,
    851         try_from_hf_gcs=try_from_hf_gcs,
--> 852         use_auth_token=use_auth_token,
    853     )
    854 

/usr/local/lib/python3.7/dist-packages/datasets/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)
    614                     if not downloaded_from_gcs:
    615                         self._download_and_prepare(
--> 616                             dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
    617                         )
    618                     # Sync info

/usr/local/lib/python3.7/dist-packages/datasets/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)
    691             try:
    692                 # Prepare split will record examples associated to the split
--> 693                 self._prepare_split(split_generator, **prepare_split_kwargs)
    694             except OSError as e:
    695                 raise OSError(

/usr/local/lib/python3.7/dist-packages/datasets/builder.py in _prepare_split(self, split_generator)
   1107                     disable=bool(logging.get_verbosity() == logging.NOTSET),
   1108                 ):
-> 1109                     example = self.info.features.encode_example(record)
   1110                     writer.write(example, key)
   1111             finally:

/usr/local/lib/python3.7/dist-packages/datasets/features.py in encode_example(self, example)
   1015         """"""
   1016         example = cast_to_python_objects(example)
-> 1017         return encode_nested_example(self, example)
   1018 
   1019     def encode_batch(self, batch):

/usr/local/lib/python3.7/dist-packages/datasets/features.py in encode_nested_example(schema, obj)
    863     if isinstance(schema, dict):
    864         return {
--> 865             k: encode_nested_example(sub_schema, sub_obj) for k, (sub_schema, sub_obj) in utils.zip_dict(schema, obj)
    866         }
    867     elif isinstance(schema, (list, tuple)):

/usr/local/lib/python3.7/dist-packages/datasets/features.py in <dictcomp>(.0)
    863     if isinstance(schema, dict):
    864         return {
--> 865             k: encode_nested_example(sub_schema, sub_obj) for k, (sub_schema, sub_obj) in utils.zip_dict(schema, obj)
    866         }
    867     elif isinstance(schema, (list, tuple)):

/usr/local/lib/python3.7/dist-packages/datasets/features.py in encode_nested_example(schema, obj)
    890     # ClassLabel will convert from string to int, TranslationVariableLanguages does some checks
    891     elif isinstance(schema, (ClassLabel, TranslationVariableLanguages, Value, _ArrayXD)):
--> 892         return schema.encode_example(obj)
    893     # Other object should be directly convertible to a native Arrow type (like Translation and Translation)
    894     return obj

/usr/local/lib/python3.7/dist-packages/datasets/features.py in encode_example(self, example_data)
    665         # If a string is given, convert to associated integer
    666         if isinstance(example_data, str):
--> 667             example_data = self.str2int(example_data)
    668 
    669         # Allowing -1 to mean no label.

/usr/local/lib/python3.7/dist-packages/datasets/features.py in str2int(self, values)
    623                 if value not in self._str2int:
    624                     value = str(value).strip()
--> 625                 output.append(self._str2int[str(value)])
    626             else:
    627                 # No names provided, try to integerize

KeyError: ' '
```"
https://github.com/huggingface/datasets/issues/2825,The datasets.map function does not load cached dataset after moving python script,"[""This also happened to me on COLAB.\r\nDetails:\r\nI ran the `run_mlm.py` in two different notebooks. \r\nIn the first notebook, I do tokenization since I can get 4 CPU cores without any GPUs, and save the cache into a folder which I copy to drive.\r\nIn the second notebook, I copy the cache folder from drive and re-run the run_mlm.py script (this time I uncomment the trainer code which happens after the tokenization)\r\n\r\nNote: I didn't change anything in the arguments, not even the preprocessing_num_workers\r\n ""
 ""Thanks for reporting ! This is indeed a bug, I'm looking into it""
 ""#2854 fixed the issue :)\r\n\r\nWe'll do a new release of `datasets` soon to make the fix available.\r\nIn the meantime, feel free to try it out by installing `datasets` from source\r\n\r\nIf you have other issues or any question, feel free to re-open the issue :)""]","## Describe the bug
The datasets.map function caches the processed data to a certain directory. When the map function is called another time with totally the same parameters, the cached data are supposed to be reloaded instead of re-processing. However, it doesn't reuse cached data sometimes. I use the common data processing in different tasks, the datasets are processed again, the only difference is that I run them in different files.

## Steps to reproduce the bug
Just run the following codes in different .py files.
```python
if __name__ == '__main__':
    from datasets import load_dataset
    from transformers import AutoTokenizer
    raw_datasets = load_dataset(""wikitext"", ""wikitext-2-raw-v1"")

    tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncased"")


    def tokenize_function(examples):
        return tokenizer(examples[""text""], padding=""max_length"", truncation=True)


    tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
```

## Expected results
The map function should reload data in the second or any later runs.

## Actual results
The processing happens in each run.

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: 1.8.0
- Platform: linux
- Python version: 3.7.6
- PyArrow version: 3.0.0

This is the first time I report a bug. If there is any problem or confusing description, please let me know 😄.
"
https://github.com/huggingface/datasets/issues/2823,HF_DATASETS_CACHE variable in Windows,"[""Agh - I'm a muppet. No quote marks are needed.\r\nset HF_DATASETS_CACHE = C:\\Datasets\r\nworks as intended.""]","I can't seem to use a custom Cache directory in Windows. I have tried:

set HF_DATASETS_CACHE = ""C:\Datasets""
set HF_DATASETS_CACHE = ""C:/Datasets""
set HF_DATASETS_CACHE = ""C:\\Datasets""
set HF_DATASETS_CACHE = ""r'C:\Datasets'""
set HF_DATASETS_CACHE = ""\Datasets""
set HF_DATASETS_CACHE = ""/Datasets""

In each instance I get the ""[WinError 123] The filename, directory name, or volume label syntax is incorrect"" error when attempting to load a dataset"
https://github.com/huggingface/datasets/issues/2821,Cannot load linnaeus dataset,"[""Thanks for reporting ! #2852 fixed this error\r\n\r\nWe'll do a new release of `datasets` soon :)""]","## Describe the bug
The [linnaeus](https://huggingface.co/datasets/linnaeus) dataset cannot be loaded. To reproduce:
```
from datasets import load_dataset

datasets = load_dataset(""linnaeus"")
```
This results in:
```
Downloading and preparing dataset linnaeus/linnaeus (download: 17.36 MiB, generated: 8.74 MiB, post-processed: Unknown size, total: 26.10 MiB) to /root/.cache/huggingface/datasets/linnaeus/linnaeus/1.0.0/2ff05dbc256108233262f596e09e322dbc3db067202de14286913607cd9cb704...
---------------------------------------------------------------------------
ConnectionError                           Traceback (most recent call last)
<ipython-input-4-7ef3a88f6276> in <module>()
      1 from datasets import load_dataset
      2 
----> 3 datasets = load_dataset(""linnaeus"")

11 frames
/usr/local/lib/python3.7/dist-packages/datasets/utils/file_utils.py in get_from_cache(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, use_auth_token)
    603             raise FileNotFoundError(""Couldn't find file at {}"".format(url))
    604         _raise_if_offline_mode_is_enabled(f""Tried to reach {url}"")
--> 605         raise ConnectionError(""Couldn't reach {}"".format(url))
    606 
    607     # Try a second time

ConnectionError: Couldn't reach https://drive.google.com/u/0/uc?id=1OletxmPYNkz2ltOr9pyT0b0iBtUWxslh&export=download/
```"
https://github.com/huggingface/datasets/issues/2820,Downloading “reddit” dataset keeps timing out.,"['```\r\nUsing custom data configuration default\r\nDownloading and preparing dataset reddit/default (download: 2.93 GiB, generated: 17.64 GiB, post-processed: Unknown size, total: 20.57 GiB) to /Volumes/My Passport for Mac/og-chat-data/reddit/default/1.0.0/98ba5abea674d3178f7588aa6518a5510dc0c6fa8176d9653a3546d5afcb3969...\r\nDownloading: 13%\r\n403M/3.14G [44:39<2:27:09, 310kB/s]\r\n---------------------------------------------------------------------------\r\ntimeout                                   Traceback (most recent call last)\r\n/usr/local/anaconda3/envs/og-data-env/lib/python3.9/site-packages/urllib3/response.py in _error_catcher(self)\r\n    437             try:\r\n--> 438                 yield\r\n    439 \r\n\r\n/usr/local/anaconda3/envs/og-data-env/lib/python3.9/site-packages/urllib3/response.py in read(self, amt, decode_content, cache_content)\r\n    518                 cache_content = False\r\n--> 519                 data = self._fp.read(amt) if not fp_closed else b""""\r\n    520                 if (\r\n\r\n/usr/local/anaconda3/envs/og-data-env/lib/python3.9/http/client.py in read(self, amt)\r\n    458             b = bytearray(amt)\r\n--> 459             n = self.readinto(b)\r\n    460             return memoryview(b)[:n].tobytes()\r\n\r\n/usr/local/anaconda3/envs/og-data-env/lib/python3.9/http/client.py in readinto(self, b)\r\n    502         # (for example, reading in 1k chunks)\r\n--> 503         n = self.fp.readinto(b)\r\n    504         if not n and b:\r\n\r\n/usr/local/anaconda3/envs/og-data-env/lib/python3.9/socket.py in readinto(self, b)\r\n    703             try:\r\n--> 704                 return self._sock.recv_into(b)\r\n    705             except timeout:\r\n\r\n/usr/local/anaconda3/envs/og-data-env/lib/python3.9/ssl.py in recv_into(self, buffer, nbytes, flags)\r\n   1240                   self.__class__)\r\n-> 1241             return self.read(nbytes, buffer)\r\n   1242         else:\r\n\r\n/usr/local/anaconda3/envs/og-data-env/lib/python3.9/ssl.py in read(self, len, buffer)\r\n   1098             if buffer is not None:\r\n-> 1099                 return self._sslobj.read(len, buffer)\r\n   1100             else:\r\n\r\ntimeout: The read operation timed out\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nReadTimeoutError                          Traceback (most recent call last)\r\n/usr/local/anaconda3/envs/og-data-env/lib/python3.9/site-packages/requests/models.py in generate()\r\n    757                 try:\r\n--> 758                     for chunk in self.raw.stream(chunk_size, decode_content=True):\r\n    759                         yield chunk\r\n\r\n/usr/local/anaconda3/envs/og-data-env/lib/python3.9/site-packages/urllib3/response.py in stream(self, amt, decode_content)\r\n    575             while not is_fp_closed(self._fp):\r\n--> 576                 data = self.read(amt=amt, decode_content=decode_content)\r\n    577 \r\n\r\n/usr/local/anaconda3/envs/og-data-env/lib/python3.9/site-packages/urllib3/response.py in read(self, amt, decode_content, cache_content)\r\n    540                         # Content-Length are caught.\r\n--> 541                         raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\r\n    542 \r\n\r\n/usr/local/anaconda3/envs/og-data-env/lib/python3.9/contextlib.py in __exit__(self, type, value, traceback)\r\n    134             try:\r\n--> 135                 self.gen.throw(type, value, traceback)\r\n    136             except StopIteration as exc:\r\n\r\n/usr/local/anaconda3/envs/og-data-env/lib/python3.9/site-packages/urllib3/response.py in _error_catcher(self)\r\n    442                 # there is yet no clean way to get at it from this context.\r\n--> 443                 raise ReadTimeoutError(self._pool, None, ""Read timed out."")\r\n    444 \r\n\r\nReadTimeoutError: HTTPSConnectionPool(host=\'zenodo.org\', port=443): Read timed out.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nConnectionError                           Traceback (most recent call last)\r\n/var/folders/3f/md0t9sgj6rz8xy01fskttqdc0000gn/T/ipykernel_89016/1133441872.py in <module>\r\n      1 from datasets import load_dataset\r\n      2 \r\n----> 3 dataset = load_dataset(""reddit"", ignore_verifications=True, cache_dir=""/Volumes/My Passport for Mac/og-chat-data"")\r\n\r\n/usr/local/anaconda3/envs/og-data-env/lib/python3.9/site-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, script_version, use_auth_token, task, streaming, **config_kwargs)\r\n    845 \r\n    846     # Download and prepare data\r\n--> 847     builder_instance.download_and_prepare(\r\n    848         download_config=download_config,\r\n    849         download_mode=download_mode,\r\n\r\n/usr/local/anaconda3/envs/og-data-env/lib/python3.9/site-packages/datasets/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)\r\n    613                             logger.warning(""HF google storage unreachable. Downloading and preparing it from source"")\r\n    614                     if not downloaded_from_gcs:\r\n--> 615                         self._download_and_prepare(\r\n    616                             dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    617                         )\r\n\r\n/usr/local/anaconda3/envs/og-data-env/lib/python3.9/site-packages/datasets/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    669         split_dict = SplitDict(dataset_name=self.name)\r\n    670         split_generators_kwargs = self._make_split_generators_kwargs(prepare_split_kwargs)\r\n--> 671         split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n    672 \r\n    673         # Checksums verification\r\n\r\n~/.cache/huggingface/modules/datasets_modules/datasets/reddit/98ba5abea674d3178f7588aa6518a5510dc0c6fa8176d9653a3546d5afcb3969/reddit.py in _split_generators(self, dl_manager)\r\n     73     def _split_generators(self, dl_manager):\r\n     74         """"""Returns SplitGenerators.""""""\r\n---> 75         dl_path = dl_manager.download_and_extract(_URL)\r\n     76         return [\r\n     77             datasets.SplitGenerator(\r\n\r\n/usr/local/anaconda3/envs/og-data-env/lib/python3.9/site-packages/datasets/utils/download_manager.py in download_and_extract(self, url_or_urls)\r\n    287             extracted_path(s): `str`, extracted paths of given URL(s).\r\n    288         """"""\r\n--> 289         return self.extract(self.download(url_or_urls))\r\n    290 \r\n    291     def get_recorded_sizes_checksums(self):\r\n\r\n/usr/local/anaconda3/envs/og-data-env/lib/python3.9/site-packages/datasets/utils/download_manager.py in download(self, url_or_urls)\r\n    195 \r\n    196         start_time = datetime.now()\r\n--> 197         downloaded_path_or_paths = map_nested(\r\n    198             download_func,\r\n    199             url_or_urls,\r\n\r\n/usr/local/anaconda3/envs/og-data-env/lib/python3.9/site-packages/datasets/utils/py_utils.py in map_nested(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, types)\r\n    194     # Singleton\r\n    195     if not isinstance(data_struct, dict) and not isinstance(data_struct, types):\r\n--> 196         return function(data_struct)\r\n    197 \r\n    198     disable_tqdm = bool(logger.getEffectiveLevel() > logging.INFO) or not utils.is_progress_bar_enabled()\r\n\r\n/usr/local/anaconda3/envs/og-data-env/lib/python3.9/site-packages/datasets/utils/download_manager.py in _download(self, url_or_filename, download_config)\r\n    218             # append the relative path to the base_path\r\n    219             url_or_filename = url_or_path_join(self._base_path, url_or_filename)\r\n--> 220         return cached_path(url_or_filename, download_config=download_config)\r\n    221 \r\n    222     def iter_archive(self, path):\r\n\r\n/usr/local/anaconda3/envs/og-data-env/lib/python3.9/site-packages/datasets/utils/file_utils.py in cached_path(url_or_filename, download_config, **download_kwargs)\r\n    286     if is_remote_url(url_or_filename):\r\n    287         # URL, so get it from the cache (downloading if necessary)\r\n--> 288         output_path = get_from_cache(\r\n    289             url_or_filename,\r\n    290             cache_dir=cache_dir,\r\n\r\n/usr/local/anaconda3/envs/og-data-env/lib/python3.9/site-packages/datasets/utils/file_utils.py in get_from_cache(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, use_auth_token)\r\n    643                 ftp_get(url, temp_file)\r\n    644             else:\r\n--> 645                 http_get(\r\n    646                     url,\r\n    647                     temp_file,\r\n\r\n/usr/local/anaconda3/envs/og-data-env/lib/python3.9/site-packages/datasets/utils/file_utils.py in http_get(url, temp_file, proxies, resume_size, headers, cookies, timeout, max_retries)\r\n    451         disable=bool(logging.get_verbosity() == logging.NOTSET),\r\n    452     )\r\n--> 453     for chunk in response.iter_content(chunk_size=1024):\r\n    454         if chunk:  # filter out keep-alive new chunks\r\n    455             progress.update(len(chunk))\r\n\r\n/usr/local/anaconda3/envs/og-data-env/lib/python3.9/site-packages/requests/models.py in generate()\r\n    763                     raise ContentDecodingError(e)\r\n    764                 except ReadTimeoutError as e:\r\n--> 765                     raise ConnectionError(e)\r\n    766             else:\r\n    767                 # Standard file-like object.\r\n\r\nConnectionError: HTTPSConnectionPool(host=\'zenodo.org\', port=443): Read timed out.\r\n```'
 'Hey @lhoestq should I try to fix this issue ?'
 'It also doesn\'t seem to be ""smart caching"" and I received an error about a file not being found...'
 'To be clear, the error I get when I try to ""re-instantiate"" the download after failure is: \r\n```\r\nOSError: Cannot find data file. \r\nOriginal error:\r\n[Errno 20] Not a directory: <HOME>/.cache/huggingface/datasets/downloads/1ec12301abba4daa60eb3a90e53529b5b173296b22dc3bef3186e205c75e594c/corpus-webis-tldr-17.json\'\r\n```'
 ""Here is a new error:\r\n```\r\nConnectionError: Couldn't reach https://zenodo.org/record/1043504/files/corpus-webis-tldr-17.zip?download=1\r\n```""
 ""Hi ! Since https://github.com/huggingface/datasets/pull/2803 we've changed the time out from 10sec to 100sec.\r\nThis should prevent the `ReadTimeoutError`. Feel free to try it out by installing `datasets` from source\r\n```\r\npip install git+https://github.com/huggingface/datasets.git\r\n```\r\n\r\nWhen re-running your code you said you get a `OSError`, could you try deleting the file at the path returned by the error ? (the one after `[Errno 20] Not a directory:`). Ideally when a download fails you should be able to re-run it without error; there might be an issue here.\r\n\r\nFinally not sure what we can do about `ConnectionError`, this must be an issue from zenodo. If it happens you simply need to try again\r\n""
 ""@lhoestq thanks for the update. The directory specified by the OSError ie. \r\n```\r\n1ec12301abba4daa60eb3a90e53529b5b173296b22dc3bef3186e205c75e594c/corpus-webis-tldr-17.json \r\n```\r\n was not actually in that directory so I can't delete it. ""
 'Oh, then could you try deleting the parent directory `1ec12301abba4daa60eb3a90e53529b5b173296b22dc3bef3186e205c75e594c` instead ?\r\nThis way the download manager will know that it has to uncompress the data again'
 'It seems to have worked. It only took like 20min! I think the extra timeout length did the trick! One thing is that it downloaded a total of 41gb instead of 20gb but at least it finished. '
 'Great ! The timeout change will be available in the next release of `datasets` :)']","## Describe the bug
A clear and concise description of what the bug is.
Everytime I try and download the reddit dataset it times out before finishing and I have to try again.

There is some timeout error that I will post once it happens again.

## Steps to reproduce the bug
```python
from datasets import load_dataset

dataset = load_dataset(""reddit"", ignore_verifications=True, cache_dir=""/Volumes/My Passport for Mac/og-chat-data"")
```

## Expected results
A clear and concise description of the expected results.

I would expect the download to finish, or at least provide a parameter to extend the read timeout window.

## Actual results
Specify the actual results or traceback.

Shown below in error message.

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version:  1.11.0
- Platform: macOS 
- Python version: 3.9.6 (conda env)
- PyArrow version: N/A
"
https://github.com/huggingface/datasets/issues/2818,cannot load data from my loacal path,"['Hi ! The `data_files` parameter must be a string, a list/tuple or a python dict.\r\n\r\nCan you check the type of your `config.train_path` please ? Or use `data_files=str(config.train_path)` ?']","## Describe the bug
I just want to directly load data from my local path,but find a bug.And I compare it with pandas to provide my local path is real.

here is my code
```python3
# print my local path
print(config.train_path)
# read data and print data length
tarin=pd.read_csv(config.train_path)
print(len(tarin))

# loading data by load_dataset 
data = load_dataset('csv',data_files=config.train_path)

print(len(data))
```


## Steps to reproduce the bug
```python
C:\Users\wie\Documents\项目\文本分类\data\train.csv
7613
Traceback (most recent call last):
  File ""c:/Users/wie/Documents/项目/文本分类/lib/DataPrecess.py"", line 17, in <module>
    data = load_dataset('csv',data_files=config.train_path)
  File ""C:\Users\wie\Miniconda3\lib\site-packages\datasets\load.py"", line 830, in load_dataset
    **config_kwargs,
  File ""C:\Users\wie\Miniconda3\lib\site-packages\datasets\load.py"", line 710, in load_dataset_builder
    **config_kwargs,
  File ""C:\Users\wie\Miniconda3\lib\site-packages\datasets\builder.py"", line 271, in __init__
    **config_kwargs,
  File ""C:\Users\wie\Miniconda3\lib\site-packages\datasets\builder.py"", line 386, in _create_builder_config
    config_kwargs, custom_features=custom_features, use_auth_token=self.use_auth_token
  File ""C:\Users\wie\Miniconda3\lib\site-packages\datasets\builder.py"", line 156, in create_config_id
    raise ValueError(""Please provide a valid `data_files` in `DatasetBuilder`"")
ValueError: Please provide a valid `data_files` in `DatasetBuilder`
```

## Expected results
A clear and concise description of the expected results.

## Actual results
Specify the actual results or traceback.

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version:  1.11.0
- Platform: win10
- Python version: 3.7.9
- PyArrow version: 5.0.0
"
https://github.com/huggingface/datasets/issues/2816,Add Mostly Basic Python Problems Dataset,['I started working on that.'],"## Adding a Dataset
- **Name:** Mostly Basic Python Problems Dataset
- **Description:** The benchmark consists of around 1,000 crowd-sourced Python programming problems, designed to be solvable by entry level programmers, covering programming fundamentals, standard library functionality, and so on. Each problem consists of a task description, code solution and 3 automated test cases.
- **Paper:** *link to the dataset paper if available*
- **Data:** https://github.com/google-research/google-research/tree/master/mbpp
- **Motivation:** Simple, small dataset related to coding problems.

Instructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).
"
https://github.com/huggingface/datasets/issues/2813,Remove compression from xopen,"['After discussing with @lhoestq, a reasonable alternative:\r\n- `download_manager.extract(urlpath)` adds prefixes to `urlpath` in the same way as `fsspec` does for protocols, but we implement custom prefixes for all compression formats: \r\n  `bz2::http://domain.org/filename.bz2`\r\n- `xopen` parses the `urlpath` and extracts the `compression` parameter and passes it to `fsspec.open`:\r\n  `fsspec.open(""http://domain.org/filename.bz2"", compression=""bz2"")`\r\n\r\nPros:\r\n- clean solution that continues giving support to all compression formats\r\n- no breaking change when opening non-decompressed files: if no compression-protocol-like is passed, fsspec.open does not uncompress (passes compression=None)\r\n\r\nCons:\r\n- we create a ""private"" convention for the format of `urlpath`: although similar to `fsspec` protocols, we add custom prefixes for the `compression` argument']","We implemented support for streaming with 2 requirements:
- transparent use for the end user: just needs to pass the parameter `streaming=True`
- no additional work for the contributors: previous loading scripts should also work in streaming mode with no (or minor) changes; and new loading scripts should not involve additional code to support streaming

In order to fulfill these requirements, streaming implementation patched some Python functions:
- the `open(urlpath)` function was patched with `fsspec.open(urlpath)`
- the `os.path.join(urlpath, *others)` function was patched in order to add to `urlpath` hops (`::`) and extractor protocols (`zip://`), which are required by `fsspec.open`

Recently, we implemented support for streaming all archive+compression formats: zip, tar, gz, bz2, lz4, xz, zst; tar.gz, tar.bz2,...
Under the hood, the implementation:
- passes an additional parameter `compression` to `fsspec.open`, so that it performs the decompression on the fly: `fsspec.open(urlpath, compression=...)`

Some concerns have been raised about passing the parameter `compression` to `fsspec.open`:
- https://github.com/huggingface/datasets/pull/2786#discussion_r689550254
- #2811 

The main argument is that if `open` decompresses the file and afterwards we call `gzip.open` on it, that will raise an error in `oscar` dataset:
```python
gzip.open(open(urlpath
```
While this is true:
- it is not natural/usual to call `open` inside `gzip.open` (never seen this before)
- indeed, this was recently (2 months ago) coded that way in `datasets` in order to allow streaming support (with previous implementation of streaming)

In this particular case, there is a natural fix solution: #2811:
- Revert the `open` inside the `gzip.open` (change done 2 months ago): `gzip.open(open(urlpath` => `gzip.open(urlpath`
- Patch `gzip.open(urlpath` with `fsspec.open(urlpath, compression=""gzip""` 

Are there other issues apart from this?

Note that there is an issue just because the open inside of the gzip.open. There is no issue in the other cases where datasets loading scripts use just
- `gzip.open` 
- `open` (after having called dl_manager.download_and_extract)

TODO:
- [ ] Is this really an issue? Please enumerate the `datasets` loading scripts where this is problematic.
  - For the moment, there are only 3 datasets where we have an `open` inside a `gzip.open`:
    - oscar (since 23 June), mc4 (since 2 July) and c4 (since 2 July)
  - In the 3 datasets, the only reason to put an open inside a gzip.open was indeed to force supporting streaming
- [ ] If this is indeed an issue, which are the possible alternatives? Pros/cons?"
https://github.com/huggingface/datasets/issues/2799,Loading JSON throws ArrowNotImplementedError,"['Hi @lewtun, thanks for reporting.\r\n\r\nApparently, `pyarrow.json` tries to cast timestamp-like fields in your JSON file to pyarrow timestamp type, and it fails with `ArrowNotImplementedError`.\r\n\r\nI will investigate if there is a way to tell pyarrow not to try that timestamp casting.'
 'I think the issue is more complex than that...\r\n\r\nI just took one of your JSON lines and pyarrow.json read it without problem.'
 '> I just took one of your JSON lines an pyarrow.json read it without problem.\r\n\r\nyes, and for some peculiar reason the error is non-deterministic (i was eventually able to load the whole dataset by just re-running the `load_dataset` cell multiple times 🤔)\r\n\r\nthanks for looking into this 🙏 !'
 'I think the error is generated by the `pyarrow.json.read()` option: `read_options=paj.ReadOptions(block_size=block_size)`...\r\ncc: @lhoestq '
 ""The code works fine on my side.\r\nNot sure what's going on here :/\r\n\r\nI remember we did a few changes in the JSON loader in #2638 , did you do an update `datasets` when debugging this ?\r\n""
 'OK after upgrading `datasets` to v1.12.1 the issue seems to have gone away. Closing this now :)'
 'Oops, I spoke too soon 😓 \r\n\r\nAfter deleting the cache and trying the above code snippet again I am hitting the same error. You can also reproduce it in the Colab notebook I linked to in the issue description. ']","## Describe the bug
I have created a [dataset](https://huggingface.co/datasets/lewtun/github-issues-test) of GitHub issues in line-separated JSON format and am finding that I cannot load it with the `json` loading script (see stack trace below).

Curiously, there is no problem loading the dataset with `pandas` which suggests some incorrect type inference is being made on the `datasets` side. For example, the stack trace indicates that some URL fields are being parsed as timestamps.

You can find a Colab notebook which reproduces the error [here](https://colab.research.google.com/drive/1YUCM0j1vx5ZrouQbYSzal6RwB4-Aoh4o?usp=sharing).

**Edit:** If one repeatedly tries to load the dataset, it _eventually_ works but I think it would still be good to understand why it fails in the first place :)

## Steps to reproduce the bug
```python
from datasets import load_dataset
from huggingface_hub import hf_hub_url
import pandas as pd

# returns https://huggingface.co/datasets/lewtun/github-issues-test/resolve/main/issues-datasets.jsonl
data_files = hf_hub_url(repo_id=""lewtun/github-issues-test"", filename=""issues-datasets.jsonl"", repo_type=""dataset"")
# throws ArrowNotImplementedError
dset = load_dataset(""json"", data_files=data_files, split=""test"")
# no problem with pandas ...
df = pd.read_json(data_files, orient=""records"", lines=True)
df.head()
```

## Expected results
I can load any line-separated JSON file, similar to `pandas`.

## Actual results
```
---------------------------------------------------------------------------
ArrowNotImplementedError                  Traceback (most recent call last)
<ipython-input-7-5b8e82b6c3a2> in <module>()
----> 1 dset = load_dataset(""json"", data_files=data_files, split=""test"")

9 frames
/usr/local/lib/python3.7/dist-packages/pyarrow/error.pxi in pyarrow.lib.check_status()

ArrowNotImplementedError: JSON conversion to struct<url: timestamp[s], html_url: timestamp[s], labels_url: timestamp[s], id: int64, node_id: timestamp[s], number: int64, title: timestamp[s], description: timestamp[s], creator: struct<login: timestamp[s], id: int64, node_id: timestamp[s], avatar_url: timestamp[s], gravatar_id: timestamp[s], url: timestamp[s], html_url: timestamp[s], followers_url: timestamp[s], following_url: timestamp[s], gists_url: timestamp[s], starred_url: timestamp[s], subscriptions_url: timestamp[s], organizations_url: timestamp[s], repos_url: timestamp[s], events_url: timestamp[s], received_events_url: timestamp[s], type: timestamp[s], site_admin: bool>, open_issues: int64, closed_issues: int64, state: timestamp[s], created_at: timestamp[s], updated_at: timestamp[s], due_on: timestamp[s], closed_at: timestamp[s]> is not supported
```

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: 1.11.0
- Platform: Linux-5.4.104+-x86_64-with-Ubuntu-18.04-bionic
- Python version: 3.7.11
- PyArrow version: 3.0.0
"
https://github.com/huggingface/datasets/issues/2788,How to sample every file in a list of files making up a split in a dataset when loading?,"['Hi ! This is not possible just with `load_dataset`.\r\n\r\nYou can do something like this instead:\r\n```python\r\nseed=42\r\ndata_files_dict = {\r\n    ""train"": [train_file1, train_file2],\r\n    ""test"": [test_file1, test_file2],\r\n    ""val"": [val_file1, val_file2]\r\n}\r\ndataset = datasets.load_dataset(\r\n    ""csv"",\r\n    data_files=data_files_dict,\r\n).shuffle(seed=seed)\r\n\r\nsample_dataset = {splitname: split.select(range(8)) for splitname, split in dataset.items()}\r\n```\r\n\r\nAnother alternative is loading each file separately with `split=""train[:8]""` and then use `concatenate_datasets` to merge the sample of each file.']","I am loading a dataset with multiple train, test, and validation files like this:

```
data_files_dict = {
    ""train"": [train_file1, train_file2],
    ""test"": [test_file1, test_file2],
    ""val"": [val_file1, val_file2]
}
dataset = datasets.load_dataset(
    ""csv"",
    data_files=data_files_dict,
    split=['train[:8]', 'test[:8]', 'val[:8]']
)

```

However, this only selects the first 8 rows from train_file1, test_file1, val_file1, since they are the first files in the lists.

I'm trying to formulate a split argument that can sample from each file specified in my list of files that make up each split.

Is this type of splitting supported? If so, how can I do it?"
https://github.com/huggingface/datasets/issues/2787,ConnectionError: Couldn't reach https://raw.githubusercontent.com,"['the bug code locate in ：\r\n    if data_args.task_name is not None:\r\n        # Downloading and loading a dataset from the hub.\r\n        datasets = load_dataset(""glue"", data_args.task_name, cache_dir=model_args.cache_dir)'
 'Hi @jinec,\r\n\r\nFrom time to time we get this kind of `ConnectionError` coming from the github.com website: https://raw.githubusercontent.com\r\n\r\nNormally, it should work if you wait a little and then retry.\r\n\r\nCould you please confirm if the problem persists?'
 'cannot connect，even by Web browser，please check that  there is some  problems。'
 'I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem...'
 '> I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem...\r\n\r\nI can not access https://raw.githubusercontent.com/huggingface/datasets either,   I am in China'
 'Finally i can access it, by the superfast software. Thanks']","Hello,
I am trying to run run_glue.py and it gives me this error -

Traceback (most recent call last):
  File ""E:/BERT/pytorch_hugging/transformers/examples/pytorch/text-classification/run_glue.py"", line 546, in <module>
    main()
  File ""E:/BERT/pytorch_hugging/transformers/examples/pytorch/text-classification/run_glue.py"", line 250, in main
    datasets = load_dataset(""glue"", data_args.task_name, cache_dir=model_args.cache_dir)
  File ""C:\install\Anaconda3\envs\huggingface\lib\site-packages\datasets\load.py"", line 718, in load_dataset
    use_auth_token=use_auth_token,
  File ""C:\install\Anaconda3\envs\huggingface\lib\site-packages\datasets\load.py"", line 320, in prepare_module
    local_path = cached_path(file_path, download_config=download_config)
  File ""C:\install\Anaconda3\envs\huggingface\lib\site-packages\datasets\utils\file_utils.py"", line 291, in cached_path
    use_auth_token=download_config.use_auth_token,
  File ""C:\install\Anaconda3\envs\huggingface\lib\site-packages\datasets\utils\file_utils.py"", line 623, in get_from_cache
    raise ConnectionError(""Couldn't reach {}"".format(url))
ConnectionError: Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py

Trying to do python run_glue.py  --model_name_or_path
bert-base-cased
--task_name
mrpc
--do_train
--do_eval
--max_seq_length
128
--per_device_train_batch_size
32
--learning_rate
2e-5
--num_train_epochs
3
--output_dir
./tmp/mrpc/

Is this something on my end? From what I can tell, this was re-fixeded by @fullyz a few months ago.
Thank you!
"
https://github.com/huggingface/datasets/issues/2775,`generate_random_fingerprint()` deterministic with 🤗Transformers' `set_seed()`,"['I dug into what I believe is the root of this issue and added a repro in my comment. If this is better addressed as a cross-team issue, let me know and I can open an issue in the Transformers repo'
 ""Hi !\r\n\r\nIMO we shouldn't try to modify `set_seed` from transformers but maybe make `datasets` have its own RNG just to generate random fingerprints.\r\n\r\nAny opinion on this @LysandreJik ?""
 'Yes, this sounds good @lhoestq ']","## Describe the bug

**Update:** I dug into this to try to reproduce the underlying issue, and I believe it's that `set_seed()` from the `transformers` library makes the ""random"" fingerprint identical each time. I believe this is still a bug, because `datasets` is used exactly this way in `transformers` after `set_seed()` has been called, and I think that using `set_seed()` is a standard procedure to aid reproducibility. I've added more details to reproduce this below.

Hi there! I'm using my own local dataset and custom preprocessing function. My preprocessing function seems to be unpickle-able, perhaps because it is from a closure (will debug this separately). I get this warning, which is expected:

https://github.com/huggingface/datasets/blob/450b9174765374111e5c6daab0ed294bc3d9b639/src/datasets/fingerprint.py#L260-L265

However, what's not expected is that the `datasets` actually _does_ seem to cache and reuse this dataset between runs! After that line, the next thing that's logged looks like:

```text
 Loading cached processed dataset at /home/xxx/.cache/huggingface/datasets/csv/default-xxx/0.0.0/xxx/cache-xxx.arrow
```

The path is exactly the same each run (e.g., last 26 runs).

This becomes a problem because I'll pass in the `--max_eval_samples` flag to the HuggingFace example script I'm running off of ([run_swag.py](https://github.com/huggingface/transformers/blob/master/examples/pytorch/multiple-choice/run_swag.py)).  The fact that the cached dataset is reused means this flag gets ignored. I'll try to load 100 examples, and it will load the full cached 1,000,000.

I think that

https://github.com/huggingface/datasets/blob/450b9174765374111e5c6daab0ed294bc3d9b639/src/datasets/fingerprint.py#L248

... is actually consistent because randomness is being controlled in HuggingFace/Transformers for reproducibility. I've added a demo of this below.

## Steps to reproduce the bug

```python
# Contents of print_fingerprint.py
from transformers import set_seed
from datasets.fingerprint import generate_random_fingerprint
set_seed(42)
print(generate_random_fingerprint())
```

```bash
for i in {0..10}; do
    python print_fingerprint.py
done

1c80317fa3b1799d
1c80317fa3b1799d
1c80317fa3b1799d
1c80317fa3b1799d
1c80317fa3b1799d
1c80317fa3b1799d
1c80317fa3b1799d
1c80317fa3b1799d
1c80317fa3b1799d
1c80317fa3b1799d
1c80317fa3b1799d
```

## Expected results
After the ""random hash"" warning is emitted, a random hash is generated, and no outdated cached datasets are reused.

## Actual results
After the ""random hash"" warning is emitted, an identical hash is generated each time, and an outdated cached dataset is reused each run.

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->

- `datasets` version: 1.9.0
- Platform: Linux-5.8.0-1038-gcp-x86_64-with-glibc2.31
- Python version: 3.9.6
- PyArrow version: 4.0.1"
https://github.com/huggingface/datasets/issues/2768,`ArrowInvalid: Added column's length must match table's length.` after using `select`,"['Hi,\r\n\r\nthe `select` method creates an indices mapping and doesn\'t modify the underlying PyArrow table by default for better performance. To modify the underlying table after the `select` call, call `flatten_indices` on the dataset object as follows:\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset(""tweets_hate_speech_detection"")[\'train\'].select(range(128))\r\nds = ds.flatten_indices()\r\nds = ds.add_column(\'ones\', [1]*128)\r\n```'
 'Thanks for the question @lvwerra. And thanks for the answer @mariosasko. ^^']","## Describe the bug
I would like to add a column to a downsampled dataset. However I get an error message saying the length don't match with the length of the unsampled dataset indicated. I suspect that the dataset size is not updated when calling `select`.

## Steps to reproduce the bug
```python
from datasets import load_dataset

ds = load_dataset(""tweets_hate_speech_detection"")['train'].select(range(128))
ds = ds.add_column('ones', [1]*128)
```

## Expected results
I would expect a new column named `ones` filled with `1`. When I check the length of `ds` it says `128`. Interestingly, it works when calling `ds = ds.map(lambda x: x)` before adding the column.

## Actual results
Specify the actual results or traceback.
```python
---------------------------------------------------------------------------
ArrowInvalid                              Traceback (most recent call last)
/var/folders/l4/2905jygx4tx5jv8_kn03vxsw0000gn/T/ipykernel_6301/868709636.py in <module>
      1 from datasets import load_dataset
      2 ds = load_dataset(""tweets_hate_speech_detection"")['train'].select(range(128))
----> 3 ds = ds.add_column('ones', [0]*128)

~/git/semantic-clustering/env/lib/python3.8/site-packages/datasets/arrow_dataset.py in wrapper(*args, **kwargs)
    183         }
    184         # apply actual function
--> 185         out: Union[""Dataset"", ""DatasetDict""] = func(self, *args, **kwargs)
    186         datasets: List[""Dataset""] = list(out.values()) if isinstance(out, dict) else [out]
    187         # re-apply format to the output

~/git/semantic-clustering/env/lib/python3.8/site-packages/datasets/fingerprint.py in wrapper(*args, **kwargs)
    395             # Call actual function
    396 
--> 397             out = func(self, *args, **kwargs)
    398 
    399             # Update fingerprint of in-place transforms + update in-place history of transforms

~/git/semantic-clustering/env/lib/python3.8/site-packages/datasets/arrow_dataset.py in add_column(self, name, column, new_fingerprint)
   2965         column_table = InMemoryTable.from_pydict({name: column})
   2966         # Concatenate tables horizontally
-> 2967         table = ConcatenationTable.from_tables([self._data, column_table], axis=1)
   2968         # Update features
   2969         info = self.info.copy()

~/git/semantic-clustering/env/lib/python3.8/site-packages/datasets/table.py in from_tables(cls, tables, axis)
    715             table_blocks = to_blocks(table)
    716             blocks = _extend_blocks(blocks, table_blocks, axis=axis)
--> 717         return cls.from_blocks(blocks)
    718 
    719     @property

~/git/semantic-clustering/env/lib/python3.8/site-packages/datasets/table.py in from_blocks(cls, blocks)
    663             return cls(table, blocks)
    664         else:
--> 665             table = cls._concat_blocks_horizontally_and_vertically(blocks)
    666             return cls(table, blocks)
    667 

~/git/semantic-clustering/env/lib/python3.8/site-packages/datasets/table.py in _concat_blocks_horizontally_and_vertically(cls, blocks)
    623             if not tables:
    624                 continue
--> 625             pa_table_horizontally_concatenated = cls._concat_blocks(tables, axis=1)
    626             pa_tables_to_concat_vertically.append(pa_table_horizontally_concatenated)
    627         return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)

~/git/semantic-clustering/env/lib/python3.8/site-packages/datasets/table.py in _concat_blocks(blocks, axis)
    612                 else:
    613                     for name, col in zip(table.column_names, table.columns):
--> 614                         pa_table = pa_table.append_column(name, col)
    615             return pa_table
    616         else:

~/git/semantic-clustering/env/lib/python3.8/site-packages/pyarrow/table.pxi in pyarrow.lib.Table.append_column()

~/git/semantic-clustering/env/lib/python3.8/site-packages/pyarrow/table.pxi in pyarrow.lib.Table.add_column()

~/git/semantic-clustering/env/lib/python3.8/site-packages/pyarrow/error.pxi in pyarrow.lib.pyarrow_internal_check_status()

~/git/semantic-clustering/env/lib/python3.8/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()

ArrowInvalid: Added column's length must match table's length. Expected length 31962 but got length 128
```

## Environment info
- `datasets` version: 1.11.0
- Platform: macOS-10.16-x86_64-i386-64bit
- Python version: 3.8.5
- PyArrow version: 5.0.0
"
https://github.com/huggingface/datasets/issues/2767,equal operation to perform unbatch for huggingface datasets ,"['Hi @lhoestq \r\nMaybe this is clearer to explain like this, currently map function, map one example to ""one"" modified one, lets assume we want to map one example to ""multiple"" examples, in which we do not know in advance how many examples they would be per each entry. I greatly appreciate telling me how I can handle this operation, thanks a lot'
 'Hi,\r\nthis is also my question on how to perform similar operation as ""unbatch"" in tensorflow in great huggingface dataset library. \r\nthanks.'
 'Hi,\r\n\r\n`Dataset.map` in the batched mode allows you to map a single row to multiple rows. So to perform ""unbatch"", you can do the following:\r\n```python\r\nimport collections\r\n\r\ndef unbatch(batch):\r\n    new_batch = collections.defaultdict(list)\r\n    keys = batch.keys()\r\n    for values in zip(*batch.values()):\r\n        ex = {k: v for k, v in zip(keys, values)}\r\n        inputs = f""record query: {ex[\'query\']} entities: {\', \'.join(ex[\'entities\'])} passage: {ex[\'passage\']}""\r\n        new_batch[""inputs""].extend([inputs] * len(ex[""answers""]))\r\n        new_batch[""targets""].extend(ex[""answers""])\r\n    return new_batch\r\n\r\ndset = dset.map(unbatch, batched=True, remove_columns=dset.column_names)\r\n```'
 'Dear @mariosasko \r\nFirst, thank you very much for coming back to me on this, I appreciate it a lot. I tried this solution,  I am getting errors, do you mind\r\ngiving me one test example to be able to run your code, to understand better the format of the inputs to your function?\r\nin this function https://github.com/google-research/text-to-text-transfer-transformer/blob/3c58859b8fe72c2dbca6a43bc775aa510ba7e706/t5/data/preprocessors.py#L952 they copy each example to the number of ""answers"", do you mean one should not do the copying part and use directly your function? \r\n\r\n\r\nthank you very much for your help and time.'
 'Hi @mariosasko \r\nI think finally I got this, I think you mean to do things in one step, here is the full example for completeness:\r\n\r\n```\r\ndef unbatch(batch):\r\n    new_batch = collections.defaultdict(list)\r\n    keys = batch.keys()\r\n    for values in zip(*batch.values()):\r\n        ex = {k: v for k, v in zip(keys, values)}\r\n        # updates the passage.\r\n        passage = ex[\'passage\']\r\n        passage = re.sub(r\'(\\.|\\?|\\!|\\""|\\\')\\n@highlight\\n\', r\'\\1 \', passage)\r\n        passage = re.sub(r\'\\n@highlight\\n\', \'. \', passage)\r\n        inputs = f""record query: {ex[\'query\']} entities: {\', \'.join(ex[\'entities\'])} passage: {passage}""\r\n        # duplicates the samples based on  number of answers.\r\n        num_answers = len(ex[""answers""])\r\n        num_duplicates = np.maximum(1, num_answers)\r\n        new_batch[""inputs""].extend([inputs] * num_duplicates) #len(ex[""answers""]))\r\n        new_batch[""targets""].extend(ex[""answers""] if num_answers > 0 else [""<unk>""])\r\n    return new_batch\r\n\r\ndata = datasets.load_dataset(\'super_glue\', \'record\', split=""train"", script_version=""master"")\r\ndata = data.map(unbatch, batched=True, remove_columns=data.column_names)\r\n```\r\n\r\nThanks a lot again, this was a super great way to do it.']","Hi
I need to use ""unbatch"" operation in tensorflow on a huggingface dataset, I could not find this operation, could you kindly direct me how I can do it, here is the problem I am trying to solve:

I am considering ""record"" dataset in SuperGlue and I need to replicate each entery of the dataset for each answer, to make it similar to what T5 originally did:

https://github.com/google-research/text-to-text-transfer-transformer/blob/3c58859b8fe72c2dbca6a43bc775aa510ba7e706/t5/data/preprocessors.py#L925

Here please find an example:

  For example, a typical example from ReCoRD might look like
  {
      'passsage': 'This is the passage.',
      'query': 'A @placeholder is a bird.',
      'entities': ['penguin', 'potato', 'pigeon'],
      'answers': ['penguin', 'pigeon'],
  }
  and I need a prosessor which would turn this example into the following two examples:
  {
      'inputs': 'record query: A @placeholder is a bird. entities: penguin, '
                'potato, pigeon passage: This is the passage.',
      'targets': 'penguin',
  }
  and
  {
      'inputs': 'record query: A @placeholder is a bird. entities: penguin, '
                'potato, pigeon passage: This is the passage.',
      'targets': 'pigeon',
  }


For doing this, one need unbatch, as each entry can map to multiple samples depending on the number of answers, I am not sure how to perform this operation with  huggingface datasets library and greatly appreciate your help

@lhoestq 

Thank you very much.
"
https://github.com/huggingface/datasets/issues/2765,BERTScore Error,"['Hi,\r\n\r\nThe `use_fast_tokenizer` argument has been recently added to the bert-score lib. I\'ve opened a PR with the fix. In the meantime, you can try to downgrade the version of bert-score with the following command to make the code work:\r\n```\r\npip uninstall bert-score\r\npip install ""bert-score<0.3.10""\r\n```']","## Describe the bug
A clear and concise description of what the bug is.

## Steps to reproduce the bug
```python
predictions = [""hello there"", ""general kenobi""]
references = [""hello there"", ""general kenobi""]
bert = load_metric('bertscore')
bert.compute(predictions=predictions, references=references,lang='en')
```

# Bug
`TypeError: get_hash() missing 1 required positional argument: 'use_fast_tokenizer'`


## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version:
- Platform: Colab 
- Python version:
- PyArrow version:
"
https://github.com/huggingface/datasets/issues/2763,English wikipedia datasets is not clean,['Hi ! Certain users might need these data (for training or simply to explore/index the dataset).\r\n\r\nFeel free to implement a map function that gets rid of these paragraphs and process the wikipedia dataset with it before training'],"## Describe the bug
Wikipedia english dumps contain many wikipedia paragraphs like ""References"", ""Category:"" and ""See Also"" that should not be used for training.

## Steps to reproduce the bug
```python
# Sample code to reproduce the bug
from datasets import load_dataset
w = load_dataset('wikipedia', '20200501.en')
print(w['train'][0]['text'])
```

> 'Yangliuqing () is a market town in Xiqing District, in the western suburbs of Tianjin, People\'s Republic of China. Despite its relatively small size, it has been named since 2006 in the ""famous historical and cultural market towns in China"".\n\nIt is best known in China for creating nianhua or Yangliuqing nianhua. For more than 400 years, Yangliuqing has in effect specialised in the creation of these woodcuts for the New Year.  wood block prints using vivid colourschemes to portray traditional scenes of children\'s games often interwoven with auspiciouse objects.\n\n, it had 27 residential communities () and 25 villages under its administration.\n\nShi Family Grand Courtyard\n\nShi Family Grand Courtyard (Tiānjīn Shí Jiā Dà Yuàn, 天津石家大院) is situated in Yangliuqing Town of Xiqing District, which is the former residence of wealthy merchant Shi Yuanshi - the 4th son of Shi Wancheng, one of the eight great masters in Tianjin. First built in 1875, it covers over 6,000 square meters, including large and small yards and over 200 folk houses, a theater and over 275 rooms that served as apartments and places of business and worship for this powerful family. Shifu Garden, which finished its expansion in October 2003, covers 1,200 square meters, incorporates the elegance of imperial garden and delicacy of south garden. Now the courtyard of Shi family covers about 10,000 square meters, which is called the first mansion in North China. Now it serves as the folk custom museum in Yangliuqing, which has a large collection of folk custom museum in Yanliuqing, which has a large collection of folk art pieces like Yanliuqing New Year pictures, brick sculpture.\n\nShi\'s ancestor came from Dong\'e County in Shandong Province, engaged in water transport of grain. As the wealth gradually accumulated, the Shi Family moved to Yangliuqing and bought large tracts of land and set up their residence. Shi Yuanshi came from the fourth generation of the family, who was a successful businessman and a good household manager, and the residence was thus enlarged for several times until it acquired the present scale. It is believed to be the first mansion in the west of Tianjin.\n\nThe residence is symmetric based on the axis formed by a passageway in the middle, on which there are four archways. On the east side of the courtyard, there are traditional single-story houses with rows of rooms around the four sides, which was once the living area for the Shi Family. The rooms on north side were the accountants\' office. On the west are the major constructions including the family hall for worshipping Buddha, theater and the south reception room. On both sides of the residence are side yard rooms for maids and servants.\n\nToday, the Shi mansion, located in the township of Yangliuqing to the west of central Tianjin, stands as a surprisingly well-preserved monument to China\'s pre-revolution mercantile spirit. It also serves as an on-location shoot for many of China\'s popular historical dramas. Many of the rooms feature period furniture, paintings and calligraphy, and the extensive Shifu Garden.\n\nPart of the complex has been turned into the Yangliuqing Museum, which includes displays focused on symbolic aspects of the courtyards\'  construction, local folk art and customs, and traditional period furnishings and crafts.\n\n**See also \n\nList of township-level divisions of Tianjin\n\nReferences \n\n http://arts.cultural-china.com/en/65Arts4795.html\n\nCategory:Towns in Tianjin'**

## Expected results
I expect no junk in the data.

## Actual results
Specify the actual results or traceback.

## Environment info
- `datasets` version: 1.10.2
- Platform: macOS-10.15.7-x86_64-i386-64bit
- Python version: 3.8.5
- PyArrow version: 3.0.0
"
https://github.com/huggingface/datasets/issues/2762,Add RVL-CDIP dataset,['cc @nateraw '],"## Adding a Dataset
- **Name:** RVL-CDIP
- **Description:** The RVL-CDIP (Ryerson Vision Lab Complex Document Information Processing) dataset consists of 400,000 grayscale images in 16 classes, with 25,000 images per class. There are 320,000 training images, 40,000 validation images, and 40,000 test images. The images are sized so their largest dimension does not exceed 1000 pixels.
- **Paper:** https://www.cs.cmu.edu/~aharley/icdar15/
- **Data:** https://www.cs.cmu.edu/~aharley/rvl-cdip/
- **Motivation:** I'm currently adding LayoutLMv2 and LayoutXLM to HuggingFace Transformers. LayoutLM (v1) already exists in the library. This dataset has a large value for document image classification (i.e. classifying scanned documents). LayoutLM models obtain SOTA on this dataset, so would be great to directly use it in notebooks.

Instructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).
"
https://github.com/huggingface/datasets/issues/2761,Error loading C4 realnewslike dataset,"[""Hi @danshirron, \r\n`c4` was updated few days back by @lhoestq. The new configs are `['en', 'en.noclean', 'en.realnewslike', 'en.webtextlike'].` You'll need to remove any older version of this dataset you previously downloaded and then run `load_dataset` again with new configuration.""
 '@bhavitvyamalik @lhoestq , just tried the above and got:\r\n>>> a=datasets.load_dataset(\'c4\',\'en.realnewslike\')\r\nDownloading: 3.29kB [00:00, 1.66MB/s]                                                                                                                                                                                                                                          \r\nDownloading: 2.40MB [00:00, 12.6MB/s]                                                                                                                                                                                                                                          \r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/dshirron/.local/lib/python3.8/site-packages/datasets/load.py"", line 819, in load_dataset\r\n    builder_instance = load_dataset_builder(\r\n  File ""/home/dshirron/.local/lib/python3.8/site-packages/datasets/load.py"", line 701, in load_dataset_builder\r\n    builder_instance: DatasetBuilder = builder_cls(\r\n  File ""/home/dshirron/.local/lib/python3.8/site-packages/datasets/builder.py"", line 1049, in __init__\r\n    super(GeneratorBasedBuilder, self).__init__(*args, **kwargs)\r\n  File ""/home/dshirron/.local/lib/python3.8/site-packages/datasets/builder.py"", line 268, in __init__\r\n    self.config, self.config_id = self._create_builder_config(\r\n  File ""/home/dshirron/.local/lib/python3.8/site-packages/datasets/builder.py"", line 360, in _create_builder_config\r\n    raise ValueError(\r\nValueError: BuilderConfig en.realnewslike not found. Available: [\'en\', \'realnewslike\', \'en.noblocklist\', \'en.noclean\']\r\n>>> \r\n\r\ndatasets version is 1.11.0\r\n'
 ""I think I had an older version of datasets installed and that's why I commented the old configurations in my last comment, my bad! I re-checked and updated it to latest version (`datasets==1.11.0`) and it's showing `available configs: ['en', 'realnewslike', 'en.noblocklist', 'en.noclean']`. \r\n\r\nI tried `raw_datasets = load_dataset('c4', 'realnewslike')` and the download started. Make sure you don't have any old copy of this dataset and you download it fresh using the latest version of datasets. Sorry for the mix up!""
 'It works. I probably had some issue with the cache. after cleaning it im able to download the dataset. Thanks']","## Describe the bug
Error loading C4 realnewslike dataset. Validation part mismatch

## Steps to reproduce the bug
```python
 raw_datasets = load_dataset('c4', 'realnewslike', cache_dir=model_args.cache_dir)
## Expected results
success on data loading
## Actual results
Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15.3M/15.3M [00:00<00:00, 28.1MB/s]Traceback (most recent call last):                                                                                                                                                                                                                                             
  File ""run_mlm_tf.py"", line 794, in <module>                                                                                                                                                                                                                                  
    main()                                                                                                                                                                                                                                                                     
  File ""run_mlm_tf.py"", line 425, in main                                                                                                                                                                                                                                      
    raw_datasets = load_dataset(data_args.dataset_name, data_args.dataset_config_name, cache_dir=model_args.cache_dir)                                                                                                                                                           File ""/home/dshirron/.local/lib/python3.8/site-packages/datasets/load.py"", line 843, in load_dataset                                                                                                                                                                         
    builder_instance.download_and_prepare(                                                                                                                                                                                                                                     
  File ""/home/dshirron/.local/lib/python3.8/site-packages/datasets/builder.py"", line 608, in download_and_prepare                                                                                                                                                              
    self._download_and_prepare(                                                                                                                                                                                                                                                
  File ""/home/dshirron/.local/lib/python3.8/site-packages/datasets/builder.py"", line 698, in _download_and_prepare                                                                                                                                                                 verify_splits(self.info.splits, split_dict)                                                                                                                                                                                                                                  File ""/home/dshirron/.local/lib/python3.8/site-packages/datasets/utils/info_utils.py"", line 74, in verify_splits                                                                                                                                                             
    raise NonMatchingSplitsSizesError(str(bad_splits))                                                                                                                                                                                                                         
datasets.utils.info_utils.NonMatchingSplitsSizesError: [{'expected': SplitInfo(name='validation', num_bytes=38165657946, num_examples=13799838, dataset_name='c4'), 'recorded': SplitInfo(name='validation', num_bytes=37875873, num_examples=13863, dataset_name='c4')}] 

## Environment info
- `datasets` version: 1.10.2
- Platform: Linux-5.4.0-58-generic-x86_64-with-glibc2.29
- Python version: 3.8.10
- PyArrow version: 4.0.1"
https://github.com/huggingface/datasets/issues/2759,the meteor metric seems not consist with the official version,"['the issue is caused by the differences between varied meteor versions:\r\nmeteor1.0 is for https://aclanthology.org/W07-0734.pdf\r\nmeteor1.5 is for https://aclanthology.org/W14-3348.pdf\r\n\r\nhere is a very similar issue in NLTK\r\nhttps://github.com/nltk/nltk/issues/2655'
 'Hi @jianguda, thanks for reporting.\r\n\r\nCurrently, at 🤗 `datasets` we are using METEOR 1.0 (indeed using NLTK: `from nltk.translate import meteor_score`): See the [citation here](https://github.com/huggingface/datasets/blob/master/metrics/meteor/meteor.py#L23-L35).\r\n\r\nIf there is some open source implementation of METEOR 1.5, that could be an interesting contribution! 😉 ']","## Describe the bug
The computed meteor score seems strange because the value is very different from the scores computed by other tools. For example, I use the meteor score computed by [NLGeval](https://github.com/Maluuba/nlg-eval) as the reference (which reuses the official jar file for the computation)

## Steps to reproduce the bug
```python
from datasets import load_metric
from nlgeval import NLGEval, compute_individual_metrics

meteor = load_metric('meteor')
predictions = [""It is a guide to action which ensures that the military always obeys the commands of the party""]
references = [""It is a guide to action that ensures that the military will forever heed Party commands""]
results = meteor.compute(predictions=predictions, references=references)
# print the actual result
print(round(results[""meteor""], 4))
metrics_dict = compute_individual_metrics(references, predictions[0])
# print the expected result
print(round(metrics_dict[""METEOR""], 4))
```
By the way, you need to install the `nlg-eval` library first. Please check the installation guide [here](https://github.com/Maluuba/nlg-eval#setup), thanks!

## Expected results
`0.4474`

## Actual results
`0.7398`

## Environment info
- `datasets` version: 1.10.2
- Platform: macOS-10.16-x86_64-i386-64bit
- Python version: 3.8.5
- PyArrow version: 4.0.1
"
https://github.com/huggingface/datasets/issues/2757,Unexpected type after `concatenate_datasets`,"['Hi @JulesBelveze, thanks for your question.\r\n\r\nNote that 🤗  `datasets` internally store their data in Apache Arrow format.\r\n\r\nHowever, when accessing dataset columns, by default they are returned as native Python objects (lists in this case).\r\n\r\nIf you would like their columns to be returned in a more suitable format for your use case (torch arrays), you can use the method `set_format()`:\r\n```python\r\nconcat_dataset.set_format(type=""torch"")\r\n```\r\n\r\nYou have detailed information in our docs:\r\n- [Using a Dataset with PyTorch/Tensorflow](https://huggingface.co/docs/datasets/torch_tensorflow.html)\r\n- [Dataset.set_format()](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.set_format)'
 'Thanks @albertvillanova it indeed\xa0did the job 😃 \r\nThanks for your answer!']","## Describe the bug
I am trying to concatenate two `Dataset` using `concatenate_datasets` but it turns out that after concatenation the features are casted from `torch.Tensor` to `list`. 
It then leads to a weird tensors when trying to convert it to a `DataLoader`. However, if I use each `Dataset` separately everything behave as expected.

## Steps to reproduce the bug
```python
>>> featurized_teacher
Dataset({
    features: ['t_labels', 't_input_ids', 't_token_type_ids', 't_attention_mask'],
    num_rows: 502
})
>>> for f in featurized_teacher.features:
     print(featurized_teacher[f].shape)
torch.Size([502])
torch.Size([502, 300])
torch.Size([502, 300])
torch.Size([502, 300])

>>> featurized_student
Dataset({
    features: ['s_features', 's_labels'],
    num_rows: 502
})
>>> for f in featurized_student.features:
     print(featurized_student[f].shape)
torch.Size([502, 64])
torch.Size([502])
```
The shapes seem alright to me. Then the results after concatenation are as follow:
```python
>>> concat_dataset = datasets.concatenate_datasets([featurized_student, featurized_teacher], axis=1)
>>> type(concat_dataset[""t_labels""])
<class 'list'>
```
One would expect to obtain the same type as the one before concatenation.

Am I doing something wrong here? Any idea on how to fix this unexpected behavior?

## Environment info
- `datasets` version: 1.9.0
- Platform: macOS-10.14.6-x86_64-i386-64bit
- Python version: 3.9.5
- PyArrow version: 3.0.0
"
https://github.com/huggingface/datasets/issues/2750,Second concatenation of datasets produces errors,"['@albertvillanova '
 ""Hi @Aktsvigun, thanks for reporting.\r\n\r\nI'm investigating this.""
 'Hi @albertvillanova ,\r\nany update on this? Can I probably help in some way?'
 'Hi @Aktsvigun! We are planning to address this issue before our next release, in a couple of weeks at most. 😅 \r\n\r\nIn the meantime, if you would like to contribute, feel free to open a Pull Request. You are welcome. Here you can find more information: [How to contribute to Datasets?](CONTRIBUTING.md)']","Hi,

I am need to concatenate my dataset with others several times, and after I concatenate it for the second time, the features of features (e.g. tags names) are collapsed. This hinders, for instance, the usage of tokenize function with `data.map`.

```
from datasets import load_dataset, concatenate_datasets

data = load_dataset('trec')['train']
concatenated = concatenate_datasets([data, data])
concatenated_2 = concatenate_datasets([concatenated, concatenated])
print('True features of features:', concatenated.features)
print('\nProduced features of features:', concatenated_2.features)
```
outputs 

```
True features of features: {'label-coarse': ClassLabel(num_classes=6, names=['DESC', 'ENTY', 'ABBR', 'HUM', 'NUM', 'LOC'], names_file=None, id=None), 'label-fine': ClassLabel(num_classes=47, names=['manner', 'cremat', 'animal', 'exp', 'ind', 'gr', 'title', 'def', 'date', 'reason', 'event', 'state', 'desc', 'count', 'other', 'letter', 'religion', 'food', 'country', 'color', 'termeq', 'city', 'body', 'dismed', 'mount', 'money', 'product', 'period', 'substance', 'sport', 'plant', 'techmeth', 'volsize', 'instru', 'abb', 'speed', 'word', 'lang', 'perc', 'code', 'dist', 'temp', 'symbol', 'ord', 'veh', 'weight', 'currency'], names_file=None, id=None), 'text': Value(dtype='string', id=None)}

Produced features of features: {'label-coarse': Value(dtype='int64', id=None), 'label-fine': Value(dtype='int64', id=None), 'text': Value(dtype='string', id=None)}
```

I am using `datasets` v.1.11.0"
https://github.com/huggingface/datasets/issues/2749,Raise a proper exception when trying to stream a dataset that requires to manually download files,"['Hi @severo, thanks for reporting.\r\n\r\nAs discussed, datasets requiring manual download should be:\r\n- programmatically identifiable\r\n- properly handled with more clear error message when trying to load them with streaming\r\n\r\nIn relation with programmatically identifiability, note that for datasets requiring manual download, their builder have a property `manual_download_instructions` which is not None:\r\n```python\r\n# Dataset requiring manual download:\r\nbuilder.manual_download_instructions is not None\r\n```'
 'Thanks @albertvillanova ']","## Describe the bug

At least for 'reclor', 'telugu_books', 'turkish_movie_sentiment', 'ubuntu_dialogs_corpus', 'wikihow', trying to `load_dataset` in streaming mode raises a `TypeError` without any detail about why it fails.

## Steps to reproduce the bug

```python
from datasets import load_dataset
dataset = load_dataset(""reclor"", streaming=True)
```

## Expected results

Ideally: raise a specific exception, something like `ManualDownloadError`.

Or at least give the reason in the message, as when we load in normal mode:

```python
from datasets import load_dataset
dataset = load_dataset(""reclor"")
```

```
AssertionError: The dataset reclor with config default requires manual data.
 Please follow the manual download instructions:   to use ReClor you need to download it manually. Please go to its homepage (http://whyu.me/reclor/) fill the google
  form and you will receive a download link and a password to extract it.Please extract all files in one folder and use the path folder in datasets.load_dataset('reclor', data_dir='path/to/folder/folder_name')
  .
 Manual data can be loaded with `datasets.load_dataset(reclor, data_dir='<path/to/manual/data>')
```

## Actual results

```
TypeError: expected str, bytes or os.PathLike object, not NoneType
```

## Environment info

- `datasets` version: 1.11.0
- Platform: macOS-11.5-x86_64-i386-64bit
- Python version: 3.8.11
- PyArrow version: 4.0.1
"
https://github.com/huggingface/datasets/issues/2746,Cannot load `few-nerd` dataset,"['Hi @Mehrad0711,\r\n\r\nI\'m afraid there is no ""canonical"" Hugging Face dataset named ""few-nerd"".\r\n\r\nThere are 2 kinds of datasets hosted at the Hugging Face Hub:\r\n- canonical datasets (their identifier contains no slash ""/""): we, the Hugging Face team, supervise their implementation and we make sure they work correctly by means of our test suite\r\n- community datasets (their identifier contains a slash ""/"", where before the slash it is the username or the organization name): those datasets are uploaded to the Hub by the community, and we, the Hugging Face team, do not supervise them; it is the responsibility of the user/organization implementing them properly if they want them to be used by other users.\r\n\r\nIn this specific case, there is no ""canonical"" dataset named ""few-nerd"". On the other hand, there are two ""community"" datasets named ""few-nerd"":\r\n- [""nbroad/few-nerd""](https://huggingface.co/datasets/nbroad/few-nerd)\r\n- [""dfki-nlp/few-nerd""](https://huggingface.co/datasets/dfki-nlp/few-nerd)\r\n\r\nIf they were properly implemented, you should be able to load them this way:\r\n```python\r\n# ""nbroad/few-nerd"" community dataset\r\nds = load_dataset(""nbroad/few-nerd"", ""supervised"")\r\n\r\n# ""dfki-nlp/few-nerd"" community dataset\r\nds = load_dataset(""dfki-nlp/few-nerd"", ""supervised"")\r\n```\r\n\r\nHowever, they are not correctly implemented and both of them give errors:\r\n- ""nbroad/few-nerd"":\r\n  ```\r\n  TypeError: expected str, bytes or os.PathLike object, not dict\r\n  ```\r\n- ""dfki-nlp/few-nerd"":\r\n  ```\r\n  ConnectionError: Couldn\'t reach https://cloud.tsinghua.edu.cn/f/09265750ae6340429827/?dl=1\r\n  ```\r\n\r\nYou could try to contact their users/organizations to inform them about their bugs and ask them if they are planning to fix them. Alternatively you could try to implement your own script for this dataset.'
 'Thanks @albertvillanova for your detailed explanation! I will resort to my own scripts for now. ']","## Describe the bug

Cannot load `few-nerd` dataset.

## Steps to reproduce the bug
```python
from datasets import load_dataset
load_dataset('few-nerd', 'supervised')
```

## Actual results

Executing above code will give the following error:

```
Using the latest cached version of the module from /Users/Mehrad/.cache/huggingface/modules/datasets_modules/datasets/few-nerd/62464ace912a40a0f33a11a8310f9041c9dc3590ff2b3c77c14d83ca53cfec53 (last modified on Wed Jun  2 11:34:25 2021) since it couldn't be found locally at /Users/Mehrad/Documents/GitHub/genienlp/few-nerd/few-nerd.py, or remotely (FileNotFoundError).
Downloading and preparing dataset few_nerd/supervised (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /Users/Mehrad/.cache/huggingface/datasets/few_nerd/supervised/0.0.0/62464ace912a40a0f33a11a8310f9041c9dc3590ff2b3c77c14d83ca53cfec53...
Traceback (most recent call last):
  File ""/Users/Mehrad/opt/anaconda3/lib/python3.7/site-packages/datasets/builder.py"", line 693, in _download_and_prepare
    self._prepare_split(split_generator, **prepare_split_kwargs)
  File ""/Users/Mehrad/opt/anaconda3/lib/python3.7/site-packages/datasets/builder.py"", line 1107, in _prepare_split
    disable=bool(logging.get_verbosity() == logging.NOTSET),
  File ""/Users/Mehrad/opt/anaconda3/lib/python3.7/site-packages/tqdm/std.py"", line 1133, in __iter__
    for obj in iterable:
  File ""/Users/Mehrad/.cache/huggingface/modules/datasets_modules/datasets/few-nerd/62464ace912a40a0f33a11a8310f9041c9dc3590ff2b3c77c14d83ca53cfec53/few-nerd.py"", line 196, in _generate_examples
    with open(filepath, encoding=""utf-8"") as f:
FileNotFoundError: [Errno 2] No such file or directory: '/Users/Mehrad/.cache/huggingface/datasets/downloads/supervised/train.json'
```
The bug is probably in identifying and downloading the dataset. If I download the json splits directly from [link](https://github.com/nbroad1881/few-nerd/tree/main/uncompressed) and put them under the downloads directory, they will be processed into arrow format correctly. 

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: 1.11.0
- Python version: 3.8
- PyArrow version: 1.0.1
"
https://github.com/huggingface/datasets/issues/2743,Dataset JSON is incorrect,"['As discussed, the metadata JSON files must be regenerated because the keys were nor properly generated and they will not be read by the builder:\r\n> Indeed there is some problem/bug while reading the datasets_info.json file: there is a mismatch with the config.name keys in the file...\r\nIn the meanwhile, in order to be able to use the datasets_info.json file content, you can create the builder without passing the name :\r\n```\r\nIn [25]: builder = datasets.load_dataset_builder(""journalists_questions"")\r\nIn [26]: builder.info.splits\r\nOut[26]: {\'train\': SplitInfo(name=\'train\', num_bytes=342296, num_examples=10077, dataset_name=\'journalists_questions\')}\r\n```\r\n\r\nAfter regenerating the metadata JSON file for this dataset, I get the right key:\r\n```\r\n{""plain_text"": {""description"": ""The journalists_questions corpus (\r\n```'
 'Thanks!']","## Describe the bug

The JSON file generated for https://github.com/huggingface/datasets/blob/573f3d35081cee239d1b962878206e9abe6cde91/datasets/journalists_questions/journalists_questions.py is https://github.com/huggingface/datasets/blob/573f3d35081cee239d1b962878206e9abe6cde91/datasets/journalists_questions/dataset_infos.json.

The only config should be `plain_text`, but the first key in the JSON is `journalists_questions` (the dataset id) instead.

```json
{
  ""journalists_questions"": {
    ""description"": ""The journalists_questions corpus (version 1.0) is a collection of 10K human-written Arabic\ntweets manually labeled for question identification over Arabic tweets posted by journalists.\n"",
    ...
```

## Steps to reproduce the bug

Look at the files.

## Expected results

The first key should be `plain_text`:

```json
{
  ""plain_text"": {
    ""description"": ""The journalists_questions corpus (version 1.0) is a collection of 10K human-written Arabic\ntweets manually labeled for question identification over Arabic tweets posted by journalists.\n"",
    ...
```

## Actual results

```json
{
  ""journalists_questions"": {
    ""description"": ""The journalists_questions corpus (version 1.0) is a collection of 10K human-written Arabic\ntweets manually labeled for question identification over Arabic tweets posted by journalists.\n"",
    ...
```

"
https://github.com/huggingface/datasets/issues/2742,Improve detection of streamable file types,['maybe we should rather attempt to download a `Range` from the server and see if it works?'],"**Is your feature request related to a problem? Please describe.**

```python
from datasets import load_dataset_builder
from datasets.utils.streaming_download_manager import StreamingDownloadManager
builder = load_dataset_builder(""journalists_questions"", name=""plain_text"")
builder._split_generators(StreamingDownloadManager(base_path=builder.base_path))
```

raises

```
NotImplementedError: Extraction protocol for file at https://drive.google.com/uc?export=download&id=1CBrh-9OrSpKmPQBxTK_ji6mq6WTN_U9U is not implemented yet
```

But the file at https://drive.google.com/uc?export=download&id=1CBrh-9OrSpKmPQBxTK_ji6mq6WTN_U9U is a text file and it can be streamed:

```bash
curl --header ""Range: bytes=0-100"" -L https://drive.google.com/uc\?export\=download\&id\=1CBrh-9OrSpKmPQBxTK_ji6mq6WTN_U9U
506938088174940160      yes     1
302221719412830209      yes     1
289761704907268096      yes     1
513820885032378369      yes     %
```

Yet, it's wrongly categorized as a file type that cannot be streamed because the test is currently based on 1. the presence of a file extension at the end of the URL (here: no extension), and 2. the inclusion of this extension in a list of supported formats.

**Describe the solution you'd like**

In the case of an URL (instead of a local path), ask for the MIME type, and decide on that value? Note that it would not work in that case, because the value of `content_type` is `text/html; charset=UTF-8`.

**Describe alternatives you've considered**

Add a variable in the dataset script to set the data format by hand.
"
https://github.com/huggingface/datasets/issues/2737,SacreBLEU update,"['Hi @devrimcavusoglu, \r\nI tried your code with latest version of `datasets`and `sacrebleu==1.5.1` and it\'s running fine after changing one small thing:\r\n```\r\nsacrebleu = datasets.load_metric(\'sacrebleu\')\r\npredictions = [""It is a guide to action which ensures that the military always obeys the commands of the party""]\r\nreferences = [[""It is a guide to action that ensures that the military will forever heed Party commands""]]  # double brackets here should do the work\r\nresults = sacrebleu.compute(predictions=predictions, references=references)\r\nprint(results)\r\noutput: {\'score\': 41.180376356915765, \'counts\': [11, 8, 6, 4], \'totals\': [18, 17, 16, 15], \'precisions\': [61.111111111111114, 47.05882352941177, 37.5, 26.666666666666668], \'bp\': 1.0, \'sys_len\': 18, \'ref_len\': 16}\r\n```'
 ""@bhavitvyamalik hmm. I forgot double brackets, but still didn't work when used it with double brackets. It may be an isseu with platform (using win-10 currently), or versions. What is your platform and your version info for datasets, python, and sacrebleu ?""
 ""You can check that here, I've reproduced your code in [Google colab](https://colab.research.google.com/drive/1X90fHRgMLKczOVgVk7NDEw_ciZFDjaCM?usp=sharing). Looks like there was some issue in `sacrebleu` which was fixed later from what I've found [here](https://github.com/pytorch/fairseq/issues/2049#issuecomment-622367967). Upgrading `sacrebleu` to latest version should work.""
 ""It seems that next release of `sacrebleu` (v2.0.0) will break our `datasets` implementation to compute it. See my Google Colab: https://colab.research.google.com/drive/1SKmvvjQi6k_3OHsX5NPkZdiaJIfXyv9X?usp=sharing\r\n\r\nI'm reopening this Issue and making a Pull Request to fix it.""]","With the latest release of [sacrebleu](https://github.com/mjpost/sacrebleu), `datasets.metrics.sacrebleu` is broken, and getting error.

    AttributeError: module 'sacrebleu' has no attribute 'DEFAULT_TOKENIZER'

this happens since in new version of sacrebleu there is no `DEFAULT_TOKENIZER`, but sacrebleu.py tries to import it anyways. This can be fixed currently with fixing `sacrebleu==1.5.0`

## Steps to reproduce the bug
```python
sacrebleu= datasets.load_metric('sacrebleu')
predictions = [""It is a guide to action which ensures that the military always obeys the commands of the party""]
references = [""It is a guide to action that ensures that the military will forever heed Party commands""]
results = sacrebleu.compute(predictions=predictions, references=references)
print(results)
```

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: 1.11.0
- Platform: Windows-10-10.0.19041-SP0
- Python version: Python 3.8.0
- PyArrow version: 5.0.0
"
https://github.com/huggingface/datasets/issues/2736,Add Microsoft Building Footprints dataset,"[""Motivation: this can be a useful dataset for researchers working on climate change adaptation, urban studies, geography, etc. I'll see if I can figure out how to add it!""]","## Adding a Dataset
- **Name:** Microsoft Building Footprints
- **Description:** With the goal to increase the coverage of building footprint data available as open data for OpenStreetMap and humanitarian efforts, we have released millions of building footprints as open data available to download free of charge.
- **Paper:** *link to the dataset paper if available*
- **Data:** https://www.microsoft.com/en-us/maps/building-footprints
- **Motivation:** this can be a useful dataset for researchers working on climate change adaptation, urban studies, geography, etc.

Instructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).

Reported by: @sashavor"
https://github.com/huggingface/datasets/issues/2730,Update CommonVoice with new release,"['cc @patrickvonplaten?'
 'Does anybody know if there is a bundled link, which would allow direct data download instead of manual? \r\nSomething similar to: `https://voice-prod-bundler-ee1969a6ce8178826482b88e843c335139bd3fb4.s3.amazonaws.com/cv-corpus-6.1-2020-12-11/ab.tar.gz` ? cc @patil-suraj \r\n'
 'Also see: https://github.com/common-voice/common-voice-bundler/issues/15']","## Adding a Dataset
- **Name:** CommonVoice mid-2021 release
- **Description:** more data in CommonVoice: Languages that have increased the most by percentage are Thai (almost 20x growth, from 12 hours to 250 hours), Luganda (almost 9x growth, from 8 to 80), Esperanto (7x growth, from 100 to 840), and Tamil (almost 8x, from 24 to 220).
- **Paper:** https://discourse.mozilla.org/t/common-voice-2021-mid-year-dataset-release/83812
- **Data:** https://commonvoice.mozilla.org/en/datasets
- **Motivation:** More data and more varied. I think we just need to add configs in the existing dataset script.

Instructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).
"
https://github.com/huggingface/datasets/issues/2728,Concurrent use of same dataset (already downloaded),"['Launching simultaneous job relying on the same datasets try some writing issue. I guess it is unexpected since I only need to load some already downloaded file.'
 'If i have two jobs that use the same dataset. I got :\r\n\r\n\r\n  File ""compute_measures.py"", line 181, in <module>\r\n    train_loader, val_loader, test_loader = get_dataloader(args)\r\n  File ""/gpfsdswork/projects/rech/toto/intRAOcular/dataset_utils.py"", line 69, in get_dataloader\r\n    dataset_train = load_dataset(\'paws\', ""labeled_final"", split=\'train\', download_mode=""reuse_cache_if_exists"")\r\n  File ""/gpfslocalsup/pub/anaconda-py3/2020.02/envs/pytorch-gpu-1.7.0/lib/python3.7/site-packages/datasets/load.py"", line 748, in load_dataset\r\n    use_auth_token=use_auth_token,\r\n  File ""/gpfslocalsup/pub/anaconda-py3/2020.02/envs/pytorch-gpu-1.7.0/lib/python3.7/site-packages/datasets/builder.py"", line 582, in download_and_prepare\r\n    self._save_info()\r\n  File ""/gpfslocalsup/pub/anaconda-py3/2020.02/envs/pytorch-gpu-1.7.0/lib/python3.7/site-packages/datasets/builder.py"", line 690, in _save_info\r\n    self.info.write_to_directory(self._cache_dir)\r\n  File ""/gpfslocalsup/pub/anaconda-py3/2020.02/envs/pytorch-gpu-1.7.0/lib/python3.7/site-packages/datasets/info.py"", line 195, in write_to_directory\r\n    with open(os.path.join(dataset_info_dir, config.LICENSE_FILENAME), ""wb"") as f:\r\nFileNotFoundError: [Errno 2] No such file or directory: \'/gpfswork/rech/toto/datasets/paws/labeled_final/1.1.0/09d8fae989bb569009a8f5b879ccf2924d3e5cd55bfe2e89e6dab1c0b50ecd34.incomplete/LICENSE\''
 'You can probably have a solution much faster than me (first time I use the library). But I suspect some write function are used when loading the dataset from cache.'
 'I have the same issue:\r\n```\r\nTraceback (most recent call last):\r\n  File ""/dccstor/tslm/envs/anaconda3/envs/trf-a100/lib/python3.9/site-packages/datasets/builder.py"", line 652, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File ""/dccstor/tslm/envs/anaconda3/envs/trf-a100/lib/python3.9/site-packages/datasets/builder.py"", line 1040, in _prepare_split\r\n    with ArrowWriter(features=self.info.features, path=fpath) as writer:\r\n  File ""/dccstor/tslm/envs/anaconda3/envs/trf-a100/lib/python3.9/site-packages/datasets/arrow_writer.py"", line 192, in __init__\r\n    self.stream = pa.OSFile(self._path, ""wb"")\r\n  File ""pyarrow/io.pxi"", line 829, in pyarrow.lib.OSFile.__cinit__\r\n  File ""pyarrow/io.pxi"", line 844, in pyarrow.lib.OSFile._open_writable\r\n  File ""pyarrow/error.pxi"", line 122, in pyarrow.lib.pyarrow_internal_check_status\r\n  File ""pyarrow/error.pxi"", line 97, in pyarrow.lib.check_status\r\nFileNotFoundError: [Errno 2] Failed to open local file \'/dccstor/tslm-gen/.cache/csv/default-387f1f95c084d4df/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0.incomplete/csv-validation.arrow\'. Detail: [errno 2] No such file or directory\r\nDuring handling of the above exception, another exception occurred:\r\nTraceback (most recent call last):\r\n  File ""/dccstor/tslm/elron/tslm-gen/train.py"", line 510, in <module>\r\n    main()\r\n  File ""/dccstor/tslm/elron/tslm-gen/train.py"", line 246, in main\r\n    datasets = prepare_dataset(dataset_args, logger)\r\n  File ""/dccstor/tslm/elron/tslm-gen/data.py"", line 157, in prepare_dataset\r\n    datasets = load_dataset(extension, data_files=data_files, split=dataset_split, cache_dir=dataset_args.dataset_cache_dir, na_filter=False, download_mode=dataset_args.dataset_generate_mode)\r\n  File ""/dccstor/tslm/envs/anaconda3/envs/trf-a100/lib/python3.9/site-packages/datasets/load.py"", line 742, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File ""/dccstor/tslm/envs/anaconda3/envs/trf-a100/lib/python3.9/site-packages/datasets/builder.py"", line 574, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File ""/dccstor/tslm/envs/anaconda3/envs/trf-a100/lib/python3.9/site-packages/datasets/builder.py"", line 654, in _download_and_prepare\r\n    raise OSError(\r\nOSError: Cannot find data file. \r\nOriginal error:\r\n[Errno 2] Failed to open local file \'/dccstor/tslm-gen/.cache/csv/default-387f1f95c084d4df/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0.incomplete/csv-validation.arrow\'. Detail: [errno 2] No such file or directory\r\n```']","## Describe the bug
When launching several jobs at the same time loading the same dataset trigger some errors see (last comments).

## Steps to reproduce the bug
export HF_DATASETS_CACHE=/gpfswork/rech/toto/datasets
for MODEL in ""bert-base-uncased"" ""roberta-base"" ""distilbert-base-cased""; do #  ""bert-base-uncased"" ""bert-large-cased"" ""roberta-large"" ""albert-base-v1"" ""albert-large-v1""; do
  for TASK_NAME in ""mrpc"" ""rte"" 'imdb' ""paws"" ""mnli""; do
    export OUTPUT_DIR=${MODEL}_${TASK_NAME}
    sbatch --job-name=${OUTPUT_DIR} \
      --gres=gpu:1 \
      --no-requeue \
      --cpus-per-task=10 \
      --hint=nomultithread \
      --time=1:00:00 \
      --output=jobinfo/${OUTPUT_DIR}_%j.out \
      --error=jobinfo/${OUTPUT_DIR}_%j.err \
      --qos=qos_gpu-t4 \
      --wrap=""module purge; module load pytorch-gpu/py3/1.7.0 ; export HF_DATASETS_OFFLINE=1; export HF_DATASETS_CACHE=/gpfswork/rech/toto/datasets;  python compute_measures.py --seed=$SEED --saving_path=results --batch_size=$BATCH_SIZE --task_name=$TASK_NAME --model_name=/gpfswork/rech/toto/transformers_models/$MODEL""

  done
done



```python
# Sample code to reproduce the bug
        dataset_train = load_dataset('imdb', split='train', download_mode=""reuse_cache_if_exists"")
        dataset_train = dataset_train.map(lambda e: tokenizer(e['text'], truncation=True, padding='max_length'),
                                          batched=True).select(list(range(args.filter)))

        dataset_val = load_dataset('imdb', split='train', download_mode=""reuse_cache_if_exists"")
        dataset_val = dataset_val.map(lambda e: tokenizer(e['text'], truncation=True, padding='max_length'),
                                      batched=True).select(list(range(args.filter, args.filter + 5000)))

        dataset_test = load_dataset('imdb', split='test', download_mode=""reuse_cache_if_exists"")
        dataset_test = dataset_test.map(lambda e: tokenizer(e['text'], truncation=True, padding='max_length'),
                                        batched=True)
```

## Expected results
I believe I am doing something wrong with the objects. 

## Actual results
Traceback (most recent call last):
  File ""/gpfslocalsup/pub/anaconda-py3/2020.02/envs/pytorch-gpu-1.7.0/lib/python3.7/site-packages/datasets/builder.py"", line 652, in _download_and_prepare
    self._prepare_split(split_generator, **prepare_split_kwargs)
  File ""/gpfslocalsup/pub/anaconda-py3/2020.02/envs/pytorch-gpu-1.7.0/lib/python3.7/site-packages/datasets/builder.py"", line 983, in _prepare_split
    check_duplicates=True,
  File ""/gpfslocalsup/pub/anaconda-py3/2020.02/envs/pytorch-gpu-1.7.0/lib/python3.7/site-packages/datasets/arrow_writer.py"", line 192, in __init__
    self.stream = pa.OSFile(self._path, ""wb"")
  File ""pyarrow/io.pxi"", line 829, in pyarrow.lib.OSFile.__cinit__
  File ""pyarrow/io.pxi"", line 844, in pyarrow.lib.OSFile._open_writable
  File ""pyarrow/error.pxi"", line 122, in pyarrow.lib.pyarrow_internal_check_status
  File ""pyarrow/error.pxi"", line 97, in pyarrow.lib.check_status
FileNotFoundError: [Errno 2] Failed to open local file '/gpfswork/rech/tts/unm25jp/datasets/paws/labeled_final/1.1.0/09d8fae989bb569009a8f5b879ccf2924d3e5cd55bfe2e89e6dab1c0b50ecd34.incomplete/paws-test.arrow'. Detail: [errno 2] No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""compute_measures.py"", line 181, in <module>
    train_loader, val_loader, test_loader = get_dataloader(args)
  File ""/gpfsdswork/projects/rech/toto/intRAOcular/dataset_utils.py"", line 69, in get_dataloader
    dataset_train = load_dataset('paws', ""labeled_final"", split='train', download_mode=""reuse_cache_if_exists"")
  File ""/gpfslocalsup/pub/anaconda-py3/2020.02/envs/pytorch-gpu-1.7.0/lib/python3.7/site-packages/datasets/load.py"", line 748, in load_dataset
    use_auth_token=use_auth_token,
  File ""/gpfslocalsup/pub/anaconda-py3/2020.02/envs/pytorch-gpu-1.7.0/lib/python3.7/site-packages/datasets/builder.py"", line 575, in download_and_prepare
    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
  File ""/gpfslocalsup/pub/anaconda-py3/2020.02/envs/pytorch-gpu-1.7.0/lib/python3.7/site-packages/datasets/builder.py"", line 658, in _download_and_prepare
    + str(e)
OSError: Cannot find data file.
Original error:
[Errno 2] Failed to open local file '/gpfswork/rech/toto/datasets/paws/labeled_final/1.1.0/09d8fae989bb569009a8f5b879ccf2924d3e5cd55bfe2e89e6dab1c0b50ecd34.incomplete/paws-test.arrow'. Detail: [errno 2] No such file or directory

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: datasets==1.8.0
- Platform: linux (jeanzay)
- Python version: pyarrow==2.0.0
- PyArrow version: 3.7.8
"
https://github.com/huggingface/datasets/issues/2727,Error in loading the Arabic Billion Words Corpus,"['I modified the dataset loading script to catch the `IndexError` and inspect the records at which the error is happening, and I found this:\r\nFor the `Techreen` config, the error happens in 36 records when trying to find the `Text` or `Dateline` tags. All these 36 records look something like:\r\n```\r\n<Techreen>\r\n <ID>TRN_ARB_0248167</ID>\r\n <URL>http://tishreen.news.sy/tishreen/public/read/248240</URL>\r\n <Headline>Removed, because the original articles was in English</Headline>\r\n</Techreen>\r\n```\r\n\r\nand all the 288 faulty records in the `Almustaqbal` config look like:\r\n```\r\n<Almustaqbal>\r\n <ID>MTL_ARB_0028398</ID>\r\n \r\n <URL>http://www.almustaqbal.com/v4/article.aspx?type=NP&ArticleID=179015</URL>\r\n <Headline> Removed because it is not available in the original site</Headline>\r\n</Almustaqbal>\r\n```\r\n\r\nso the error is happening because the articles were removed and so the associated records lack the `Text` tag.\r\n\r\nIn this case, I think we just need to catch the `IndexError` and ignore (pass) it.\r\n'
 ""Thanks @M-Salti for reporting this issue and for your investigation.\r\n\r\nIndeed, those `IndexError` should be catched and the corresponding record should be ignored.\r\n\r\nI'm opening a Pull Request to fix it.""]","## Describe the bug
I get `IndexError: list index out of range` when trying to load the `Techreen` and `Almustaqbal` configs of the dataset.

## Steps to reproduce the bug
```python
load_dataset(""arabic_billion_words"", ""Techreen"")
load_dataset(""arabic_billion_words"", ""Almustaqbal"")
```

## Expected results
The datasets load succefully.

## Actual results
```python
_extract_tags(self, sample, tag)
    139             if len(out) > 0:
    140                 break
--> 141         return out[0]
    142 
    143     def _clean_text(self, text):

IndexError: list index out of range
```

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: 1.10.2
- Platform: Ubuntu 18.04.5 LTS
- Python version: 3.7.11
- PyArrow version: 3.0.0
"
https://github.com/huggingface/datasets/issues/2724,404 Error when loading remote data files from private repo,"['I guess the issue is when computing the ETags of the remote files. Indeed `use_auth_token` must be passed to `request_etags` here:\r\n\r\nhttps://github.com/huggingface/datasets/blob/35b5e4bc0cb2ed896e40f3eb2a4aa3de1cb1a6c5/src/datasets/builder.py#L160-L160'
 'Yes, I remember having properly implemented that: \r\n- https://github.com/huggingface/datasets/commit/7a9c62f7cef9ecc293f629f859d4375a6bd26dc8#diff-f933ce41f71c6c0d1ce658e27de62cbe0b45d777e9e68056dd012ac3eb9324f7R160\r\n- https://github.com/huggingface/datasets/pull/2628/commits/6350a03b4b830339a745f7b1da46ece784ca734c\r\n\r\nBut a subsequent refactoring accidentally removed it...'
 'I have opened a PR to fix it @lewtun.']","## Describe the bug
When loading remote data files from a private repo, a 404 error is raised.

## Steps to reproduce the bug
```python
url = hf_hub_url(""lewtun/asr-preds-test"", ""preds.jsonl"", repo_type=""dataset"")
dset = load_dataset(""json"", data_files=url, use_auth_token=True)
# HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/datasets/lewtun/asr-preds-test/resolve/main/preds.jsonl
```

## Expected results
Load dataset.

## Actual results
404 Error.

"
https://github.com/huggingface/datasets/issues/2722,Missing cache file,"['This could be solved by going to the glue/ directory and delete sst2 directory, then load the dataset again will help you redownload the dataset.'
 'Hi ! Not sure why this file was missing, but yes the way to fix this is to delete the sst2 directory and to reload the dataset']","Strangely missing cache file after I  restart my program again.

`glue_dataset = datasets.load_dataset('glue', 'sst2')`

`FileNotFoundError: [Errno 2] No such file or directory: /Users/chris/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96d6053ad/dataset_info.json'`
"
https://github.com/huggingface/datasets/issues/2716,Calling shuffle on IterableDataset will disable batching in case any functions were mapped,"['Hi :) Good catch ! Feel free to open a PR if you want to contribute, this would be very welcome ;)'
 'Have raised the PR [here](https://github.com/huggingface/datasets/pull/2717)'
 'Fixed by #2717.']","When using dataset in streaming mode, if one applies `shuffle` method on the dataset and `map` method for which `batched=True` than the batching operation will not happen, instead `batched` will be set to `False`

I did RCA on the dataset codebase, the problem is emerging from [this line of code](https://github.com/huggingface/datasets/blob/d25a0bf94d9f9a9aa6cabdf5b450b9c327d19729/src/datasets/iterable_dataset.py#L197) here as it is
`self.ex_iterable.shuffle_data_sources(seed), function=self.function, batch_size=self.batch_size`, as one can see it is missing batched argument, which means that the iterator fallsback to default constructor value, which in this case is `False`.
To remedy the problem we can change this line to
`self.ex_iterable.shuffle_data_sources(seed), function=self.function, batched=self.batched, batch_size=self.batch_size`
"
https://github.com/huggingface/datasets/issues/2714,add more precise information for size,['We already have this information in the dataset_infos.json files of each dataset.\r\nMaybe we can parse these files in the backend to return their content with the endpoint at huggingface.co/api/datasets\r\n\r\nFor now if you want to access this info you have to load the json for each dataset. For example:\r\n- for a dataset on github like `squad` \r\n-   https://raw.githubusercontent.com/huggingface/datasets/master/datasets/squad/dataset_infos.json\r\n- for a community dataset on the hub like `lhoestq/squad`:\r\n  https://huggingface.co/datasets/lhoestq/squad/resolve/main/dataset_infos.json'],"For the import into ELG, we would like a more precise description of the size of the dataset, instead of the current size categories. The size can be expressed in bytes, or any other preferred size unit. As suggested in the slack channel, perhaps this could be computed with a regex for existing datasets."
https://github.com/huggingface/datasets/issues/2709,Missing documentation for wnut_17 (ner_tags),"['Hi @maxpel, thanks for reporting this issue.\r\n\r\nIndeed, the documentation in the dataset card is not complete. I’m opening a Pull Request to fix it.\r\n\r\nAs the paper explains, there are 6 entity types and we have ordered them alphabetically: `corporation`, `creative-work`, `group`, `location`, `person` and `product`. \r\n\r\nEach of these entity types has 2 possible IOB2 format tags: \r\n- `B-`: to indicate that the token is the beginning of an entity name, and the \r\n- `I-`: to indicate that the token is inside an entity name. \r\n\r\nAdditionally, there is the standalone IOB2 tag \r\n- `O`: that indicates that the token belongs to no named entity. \r\n\r\nIn total there are 13 possible tags, which correspond to the following integer numbers:\r\n\r\n0. `O`\r\n1. `B-corporation`\r\n2. `I-corporation`\r\n3. `B-creative-work`\r\n4. `I-creative-work`\r\n5. `B-group`\r\n6. `I-group`\r\n7. `B-location`\r\n8. `I-location`\r\n9. `B-person`\r\n10. `I-person`\r\n11. `B-product`\r\n12. `I-product`']","On the info page of the wnut_17 data set (https://huggingface.co/datasets/wnut_17), the model output of ner-tags is only documented for these 5 cases:

`ner_tags: a list of classification labels, with possible values including O (0), B-corporation (1), I-corporation (2), B-creative-work (3), I-creative-work (4).`

I trained a model with the data and it gives me 13 classes:

```
""id2label"": {
    ""0"": 0,
    ""1"": 1,
    ""2"": 2,
    ""3"": 3,
    ""4"": 4,
    ""5"": 5,
    ""6"": 6,
    ""7"": 7,
    ""8"": 8,
    ""9"": 9,
    ""10"": 10,
    ""11"": 11,
    ""12"": 12
  }

  ""label2id"": {
    ""0"": 0,
    ""1"": 1,
    ""10"": 10,
    ""11"": 11,
    ""12"": 12,
    ""2"": 2,
    ""3"": 3,
    ""4"": 4,
    ""5"": 5,
    ""6"": 6,
    ""7"": 7,
    ""8"": 8,
    ""9"": 9
  }
```
The paper (https://www.aclweb.org/anthology/W17-4418.pdf) explains those 6 categories, but the ordering does not match:

```
1. person
2. location (including GPE, facility)
3. corporation
4. product (tangible goods, or well-defined
services)
5. creative-work (song, movie, book and
so on)
6. group (subsuming music band, sports team,
and non-corporate organisations)
```
I would be very helpful for me, if somebody could clarify the model ouputs and explain the ""B-"" and ""I-"" prefixes to me.

Really great work with that and the other packages, I couldn't believe that training the model with that data was basically a one-liner!"
https://github.com/huggingface/datasets/issues/2708,QASC: incomplete training set ,"['Hi @danyaljj, thanks for reporting.\r\n\r\nUnfortunately, I have not been able to reproduce your problem. My train split has 8134 examples:\r\n```ipython\r\nIn [10]: ds[""train""]\r\nOut[10]:\r\nDataset({\r\n    features: [\'id\', \'question\', \'choices\', \'answerKey\', \'fact1\', \'fact2\', \'combinedfact\', \'formatted_question\'],\r\n    num_rows: 8134\r\n})\r\n\r\nIn [11]: ds[""train""].shape\r\nOut[11]: (8134, 8)\r\n```\r\nand the content of the last 5 examples is:\r\n```ipython\r\nIn [12]: for i in range(8129, 8134):\r\n    ...:     print(json.dumps(ds[""train""][i]))\r\n    ...:\r\n{""id"": ""3KAKFY4PGU1LGXM77JAK2700NGCI3X"", ""question"": ""Chitin can be used for protection by whom?"", ""choices"": {""text"": [""Fungi"", ""People"", ""Man"", ""Fish"", ""trees"", ""Dogs"", ""animal"", ""Birds""], ""label"": [""A"", ""B"",\r\n ""C"", ""D"", ""E"", ""F"", ""G"", ""H""]}, ""answerKey"": ""D"", ""fact1"": ""scales are used for protection by scaled animals"", ""fact2"": ""Fish scales are also composed of chitin."", ""combinedfact"": ""Chitin can be used for prote\r\nction by fish."", ""formatted_question"": ""Chitin can be used for protection by whom? (A) Fungi (B) People (C) Man (D) Fish (E) trees (F) Dogs (G) animal (H) Birds""}\r\n{""id"": ""336YQZE83VDAQVZ26HW59X51JZ9M5M"", ""question"": ""Which type of animal uses plates for protection?"", ""choices"": {""text"": [""squids"", ""reptiles"", ""sea urchins"", ""fish"", ""amphibians"", ""Frogs"", ""mammals"", ""salm\r\non""], ""label"": [""A"", ""B"", ""C"", ""D"", ""E"", ""F"", ""G"", ""H""]}, ""answerKey"": ""B"", ""fact1"": ""scales are used for protection by scaled animals"", ""fact2"": ""Reptiles have scales or plates."", ""combinedfact"": ""Reptiles use\r\n their plates for protection."", ""formatted_question"": ""Which type of animal uses plates for protection? (A) squids (B) reptiles (C) sea urchins (D) fish (E) amphibians (F) Frogs (G) mammals (H) salmon""}\r\n{""id"": ""3WZ36BJEV3FGS66VGOOUYX0LN8GTBU"", ""question"": ""What are used for protection by fish?"", ""choices"": {""text"": [""scales"", ""fins"", ""streams."", ""coral"", ""gills"", ""Collagen"", ""mussels"", ""whiskers""], ""label"": [""\r\nA"", ""B"", ""C"", ""D"", ""E"", ""F"", ""G"", ""H""]}, ""answerKey"": ""A"", ""fact1"": ""scales are used for protection by scaled animals"", ""fact2"": ""Fish are backboned aquatic animals."", ""combinedfact"": ""scales are used for prote\r\nction by fish "", ""formatted_question"": ""What are used for protection by fish? (A) scales (B) fins (C) streams. (D) coral (E) gills (F) Collagen (G) mussels (H) whiskers""}\r\n{""id"": ""3Z2R0DQ0JHDKFAO2706OYIXGNA4E28"", ""question"": ""What are pangolins covered in?"", ""choices"": {""text"": [""tunicates"", ""Echinoids"", ""shells"", ""exoskeleton"", ""blastoids"", ""barrel-shaped"", ""protection"", ""white""\r\n], ""label"": [""A"", ""B"", ""C"", ""D"", ""E"", ""F"", ""G"", ""H""]}, ""answerKey"": ""G"", ""fact1"": ""scales are used for protection by scaled animals"", ""fact2"": ""Pangolins have an elongate and tapering body covered above with ov\r\nerlapping scales."", ""combinedfact"": ""Pangolins are covered in overlapping protection."", ""formatted_question"": ""What are pangolins covered in? (A) tunicates (B) Echinoids (C) shells (D) exoskeleton (E) blastoids\r\n (F) barrel-shaped (G) protection (H) white""}\r\n{""id"": ""3PMBY0YE272GIWPNWIF8IH5RBHVC9S"", ""question"": ""What are covered with protection?"", ""choices"": {""text"": [""apples"", ""trees"", ""coral"", ""clams"", ""roses"", ""wings"", ""hats"", ""fish""], ""label"": [""A"", ""B"", ""C"", ""D\r\n"", ""E"", ""F"", ""G"", ""H""]}, ""answerKey"": ""H"", ""fact1"": ""scales are used for protection by scaled animals"", ""fact2"": ""Fish are covered with scales."", ""combinedfact"": ""Fish are covered with protection"", ""formatted_q\r\nuestion"": ""What are covered with protection? (A) apples (B) trees (C) coral (D) clams (E) roses (F) wings (G) hats (H) fish""}\r\n```\r\n\r\nCould you please load again your dataset and print its shape, like this:\r\n```python\r\nds = load_dataset(""qasc"", split=""train)\r\nprint(ds.shape)\r\n```\r\nand confirm which is your output?'
 'Hmm .... it must have been a mistake on my side. Sorry for the hassle! ']","## Describe the bug
The training instances are not loaded properly. 

## Steps to reproduce the bug
```python
from datasets import load_dataset

dataset = load_dataset(""qasc"", script_version='1.10.2')
 
def load_instances(split): 
    instances = dataset[split]
    print(f""split: {split} - size: {len(instances)}"")
    for x in instances:
        print(json.dumps(x))


load_instances('test')
load_instances('validation')
load_instances('train')
```

##  results
For test and validation, we can see the examples in the output (which is good!): 
```
split: test - size: 920
{""answerKey"": """", ""choices"": {""label"": [""A"", ""B"", ""C"", ""D"", ""E"", ""F"", ""G"", ""H""], ""text"": [""Anthax"", ""under water"", ""uterus"", ""wombs"", ""two"", ""moles"", ""live"", ""embryo""]}, ""combinedfact"": """", ""fact1"": """", ""fact2"": """", ""formatted_question"": ""What type of birth do therian mammals have? (A) Anthax (B) under water (C) uterus (D) wombs (E) two (F) moles (G) live (H) embryo"", ""id"": ""3C44YUNSI1OBFBB8D36GODNOZN9DPA"", ""question"": ""What type of birth do therian mammals have?""}
{""answerKey"": """", ""choices"": {""label"": [""A"", ""B"", ""C"", ""D"", ""E"", ""F"", ""G"", ""H""], ""text"": [""Corvidae"", ""arthropods"", ""birds"", ""backbones"", ""keratin"", ""Jurassic"", ""front paws"", ""Parakeets.""]}, ""combinedfact"": """", ""fact1"": """", ""fact2"": """", ""formatted_question"": ""By what time had mouse-sized viviparous mammals evolved? (A) Corvidae (B) arthropods (C) birds (D) backbones (E) keratin (F) Jurassic (G) front paws (H) Parakeets."", ""id"": ""3B1NLC6UGZVERVLZFT7OUYQLD1SGPZ"", ""question"": ""By what time had mouse-sized viviparous mammals evolved?""}
{""answerKey"": """", ""choices"": {""label"": [""A"", ""B"", ""C"", ""D"", ""E"", ""F"", ""G"", ""H""], ""text"": [""Reduced friction"", ""causes infection"", ""vital to a good life"", ""prevents water loss"", ""camouflage from consumers"", ""Protection against predators"", ""spur the growth of the plant"", ""a smooth surface""]}, ""combinedfact"": """", ""fact1"": """", ""fact2"": """", ""formatted_question"": ""What does a plant's skin do? (A) Reduced friction (B) causes infection (C) vital to a good life (D) prevents water loss (E) camouflage from consumers (F) Protection against predators (G) spur the growth of the plant (H) a smooth surface"", ""id"": ""3QRYMNZ7FYGITFVSJET3PS0F4S0NT9"", ""question"": ""What does a plant's skin do?""}
...
```
However, only a few instances are loaded for the training split, which is not correct. 

## Environment info
- `datasets` version: '1.10.2' 
- Platform: MaxOS 
- Python version:3.7
- PyArrow version: 3.0.0
"
https://github.com/huggingface/datasets/issues/2707,404 Not Found Error when loading LAMA dataset,"['Hi @dwil2444! I was able to reproduce your error when I downgraded to v1.1.2. Updating to the latest version of Datasets fixed the error for me :)'
 'Hi @dwil2444, thanks for reporting.\r\n\r\nCould you please confirm which `datasets` version you were using and if the problem persists after you update it to the latest version: `pip install -U datasets`?\r\n\r\nThanks @stevhliu for the hint to fix this! ;)'
 '@stevhliu @albertvillanova  updating to the latest version of datasets did in fact fix this issue. Thanks a lot for your help!']","The [LAMA](https://huggingface.co/datasets/viewer/?dataset=lama) probing dataset is not available for download:  

Steps to Reproduce: 

1. `from datasets import load_dataset`
2. `dataset = load_dataset('lama', 'trex')`. 


Results:  
`FileNotFoundError: Couldn't find file locally at lama/lama.py, or remotely at https://raw.githubusercontent.com/huggingface/datasets/1.1.2/datasets/lama/lama.py or https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/lama/lama.py`"
https://github.com/huggingface/datasets/issues/2705,404 not found error on loading WIKIANN dataset,"['Hi @ronbutan, thanks for reporting.\r\n\r\nYou are right: we have recently found that the link to the original PAN-X dataset (also called WikiANN), hosted at Dropbox, is no longer working.\r\n\r\nWe have opened an issue in the GitHub repository of the original dataset (afshinrahimi/mmner#4) and we have also contacted the author by email to ask if they are planning to fix this issue. See the details here: https://github.com/huggingface/datasets/issues/2691#issuecomment-885463027\r\n\r\nI close this issue because it is the same as in #2691. Feel free to subscribe to that other issue to be informed about any updates.']","## Describe the bug
Unable to retreive wikiann English dataset

## Steps to reproduce the bug
```python
from datasets import list_datasets, load_dataset, list_metrics, load_metric
WIKIANN = load_dataset(""wikiann"",""en"")
```

## Expected results
Colab notebook should display successful download status

## Actual results
FileNotFoundError: Couldn't find file at https://www.dropbox.com/s/12h3qqog6q4bjve/panx_dataset.tar?dl=1

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: 1.10.1
- Platform: Linux-5.4.104+-x86_64-with-Ubuntu-18.04-bionic
- Python version: 3.7.11
- PyArrow version: 3.0.0
"
https://github.com/huggingface/datasets/issues/2700,from datasets import Dataset is failing ,"['Hi @kswamy15, thanks for reporting.\r\n\r\nWe are fixing this critical issue and making an urgent patch release of the `datasets` library today.\r\n\r\nIn the meantime, you can circumvent this issue by updating the `tqdm` library: `!pip install -U tqdm`']","## Describe the bug
A clear and concise description of what the bug is.

## Steps to reproduce the bug
```python
# Sample code to reproduce the bug
from datasets import Dataset
```

## Expected results
A clear and concise description of the expected results.

## Actual results
Specify the actual results or traceback.
/usr/local/lib/python3.7/dist-packages/datasets/utils/file_utils.py in <module>()
     25 import posixpath
     26 import requests
---> 27 from tqdm.contrib.concurrent import thread_map
     28 
     29 from .. import __version__, config, utils

ModuleNotFoundError: No module named 'tqdm.contrib.concurrent'

---------------------------------------------------------------------------
NOTE: If your import is failing due to a missing package, you can
manually install dependencies using either !pip or !apt.

To view examples of installing some common dependencies, click the
""Open Examples"" button below.
---------------------------------------------------------------------------

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: latest version as of 07/21/2021
- Platform: Google Colab
- Python version: 3.7
- PyArrow version:
"
https://github.com/huggingface/datasets/issues/2699,cannot combine splits merging and streaming?,"[""Hi ! That's missing indeed. We'll try to implement this for the next version :)\r\n\r\nI guess we just need to implement #2564 first, and then we should be able to add support for splits combinations""]","this does not work:
`dataset = datasets.load_dataset('mc4','iw',split='train+validation',streaming=True)`
with error:
`ValueError: Bad split: train+validation. Available splits: ['train', 'validation']`

these work:
`dataset = datasets.load_dataset('mc4','iw',split='train+validation')`
`dataset = datasets.load_dataset('mc4','iw',split='train',streaming=True)`
`dataset = datasets.load_dataset('mc4','iw',split='validation',streaming=True)`

i could not find a reference to this in the documentation and the error message is confusing. also would be nice to allow streaming for the merged splits"
https://github.com/huggingface/datasets/issues/2695,Cannot import load_dataset on Colab,"['I\'m facing the same issue on Colab today too.\r\n\r\n```\r\nModuleNotFoundError                       Traceback (most recent call last)\r\n<ipython-input-4-5833ac0f5437> in <module>()\r\n      3 \r\n      4 from ray import tune\r\n----> 5 from datasets import DatasetDict, Dataset\r\n      6 from datasets import load_dataset, load_metric\r\n      7 from dataclasses import dataclass\r\n\r\n7 frames\r\n/usr/local/lib/python3.7/dist-packages/datasets/utils/file_utils.py in <module>()\r\n     25 import posixpath\r\n     26 import requests\r\n---> 27 from tqdm.contrib.concurrent import thread_map\r\n     28 \r\n     29 from .. import __version__, config, utils\r\n\r\nModuleNotFoundError: No module named \'tqdm.contrib.concurrent\'\r\n\r\n---------------------------------------------------------------------------\r\nNOTE: If your import is failing due to a missing package, you can\r\nmanually install dependencies using either !pip or !apt.\r\n\r\nTo view examples of installing some common dependencies, click the\r\n""Open Examples"" button below.\r\n---------------------------------------------------------------------------\r\n```'
 '@phosseini \r\nI think it is related to [1.10.0](https://github.com/huggingface/datasets/actions/runs/1052653701) release done 3 hours ago. (cc: @lhoestq )\r\nFor now I just downgraded to 1.9.0 and it is working fine.'
 '> @phosseini\r\n> I think it is related to [1.10.0](https://github.com/huggingface/datasets/actions/runs/1052653701) release done 3 hours ago. (cc: @lhoestq )\r\n> For now I just downgraded to 1.9.0 and it is working fine.\r\n\r\nSame here, downgraded to 1.9.0 for now and works fine.'
 'Hi, \r\n\r\nupdating tqdm to the newest version resolves the issue for me. You can do this as follows in Colab:\r\n```\r\n!pip install tqdm --upgrade\r\n```'
 'Hi @bayartsogt-ya and @phosseini, thanks for reporting.\r\n\r\nWe are fixing this critical issue and making an urgent patch release of the `datasets` library today.\r\n\r\nIn the meantime, as pointed out by @mariosasko, you can circumvent this issue by updating the `tqdm` library: \r\n```\r\n!pip install -U tqdm\r\n```']","## Describe the bug
Got tqdm concurrent module not found error during importing load_dataset from datasets.

## Steps to reproduce the bug
Here [colab notebook](https://colab.research.google.com/drive/1pErWWnVP4P4mVHjSFUtkePd8Na_Qirg4?usp=sharing) to reproduce the error

On colab:
```python
!pip install datasets
from datasets import load_dataset
```

## Expected results
Works without error

## Actual results
Specify the actual results or traceback.
```
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-2-8cc7de4c69eb> in <module>()
----> 1 from datasets import load_dataset, load_metric, Metric, MetricInfo, Features, Value
      2 from sklearn.metrics import mean_squared_error

/usr/local/lib/python3.7/dist-packages/datasets/__init__.py in <module>()
     31     )
     32 
---> 33 from .arrow_dataset import Dataset, concatenate_datasets
     34 from .arrow_reader import ArrowReader, ReadInstruction
     35 from .arrow_writer import ArrowWriter

/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py in <module>()
     40 from tqdm.auto import tqdm
     41 
---> 42 from datasets.tasks.text_classification import TextClassification
     43 
     44 from . import config, utils

/usr/local/lib/python3.7/dist-packages/datasets/tasks/__init__.py in <module>()
      1 from typing import Optional
      2 
----> 3 from ..utils.logging import get_logger
      4 from .automatic_speech_recognition import AutomaticSpeechRecognition
      5 from .base import TaskTemplate

/usr/local/lib/python3.7/dist-packages/datasets/utils/__init__.py in <module>()
     19 
     20 from . import logging
---> 21 from .download_manager import DownloadManager, GenerateMode
     22 from .file_utils import DownloadConfig, cached_path, hf_bucket_url, is_remote_url, temp_seed
     23 from .mock_download_manager import MockDownloadManager

/usr/local/lib/python3.7/dist-packages/datasets/utils/download_manager.py in <module>()
     24 
     25 from .. import config
---> 26 from .file_utils import (
     27     DownloadConfig,
     28     cached_path,

/usr/local/lib/python3.7/dist-packages/datasets/utils/file_utils.py in <module>()
     25 import posixpath
     26 import requests
---> 27 from tqdm.contrib.concurrent import thread_map
     28 
     29 from .. import __version__, config, utils

ModuleNotFoundError: No module named 'tqdm.contrib.concurrent'
```
## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: 1.10.0
- Platform: Colab
- Python version: 3.7.11
- PyArrow version: 3.0.0
"
https://github.com/huggingface/datasets/issues/2691,xtreme / pan-x cannot be downloaded,"['Hi @severo, thanks for reporting.\r\n\r\nHowever I have not been able to reproduce this issue. Could you please confirm if the problem persists for you?\r\n\r\nMaybe Dropbox (where the data source is hosted) was temporarily unavailable when you tried.'
 'Hmmm, the file (https://www.dropbox.com/s/dl/12h3qqog6q4bjve/panx_dataset.tar) really seems to be unavailable... I tried from various connexions and machines and got the same 404 error. Maybe the dataset has been loaded from the cache in your case?'
 ""Yes @severo, weird... I could access the file when I answered to you, but now I cannot longer access it either... Maybe it was from the cache as you point out.\r\n\r\nAnyway, I have opened an issue in the GitHub repository responsible for the original dataset: https://github.com/afshinrahimi/mmner/issues/4\r\nI have also contacted the maintainer by email.\r\n\r\nI'll keep you informed with their answer.""
 'Reply from the author/maintainer: \r\n> Will fix the issue and let you know during the weekend.'
 'The author told that apparently Dropbox has changed their policy and no longer allow downloading the file without having signed in first. The author asked Hugging Face to host their dataset.']","## Describe the bug

Dataset xtreme / pan-x cannot be loaded

Seems related to https://github.com/huggingface/datasets/pull/2326

## Steps to reproduce the bug

```python
dataset = load_dataset(""xtreme"", ""PAN-X.fr"")
```

## Expected results

Load the dataset

## Actual results

```
FileNotFoundError: Couldn't find file at https://www.dropbox.com/s/12h3qqog6q4bjve/panx_dataset.tar?dl=1
```

## Environment info

- `datasets` version: 1.9.0
- Platform: macOS-11.4-x86_64-i386-64bit
- Python version: 3.8.11
- PyArrow version: 4.0.1
"
https://github.com/huggingface/datasets/issues/2689,cannot save the dataset to disk after rename_column,"[""Hi ! That's because you are trying to overwrite a file that is already open and being used.\r\nIndeed `foo/dataset.arrow` is open and used by your `dataset` object.\r\n\r\nWhen you do `rename_column`, the resulting dataset reads the data from the same arrow file.\r\nIn other cases like when using `map` on the other hand, the resulting dataset reads the data from another arrow file that is the result of the map transform.\r\n\r\nTherefore overwriting a dataset after `rename_column` is not possible, but it is possible after `map`, since `rename_column` doesn't switch to using another arrow file (the actual data stay the same).""
 'Ok, thanks for clearing it up :)']","## Describe the bug
If you use `rename_column` and do no other modification, you will be unable to save the dataset using `save_to_disk`

## Steps to reproduce the bug
```python
# Sample code to reproduce the bug
In [1]: from datasets import Dataset, load_from_disk
In [5]: dataset=Dataset.from_dict({'foo': [0]})
In [7]: dataset.save_to_disk('foo')
In [8]: dataset=load_from_disk('foo')
In [10]: dataset=dataset.rename_column('foo', 'bar')
In [11]: dataset.save_to_disk('foo')
---------------------------------------------------------------------------
PermissionError                           Traceback (most recent call last)
<ipython-input-11-a3bc0d4fc339> in <module>
----> 1 dataset.save_to_disk('foo')

/mnt/beegfs/projects/meerqat/anaconda3/envs/meerqat/lib/python3.7/site-packages/datasets/arrow_dataset.py in save_to_disk(self, dataset_path
, fs)
    597             if Path(dataset_path, config.DATASET_ARROW_FILENAME) in cache_files_paths:
    598                 raise PermissionError(
--> 599                     f""Tried to overwrite {Path(dataset_path, config.DATASET_ARROW_FILENAME)} but a dataset can't overwrite itself.""
    600                 )
    601             if Path(dataset_path, config.DATASET_INDICES_FILENAME) in cache_files_paths:

PermissionError: Tried to overwrite foo/dataset.arrow but a dataset can't overwrite itself.
```

N. B. I created the dataset from dict to enable easy reproduction but the same happens if you load an existing dataset (e.g. starting from `In [8]`)

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: 1.8.0
- Platform: Linux-3.10.0-1160.11.1.el7.x86_64-x86_64-with-centos-7.9.2009-Core
- Python version: 3.7.10
- PyArrow version: 3.0.0

"
https://github.com/huggingface/datasets/issues/2688,hebrew language codes he and iw should be treated as aliases,"['Hi @eyaler, thanks for reporting.\r\n\r\nWhile you are true with respect the Hebrew language tag (""iw"" is deprecated and ""he"" is the preferred value), in the ""mc4"" dataset (which is a derived dataset) we have kept the language tags present in the original dataset: [Google C4](https://www.tensorflow.org/datasets/catalog/c4).'
 'For discoverability on the website I updated the YAML tags at the top of the mC4 dataset card https://github.com/huggingface/datasets/commit/38288087b1b02f97586e0346e8f28f4960f1fd37\r\n\r\nOnce the website is updated, mC4 will be listed in https://huggingface.co/datasets?filter=languages:he\r\n\r\n']","https://huggingface.co/datasets/mc4 not listed when searching for hebrew datasets (he) as it uses the older language code iw, preventing discoverability. "
https://github.com/huggingface/datasets/issues/2681,5 duplicate datasets,"[""Yes this was documented in the PR that added this hf->paperswithcode mapping (https://github.com/huggingface/datasets/pull/2404) and AFAICT those are slightly distinct datasets so I think it's a wontfix\r\n\r\nFor context on the paperswithcode mapping you can also refer to https://github.com/huggingface/huggingface_hub/pull/43 which contains a lot of background discussion ""
 'Thanks for the antecedents. I close.']","## Describe the bug

In 5 cases, I could find a dataset on Paperswithcode which references two Hugging Face datasets as dataset loaders. They are:

- https://paperswithcode.com/dataset/multinli -> https://huggingface.co/datasets/multi_nli and https://huggingface.co/datasets/multi_nli_mismatch
  
  <img width=""838"" alt=""Capture d’écran 2021-07-20 à 16 33 58"" src=""https://user-images.githubusercontent.com/1676121/126342757-4625522a-f788-41a3-bd1f-2a8b9817bbf5.png"">

- https://paperswithcode.com/dataset/squad -> https://huggingface.co/datasets/squad and https://huggingface.co/datasets/squad_v2
- https://paperswithcode.com/dataset/narrativeqa -> https://huggingface.co/datasets/narrativeqa and https://huggingface.co/datasets/narrativeqa_manual
- https://paperswithcode.com/dataset/hate-speech-and-offensive-language -> https://huggingface.co/datasets/hate_offensive and https://huggingface.co/datasets/hate_speech_offensive
- https://paperswithcode.com/dataset/newsph-nli -> https://huggingface.co/datasets/newsph and https://huggingface.co/datasets/newsph_nli

Possible solutions:
- don't fix (it works)
- for each pair of duplicate datasets, remove one, and create an alias to the other.

## Steps to reproduce the bug

Visit the Paperswithcode links, and look at the ""Dataset Loaders"" section

## Expected results

There should only be one reference to a Hugging Face dataset loader

## Actual results

Two Hugging Face dataset loaders
"
https://github.com/huggingface/datasets/issues/2679,Cannot load the blog_authorship_corpus due to codec errors,"[""Hi @izaskr, thanks for reporting.\r\n\r\nHowever the traceback you joined does not correspond to the codec error message: it is about other error `NonMatchingSplitsSizesError`. Maybe you missed some important part of your traceback...\r\n\r\nI'm going to have a look at the dataset anyway...""
 'Hi @izaskr, thanks again for having reported this issue.\r\n\r\nAfter investigation, I have created a Pull Request (#2685) to fix several issues with this dataset:\r\n- the `NonMatchingSplitsSizesError`\r\n- the `UnicodeDecodeError`\r\n\r\nOnce the Pull Request merged into master, you will be able to load this dataset if you install `datasets` from our GitHub repository master branch. Otherwise, you will be able to use it after our next release, by updating `datasets`: `pip install -U datasets`.'
 '@albertvillanova \r\nCan you shed light on how this fix works?\r\n\r\nWe\'re experiencing a similar issue. \r\n\r\nIf we run several runs (eg in a Wandb sweep) the first run ""works"" but then we get `NonMatchingSplitsSizesError`\r\n\r\n| run num | actual train examples # | expected example # | recorded example # |\r\n| ------- | -------------- | ----------------- | -------- |\r\n| 1       | 100            | 100               | 100      |\r\n| 2       | 102            | 100               | 102      |\r\n| 3       | 100            | 100               | 202      | \r\n| 4       | 40             | 100               | 40       |\r\n| 5       | 40             | 100               | 40       |\r\n| 6       | 40             | 100               | 40       | \r\n\r\n\r\nThe second through the nth all crash with \r\n\r\n```\r\ndatasets.utils.info_utils.NonMatchingSplitsSizesError: [{\'expected\': SplitInfo(name=\'train\', num_bytes=19980970, num_examples=100, dataset_name=\'cies\'), \'recorded\': SplitInfo(name=\'train\', num_bytes=40163811, num_examples=202, dataset_name=\'cies\')}]\r\n\r\n```']","## Describe the bug
A codec error is raised while loading the blog_authorship_corpus. 

## Steps to reproduce the bug
```
from datasets import load_dataset
raw_datasets = load_dataset(""blog_authorship_corpus"")
```


## Expected results
Loading the dataset without errors.

## Actual results
An error similar to the one below was raised for (what seems like) every XML file.
/home/izaskr/.cache/huggingface/datasets/downloads/extracted/7cf52524f6517e168604b41c6719292e8f97abbe8f731e638b13423f4212359a/blogs/788358.male.24.Arts.Libra.xml cannot be loaded. Error message: 'utf-8' codec can't decode byte 0xe7 in position 7551: invalid continuation byte

Traceback (most recent call last):         
  File ""<stdin>"", line 1, in <module>
  File ""/home/izaskr/anaconda3/envs/local_vae_older/lib/python3.8/site-packages/datasets/load.py"", line 856, in load_dataset
    builder_instance.download_and_prepare(
  File ""/home/izaskr/anaconda3/envs/local_vae_older/lib/python3.8/site-packages/datasets/builder.py"", line 583, in download_and_prepare
    self._download_and_prepare(
  File ""/home/izaskr/anaconda3/envs/local_vae_older/lib/python3.8/site-packages/datasets/builder.py"", line 671, in _download_and_prepare
    verify_splits(self.info.splits, split_dict)
  File ""/home/izaskr/anaconda3/envs/local_vae_older/lib/python3.8/site-packages/datasets/utils/info_utils.py"", line 74, in verify_splits
    raise NonMatchingSplitsSizesError(str(bad_splits))
datasets.utils.info_utils.NonMatchingSplitsSizesError: [{'expected': SplitInfo(name='train', num_bytes=610252351, num_examples=532812, dataset_name='blog_authorship_corpus'), 'recorded': SplitInfo(name='train', num_bytes=614706451, num_examples=535568, dataset_name='blog_authorship_corpus')}, {'expected': SplitInfo(name='validation', num_bytes=37500394, num_examples=31277, dataset_name='blog_authorship_corpus'), 'recorded': SplitInfo(name='validation', num_bytes=32553710, num_examples=28521, dataset_name='blog_authorship_corpus')}]


## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: 1.9.0
- Platform: Linux-4.15.0-132-generic-x86_64-with-glibc2.10
- Python version: 3.8.8
- PyArrow version: 4.0.1

"
https://github.com/huggingface/datasets/issues/2678,Import Error in Kaggle notebook,"['This looks like an issue with PyArrow. Did you try reinstalling it ?'
 ""@lhoestq I did, and then let pip handle the installation in `pip import datasets`. I also tried using conda but it gives the same error.\r\n\r\nEdit: pyarrow version on kaggle is 4.0.0, it gets replaced with 4.0.1. So, I don't think uninstalling will change anything.\r\n```\r\nInstall Trace of datasets:\r\n\r\nCollecting datasets\r\n  Downloading datasets-1.9.0-py3-none-any.whl (262 kB)\r\n     |████████████████████████████████| 262 kB 834 kB/s eta 0:00:01\r\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.4)\r\nCollecting pyarrow!=4.0.0,>=1.0.0\r\n  Downloading pyarrow-4.0.1-cp37-cp37m-manylinux2014_x86_64.whl (21.8 MB)\r\n     |████████████████████████████████| 21.8 MB 6.2 MB/s eta 0:00:01\r\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (3.4.0)\r\nRequirement already satisfied: huggingface-hub<0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.0.8)\r\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.2.4)\r\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.25.1)\r\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2021.6.1)\r\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.12.2)\r\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets) (20.9)\r\nCollecting xxhash\r\n  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\r\n     |████████████████████████████████| 243 kB 23.7 MB/s eta 0:00:01\r\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.19.5)\r\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from datasets) (4.61.1)\r\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\r\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.5)\r\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.10)\r\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2021.5.30)\r\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (4.0.0)\r\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.7.4.3)\r\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.4.1)\r\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->datasets) (2.4.7)\r\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.1)\r\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2021.1)\r\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\r\nInstalling collected packages: xxhash, pyarrow, datasets\r\n  Attempting uninstall: pyarrow\r\n    Found existing installation: pyarrow 4.0.0\r\n    Uninstalling pyarrow-4.0.0:\r\n      Successfully uninstalled pyarrow-4.0.0\r\nSuccessfully installed datasets-1.9.0 pyarrow-4.0.1 xxhash-2.0.2\r\nWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\r\n```""
 ""You may need to restart your kaggle notebook after installing a newer version of `pyarrow`.\r\n\r\nIf it doesn't work we'll probably have to create an issue on [arrow's JIRA](https://issues.apache.org/jira/projects/ARROW/issues/), and maybe ask kaggle why it could fail""
 ""> You may need to restart your kaggle notebook before after installing a newer version of `pyarrow`.\r\n> \r\n> If it doesn't work we'll probably have to create an issue on [arrow's JIRA](https://issues.apache.org/jira/projects/ARROW/issues/), and maybe ask kaggle why it could fail\r\n\r\nIt works after restarting.\r\nMy bad, I forgot to restart the notebook. Sorry for the trouble!""]","## Describe the bug
Not able to import datasets library in kaggle notebooks

## Steps to reproduce the bug
```python
!pip install datasets
import datasets
```

## Expected results
No such error

## Actual results
```
ImportError                               Traceback (most recent call last)
<ipython-input-9-652e886d387f> in <module>
----> 1 import datasets

/opt/conda/lib/python3.7/site-packages/datasets/__init__.py in <module>
     31     )
     32 
---> 33 from .arrow_dataset import Dataset, concatenate_datasets
     34 from .arrow_reader import ArrowReader, ReadInstruction
     35 from .arrow_writer import ArrowWriter

/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py in <module>
     36 import pandas as pd
     37 import pyarrow as pa
---> 38 import pyarrow.compute as pc
     39 from multiprocess import Pool, RLock
     40 from tqdm.auto import tqdm

/opt/conda/lib/python3.7/site-packages/pyarrow/compute.py in <module>
     16 # under the License.
     17 
---> 18 from pyarrow._compute import (  # noqa
     19     Function,
     20     FunctionOptions,

ImportError: /opt/conda/lib/python3.7/site-packages/pyarrow/_compute.cpython-37m-x86_64-linux-gnu.so: undefined symbol: _ZNK5arrow7compute15KernelSignature8ToStringEv
```

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: 1.9.0
- Platform: Kaggle
- Python version: 3.7.10
- PyArrow version: 4.0.1
"
https://github.com/huggingface/datasets/issues/2677,Error when downloading C4,"['Hi Thanks for reporting !\r\nIt looks like these files are not correctly reported in the list of expected files to download, let me fix that ;)'
 ""Alright this is fixed now. We'll do a new release soon to make the fix available.\r\n\r\nIn the meantime feel free to simply pass `ignore_verifications=True` to `load_dataset` to skip this error""
 '@lhoestq thank you for such a quick feedback!']","Hi,
I am trying to download `en` corpus from C4 dataset. However, I get an error caused by validation files download (see image). My code is very primitive:
`datasets.load_dataset('c4', 'en')`

Is this a bug or do I have some configurations missing on my server? 
Thanks!


<img width=""1014"" alt=""Снимок экрана 2021-07-20 в 11 37 17"" src=""https://user-images.githubusercontent.com/36672861/126289448-6e0db402-5f3f-485a-bf74-eb6e0271fc25.png"">"
https://github.com/huggingface/datasets/issues/2669,Metric kwargs are not passed to underlying external metric f1_score,"['Hi @BramVanroy, thanks for reporting.\r\n\r\nFirst, note that `""min""` is not an allowed value for `average`. According to scikit-learn [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html), `average` can only take the values: `{""micro"", ""macro"", ""samples"", ""weighted"", ""binary""} or None, default=""binary""`.\r\n\r\nSecond, you should take into account that all additional metric-specific argument should be passed in the method `compute` (and not in the method `load_metric`). You can find more information in our documentation: https://huggingface.co/docs/datasets/using_metrics.html#computing-the-metric-scores\r\n\r\nSo for example, if you would like to calculate the macro-averaged F1 score, you should use:\r\n```python\r\nimport datasets\r\n\r\nf1 = datasets.load_metric(""f1"", keep_in_memory=True)\r\nf1.add_batch(predictions=[0,2,3], references=[1, 2, 3])\r\nf1.compute(average=""macro"")\r\n```'
 ""Thanks, that was it. A bit strange though, since `load_metric` had an argument `metric_init_kwargs`. I assume that that's for specific initialisation arguments whereas `average` is for the function itself.""]","## Describe the bug
When I want to use F1 score with average=""min"", this keyword argument does not seem to be passed through to the underlying sklearn metric. This is evident because [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) throws an error telling me so.

## Steps to reproduce the bug
```python
import datasets
f1 = datasets.load_metric(""f1"", keep_in_memory=True, average=""min"")
f1.add_batch(predictions=[0,2,3], references=[1, 2, 3])
f1.compute()
```

## Expected results
No error, because `average=""min""` should be passed correctly to f1_score in sklearn.

## Actual results

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\bramv\.virtualenvs\pipeline-TpEsXVex\lib\site-packages\datasets\metric.py"", line 402, in compute
    output = self._compute(predictions=predictions, references=references, **kwargs)
  File ""C:\Users\bramv\.cache\huggingface\modules\datasets_modules\metrics\f1\82177930a325d4c28342bba0f116d73f6d92fb0c44cd67be32a07c1262b61cfe\f1.py"", line 97, in _compute
    ""f1"": f1_score(
  File ""C:\Users\bramv\.virtualenvs\pipeline-TpEsXVex\lib\site-packages\sklearn\utils\validation.py"", line 63, in inner_f
    return f(*args, **kwargs)
  File ""C:\Users\bramv\.virtualenvs\pipeline-TpEsXVex\lib\site-packages\sklearn\metrics\_classification.py"", line 1071, in f1_score
    return fbeta_score(y_true, y_pred, beta=1, labels=labels,
  File ""C:\Users\bramv\.virtualenvs\pipeline-TpEsXVex\lib\site-packages\sklearn\utils\validation.py"", line 63, in inner_f
    return f(*args, **kwargs)
  File ""C:\Users\bramv\.virtualenvs\pipeline-TpEsXVex\lib\site-packages\sklearn\metrics\_classification.py"", line 1195, in fbeta_score
    _, _, f, _ = precision_recall_fscore_support(y_true, y_pred,
  File ""C:\Users\bramv\.virtualenvs\pipeline-TpEsXVex\lib\site-packages\sklearn\utils\validation.py"", line 63, in inner_f
    return f(*args, **kwargs)
  File ""C:\Users\bramv\.virtualenvs\pipeline-TpEsXVex\lib\site-packages\sklearn\metrics\_classification.py"", line 1464, in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels,
  File ""C:\Users\bramv\.virtualenvs\pipeline-TpEsXVex\lib\site-packages\sklearn\metrics\_classification.py"", line 1294, in _check_set_wise_labels
    raise ValueError(""Target is %s but average='binary'. Please ""
ValueError: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].
```

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: 1.9.0
- Platform: Windows-10-10.0.19041-SP0
- Python version: 3.9.2
- PyArrow version: 4.0.1"
https://github.com/huggingface/datasets/issues/2663,[`to_json`] add multi-proc sharding support,"['Hi @stas00, \r\nI want to work on this issue and I was thinking why don\'t we use `imap` [in this loop](https://github.com/huggingface/datasets/blob/440b14d0dd428ae1b25881aa72ba7bbb8ad9ff84/src/datasets/io/json.py#L99)? This way, using offset (which is being used to slice the pyarrow table) we can convert  pyarrow table to `json` using multiprocessing. I\'ve a small code snippet for some clarity:\r\n```\r\nresult = list(\r\n            pool.imap(self._apply_df, [(offset, batch_size) for offset in range(0, len(self.dataset), batch_size)])\r\n        )\r\n```\r\n`_apply_df` is a function which will return `batch.to_pandas().to_json(path_or_buf=None, orient=""records"", lines=True)` which is basically json version of the batched pyarrow table. Later on we can concatenate it to form json file? \r\n\r\nI think the only downside here is to write file from `imap` output (output would be a list and we\'ll need to iterate over it and write in a file) which might add a little overhead cost. What do you think about this?'
 'Followed up in https://github.com/huggingface/datasets/pull/2747']","As discussed on slack it appears that `to_json` is quite slow on huge datasets like OSCAR.

I implemented sharded saving, which is much much faster - but the tqdm bars all overwrite each other, so it's hard to make sense of the progress, so if possible ideally this multi-proc support could be implemented internally in `to_json` via `num_proc` argument. I guess `num_proc` will be the number of shards?

I think the user will need to use this feature wisely, since too many processes writing to say normal style HD is likely to be slower than one process.

I'm not sure whether the user should be responsible to concatenate the shards at the end  or `datasets`, either way works for my needs.

The code I was using:

```
from multiprocessing import cpu_count, Process, Queue

[...]

filtered_dataset = concat_dataset.map(filter_short_documents, batched=True, batch_size=256, num_proc=cpu_count())

DATASET_NAME = ""oscar""
SHARDS = 10
def process_shard(idx):
    print(f""Sharding {idx}"")
    ds_shard = filtered_dataset.shard(SHARDS, idx, contiguous=True)
    # ds_shard = ds_shard.shuffle() # remove contiguous=True above if shuffling
    print(f""Saving {DATASET_NAME}-{idx}.jsonl"")
    ds_shard.to_json(f""{DATASET_NAME}-{idx}.jsonl"", orient=""records"", lines=True, force_ascii=False)

queue = Queue()
processes = [Process(target=process_shard, args=(idx,)) for idx in range(SHARDS)]
for p in processes:
    p.start()

for p in processes:
    p.join()
```

Thank you!

@lhoestq "
https://github.com/huggingface/datasets/issues/2655,Allow the selection of multiple columns at once,"[""Hi! I was looking into this and hope you can clarify a point. Your my_dataset variable would be of type DatasetDict which means the alternative you've described (dict comprehension) is what makes sense. \r\nIs there a reason why you wouldn't want to convert my_dataset to a pandas df if you'd like to use it like one? Please let me know if I'm missing something.""
 'Hi! Sorry for the delay.\r\n\r\nIn this case, the dataset would be a `datasets.Dataset` and we want to select multiple columns, the `idx` and `label` columns for example.\r\n\r\nMy issue is that my dataset is too big for memory if I load everything into pandas.']","**Is your feature request related to a problem? Please describe.**

Similar to pandas, it would be great if we could select multiple columns at once.


**Describe the solution you'd like**
```python
my_dataset = ...  # Has columns ['idx', 'sentence', 'label']
idx, label = my_dataset[['idx', 'label']]
```

**Describe alternatives you've considered**
we can do `[dataset[col] for col in ('idx', 'label')]`

**Additional context**
This is of course very minor.
"
https://github.com/huggingface/datasets/issues/2654,Give a user feedback if the dataset he loads is streamable or not,"['#self-assign'
 'I understand it already raises a `NotImplementedError` exception, eg:\r\n\r\n```\r\n>>> dataset = load_dataset(""journalists_questions"", name=""plain_text"", split=""train"", streaming=True)\r\n\r\n[...]\r\nNotImplementedError: Extraction protocol for file at https://drive.google.com/uc?export=download&id=1CBrh-9OrSpKmPQBxTK_ji6mq6WTN_U9U is not implemented yet\r\n```\r\n']","**Is your feature request related to a problem? Please describe.**
I would love to know if a `dataset` is with the current implementation streamable or not. 

**Describe the solution you'd like**
We could show a warning when a dataset is loaded with `load_dataset('...',streaming=True)` when its lot streamable, e.g. if it is an archive. 

**Describe alternatives you've considered**
Add a new metadata tag for ""streaming""
"
https://github.com/huggingface/datasets/issues/2653,Add SD task for SUPERB,"[""Note that this subset requires us to:\r\n\r\n* generate the LibriMix corpus from LibriSpeech\r\n* prepare the corpus for diarization\r\n\r\nAs suggested by @lhoestq we should perform these steps locally and add the prepared data to this public repo on the Hub: https://huggingface.co/datasets/superb/superb-data\r\n\r\nThen we can use the URLs for the files to load the data in `superb`'s dataset loading script.\r\n\r\nFor consistency, I suggest we name the folders in `superb-data` in the same way as the configs in the dataset loading script - e.g. use `sd` for speech diarization in both places :)""
 '@lewtun @lhoestq: \r\n\r\nI have already generated the LibriMix corpus and prepared the corpus for diarization. The output is 3 dirs (train, dev, test), each one containing 6 files: reco2dur  rttm  segments  spk2utt  utt2spk  wav.scp\r\n\r\nNext steps:\r\n- Upload these files to the superb-data repo\r\n- Transcribe the corresponding s3prl processing of these files into our superb loading script\r\n\r\nNote that processing of these files is a bit more intricate than usual datasets: https://github.com/s3prl/s3prl/blob/master/s3prl/downstream/diarization/dataset.py#L233\r\n\r\n']","Include the SD (Speaker Diarization) task as described in the [SUPERB paper](https://arxiv.org/abs/2105.01051) and `s3prl` [instructions](https://github.com/s3prl/s3prl/tree/master/s3prl/downstream#sd-speaker-diarization).

Steps:
- [x] Generate the LibriMix corpus
- [x] Prepare the corpus for diarization
- [x] Upload these files to the superb-data repo
- [x] Transcribe the corresponding s3prl processing of these files into our superb loading script
- [ ] README: tags + description sections

Related to #2619.

cc: @lewtun 
"
https://github.com/huggingface/datasets/issues/2651,Setting log level higher than warning does not suppress progress bar,"['Hi,\r\n\r\nyou can suppress progress bars by patching logging as follows:\r\n```python\r\nimport datasets\r\nimport logging\r\ndatasets.logging.get_verbosity = lambda: logging.NOTSET\r\n# map call ...\r\n```'
 'Thank you, it worked :)'
 'See https://github.com/huggingface/datasets/issues/2528 for reference'
 'Note also that you can disable the progress bar with\r\n\r\n```python\r\nfrom datasets.utils import disable_progress_bar\r\ndisable_progress_bar()\r\n```\r\n\r\nSee https://github.com/huggingface/datasets/blob/8814b393984c1c2e1800ba370de2a9f7c8644908/src/datasets/utils/tqdm_utils.py#L84']","## Describe the bug
I would like to disable progress bars for `.map` method (and other methods like `.filter` and `load_dataset` as well).
According to #1627 one can suppress it by setting log level higher than `warning`, however doing so doesn't suppress it with version 1.9.0.

I also tried to set `DATASETS_VERBOSITY` environment variable to `error` or `critical` but it also didn't work.

## Steps to reproduce the bug
```python
import datasets

from datasets.utils.logging import set_verbosity_error

set_verbosity_error()

def dummy_map(batch):
    return batch

common_voice_train = datasets.load_dataset(""common_voice"", ""de"", split=""train"")
common_voice_test = datasets.load_dataset(""common_voice"", ""de"", split=""test"")

common_voice_train.map(dummy_map)
```

## Expected results
- The progress bar for `.map` call won't be shown

## Actual results
- The progress bar for `.map` is still shown 

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: 1.9.0
- Platform: Linux-5.4.0-1045-aws-x86_64-with-Ubuntu-18.04-bionic
- Python version: 3.7.5
- PyArrow version: 4.0.1
"
https://github.com/huggingface/datasets/issues/2648,Add web_split dataset for Paraphase and Rephrase benchmark,['#take'],"## Describe:
For getting simple sentences from complex sentence there are dataset and task like wiki_split that is available in hugging face datasets. This web_split is a very similar dataset. There some research paper which states that by combining these two datasets we if we train the model it will yield better results on both tests data.

This dataset is made from web NLG data.

All the dataset related details are provided in the below repository

Github link: https://github.com/shashiongithub/Split-and-Rephrase

"
https://github.com/huggingface/datasets/issues/2646,downloading of yahoo_answers_topics dataset failed,"['Hi ! I just tested and it worked fine today for me.\r\n\r\nI think this is because the dataset is stored on Google Drive which has a quota limit for the number of downloads per day, see this similar issue https://github.com/huggingface/datasets/issues/996 \r\n\r\nFeel free to try again today, now that the quota was reset']","## Describe the bug
I get an error datasets.utils.info_utils.NonMatchingChecksumError: Checksums didn't match for dataset source files when I try to download the yahoo_answers_topics dataset

## Steps to reproduce the bug
   self.dataset = load_dataset(
                'yahoo_answers_topics', cache_dir=self.config['yahoo_cache_dir'], split='train[:90%]')
# Sample code to reproduce the bug
   self.dataset = load_dataset(
                'yahoo_answers_topics', cache_dir=self.config['yahoo_cache_dir'], split='train[:90%]')

## Expected results
A clear and concise description of the expected results.


## Actual results
Specify the actual results or traceback.
datasets.utils.info_utils.NonMatchingChecksumError: Checksums didn't match for dataset source files

"
https://github.com/huggingface/datasets/issues/2645,load_dataset processing failed with OS error after downloading a dataset,"['Hi ! It looks like an issue with pytorch.\r\n\r\nCould you try to run `import torch` and see if it raises an error ?'
 '> Hi ! It looks like an issue with pytorch.\r\n> \r\n> Could you try to run `import torch` and see if it raises an error ?\r\n\r\nIt works. Thank you!']","## Describe the bug
After downloading a dataset like opus100, there is a bug that 
OSError: Cannot find data file.
Original error:
dlopen: cannot load any more object with static TLS

## Steps to reproduce the bug
```python
from datasets import load_dataset
this_dataset = load_dataset('opus100', 'af-en')
```

## Expected results
there is no error when running load_dataset.

## Actual results
Specify the actual results or traceback.

Traceback (most recent call last):
  File ""/home/anaconda3/lib/python3.6/site-packages/datasets/builder.py"", line 652, in _download_and_prep
    self._prepare_split(split_generator, **prepare_split_kwargs)
  File ""/home/anaconda3/lib/python3.6/site-packages/datasets/builder.py"", line 989, in _prepare_split
    example = self.info.features.encode_example(record)
  File ""/home/anaconda3/lib/python3.6/site-packages/datasets/features.py"", line 952, in encode_example
    example = cast_to_python_objects(example)
  File ""/home/anaconda3/lib/python3.6/site-packages/datasets/features.py"", line 219, in cast_to_python_ob
    return _cast_to_python_objects(obj)[0]
  File ""/home/anaconda3/lib/python3.6/site-packages/datasets/features.py"", line 165, in _cast_to_python_o
    import torch
  File ""/home/anaconda3/lib/python3.6/site-packages/torch/__init__.py"", line 188, in <module>
    _load_global_deps()
  File ""/home/anaconda3/lib/python3.6/site-packages/torch/__init__.py"", line 141, in _load_global_deps
    ctypes.CDLL(lib_path, mode=ctypes.RTLD_GLOBAL)
  File ""/home/anaconda3/lib/python3.6/ctypes/__init__.py"", line 348, in __init__
    self._handle = _dlopen(self._name, mode)
OSError: dlopen: cannot load any more object with static TLS

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""download_hub_opus100.py"", line 9, in <module>
    this_dataset = load_dataset('opus100', language_pair)
  File ""/home/anaconda3/lib/python3.6/site-packages/datasets/load.py"", line 748, in load_dataset
    use_auth_token=use_auth_token,
  File ""/home/anaconda3/lib/python3.6/site-packages/datasets/builder.py"", line 575, in download_and_prepa
    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
  File ""/home/anaconda3/lib/python3.6/site-packages/datasets/builder.py"", line 658, in _download_and_prep
    + str(e)
OSError: Cannot find data file.
Original error:
dlopen: cannot load any more object with static TLS


## Environment info
- `datasets` version: 1.8.0
- Platform: Linux-3.13.0-32-generic-x86_64-with-debian-jessie-sid
- Python version: 3.6.6
- PyArrow version: 3.0.0


"
https://github.com/huggingface/datasets/issues/2644,Batched `map` not allowed to return 0 items,"['Hi ! Thanks for reporting. Indeed it looks like type inference makes it fail. We should probably just ignore this step until a non-empty batch is passed.'
 ""Sounds good! Do you want me to propose a PR? I'm quite busy right now, but if it's not too urgent I could take a look next week.""
 ""Sure if you're interested feel free to open a PR :)\r\n\r\nYou can also ping me anytime if you have questions or if I can help !""
 'Sorry to ping you, @lhoestq, did you have a chance to take a look at the proposed PR? Thank you!'
 ""Yes and it's all good, thank you :)\r\n\r\nFeel free to close this issue if it's good for you""
 ""Everything's good, thanks!""]","## Describe the bug
I'm trying to use `map` to filter a large dataset by selecting rows that match an expensive condition (files referenced by one of the columns need to exist in the filesystem, so we have to `stat` them). According to [the documentation](https://huggingface.co/docs/datasets/processing.html#augmenting-the-dataset), `a batch mapped function can take as input a batch of size N and return a batch of size M where M can be greater or less than N and can even be zero`.

However, when the returned batch has a size of zero (neither item in the batch fulfilled the condition), we get an `index out of bounds` error. I think that `arrow_writer.py` is [trying to infer the returned types using the first element returned](https://github.com/huggingface/datasets/blob/master/src/datasets/arrow_writer.py#L100), but no elements were returned in this case.

For this error to happen, I'm returning a dictionary that contains empty lists for the keys I want to keep, see below. If I return an empty dictionary instead (no keys), then a different error eventually occurs.

## Steps to reproduce the bug
```python
def select_rows(examples):
    # `key` is a column name that exists in the original dataset
    # The following line simulates no matches found, so we return an empty batch
    result = {'key': []}
    return result

filtered_dataset = dataset.map(
    select_rows,
    remove_columns = dataset.column_names,
    batched = True,
    num_proc = 1,
    desc = ""Selecting rows with images that exist""
)
```

The code above immediately triggers the exception. If we use the following instead:

```python
def select_rows(examples):
    # `key` is a column name that exists in the original dataset
    result = {'key': []}   # or defaultdict or whatever
    
    # code to check for condition and append elements to result
    # some_items_found will be set to True if there were any matching elements in the batch
    
    return result if some_items_found else {}
```

Then it _seems_ to work, but it eventually fails with some sort of schema error. I believe it may happen when an empty batch is followed by a non-empty one, but haven't set up a test to verify it.

In my opinion, returning a dictionary with empty lists and valid column names should be accepted as a valid result with zero items.

## Expected results
The dataset would be filtered and only the matching fields would be returned.

## Actual results
An exception is encountered, as described. Using a workaround makes it fail further along the line.

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: 1.9.1.dev0
- Platform: Linux-5.4.0-53-generic-x86_64-with-glibc2.17
- Python version: 3.8.10
- PyArrow version: 4.0.1
"
https://github.com/huggingface/datasets/issues/2643,Enum used in map functions will raise a RecursionError with dill.,"[""I'm running into this as well. (Thank you so much for reporting @jorgeecardona — was staring at this massive stack trace and unsure what exactly was wrong!)""
 ""Hi ! Thanks for reporting :)\r\n\r\nUntil this is fixed on `dill`'s side, we could implement a custom saving in our Pickler indefined in utils.py_utils.py\r\nThere is already a suggestion in this message about how to do it:\r\nhttps://github.com/uqfoundation/dill/issues/250#issuecomment-852566284\r\n\r\nLet me know if such a workaround could help, and feel free to open a PR if you want to contribute !""]","## Describe the bug

Enums used in functions pass to `map` will fail at pickling with a maximum recursion exception as described here: https://github.com/uqfoundation/dill/issues/250#issuecomment-852566284

In my particular case, I use an enum to define an argument with fixed options using the `TraininigArguments` dataclass as base class and the `HfArgumentParser`. In the same file I use a `ds.map` that tries to pickle the content of the module including the definition of the enum that runs into the dill bug described above.

## Steps to reproduce the bug
```python
from datasets import load_dataset
from enum import Enum

class A(Enum):
    a = 'a'

def main():
    a = A.a
    
    def f(x):
        return {} if a == a.a else x
    
    ds = load_dataset('cnn_dailymail', '3.0.0')['test']
    ds = ds.map(f, num_proc=15)

if __name__ == ""__main__"":
    main()
```

## Expected results
The known problem with dill could be prevented as explained in the link above (workaround.) Since `HFArgumentParser` nicely uses the enum class for choices it makes sense to also deal with this bug under the hood.

## Actual results

```python
  File ""/home/xxxx/miniconda3/lib/python3.8/site-packages/dill/_dill.py"", line 1373, in save_type
    pickler.save_reduce(_create_type, (type(obj), obj.__name__,
  File ""/home/xxxx/miniconda3/lib/python3.8/pickle.py"", line 690, in save_reduce
    save(args)
  File ""/home/xxxx/miniconda3/lib/python3.8/pickle.py"", line 558, in save
    f(self, obj)  # Call unbound method with explicit self
  File ""/home/xxxx/miniconda3/lib/python3.8/pickle.py"", line 899, in save_tuple
    save(element)
  File ""/home/xxxx/miniconda3/lib/python3.8/pickle.py"", line 534, in save
    self.framer.commit_frame()
  File ""/home/xxxx/miniconda3/lib/python3.8/pickle.py"", line 220, in commit_frame
    if f.tell() >= self._FRAME_SIZE_TARGET or force:
RecursionError: maximum recursion depth exceeded while calling a Python object
```

## Environment info
- `datasets` version: 1.8.0
- Platform: Linux-5.9.0-4-amd64-x86_64-with-glibc2.10
- Python version: 3.8.5
- PyArrow version: 3.0.0
"
https://github.com/huggingface/datasets/issues/2642,Support multi-worker with streaming dataset (IterableDataset).,"['Hi ! This is a great idea :)\r\nI think we could have something similar to what we have in `datasets.Dataset.map`, i.e. a `num_proc` parameter that tells how many processes to spawn to parallelize the data processing. \r\n\r\nRegarding AUTOTUNE, this could be a nice feature as well, we could see how to add it in a second step']","**Is your feature request related to a problem? Please describe.**
The current `.map` does not support multi-process, CPU can become bottleneck if the pre-processing is complex (e.g. t5 span masking).

**Describe the solution you'd like**
Ideally `.map` should support multi-worker like tfds, with `AUTOTUNE`.

**Describe alternatives you've considered**
A simpler solution is to shard the dataset and process it in parallel with pytorch dataloader. The shard does not need to be of equal size.
* https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset

**Additional context**
"
https://github.com/huggingface/datasets/issues/2641,"load_dataset(""financial_phrasebank"") NonMatchingChecksumError","[""Hi! It's probably because this dataset is stored on google drive and it has a per day quota limit. It should work if you retry, I was able to initiate the download.\r\n\r\nSimilar issue [here](https://github.com/huggingface/datasets/issues/2646)""
 'Hi ! Loading the dataset works on my side as well.\r\nFeel free to try again and let us know if it works for you know'
 'Thank you! I\'ve been trying periodically for the past month, and no luck yet with this particular dataset. Just tried again and still hitting the checksum error.\r\n\r\nCode:\r\n\r\n`dataset = load_dataset(""financial_phrasebank"", ""sentences_allagree"") `\r\n\r\nTraceback:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nNonMatchingChecksumError                  Traceback (most recent call last)\r\n<ipython-input-2-55cc2144f31e> in <module>\r\n----> 1 dataset = load_dataset(""financial_phrasebank"", ""sentences_allagree"")\r\n\r\n/opt/conda/lib/python3.7/site-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, script_version, use_auth_token, task, streaming, **config_kwargs)\r\n    859         ignore_verifications=ignore_verifications,\r\n    860         try_from_hf_gcs=try_from_hf_gcs,\r\n--> 861         use_auth_token=use_auth_token,\r\n    862     )\r\n    863 \r\n\r\n/opt/conda/lib/python3.7/site-packages/datasets/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)\r\n    582                     if not downloaded_from_gcs:\r\n    583                         self._download_and_prepare(\r\n--> 584                             dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    585                         )\r\n    586                     # Sync info\r\n\r\n/opt/conda/lib/python3.7/site-packages/datasets/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    642         if verify_infos:\r\n    643             verify_checksums(\r\n--> 644                 self.info.download_checksums, dl_manager.get_recorded_sizes_checksums(), ""dataset source files""\r\n    645             )\r\n    646 \r\n\r\n/opt/conda/lib/python3.7/site-packages/datasets/utils/info_utils.py in verify_checksums(expected_checksums, recorded_checksums, verification_name)\r\n     38     if len(bad_urls) > 0:\r\n     39         error_msg = ""Checksums didn\'t match"" + for_verification_name + "":\\n""\r\n---> 40         raise NonMatchingChecksumError(error_msg + str(bad_urls))\r\n     41     logger.info(""All the checksums matched successfully"" + for_verification_name)\r\n     42 \r\n\r\nNonMatchingChecksumError: Checksums didn\'t match for dataset source files:\r\n[\'https://www.researchgate.net/profile/Pekka_Malo/publication/251231364_FinancialPhraseBank-v10/data/0c96051eee4fb1d56e000000/FinancialPhraseBank-v10.zip\']\r\n```']","## Describe the bug
Attempting to download the financial_phrasebank dataset results in a NonMatchingChecksumError

## Steps to reproduce the bug
```python
from datasets import load_dataset
dataset = load_dataset(""financial_phrasebank"", 'sentences_allagree')
```

## Expected results
I expect to see the financial_phrasebank dataset downloaded successfully

## Actual results
NonMatchingChecksumError: Checksums didn't match for dataset source files:
['https://www.researchgate.net/profile/Pekka_Malo/publication/251231364_FinancialPhraseBank-v10/data/0c96051eee4fb1d56e000000/FinancialPhraseBank-v10.zip']

## Environment info
- `datasets` version: 1.9.0
- Platform: Linux-4.14.232-177.418.amzn2.x86_64-x86_64-with-debian-10.6
- Python version: 3.7.10
- PyArrow version: 4.0.1
"
https://github.com/huggingface/datasets/issues/2630,Progress bars are not properly rendered in Jupyter notebook,"['To add my experience when trying to debug this issue:\r\n\r\nSeems like previously the workaround given [here](https://github.com/tqdm/tqdm/issues/485#issuecomment-473338308) worked around this issue. But with the latest version of jupyter/tqdm I still get terminal warnings that IPython tried to send a message from a forked process.'
 'Hi @mludv, thanks for the hint!!! :) \r\n\r\nWe will definitely take it into account to try to fix this issue... It seems somehow related to `multiprocessing` and `tqdm`...']","## Describe the bug
The progress bars are not Jupyter widgets; regular progress bars appear (like in a terminal).

## Steps to reproduce the bug
```python
ds.map(tokenize, num_proc=10)
```

## Expected results
Jupyter widgets displaying the progress bars.

## Actual results
Simple plane progress bars.

cc: Reported by @thomwolf "
https://github.com/huggingface/datasets/issues/2629,Load datasets from the Hub without requiring a dataset script,"['This is so cool, let us know if we can help with anything on the hub side (@Pierrci @elishowk) 🎉 ']","As a user I would like to be able to upload my csv/json/text/parquet/etc. files in a dataset repository on the Hugging Face Hub and be able to load this dataset with `load_dataset` without having to implement a dataset script.

Moreover I would like to be able to specify which file goes into which split using the `data_files` argument.

This feature should be compatible with private repositories and dataset streaming.

This can be implemented by checking the extension of the files in the dataset repository and then by using the right dataset builder that is already packaged in the library (csv/json/text/parquet/etc.)"
https://github.com/huggingface/datasets/issues/2624,can't set verbosity for `metric.py`,['Thanks @thomas-happify for reporting and thanks @mariosasko for the fix.'],"## Describe the bug
```
[2021-07-10 20:13:11,528][datasets.utils.filelock][INFO] - Lock 139705371374976 acquired on /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow.lock
[2021-07-10 20:13:11,529][datasets.arrow_writer][INFO] - Done writing 32 examples in 6100 bytes /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow.
[2021-07-10 20:13:11,531][datasets.arrow_dataset][INFO] - Set __getitem__(key) output type to python objects for no columns  (when key is int or slice) and don't output other (un-formatted) columns.
[2021-07-10 20:13:11,543][/conda/envs/myenv/lib/python3.8/site-packages/datasets/metric.py][INFO] - Removing /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow
```
As you can see, `datasets` logging come from different places. 
`filelock`, `arrow_writer` & `arrow_dataset` comes from `datasets.*` which are expected 
However, `metric.py` logging comes from `/conda/envs/myenv/lib/python3.8/site-packages/datasets/`

So when setting `datasets.utils.logging.set_verbosity_error()`,  it still logs the last message which is annoying during evaluation. 

I had to do 
```
logging.getLogger(""/conda/envs/myenv/lib/python3.8/site-packages/datasets/metric"").setLevel(logging.ERROR)
``` 
to fully mute these messages

## Expected results
it shouldn't log these messages when setting `datasets.utils.logging.set_verbosity_error()`

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: tried both 1.8.0 & 1.9.0
- Platform: Ubuntu 18.04.5 LTS 
- Python version: 3.8.10
- PyArrow version: 3.0.0
"
https://github.com/huggingface/datasets/issues/2622,Integration with AugLy,"['Hi,\r\n\r\nyou can define your own custom formatting with `Dataset.set_transform()` and then run the tokenizer with the batches of augmented data as follows:\r\n```python\r\ndset = load_dataset(""imdb"", split=""train"")  # Let\'s say we are working with the IMDB dataset\r\ndset.set_transform(lambda ex: {""text"": augly_text_augmentation(ex[""text""])}, columns=""text"", output_all_columns=True)\r\ndataloader = torch.utils.data.DataLoader(dset, batch_size=32)\r\nfor epoch in range(5):\r\n    for batch in dataloader:\r\n       tokenizer_output = tokenizer(batch.pop(""text""), padding=True, truncation=True, return_tensors=""pt"")\r\n       batch.update(tokenizer_output)\r\n       output = model(**batch)\r\n       ...\r\n```']","**Is your feature request related to a problem? Please describe.**
Facebook recently launched a library, [AugLy](https://github.com/facebookresearch/AugLy) , that has a unified API for augmentations for image, video and text.

It would be pretty exciting to have it hooked up to HF libraries so that we can make NLP models robust to misspellings or to punctuation, or emojis etc. Plus, with Transformers supporting more CV use cases, having augmentations support becomes crucial.

**Describe the solution you'd like**
The biggest difference between augmentations and preprocessing is that preprocessing happens only once, but you are running augmentations once per epoch. AugLy operates on text directly, so this breaks the typical workflow where we would run the tokenizer once, set format to pt tensors and be ready for the Dataloader.

**Describe alternatives you've considered**

One possible way of implementing these is to make a custom Dataset class where getitem(i) runs the augmentation and the tokenizer every time, though this would slow training down considerably given we wouldn't even run the tokenizer in batches.
"
https://github.com/huggingface/datasets/issues/2618,`filelock.py` Error,"['Hi @liyucheng09, thanks for reporting.\r\n\r\nApparently this issue has to do with your environment setup. One question: is your data in an NFS share? Some people have reported this error when using `fcntl` to write to an NFS share... If this is the case, then it might be that your NFS just may not be set up to provide file locks. You should ask your system administrator, or try these commands in the terminal:\r\n```shell\r\nsudo systemctl enable rpc-statd\r\nsudo systemctl start rpc-statd\r\n```']","## Describe the bug

It seems that the `filelock.py` went error. 

```
>>> ds=load_dataset('xsum')

^CTraceback (most recent call last):
  File ""/user/HS502/yl02706/.conda/envs/lyc/lib/python3.6/site-packages/datasets/utils/filelock.py"", line 402, in _acquire
    fcntl.flock(fd, fcntl.LOCK_EX | fcntl.LOCK_NB)
OSError: [Errno 37] No locks available
```

According to error log, it is OSError, but there is an `except` in the `_acquire` function.

```
    def _acquire(self):
        open_mode = os.O_WRONLY | os.O_CREAT | os.O_EXCL | os.O_TRUNC
        try:
            fd = os.open(self._lock_file, open_mode)
        except (IOError, OSError):
            pass
        else:
            self._lock_file_fd = fd
        return None
```

I don't know why it stucked rather than `pass` directly.

I am not quite familiar with filelock operation, so any help is highly appriciated.

## Steps to reproduce the bug
```python

ds = load_dataset('xsum')
```

## Expected results
A clear and concise description of the expected results.

## Actual results
```
>>> ds=load_dataset('xsum')

^CTraceback (most recent call last):
  File ""/user/HS502/yl02706/.conda/envs/lyc/lib/python3.6/site-packages/datasets/utils/filelock.py"", line 402, in _acquire
    fcntl.flock(fd, fcntl.LOCK_EX | fcntl.LOCK_NB)
OSError: [Errno 37] No locks available

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/user/HS502/yl02706/.conda/envs/lyc/lib/python3.6/site-packages/datasets/load.py"", line 818, in load_dataset
    use_auth_token=use_auth_token,
  File ""/user/HS502/yl02706/.conda/envs/lyc/lib/python3.6/site-packages/datasets/load.py"", line 470, in prepare_module
    with FileLock(lock_path):
  File ""/user/HS502/yl02706/.conda/envs/lyc/lib/python3.6/site-packages/datasets/utils/filelock.py"", line 323, in __enter__
    self.acquire()
  File ""/user/HS502/yl02706/.conda/envs/lyc/lib/python3.6/site-packages/datasets/utils/filelock.py"", line 272, in acquire
    self._acquire()
  File ""/user/HS502/yl02706/.conda/envs/lyc/lib/python3.6/site-packages/datasets/utils/filelock.py"", line 402, in _acquire
    fcntl.flock(fd, fcntl.LOCK_EX | fcntl.LOCK_NB)
KeyboardInterrupt
```

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->

- `datasets` version: 1.9.0
- Platform: Linux-4.15.0-135-generic-x86_64-with-debian-buster-sid
- Python version: 3.6.13
- PyArrow version: 4.0.1
"
https://github.com/huggingface/datasets/issues/2615,Jsonlines export error,"[""Thanks for reporting @TevenLeScao! I'm having a look...""
 '(not sure what just happened on the assignations sorry)'
 'For some reason this happens (both `datasets` version are on master) only on Python 3.6 and not Python 3.8.'
 '@TevenLeScao we are using `pandas` to serialize the dataset to JSON Lines. So it must be due to pandas. Could you please check the pandas version causing the issue?'
 '@TevenLeScao I have just checked it: this was a bug in `pandas` and it was fixed in version 1.2: https://github.com/pandas-dev/pandas/pull/36898'
 ""Thanks ! I'm creating a PR""
 'Well I though it was me who has taken on this issue... 😅 '
 'Sorry, I was also talking to teven offline so I already had the PR ready before noticing x)'
 'I was also already working in my PR... Nevermind. Next time we should pay attention if there is somebody (self-)assigned to an issue and if he/she is still working on it before overtaking it... 😄 '
 'The fix is available on `master` @TevenLeScao , thanks for reporting']","## Describe the bug
When exporting large datasets in jsonlines (c4 in my case) the created file has an error every 9999 lines: the 9999th and 10000th are concatenated, thus breaking the jsonlines format. This sounds like it is related to batching, which is by 10000 by default

## Steps to reproduce the bug
This what I'm running:

in python:

```
from datasets import load_dataset
ptb = load_dataset(""ptb_text_only"")
ptb[""train""].to_json(""ptb.jsonl"")
```

then out of python:

```
head -10000 ptb.jsonl
```

## Expected results
Properly separated lines

## Actual results
The last line is a concatenation of two lines

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->

- `datasets` version: 1.9.1.dev0
- Platform: Linux-5.4.0-1046-gcp-x86_64-with-Ubuntu-18.04-bionic
- Python version: 3.6.9
- PyArrow version: 4.0.1"
https://github.com/huggingface/datasets/issues/2607,Streaming local gzip compressed JSON line files is not working,"[""Updating to pyarrow-4.0.1 didn't fix the issue""
 'Here is an exemple dataset with 2 of these compressed JSON files: https://huggingface.co/datasets/thomwolf/github-python'
 'Hi @thomwolf, thanks for reporting.\r\n\r\nIt seems this might be due to the fact that the JSON Dataset builder uses `pyarrow.json` (`paj.read_json`) to read the data without using the Python standard `open(file,...` (which is the one patched with `xopen` to work in streaming mode).\r\n\r\nThis has to be fixed.'
 'Sorry for reopening this, but I\'m having the same issue as @thomwolf when streaming a gzipped JSON Lines file from the hub. Or is that just not possible by definition?\r\nI installed `datasets`in editable mode from source (so probably includes the fix from #2608 ?): \r\n```\r\n>>> datasets.__version__\r\n\'1.9.1.dev0\'\r\n```\r\n\r\n```\r\n>>> msmarco = datasets.load_dataset(""webis/msmarco"", ""corpus"", streaming=True)\r\nUsing custom data configuration corpus-174d3b7155eb68db\r\n>>> msmarco_iter = iter(msmarco[\'train\'])\r\n>>> print(next(msmarco_iter))\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/media/ssd/TREC/msmarco/datasets/src/datasets/iterable_dataset.py"", line 338, in __iter__\r\n    for key, example in self._iter():\r\n  File ""/media/ssd/TREC/msmarco/datasets/src/datasets/iterable_dataset.py"", line 335, in _iter\r\n    yield from ex_iterable\r\n  File ""/media/ssd/TREC/msmarco/datasets/src/datasets/iterable_dataset.py"", line 78, in __iter__\r\n    for key, example in self.generate_examples_fn(**self.kwargs):\r\n  File ""/home/christopher/.cache/huggingface/modules/datasets_modules/datasets/msmarco/eb63dff8d83107168e973c7a655a6082d37e08d71b4ac39a0afada479c138745/msmarco.py"", line 96, in _generate_examples\r\n    with gzip.open(file, ""rt"", encoding=""utf-8"") as f:\r\n  File ""/usr/lib/python3.6/gzip.py"", line 53, in open\r\n    binary_file = GzipFile(filename, gz_mode, compresslevel)\r\n  File ""/usr/lib/python3.6/gzip.py"", line 163, in __init__\r\n    fileobj = self.myfileobj = builtins.open(filename, mode or \'rb\')\r\nFileNotFoundError: [Errno 2] No such file or directory: \'https://huggingface.co/datasets/webis/msmarco/resolve/main/msmarco_doc_00.gz\'\r\n```\r\n\r\nLoading the dataset without streaming set to True, works fine.'
 'Hi ! To make the streaming work, we extend `open` in the dataset builder to work with urls.\r\n\r\nTherefore you just need to use `open` before using `gzip.open`:\r\n```diff\r\n- with gzip.open(file, ""rt"", encoding=""utf-8"") as f:\r\n+ with gzip.open(open(file, ""rb""), ""rt"", encoding=""utf-8"") as f:\r\n```\r\n\r\nYou can see that it is the case for oscar.py and c4.py for example:\r\n\r\nhttps://github.com/huggingface/datasets/blob/8814b393984c1c2e1800ba370de2a9f7c8644908/datasets/oscar/oscar.py#L358-L358\r\n\r\nhttps://github.com/huggingface/datasets/blob/8814b393984c1c2e1800ba370de2a9f7c8644908/datasets/c4/c4.py#L88-L88\r\n\r\n'
 '@lhoestq Sorry I missed that. Thank you Quentin!']","## Describe the bug
Using streaming to iterate on local gzip compressed JSON files raise a file not exist error

## Steps to reproduce the bug
```python
from datasets import load_dataset

streamed_dataset = load_dataset('json', split='train', data_files=data_files, streaming=True)

next(iter(streamed_dataset))
```

## Actual results
```
FileNotFoundError                         Traceback (most recent call last)
<ipython-input-6-27a664e29784> in <module>
----> 1 next(iter(streamed_dataset))

~/Documents/GitHub/datasets/src/datasets/iterable_dataset.py in __iter__(self)
    336 
    337     def __iter__(self):
--> 338         for key, example in self._iter():
    339             if self.features:
    340                 # we encode the example for ClassLabel feature types for example

~/Documents/GitHub/datasets/src/datasets/iterable_dataset.py in _iter(self)
    333         else:
    334             ex_iterable = self._ex_iterable
--> 335         yield from ex_iterable
    336 
    337     def __iter__(self):

~/Documents/GitHub/datasets/src/datasets/iterable_dataset.py in __iter__(self)
     76 
     77     def __iter__(self):
---> 78         for key, example in self.generate_examples_fn(**self.kwargs):
     79             yield key, example
     80 

~/Documents/GitHub/datasets/src/datasets/iterable_dataset.py in wrapper(**kwargs)
    282     def wrapper(**kwargs):
    283         python_formatter = PythonFormatter()
--> 284         for key, table in generate_tables_fn(**kwargs):
    285             batch = python_formatter.format_batch(table)
    286             for i, example in enumerate(_batch_to_examples(batch)):

~/Documents/GitHub/datasets/src/datasets/packaged_modules/json/json.py in _generate_tables(self, files, original_files)
     85                         file,
     86                         read_options=self.config.pa_read_options,
---> 87                         parse_options=self.config.pa_parse_options,
     88                     )
     89                 except pa.ArrowInvalid as err:

~/miniconda2/envs/datasets/lib/python3.7/site-packages/pyarrow/_json.pyx in pyarrow._json.read_json()

~/miniconda2/envs/datasets/lib/python3.7/site-packages/pyarrow/_json.pyx in pyarrow._json._get_reader()

~/miniconda2/envs/datasets/lib/python3.7/site-packages/pyarrow/io.pxi in pyarrow.lib.get_input_stream()

~/miniconda2/envs/datasets/lib/python3.7/site-packages/pyarrow/io.pxi in pyarrow.lib.get_native_file()

~/miniconda2/envs/datasets/lib/python3.7/site-packages/pyarrow/io.pxi in pyarrow.lib.OSFile.__cinit__()

~/miniconda2/envs/datasets/lib/python3.7/site-packages/pyarrow/io.pxi in pyarrow.lib.OSFile._open_readable()

~/miniconda2/envs/datasets/lib/python3.7/site-packages/pyarrow/error.pxi in pyarrow.lib.pyarrow_internal_check_status()

~/miniconda2/envs/datasets/lib/python3.7/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()

FileNotFoundError: [Errno 2] Failed to open local file 'gzip://file-000000000000.json::/Users/thomwolf/github-dataset/file-000000000000.json.gz'. Detail: [errno 2] No such file or directory
```

## Environment info
- `datasets` version: 1.9.1.dev0
- Platform: Darwin-19.6.0-x86_64-i386-64bit
- Python version: 3.7.7
- PyArrow version: 1.0.0"
https://github.com/huggingface/datasets/issues/2606,[Metrics] addition of wiki_split metrics,['#take'],"**Is your feature request related to a problem? Please describe.**
While training the model on sentence split the task in English we require to evaluate the trained model on `Exact Match`, `SARI` and `BLEU` score
like this 
![image](https://user-images.githubusercontent.com/26653468/124746876-ff5a3380-df3e-11eb-9a01-4b48db7a6694.png)
While training we require metrics which can give all the output

Currently, we don't have an exact match for text normalized data

**Describe the solution you'd like**
A custom metrics for wiki_split that can calculate these three values and provide it in the form of a single dictionary
For exact match, we can refer to [this](https://github.com/huggingface/transformers/blob/master/src/transformers/data/metrics/squad_metrics.py) 

**Describe alternatives you've considered**
Two metrics are already present one more can be added for an exact match then we can run all three metrics in training script

#self-assign"
https://github.com/huggingface/datasets/issues/2604,Add option to delete temporary files (e.g. extracted files) when loading dataset,"['Hi !\r\nIf we want something more general, we could either\r\n1. delete the extracted files after the arrow data generation automatically, or \r\n2. delete each extracted file during the arrow generation right after it has been closed.\r\n\r\nSolution 2 is better to save disk space during the arrow generation. Is it what you had in mind ?\r\n\r\nThe API could look like\r\n```python\r\nload_dataset(..., delete_extracted_files_after_usage=True)\r\n```\r\n\r\nIn terms of implementation, here are some directions we could take for each solution:\r\n1. get the list of the extracted files from the DownloadManager and then delete them after the dataset is processed. This can be implemented in `download_and_prepare` I guess\r\n2. maybe wrap and mock `open` in the builder to make it delete the file when the file is closed.'
 'Also, if I delete the extracted files they need to be re-extracted again instead of loading from the Arrow cache files'
 'I think we already opened an issue about this topic (suggested by @stas00): duplicated of #2481?\r\n\r\nThis is in our TODO list... 😅 '
 'I think the deletion of each extracted file could be implemented in our CacheManager and ExtractManager (once merged to master: #2295, #2277). 😉 '
 ""Oh yes sorry, I didn't check if this was a duplicate""
 'Nevermind @thomwolf, I just mentioned the other issue so that both appear linked in GitHub and we do not forget to close both once we make the corresponding Pull Request... That was the main reason! 😄 '
 'Ok yes. I think this is an important feature to be able to use large datasets which are pretty much always compressed files.\r\n\r\nIn particular now this requires to keep the extracted file on the drive if you want to avoid reprocessing the dataset so in my case, this require using always ~400GB of drive instead of just 200GB (which is already significant). \r\n\r\nTwo nice features would be to:\r\n- allow to delete the extracted files without loosing the ability to load the dataset from the cached arrow-file\r\n- streamlined decompression when only the currently read file is extracted - this might require to read the list of files from the extracted archives before processing them?'
 'Here is a sample dataset with 2 such large compressed JSON files for debugging: https://huggingface.co/datasets/thomwolf/github-python'
 ""Note that I'm confirming that with the current master branch of dataset, deleting extracted files (without deleting the arrow cache file) lead to **re-extracting** these files when reloading the dataset instead of directly loading the arrow cache file.""
 ""Hi ! That's weird, it doesn't do that on my side (tested on master on my laptop by deleting the `extracted` folder in the download cache directory). You tested with one of the files at https://huggingface.co/datasets/thomwolf/github-python that you have locally ?""
 'Yes it’s when I load local compressed JSON line files with load_dataset(‘json’, data_files=…) '
 '@thomwolf I\'m sorry but I can\'t reproduce this problem. I\'m also using: \r\n```python\r\nds = load_dataset(""json"", split=""train"", data_files=data_files, cache_dir=cache_dir)\r\n```\r\nafter having removed the extracted files:\r\n```python\r\nassert sorted((cache_dir / ""downloads"" / ""extracted"").iterdir()) == []\r\n```\r\n\r\nI get the logging message:\r\n```shell\r\nWARNING  datasets.builder:builder.py:531 Reusing dataset json ...\r\n```'
 'Do you confirm the extracted folder stays empty after reloading?'
 '> \r\n> \r\n> Do you confirm the extracted folder stays empty after reloading?\r\n\r\nYes, I have the above mentioned assertion on the emptiness of the extracted folder:\r\n```python\r\nassert sorted((cache_dir / ""downloads"" / ""extracted"").iterdir()) == []\r\n```\r\n']","I'm loading a dataset constituted of 44 GB of compressed JSON files.

When loading the dataset with the JSON script, extracting the files create about 200 GB of uncompressed files before creating the 180GB of arrow cache tables

Having a simple way to delete the extracted files after usage (or even better, to stream extraction/delete) would be nice to avoid disk cluter.

I can maybe tackle this one in the JSON script unless you want a more general solution."
https://github.com/huggingface/datasets/issues/2598,Unable to download omp dataset,"['Hi @erikadistefano , thanks for reporting the issue.\r\n\r\nI have created a Pull Request that should fix it. \r\n\r\nOnce merged into master, feel free to update your installed `datasets` library (either by installing it from our GitHub master branch or waiting until our next release) to be able to load omp dataset.']","## Describe the bug
The omp dataset cannot be downloaded because of a DuplicatedKeysError

## Steps to reproduce the bug
from datasets import load_dataset
omp = load_dataset('omp', 'posts_labeled')
print(omp)

## Expected results
This code should download the omp dataset and print the dictionary

## Actual results
Downloading and preparing dataset omp/posts_labeled (download: 1.27 MiB, generated: 13.31 MiB, post-processed: Unknown size, total: 14.58 MiB) to /home/erika_distefano/.cache/huggingface/datasets/omp/posts_labeled/1.1.0/2fe5b067be3bff1d4588d5b0cbb9b5b22ae1b9d5b026a8ff572cd389f862735b...
0 examples [00:00, ? examples/s]2021-07-06 09:43:55.868815: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
Traceback (most recent call last):      
  File ""/home/erika_distefano/.local/lib/python3.6/site-packages/datasets/builder.py"", line 990, in _prepare_split
    writer.write(example, key)
  File ""/home/erika_distefano/.local/lib/python3.6/site-packages/datasets/arrow_writer.py"", line 338, in write
    self.check_duplicate_keys()
  File ""/home/erika_distefano/.local/lib/python3.6/site-packages/datasets/arrow_writer.py"", line 349, in check_duplicate_keys
    raise DuplicatedKeysError(key)
datasets.keyhash.DuplicatedKeysError: FAILURE TO GENERATE DATASET !
Found duplicate Key: 3326
Keys should be unique and deterministic in nature

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""hf_datasets.py"", line 32, in <module>
    omp = load_dataset('omp', 'posts_labeled')
  File ""/home/erika_distefano/.local/lib/python3.6/site-packages/datasets/load.py"", line 748, in load_dataset
    use_auth_token=use_auth_token,
  File ""/home/erika_distefano/.local/lib/python3.6/site-packages/datasets/builder.py"", line 575, in download_and_prepare
    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
  File ""/home/erika_distefano/.local/lib/python3.6/site-packages/datasets/builder.py"", line 652, in _download_and_prepare
    self._prepare_split(split_generator, **prepare_split_kwargs)
  File ""/home/erika_distefano/.local/lib/python3.6/site-packages/datasets/builder.py"", line 992, in _prepare_split
    num_examples, num_bytes = writer.finalize()
  File ""/home/erika_distefano/.local/lib/python3.6/site-packages/datasets/arrow_writer.py"", line 409, in finalize
    self.check_duplicate_keys()
  File ""/home/erika_distefano/.local/lib/python3.6/site-packages/datasets/arrow_writer.py"", line 349, in check_duplicate_keys
    raise DuplicatedKeysError(key)
datasets.keyhash.DuplicatedKeysError: FAILURE TO GENERATE DATASET !
Found duplicate Key: 3326
Keys should be unique and deterministic in nature

## Environment info
- `datasets` version: 1.8.0
- Platform: Ubuntu 18.04.4 LTS
- Python version: 3.6.9
- PyArrow version: 3.0.0
"
https://github.com/huggingface/datasets/issues/2596,Transformer Class on dataset,"['Hi ! Do you have an example in mind that shows how this could be useful ?'
 'Example:\n\nMerge 2 datasets into one datasets\n\nLabel extraction from dataset\n\ndataset(text, label)\n   —> dataset(text, newlabel)\n\nTextCleaning.\n\n\nFor image dataset, \nTransformation are easier (ie linear algebra).\n\n\n\n\n\n\n> On Jul 6, 2021, at 17:39, Quentin Lhoest ***@***.***> wrote:\n> \n> \ufeff\n> Hi ! Do you have an example in mind that shows how this could be useful ?\n> \n> —\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n'
 'There are already a few transformations that you can apply on a dataset using methods like `dataset.map()`.\r\nYou can find examples in the documentation here:\r\nhttps://huggingface.co/docs/datasets/processing.html\r\n\r\nYou can merge two datasets with `concatenate_datasets()` or do label extraction with `dataset.map()` for example'
 'Ok, sure.\n\nThanks for pointing on functional part.\nMy question is more\n“Philosophical”/Design perspective.\n\nThere are 2 perspetive:\n  Add transformation methods to \n     Dataset Class\n\n\n OR Create a Transformer Class\n   which operates on Dataset Class.\n\nT(Dataset) —> Dataset\n\ndatasetnew = MyTransform.transform(dataset)\ndatasetNew.save(path)\n\n\nWhat would be the difficulty\nof implementing a Transformer Class\noperating at dataset level ?\n\n\nthanks\n\n\n\n\n\n\n\n\n\n> On Jul 6, 2021, at 22:00, Quentin Lhoest ***@***.***> wrote:\n> \n> \ufeff\n> There are already a few transformations that you can apply on a dataset using methods like dataset.map().\n> You can find examples in the documentation here:\n> https://huggingface.co/docs/datasets/processing.html\n> \n> You can merge two datasets with concatenate_datasets() or do label extraction with dataset.map() for example\n> \n> —\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n'
 'I can imagine that this would be a useful API to implement processing pipelines as transforms. They could be used to perform higher level transforms compared to the atomic transforms allowed by methods like map, filter, etc.\r\n\r\nI guess if you find any transform that could be useful for text dataset processing, image dataset processing etc. we could definitely start having such transforms :)'
 'Thanks for reply.\n\nWhat would be the constraints\nto have\nDataset —> Dataset consistency ?\n\nMain issue would be\nlarger than memory dataset and\nserialization on disk.\n\nTechnically,\none still process at atomic level\nand try to wrap the full results\ninto Dataset…. (!)\n\nWhat would you think ?\n\n\n\n\n\n\n\n\n> On Jul 7, 2021, at 16:51, Quentin Lhoest ***@***.***> wrote:\n> \n> \ufeff\n> I can imagine that this would be a useful API to implement processing pipelines as transforms. They could be used to perform higher level transforms compared to the atomic transforms allowed by methods like map, filter, etc.\n> \n> I guess if you find any transform that could be useful for text dataset processing, image dataset processing etc. we could definitely start having such transforms :)\n> \n> —\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n'
 ""We can be pretty flexible and not impose any constraints for transforms.\r\n\r\nMoreover, this library is designed to support datasets bigger than memory. The datasets are loaded from the disk via memory mapping, without filling up RAM. Even processing functions like `map` work in a batched fashion to not fill up your RAM. So this shouldn't be an issue""
 ""Ok thanks.\n\nBut, Dataset has various flavors.\nIn current design of Dataset,\n   how the serialization on disk is done (?)\n\n\nThe main issue is serialization \nof   newdataset= Transform(Dataset)\n (ie thats why am referring to Out Of memory dataset…):\n\n   Should be part of Transform or part of dataset ?\n\n\n\n\nMaybe, not, since the output is aimed to feed model in memory (?)\n\n\n\n\n\n\n\n\n> On Jul 7, 2021, at 18:04, Quentin Lhoest ***@***.***> wrote:\n> \n> \ufeff\n> We can be pretty flexible and not impose any constraints for transforms.\n> \n> Moreover, this library is designed to support datasets bigger than memory. The datasets are loaded from the disk via memory mapping, without filling up RAM. Even processing functions like map work in a batched fashion to not fill up your RAM. So this shouldn't be an issue\n> \n> —\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n""
 ""I'm not sure I understand, could you elaborate a bit more please ?\r\n\r\nEach dataset is a wrapper of a PyArrow Table that contains all the data. The table is loaded from an arrow file on the disk.\r\nWe have an ArrowWriter and ArrowReader class to write/read arrow tables on disk or in in-memory buffers.""]","Just wondering if you have intenttion to create

TransformerClass :
    dataset --> dataset

and make determnistic transformation (ie not fit).





"
https://github.com/huggingface/datasets/issues/2595,ModuleNotFoundError: No module named 'datasets.tasks' while importing common voice datasets,"['Hi @profsatwinder.\r\n\r\nIt looks like you are using an old version of `datasets`. Please update it with `pip install -U datasets` and indicate if the problem persists.'
 '@albertvillanova Thanks for the information. I updated it to 1.9.0 and the issue is resolved. Thanks again. ']","Error traceback:
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-8-a7b592d3bca0> in <module>()
      1 from datasets import load_dataset, load_metric
      2 
----> 3 common_voice_train = load_dataset(""common_voice"", ""pa-IN"", split=""train+validation"")
      4 common_voice_test = load_dataset(""common_voice"", ""pa-IN"", split=""test"")

9 frames
/root/.cache/huggingface/modules/datasets_modules/datasets/common_voice/078d412587e9efeb0ae2e574da99c31e18844c496008d53dc5c60f4159ed639b/common_voice.py in <module>()
     19 
     20 import datasets
---> 21 from datasets.tasks import AutomaticSpeechRecognition
     22 
     23 

ModuleNotFoundError: No module named 'datasets.tasks'"
https://github.com/huggingface/datasets/issues/2591,Cached dataset overflowing disk space,"[""Hi! I'm transferring this issue over to `datasets`""
 ""I'm using the datasets concatenate dataset to combine the datasets and then train.\r\ntrain_dataset = concatenate_datasets([dataset1, dataset2, common_voice_train])\r\n\r\n""
 ""Hi @BirgerMoell.\r\n\r\nYou have several options:\r\n- to set caching to be stored on a different path location, other than the default one (`~/.cache/huggingface/datasets`):\r\n  - either setting the environment variable `HF_DATASETS_CACHE` with the path to the new cache location\r\n  - or by passing it with the parameter `cache_dir` when loading each of the datasets: `dataset = load_dataset(..., cache_dir=your_new_location)`\r\n\r\n  You can get all the information in the docs: https://huggingface.co/docs/datasets/loading_datasets.html#cache-directory\r\n- I wouldn't recommend disabling caching, because current implementation generates cache files anyway, although in a temporary directory and they are deleted when the session closes. See details here: https://huggingface.co/docs/datasets/processing.html#enable-or-disable-caching\r\n- You could alternatively load the datasets in streaming mode. This is a new feature which allows loading the datasets without downloading the entire files. More information here: https://huggingface.co/docs/datasets/dataset_streaming.html""
 'Hi @BirgerMoell,\r\n\r\nWe are planning to add a new feature to datasets, which could be interesting in your case: Add the option to delete temporary files (decompressed files) from the cache directory (see: #2481, #2604).\r\n\r\nWe will ping you once this feature is implemented, so that the size of your cache directory will be considerably reduced.']","I'm training a Swedish Wav2vec2 model on a Linux GPU and having issues that the huggingface cached dataset folder is completely filling up my disk space (I'm training on a dataset of around 500 gb).

The cache folder is 500gb (and now my disk space is full).

Is there a way to toggle caching or set the caching to be stored on a different device (I have another drive with 4 tb that could hold the caching files).

This might not technically be a bug, but I was unsure and I felt that the bug was the closest one.

Traceback (most recent call last):
  File ""/home/birger/miniconda3/envs/wav2vec2/lib/python3.7/site-packages/multiprocess/pool.py"", line 121, in worker
    result = (True, func(*args, **kwds))
  File ""/home/birger/miniconda3/envs/wav2vec2/lib/python3.7/site-packages/datasets/arrow_dataset.py"", line 186, in wrapper
    out: Union[""Dataset"", ""DatasetDict""] = func(self, *args, **kwargs)
  File ""/home/birger/miniconda3/envs/wav2vec2/lib/python3.7/site-packages/datasets/fingerprint.py"", line 397, in wrapper
    out = func(self, *args, **kwargs)
  File ""/home/birger/miniconda3/envs/wav2vec2/lib/python3.7/site-packages/datasets/arrow_dataset.py"", line 1983, in _map_single
    writer.finalize()
  File ""/home/birger/miniconda3/envs/wav2vec2/lib/python3.7/site-packages/datasets/arrow_writer.py"", line 418, in finalize
    self.pa_writer.close()
  File ""pyarrow/ipc.pxi"", line 402, in pyarrow.lib._CRecordBatchWriter.close
  File ""pyarrow/error.pxi"", line 97, in pyarrow.lib.check_status
OSError: [Errno 28] Error writing bytes to file. Detail: [errno 28] No space left on device
""""""

The above exception was the direct cause of the following exception:
"
https://github.com/huggingface/datasets/issues/2585,sqaud_v2 dataset contains misalignment between the answer text and the context value at the answer index,"[""Hi @mmajurski, thanks for reporting this issue.\r\n\r\nIndeed this misalignment arises because the source dataset context field contains leading blank spaces (and these are counted within the answer_start), while our datasets loading script removes these leading blank spaces.\r\n\r\nI'm going to fix our script so that all leading blank spaces in the source dataset are kept, and there is no misalignment between the answer text and the answer_start within the context.""
 'If you are going to be altering the data cleaning from the source Squad dataset, here is one thing to consider.\r\nThere are occasional double spaces separating words which it might be nice to get rid of. \r\n\r\nEither way, thank you.']","## Describe the bug
The built in huggingface squad_v2 dataset that you can access via datasets.load_dataset contains mis-alignment between the answers['text'] and the characters in the context at the location specified by answers['answer_start'].

For example:
id = '56d1f453e7d4791d009025bd'
answers = {'text': ['Pure Land'], 'answer_start': [146]}
However the actual text in context at location 146 is 'ure Land,'
Which is an off-by-one error from the correct answer.

## Steps to reproduce the bug
```python
import datasets

def check_context_answer_alignment(example):
    for a_idx in range(len(example['answers']['text'])):
        # check raw dataset for answer consistency between context and answer
        answer_text = example['answers']['text'][a_idx]
        a_st_idx = example['answers']['answer_start'][a_idx]
        a_end_idx = a_st_idx + len(example['answers']['text'][a_idx])
        answer_text_from_context = example['context'][a_st_idx:a_end_idx]
        if answer_text != answer_text_from_context:
            #print(example['id'])
            return False
    return True

dataset = datasets.load_dataset('squad_v2', split='train', keep_in_memory=True)

start_len = len(dataset)
dataset = dataset.filter(check_context_answer_alignment,
                                num_proc=1,
                                keep_in_memory=True)
end_len = len(dataset)
print('{} instances contain mis-alignment between the answer text and answer index.'.format(start_len - end_len))
```

## Expected results
This code should result in 0 rows being filtered out from the dataset.

## Actual results
This filter command results in 258 rows being flagged as containing a discrepancy between the text contained within answers['text'] and the text in example['context'] at the answers['answer_start'] location.

This code will reproduce the problem and produce the following count:
""258 instances contain mis-alignment between the answer text and answer index.""

## Environment info
Steps to rebuilt the Conda environment:
```
# create a virtual environment to stuff all these packages into
conda create -n round8 python=3.8 -y

# activate the virtual environment
conda activate round8

# install pytorch (best done through conda to handle cuda dependencies)
conda install pytorch torchvision torchtext cudatoolkit=11.1 -c pytorch-lts -c nvidia

pip install jsonpickle transformers datasets matplotlib
```

OS: Ubuntu 20.04
Python 3.8

Result of `conda env export`:
```
name: round8
channels:
  - pytorch-lts
  - nvidia
  - defaults
dependencies:
  - _libgcc_mutex=0.1=main
  - _openmp_mutex=4.5=1_gnu
  - blas=1.0=mkl
  - brotlipy=0.7.0=py38h27cfd23_1003
  - bzip2=1.0.8=h7b6447c_0
  - ca-certificates=2021.5.25=h06a4308_1
  - certifi=2021.5.30=py38h06a4308_0
  - cffi=1.14.5=py38h261ae71_0
  - chardet=4.0.0=py38h06a4308_1003
  - cryptography=3.4.7=py38hd23ed53_0
  - cudatoolkit=11.1.74=h6bb024c_0
  - ffmpeg=4.2.2=h20bf706_0
  - freetype=2.10.4=h5ab3b9f_0
  - gmp=6.2.1=h2531618_2
  - gnutls=3.6.15=he1e5248_0
  - idna=2.10=pyhd3eb1b0_0
  - intel-openmp=2021.2.0=h06a4308_610
  - jpeg=9b=h024ee3a_2
  - lame=3.100=h7b6447c_0
  - lcms2=2.12=h3be6417_0
  - ld_impl_linux-64=2.35.1=h7274673_9
  - libffi=3.3=he6710b0_2
  - libgcc-ng=9.3.0=h5101ec6_17
  - libgomp=9.3.0=h5101ec6_17
  - libidn2=2.3.1=h27cfd23_0
  - libopus=1.3.1=h7b6447c_0
  - libpng=1.6.37=hbc83047_0
  - libstdcxx-ng=9.3.0=hd4cf53a_17
  - libtasn1=4.16.0=h27cfd23_0
  - libtiff=4.2.0=h85742a9_0
  - libunistring=0.9.10=h27cfd23_0
  - libuv=1.40.0=h7b6447c_0
  - libvpx=1.7.0=h439df22_0
  - libwebp-base=1.2.0=h27cfd23_0
  - lz4-c=1.9.3=h2531618_0
  - mkl=2021.2.0=h06a4308_296
  - mkl-service=2.3.0=py38h27cfd23_1
  - mkl_fft=1.3.0=py38h42c9631_2
  - mkl_random=1.2.1=py38ha9443f7_2
  - ncurses=6.2=he6710b0_1
  - nettle=3.7.3=hbbd107a_1
  - ninja=1.10.2=hff7bd54_1
  - numpy=1.20.2=py38h2d18471_0
  - numpy-base=1.20.2=py38hfae3a4d_0
  - olefile=0.46=py_0
  - openh264=2.1.0=hd408876_0
  - openssl=1.1.1k=h27cfd23_0
  - pillow=8.2.0=py38he98fc37_0
  - pip=21.1.2=py38h06a4308_0
  - pycparser=2.20=py_2
  - pyopenssl=20.0.1=pyhd3eb1b0_1
  - pysocks=1.7.1=py38h06a4308_0
  - python=3.8.10=h12debd9_8
  - pytorch=1.8.1=py3.8_cuda11.1_cudnn8.0.5_0
  - readline=8.1=h27cfd23_0
  - requests=2.25.1=pyhd3eb1b0_0
  - setuptools=52.0.0=py38h06a4308_0
  - six=1.16.0=pyhd3eb1b0_0
  - sqlite=3.35.4=hdfb4753_0
  - tk=8.6.10=hbc83047_0
  - torchtext=0.9.1=py38
  - torchvision=0.9.1=py38_cu111
  - typing_extensions=3.7.4.3=pyha847dfd_0
  - urllib3=1.26.4=pyhd3eb1b0_0
  - wheel=0.36.2=pyhd3eb1b0_0
  - x264=1!157.20191217=h7b6447c_0
  - xz=5.2.5=h7b6447c_0
  - zlib=1.2.11=h7b6447c_3
  - zstd=1.4.9=haebb681_0
  - pip:
    - click==8.0.1
    - cycler==0.10.0
    - datasets==1.8.0
    - dill==0.3.4
    - filelock==3.0.12
    - fsspec==2021.6.0
    - huggingface-hub==0.0.8
    - joblib==1.0.1
    - jsonpickle==2.0.0
    - kiwisolver==1.3.1
    - matplotlib==3.4.2
    - multiprocess==0.70.12.2
    - packaging==20.9
    - pandas==1.2.4
    - pyarrow==3.0.0
    - pyparsing==2.4.7
    - python-dateutil==2.8.1
    - pytz==2021.1
    - regex==2021.4.4
    - sacremoses==0.0.45
    - tokenizers==0.10.3
    - tqdm==4.49.0
    - transformers==4.6.1
    - xxhash==2.0.2
prefix: /home/mmajurski/anaconda3/envs/round8
```
"
https://github.com/huggingface/datasets/issues/2583,Error iteration over IterableDataset using Torch DataLoader,"['Hi ! This is because you first need to format the dataset for pytorch:\r\n\r\n```python\r\n>>> import torch\r\n>>> from datasets import load_dataset\r\n>>> dataset = load_dataset(\'oscar\', ""unshuffled_deduplicated_en"", split=\'train\', streaming=True)\r\n>>> torch_iterable_dataset = dataset.with_format(""torch"")\r\n>>> assert isinstance(torch_iterable_dataset, torch.utils.data.IterableDataset)\r\n>>> dataloader = torch.utils.data.DataLoader(torch_iterable_dataset, batch_size=4)\r\n>>> next(iter(dataloader))\r\n{\'id\': tensor([0, 1, 2, 3]), \'text\': [\'Mtendere Village was inspired...]}\r\n```\r\n\r\nThis is because the pytorch dataloader expects a subclass of `torch.utils.data.IterableDataset`. Since you can\'t pass an arbitrary iterable to a pytorch dataloader, you first need to build an object that inherits from `torch.utils.data.IterableDataset` using `with_format(""torch"")` for example.\r\n'
 'Thank you for that and the example! \r\n\r\nWhat you said makes total sense; I just somehow missed that and assumed HF IterableDataset was a subclass of Torch IterableDataset. ']","## Describe the bug
I have an IterableDataset (created using streaming=True) and I am trying to create batches using Torch DataLoader class by passing this IterableDataset to it. This throws error which is pasted below. I can do the same by using Torch IterableDataset. One thing I noticed is that in the former case when I look at the dataloader.sampler class I get torch.utils.data.sampler.SequentialSampler while the latter one gives torch.utils.data.dataloader._InfiniteConstantSampler. 

I am not sure if this is how it is meant to be used, but that's what seemed reasonable to me. 

## Steps to reproduce the bug

1. Does not work.
```python
>>> from datasets import load_dataset
>>> dataset = load_dataset('oscar', ""unshuffled_deduplicated_en"", split='train', streaming=True)
>>> dataloader = torch.utils.data.DataLoader(dataset, batch_size=4)
>>> dataloader.sampler
<torch.utils.data.sampler.SequentialSampler object at 0x7f245a510208>
>>> for batch in dataloader:
...     print(batch)
```

2. Works.
```python
import torch
from torch.utils.data import Dataset, IterableDataset, DataLoader
class CustomIterableDataset(IterableDataset):
  'Characterizes a dataset for PyTorch'
  def __init__(self, data):
        'Initialization'
        self.data = data


  def __iter__(self):
        return iter(self.data)


data = list(range(12))
dataset = CustomIterableDataset(data)
dataloader = DataLoader(dataset, batch_size=4)
print(""dataloader: "", dataloader.sampler)
for batch in dataloader:
    print(batch)
```

## Expected results
To get batches of data with the batch size as 4. Output from the latter one (2) though Datasource is different here so actual data is different.
dataloader:  <torch.utils.data.dataloader._InfiniteConstantSampler object at 0x7f1cc29e2c50>
tensor([0, 1, 2, 3])
tensor([4, 5, 6, 7])
tensor([ 8,  9, 10, 11])

## Actual results
<torch.utils.data.sampler.SequentialSampler object at 0x7f245a510208>

...
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/data/leshekha/lib/HFDatasets/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 435, in __next__
    data = self._next_data()
  File ""/data/leshekha/lib/HFDatasets/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 474, in _next_data
    index = self._next_index()  # may raise StopIteration
  File ""/data/leshekha/lib/HFDatasets/lib/python3.6/site-packages/torch/utils/data/dataloader.py"", line 427, in _next_index
    return next(self._sampler_iter)  # may raise StopIteration
  File ""/data/leshekha/lib/HFDatasets/lib/python3.6/site-packages/torch/utils/data/sampler.py"", line 227, in __iter__
    for idx in self.sampler:
  File ""/data/leshekha/lib/HFDatasets/lib/python3.6/site-packages/torch/utils/data/sampler.py"", line 67, in __iter__
    return iter(range(len(self.data_source)))
TypeError: object of type 'IterableDataset' has no len()

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: '1.8.1.dev0'
- Platform: Linux
- Python version: Python 3.6.8
- PyArrow version: '3.0.0'
"
https://github.com/huggingface/datasets/issues/2573,Finding right block-size with JSON loading difficult for user,['This was actually a second error arising from a too small block-size in the json reader.\r\n\r\nFinding the right block size is difficult for the layman user'],"As reported by @thomwolf, while loading a JSON Lines file with ""json"" loading script, he gets
> json.decoder.JSONDecodeError: Extra data: line 2 column 1 (char 383)
"
https://github.com/huggingface/datasets/issues/2569,Weights of model checkpoint not initialized for RobertaModel for Bertscore,"[""Hi @suzyahyah, thanks for reporting.\r\n\r\nThe message you get is indeed not an error message, but a warning coming from Hugging Face `transformers`. The complete warning message is:\r\n```\r\nSome weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\r\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\n```\r\n\r\nIn this case, this behavior IS expected and you can safely ignore the warning message.\r\n\r\nThe reason is that you are just using RoBERTa to get the contextual embeddings of the input sentences/tokens, thus leaving away its head layer, whose weights are ignored.\r\n\r\nFeel free to reopen this issue if you need further explanations.""
 'Hi @suzyahyah, I have created a Pull Request to filter out that warning message in this specific case, since the behavior is as expected and the warning message can only cause confusion for users (as in your case).']","When applying bertscore out of the box, 

```Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']```

Following the typical usage from https://huggingface.co/docs/datasets/loading_metrics.html

```
from datasets import load_metric
metric = load_metric('bertscore')

# Example of typical usage
for batch in dataset:
    inputs, references = batch
    predictions = model(inputs)
    metric.add_batch(predictions=predictions, references=references)
score = metric.compute(lang=""en"")
#score = metric.compute(model_type=""roberta-large"") # gives the same error
```

I am concerned about this because my usage shouldn't require any further fine-tuning and most people would expect to use BertScore out of the box? I realised the huggingface code is a wrapper around https://github.com/Tiiiger/bert_score, but I think this repo is anyway relying on the model code and weights from huggingface repo.... 

## Environment info
- `datasets` version: 1.7.0
- Platform: Linux-5.4.0-1041-aws-x86_64-with-glibc2.27
- Python version:  3.9.5
- PyArrow version: 3.0.0
"
https://github.com/huggingface/datasets/issues/2561,Existing cache for local dataset builder file updates is ignored with `ignore_verifications=True`,"[""Hi ! I just tried to reproduce what you said:\r\n- create a local builder class\r\n- use `load_dataset`\r\n- update the builder class code\r\n- use `load_dataset` again (with or without `ignore_verifications=True`)\r\nAnd it creates a new cache, as expected.\r\n\r\nWhat modifications did you do to your builder's code ?""
 ""Hi @lhoestq. Thanks for your reply. I just did minor modifications for which it should not regenerate cache (for e.g. Adding a print statement). Overall, regardless of cache miss, there should be an explicit option to allow reuse of existing cache if author knows cache shouldn't be affected.""
 ""The cache is based on the hash of the dataset builder's code, so changing the code makes it recompute the cache.\r\n\r\nYou could still rename the cache directory of your previous computation to the new expected cache directory if you want to avoid having to recompute it and if you're sure that it would generate the exact same result.\r\n\r\nThe verifications are data integrity verifications: it checks the checksums of the downloaded files, as well as the size of the generated splits.""
 'Hi @apsdehal,\r\n\r\nIf you decide to follow @lhoestq\'s suggestion to rename the cache directory of your previous computation to the new expected cache directory, you can do the following to get the name of the new expected cache directory once #2500 is merged:\r\n```python\r\nfrom datasets import load_dataset_builder\r\ndataset_builder = load_dataset_builder(""path/to/your/dataset"")\r\nprint(dataset_builder.cache_dir)\r\n```\r\n\r\nThis way, you don\'t have to recompute the hash of the dataset script yourself each time you modify the script.']","## Describe the bug
If i have local file defining a dataset builder class and I load it using `load_dataset` functionality, the existing cache is ignored whenever the file is update even with `ignore_verifications=True`. This slows down debugging and cache generator for very large datasets.

## Steps to reproduce the bug

- Create a local dataset builder class
- load the local builder class file using `load_dataset` and let the cache build
- update the file's content
- The cache should rebuilt.

## Expected results

With `ignore_verifications=True`, `load_dataset` should pick up existing cache.

## Actual results

Creates new cache.

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: 1.8.0
- Platform: Linux-5.4.0-52-generic-x86_64-with-debian-bullseye-sid
- Python version: 3.7.7
- PyArrow version: 3.0.0
"
https://github.com/huggingface/datasets/issues/2559,Memory usage consistently increases when processing a dataset with `.map`,['Hi ! Can you share the function you pass to `map` ?\r\nI know you mentioned it would be hard to share some code but this would really help to understand what happened'],"## Describe the bug

I have a HF dataset with image paths stored in it and I am trying to load those image paths using `.map` with `num_proc=80`. I am noticing that the memory usage consistently keeps on increasing with time. I tried using `DEFAULT_WRITER_BATCH_SIZE=10` in the builder to decrease arrow writer's batch size but that doesn't seem to help.

## Steps to reproduce the bug

Providing code as it is would be hard. I can provide a MVP if that helps.

## Expected results

Memory usage should become consistent after some time following the launch of processing.

## Actual results

Memory usage keeps on increasing.

## Environment info

- `datasets` version: 1.8.0
- Platform: Linux-5.4.0-52-generic-x86_64-with-debian-bullseye-sid
- Python version: 3.7.7
- PyArrow version: 3.0.0"
https://github.com/huggingface/datasets/issues/2554,Multilabel metrics not supported,"['Hi @GuillemGSubies, thanks for reporting.\r\n\r\nI have made a PR to fix this issue and allow metrics to be computed also for multilabel classification problems.'
 'Looks nice, thank you very much! 🚀 ']","When I try to use a metric like F1 macro I get the following error:

```
TypeError: int() argument must be a string, a bytes-like object or a number, not 'list'
```
There is an explicit casting here:

https://github.com/huggingface/datasets/blob/fc79f61cbbcfa0e8c68b28c0a8257f17e768a075/src/datasets/features.py#L274

And looks like this is because here

https://github.com/huggingface/datasets/blob/fc79f61cbbcfa0e8c68b28c0a8257f17e768a075/metrics/f1/f1.py#L88

the features can only be integers, so we cannot use that F1 for multilabel. Instead, if I create the following F1 (ints replaced with sequence of ints), it will work:

```python
class F1(datasets.Metric):
    def _info(self):
        return datasets.MetricInfo(
            description=_DESCRIPTION,
            citation=_CITATION,
            inputs_description=_KWARGS_DESCRIPTION,
            features=datasets.Features(
                {
                    ""predictions"": datasets.Sequence(datasets.Value(""int32"")),
                    ""references"": datasets.Sequence(datasets.Value(""int32"")),
                }
            ),
            reference_urls=[""https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html""],
        )

    def _compute(self, predictions, references, labels=None, pos_label=1, average=""binary"", sample_weight=None):
        return {
            ""f1"": f1_score(
                references,
                predictions,
                labels=labels,
                pos_label=pos_label,
                average=average,
                sample_weight=sample_weight,
            ),
        }
```
"
https://github.com/huggingface/datasets/issues/2553,"load_dataset(""web_nlg"") NonMatchingChecksumError","[""Hi ! Thanks for reporting. This is due to the WebNLG repository that got updated today.\r\nI just pushed a fix at #2558 - this shouldn't happen anymore in the future.""
 ""This is fixed on `master` now :)\r\nWe'll do a new release soon !""]","Hi! It seems the WebNLG dataset gives a NonMatchingChecksumError.

## Steps to reproduce the bug

```python
from datasets import load_dataset
dataset = load_dataset('web_nlg', name=""release_v3.0_en"", split=""dev"")
```

Gives

```
NonMatchingChecksumError: Checksums didn't match for dataset source files:
['https://gitlab.com/shimorina/webnlg-dataset/-/archive/master/webnlg-dataset-master.zip']
```

## Environment info
- `datasets` version: 1.8.0
- Platform: macOS-11.3.1-x86_64-i386-64bit
- Python version: 3.9.4
- PyArrow version: 3.0.0

Also tested on Linux, with python 3.6.8"
https://github.com/huggingface/datasets/issues/2552,Keys should be unique error on code_search_net,"[""Two questions:\r\n- with `datasets-cli env` we don't have any information on the dataset script version used. Should we give access to this somehow? Either as a note in the Error message or as an argument with the name of the dataset to `datasets-cli env`?\r\n- I don't really understand why the id is duplicated in the code of `code_search_net`, how can I debug this actually?""
 'Thanks for reporting. There was indeed an issue with the keys. The key was the addition of the file id and row id, which resulted in collisions. I just opened a PR to fix this at https://github.com/huggingface/datasets/pull/2555\r\n\r\nTo help users debug this kind of errors we could try to show a message like this\r\n```python\r\nDuplicateKeysError: both 42th and 1337th examples have the same keys `48`.\r\nPlease fix the dataset script at <path/to/the/dataset/script>\r\n```\r\n\r\nThis way users who what to look for if they want to debug this issue. I opened an issue to track this: https://github.com/huggingface/datasets/issues/2556'
 'and are we sure there are not a lot of datasets which are now broken with this change?'
 ""Thanks to the dummy data, we know for sure that most of them work as expected.\r\n`code_search_net` wasn't caught because the dummy data only have one dummy data file while the dataset script can actually load several of them using `os.listdir`. Let me take a look at all the other datasets that use `os.listdir` to see if the keys are alright""
 'I found one issue on `fever` (PR here: https://github.com/huggingface/datasets/pull/2557)\r\nAll the other ones seem fine :)'
 ""Hi! Got same error when loading other dataset:\r\n```python3\r\nload_dataset('wikicorpus', 'raw_en')\r\n```\r\n\r\ntb:\r\n```pytb\r\n---------------------------------------------------------------------------\r\nDuplicatedKeysError                       Traceback (most recent call last)\r\n/opt/conda/lib/python3.8/site-packages/datasets/builder.py in _prepare_split(self, split_generator)\r\n   1109                     example = self.info.features.encode_example(record)\r\n-> 1110                     writer.write(example, key)\r\n   1111             finally:\r\n\r\n/opt/conda/lib/python3.8/site-packages/datasets/arrow_writer.py in write(self, example, key, writer_batch_size)\r\n    341             if self._check_duplicates:\r\n--> 342                 self.check_duplicate_keys()\r\n    343                 # Re-intializing to empty list for next batch\r\n\r\n/opt/conda/lib/python3.8/site-packages/datasets/arrow_writer.py in check_duplicate_keys(self)\r\n    352             if hash in tmp_record:\r\n--> 353                 raise DuplicatedKeysError(key)\r\n    354             else:\r\n\r\nDuplicatedKeysError: FAILURE TO GENERATE DATASET !\r\nFound duplicate Key: 519\r\nKeys should be unique and deterministic in nature\r\n```\r\n\r\nVersion: datasets==1.11.0""
 'Fixed by #2555.'
 ""The wikicorpus issue has been fixed by https://github.com/huggingface/datasets/pull/2844\r\n\r\nWe'll do a new release of `datasets` soon :)""]","## Describe the bug
Loading `code_search_net` seems not possible at the moment.

## Steps to reproduce the bug
```python
>>> load_dataset('code_search_net')
Downloading: 8.50kB [00:00, 3.09MB/s]                                                                                                                                           
Downloading: 19.1kB [00:00, 10.1MB/s]                                                                                                                                           
No config specified, defaulting to: code_search_net/all
Downloading and preparing dataset code_search_net/all (download: 4.77 GiB, generated: 5.99 GiB, post-processed: Unknown size, total: 10.76 GiB) to /Users/thomwolf/.cache/huggingface/datasets/code_search_net/all/1.0.0/b3e8278faf5d67da1d06981efbeac3b76a2900693bd2239bbca7a4a3b0d6e52a...
Traceback (most recent call last):         
  File ""/Users/thomwolf/Documents/GitHub/datasets/src/datasets/builder.py"", line 1067, in _prepare_split
    writer.write(example, key)
  File ""/Users/thomwolf/Documents/GitHub/datasets/src/datasets/arrow_writer.py"", line 343, in write
    self.check_duplicate_keys()
  File ""/Users/thomwolf/Documents/GitHub/datasets/src/datasets/arrow_writer.py"", line 354, in check_duplicate_keys
    raise DuplicatedKeysError(key)
datasets.keyhash.DuplicatedKeysError: FAILURE TO GENERATE DATASET !
Found duplicate Key: 48
Keys should be unique and deterministic in nature
```

## Environment info
- `datasets` version: 1.8.1.dev0
- Platform: macOS-10.15.7-x86_64-i386-64bit
- Python version: 3.8.5
- PyArrow version: 2.0.0
"
https://github.com/huggingface/datasets/issues/2549,Handling unlabeled datasets,"['Hi @nelson-liu,\r\n\r\nYou can pass the parameter `features` to `load_dataset`: https://huggingface.co/docs/datasets/_modules/datasets/load.html#load_dataset\r\n\r\nIf you look at the code of the MNLI script you referred in your question (https://github.com/huggingface/datasets/blob/master/datasets/multi_nli/multi_nli.py#L62-L77), you can see how the Features were originally specified. \r\n\r\nFeel free to use it as a template, customize it and pass it to `load_dataset` using the parameter `features`.'
 'ah got it, thanks!']","Hi!

Is there a way for datasets to produce unlabeled instances (e.g., the `ClassLabel` can be nullable).

For example, I want to use the MNLI dataset reader ( https://github.com/huggingface/datasets/blob/master/datasets/multi_nli/multi_nli.py ) on a file that doesn't have the `gold_label` field. I tried setting `""label"": data.get(""gold_label"")`, but got the following error:

```
  File ""/home/nfliu/miniconda3/envs/debias/lib/python3.7/site-packages/datasets/load.py"", line 748, in load_dataset
    use_auth_token=use_auth_token,
  File ""/home/nfliu/miniconda3/envs/debias/lib/python3.7/site-packages/datasets/builder.py"", line 575, in download_and_prepare
    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
  File ""/home/nfliu/miniconda3/envs/debias/lib/python3.7/site-packages/datasets/builder.py"", line 652, in _download_and_prepare
    self._prepare_split(split_generator, **prepare_split_kwargs)
  File ""/home/nfliu/miniconda3/envs/debias/lib/python3.7/site-packages/datasets/builder.py"", line 989, in _prepare_split
    example = self.info.features.encode_example(record)
  File ""/home/nfliu/miniconda3/envs/debias/lib/python3.7/site-packages/datasets/features.py"", line 953, in encode_example
    return encode_nested_example(self, example)
  File ""/home/nfliu/miniconda3/envs/debias/lib/python3.7/site-packages/datasets/features.py"", line 848, in encode_nested_example
    k: encode_nested_example(sub_schema, sub_obj) for k, (sub_schema, sub_obj) in utils.zip_dict(schema, obj)
  File ""/home/nfliu/miniconda3/envs/debias/lib/python3.7/site-packages/datasets/features.py"", line 848, in <dictcomp>
    k: encode_nested_example(sub_schema, sub_obj) for k, (sub_schema, sub_obj) in utils.zip_dict(schema, obj)
  File ""/home/nfliu/miniconda3/envs/debias/lib/python3.7/site-packages/datasets/features.py"", line 875, in encode_nested_example
    return schema.encode_example(obj)
  File ""/home/nfliu/miniconda3/envs/debias/lib/python3.7/site-packages/datasets/features.py"", line 653, in encode_example
    if not -1 <= example_data < self.num_classes:
TypeError: '<=' not supported between instances of 'int' and 'NoneType'
```

What's the proper way to handle reading unlabeled datasets, especially for downstream usage with Transformers?"
https://github.com/huggingface/datasets/issues/2548,Field order issue in loading json,"['Hi @luyug, thanks for reporting.\r\n\r\nThe good news is that we fixed this issue only 9 days ago: #2507.\r\n\r\nThe patch is already in the master branch of our repository and it will be included in our next `datasets` release version 1.9.0.\r\n\r\nFeel free to reopen the issue if the problem persists.']","## Describe the bug
The `load_dataset` function expects columns in alphabetical order when loading json files.

Similar bug was previously reported for csv in #623 and fixed in #684.
## Steps to reproduce the bug

For a json file `j.json`,
```
{""c"":321, ""a"": 1, ""b"": 2}
```
Running the following,
```
f= datasets.Features({'a': Value('int32'), 'b': Value('int32'), 'c': Value('int32')})
json_data = datasets.load_dataset('json', data_files='j.json', features=f)
```


## Expected results
A successful load.
## Actual results
```
File ""pyarrow/table.pxi"", line 1409, in pyarrow.lib.Table.cast
ValueError: Target schema's field names are not matching the table's field names: ['c', 'a', 'b'], ['a', 'b', 'c']
```

## Environment info
- `datasets` version: 1.8.0
- Platform: Linux-3.10.0-957.1.3.el7.x86_64-x86_64-with-glibc2.10
- Python version: 3.8.8
- PyArrow version: 3.0.0

"
https://github.com/huggingface/datasets/issues/2547,Dataset load_from_disk is too slow,"[""Hi ! It looks like an issue with the virtual disk you are using.\r\n\r\nWe load datasets using memory mapping. In general it makes it possible to load very big files instantaneously since it doesn't have to read the file (it just assigns virtual memory to the file on disk).\r\nHowever there happens to be issues with virtual disks (for example on spot instances), for which memory mapping does a pass over the entire file, and this takes a while. We are discussing about this issue here: #2252 \r\n\r\nMemory mapping is something handled by the OS so we can't do much about it, though we're still trying to figure out what's causing this behavior exactly to see what we can do.""
 ""Okay, that's exactly my case, with spot instances... Therefore this isn't something we can change in any way to be able to load the dataset faster? I mean, what do you do internally at huggingface for being able to use spot instances with datasets efficiently?""
 ""There are no solutions yet unfortunately.\r\nWe're still trying to figure out a way to make the loading instantaneous on such disks, I'll keep you posted""]","@lhoestq 
## Describe the bug
It's not normal that I have to wait 7-8 hours for a dataset to be loaded from disk, as there are no preprocessing steps, it's only loading it with load_from_disk. I have 96 cpus, however only 1 is used for this, which is inefficient. Moreover, its usage is at 1%... This is happening in the context of a language model training, therefore I'm wasting 100$ each time I have to load the dataset from disk again (because the spot instance was stopped by aws and I need to relaunch it for example). 

## Steps to reproduce the bug
Just get the oscar in  spanish (around 150GGB) and try to first save in disk and then load the processed dataset. It's not dependent on the task you're doing, it just depends on the size of the text dataset.

## Expected results
I expect the dataset to be loaded in a normal time, by using the whole machine for loading it, I mean if you store the dataset in multiple files (.arrow) and then load it from multiple files, you can use multiprocessing for that and therefore don't waste so much time. 


## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: 1.8.0
- Platform: Ubuntu 18
- Python version: 3.8


I've seen you're planning to include a streaming mode for load_dataset, but that only saves the downloading and processing time, that's not being a problem for me, you cannot save the pure loading from disk time, therefore that's not a solution for my use case or for anyone who wants to use your library for training a language model. "
https://github.com/huggingface/datasets/issues/2543,switching some low-level log.info's to log.debug?,"['Hi @stas00, thanks for pointing out this issue with logging.\r\n\r\nI agree that `datasets` can sometimes be too verbose... I can create a PR and we could discuss there the choice of the log levels for different parts of the code.']","In https://github.com/huggingface/transformers/pull/12276 we are now changing the examples to have `datasets` on the same log level as `transformers`, so that one setting can do a consistent logging across all involved components.

The trouble is that now we get a ton of these:

```
06/23/2021 12:15:31 - INFO - datasets.utils.filelock -   Lock 139627640431136 acquired on /home/stas/.cache/huggingface/metrics/sacrebleu/default/default_experiment-1-0.arrow.lock
06/23/2021 12:15:31 - INFO - datasets.arrow_writer -   Done writing 50 examples in 12280 bytes /home/stas/.cache/huggingface/metrics/sacrebleu/default/default_experiment-1-0.arrow.
06/23/2021 12:15:31 - INFO - datasets.arrow_dataset -   Set __getitem__(key) output type to python objects for no columns  (when key is int or slice) and don't output other (un-formatted) columns.
06/23/2021 12:15:31 - INFO - datasets.utils.filelock -   Lock 139627640431136 released on /home/stas/.cache/huggingface/metrics/sacrebleu/default/default_experiment-1-0.arrow.lock
```

May I suggest that these can be `log.debug` as it's no informative to the user.

More examples: these are not informative - too much information:
```
06/23/2021 12:14:26 - INFO - datasets.load -   Checking /home/stas/.cache/huggingface/datasets/downloads/459933f1fe47711fad2f6ff8110014ff189120b45ad159ef5b8e90ea43a174fa.e23e7d1259a8c6274a82a42a8936dd1b87225302c6dc9b7261beb3bc2daac640.py for additional imports.
06/23/2021 12:14:27 - INFO - datasets.builder -   Constructing Dataset for split train, validation, test, from /home/stas/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a

```

While these are:
```
06/23/2021 12:14:27 - INFO - datasets.info -   Loading Dataset Infos from /home/stas/.cache/huggingface/modules/datasets_modules/datasets/wmt16/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a
06/23/2021 12:14:27 - WARNING - datasets.builder -   Reusing dataset wmt16 (/home/stas/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a)
```

I also realize that `transformers` examples don't have do use `info` for `datasets` to let the default `warning` keep logging to less noisy.

But I think currently the log levels are slightly misused and skewed by 1 level. Many `warnings` will better be `info`s and most `info`s be `debug`.

e.g.:

```
06/23/2021 12:14:27 - WARNING - datasets.builder -   Reusing dataset wmt16 (/home/stas/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a)
```

why is this a warning? it is informing me that the cache is used, there is nothing to be worried about. I'd have it as `info`.

Warnings are typically something that's bordering error or the first thing to check when things don't work as expected.

infrequent info is there to inform of the different stages or important events.

Everything else is debug.

At least the way I understand things. 
"
https://github.com/huggingface/datasets/issues/2542,`datasets.keyhash.DuplicatedKeysError` for `drop` and `adversarial_qa/adversarialQA`,"['very much related: https://github.com/huggingface/datasets/pull/2333'
 'Hi @VictorSanh, thank you for reporting this issue with duplicated keys.\r\n\r\n- The issue with ""adversarial_qa"" was fixed 23 days ago: #2433. Current version of `datasets` (1.8.0) includes the patch.\r\n- I am investigating the issue with `drop`. I\'ll ping you to keep you informed.'
 'Hi @VictorSanh, the issue is already fixed and merged into master branch and will be included in our next release version 1.9.0.'
 'thank you!']","## Describe the bug
Failure to generate the datasets (`drop` and subset `adversarialQA` from `adversarial_qa`) because of duplicate keys.

## Steps to reproduce the bug
```python
from datasets import load_dataset
load_dataset(""drop"")
load_dataset(""adversarial_qa"", ""adversarialQA"")
```

## Expected results
The examples keys should be unique.

## Actual results
```bash
>>> load_dataset(""drop"")
Using custom data configuration default
Downloading and preparing dataset drop/default (download: 7.92 MiB, generated: 111.88 MiB, post-processed: Unknown size, total: 119.80 MiB) to /home/hf/.cache/huggingface/datasets/drop/default/0.1.0/7a94f1e2bb26c4b5c75f89857c06982967d7416e5af935a9374b9bccf5068026...
Traceback (most recent call last):         
  File ""<stdin>"", line 1, in <module>
  File ""/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/load.py"", line 751, in load_dataset
    use_auth_token=use_auth_token,
  File ""/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/builder.py"", line 575, in download_and_prepare
    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
  File ""/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/builder.py"", line 652, in _download_and_prepare
    self._prepare_split(split_generator, **prepare_split_kwargs)
  File ""/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/builder.py"", line 992, in _prepare_split
    num_examples, num_bytes = writer.finalize()
  File ""/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/arrow_writer.py"", line 409, in finalize
    self.check_duplicate_keys()
  File ""/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/arrow_writer.py"", line 349, in check_duplicate_keys
    raise DuplicatedKeysError(key)
datasets.keyhash.DuplicatedKeysError: FAILURE TO GENERATE DATASET !
Found duplicate Key: 28553293-d719-441b-8f00-ce3dc6df5398
Keys should be unique and deterministic in nature
```

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: 1.7.0
- Platform: Linux-5.4.0-1044-gcp-x86_64-with-Ubuntu-18.04-bionic
- Python version: 3.7.10
- PyArrow version: 3.0.0
"
https://github.com/huggingface/datasets/issues/2538,Loading partial dataset when debugging,"['Hi ! `load_dataset` downloads the full dataset once and caches it, so that subsequent calls to `load_dataset` just reloads the dataset from your disk.\r\nThen when you specify a `split` in `load_dataset`, it will just load the requested split from the disk. If your specified split is a sliced split (e.g. `""train[:10]""`), then it will load the 10 first rows of the train split that you have on disk.\r\n\r\nTherefore, as long as you don\'t delete your cache, all your calls to `load_dataset` will be very fast. Except the first call that downloads the dataset of course ^^'
 'That’s a use case for the new streaming feature, no?'
 'Hi @reachtarunhere.\r\n\r\nBesides the above insights provided by @lhoestq and @thomwolf, there is also a Dataset feature in progress (I plan to finish it this week): #2249, which will allow you, when calling `load_dataset`, to pass the option to download/preprocess/cache only some specific split(s), which will definitely speed up your workflow.\r\n\r\nIf this feature is interesting for you, I can ping you once it will be merged into the master branch.'
 'Thanks all for responding.\r\n\r\nHey @albertvillanova \r\n\r\nThanks. Yes, I would be interested.\r\n\r\n@lhoestq I think even if a small split is specified it loads up the full dataset from the disk (please correct me if this is not the case). Because it does seem to be slow to me even on subsequent calls. There is no repeated downloading so it seems that the cache is working.\r\n\r\nI am not aware of the streaming feature @thomwolf mentioned. So I might need to read up on it.'
 '@reshinthadithyan  I use the .select function to have a fraction of indices.']","I am using PyTorch Lightning along with datasets (thanks for so many datasets already prepared and the great splits). 

Every time I execute load_dataset  for the imdb dataset it takes some time even if I specify a split involving very few samples. I guess this due to hashing as per the other issues.

Is there a way to only load part of the dataset on load_dataset? This would really speed up my workflow.
Something like a debug mode would really help. Thanks!"
https://github.com/huggingface/datasets/issues/2536,Use `Audio` features for `AutomaticSpeechRecognition` task template,"[""I'm just retaking and working on #2324. 😉 ""]","In #2533 we added a task template for speech recognition that relies on the file paths to the audio files. As pointed out by @SBrandeis this is brittle as it doesn't port easily across different OS'. 

The solution is to use dedicated `Audio` features when casting the dataset. These features are not yet available in `datasets`, but should be included in the `AutomaticSpeechRecognition` template once they are."
https://github.com/huggingface/datasets/issues/2532,Tokenizer's normalization preprocessor cause misalignment in return_offsets_mapping for tokenizer classification task,"['Hi @jerryIsHere, thanks for reporting the issue. But are you sure this is a bug in HuggingFace **Datasets**?'
 '> Hi @jerryIsHere, thanks for reporting the issue. But are you sure this is a bug in HuggingFace **Datasets**?\r\n\r\nOh, I am sorry\r\nI would reopen the post on huggingface/transformers']","[This colab notebook](https://colab.research.google.com/drive/151gKyo0YIwnlznrOHst23oYH_a3mAe3Z?usp=sharing) implements a token classification input pipeline extending the logic from [this hugging example](https://huggingface.co/transformers/custom_datasets.html#tok-ner).

The pipeline works fine with most instance in different languages, but unfortunately, [the Japanese Kana ligature (a form of abbreviation? I don't know Japanese well)](https://en.wikipedia.org/wiki/Kana_ligature) break the alignment of `return_offsets_mapping`:
![image](https://user-images.githubusercontent.com/50871412/122904371-db192700-d382-11eb-8917-1775db76db69.png)

Without the try catch block, it riase `ValueError: NumPy boolean array indexing assignment cannot assign 88 input values to the 87 output values where the mask is true`, example shown here [(another colab notebook)](https://colab.research.google.com/drive/1MmOqf3ppzzdKKyMWkn0bJy6DqzOO0SSm?usp=sharing)

It is clear that the normalizer is the process that break the alignment, as it is observed that `tokenizer._tokenizer.normalizer.normalize_str('ヿ')` return 'コト'.

One workaround is to include `tokenizer._tokenizer.normalizer.normalize_str` before the tokenizer preprocessing pipeline, which is also provided in the [first colab notebook](https://colab.research.google.com/drive/151gKyo0YIwnlznrOHst23oYH_a3mAe3Z?usp=sharing) with the name `udposTestDatasetWorkaround`.

I guess similar logics should be included inside the tokenizer and the offsets_mapping generation process such that user don't need to include them in their code. But I don't understand the code of tokenizer well that I think I am not able to do this.

p.s.
**I am using my own dataset building script in the provided example, but the script should be equivalent to the changes made by this [update](https://github.com/huggingface/datasets/pull/2466)**
`get_dataset `is just a simple wrapping for `load_dataset`
and the `tokenizer` is just `XLMRobertaTokenizerFast.from_pretrained(""xlm-roberta-large"")`"
https://github.com/huggingface/datasets/issues/2528,Logging cannot be set to NOTSET similar to transformers,"['Hi @joshzwiebel, thanks for reporting. We are going to align with `transformers`.']","## Describe the bug
In the transformers library you can set the verbosity level to logging.NOTSET to work around the usage of tqdm and IPywidgets, however in Datasets this is no longer possible. This is because transformers set the verbosity level of tqdm with [this](https://github.com/huggingface/transformers/blob/b53bc55ba9bb10d5ee279eab51a2f0acc5af2a6b/src/transformers/file_utils.py#L1449) 
`disable=bool(logging.get_verbosity() == logging.NOTSET)`
and datasets accomplishes this like [so](https://github.com/huggingface/datasets/blob/83554e410e1ab8c6f705cfbb2df7953638ad3ac1/src/datasets/utils/file_utils.py#L493)
`not_verbose = bool(logger.getEffectiveLevel() > WARNING)`
## Steps to reproduce the bug
```python
import datasets
import logging
datasets.logging.get_verbosity = lambda : logging.NOTSET
datasets.load_dataset(""patrickvonplaten/librispeech_asr_dummy"")
```

## Expected results
The code should download and load the dataset as normal without displaying progress bars

## Actual results
```ImportError                               Traceback (most recent call last)
<ipython-input-4-aec65c0509c6> in <module>
----> 1 datasets.load_dataset(""patrickvonplaten/librispeech_asr_dummy"")

~/venv/lib/python3.7/site-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, script_version, use_auth_token, task, **config_kwargs)
    713         dataset=True,
    714         return_resolved_file_path=True,
--> 715         use_auth_token=use_auth_token,
    716     )
    717     # Set the base path for downloads as the parent of the script location

~/venv/lib/python3.7/site-packages/datasets/load.py in prepare_module(path, script_version, download_config, download_mode, dataset, force_local_path, dynamic_modules_path, return_resolved_file_path, **download_kwargs)
    350                     file_path = hf_bucket_url(path, filename=name, dataset=False)
    351                 try:
--> 352                     local_path = cached_path(file_path, download_config=download_config)
    353                 except FileNotFoundError:
    354                     raise FileNotFoundError(

~/venv/lib/python3.7/site-packages/datasets/utils/file_utils.py in cached_path(url_or_filename, download_config, **download_kwargs)
    289             use_etag=download_config.use_etag,
    290             max_retries=download_config.max_retries,
--> 291             use_auth_token=download_config.use_auth_token,
    292         )
    293     elif os.path.exists(url_or_filename):

~/venv/lib/python3.7/site-packages/datasets/utils/file_utils.py in get_from_cache(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, use_auth_token)
    668                     headers=headers,
    669                     cookies=cookies,
--> 670                     max_retries=max_retries,
    671                 )
    672 

~/venv/lib/python3.7/site-packages/datasets/utils/file_utils.py in http_get(url, temp_file, proxies, resume_size, headers, cookies, timeout, max_retries)
    493         initial=resume_size,
    494         desc=""Downloading"",
--> 495         disable=not_verbose,
    496     )
    497     for chunk in response.iter_content(chunk_size=1024):

~/venv/lib/python3.7/site-packages/tqdm/notebook.py in __init__(self, *args, **kwargs)
    217         total = self.total * unit_scale if self.total else self.total
    218         self.container = self.status_printer(
--> 219             self.fp, total, self.desc, self.ncols)
    220         self.sp = self.display
    221 

~/venv/lib/python3.7/site-packages/tqdm/notebook.py in status_printer(_, total, desc, ncols)
     95         if IProgress is None:  # #187 #451 #558 #872
     96             raise ImportError(
---> 97                 ""IProgress not found. Please update jupyter and ipywidgets.""
     98                 "" See https://ipywidgets.readthedocs.io/en/stable""
     99                 ""/user_install.html"")

ImportError: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
```
## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: 1.8.0
- Platform: Linux-5.4.95-42.163.amzn2.x86_64-x86_64-with-debian-10.8
- Python version: 3.7.10
- PyArrow version: 3.0.0
I am running this code on Deepnote and which important to this issue **does not** support IPywidgets

"
https://github.com/huggingface/datasets/issues/2522,Documentation Mistakes in Dataset: emotion,"['Hi,\r\n\r\nthis issue has been already reported in the dataset repo (https://github.com/dair-ai/emotion_dataset/issues/2), so this is a bug on their side.']","As per documentation,
Dataset: emotion
Homepage: https://github.com/dair-ai/emotion_dataset

Dataset: https://github.com/huggingface/datasets/blob/master/datasets/emotion/emotion.py

Permalink: https://huggingface.co/datasets/viewer/?dataset=emotion

Emotion is a dataset of English Twitter messages with eight basic emotions: anger, anticipation, disgust, fear, joy, sadness, surprise, and trust. For more detailed information please refer to the paper.

But when we view the data, there are only 6 emotions, anger, fear, joy, sadness, surprise, and trust."
https://github.com/huggingface/datasets/issues/2516,datasets.map pickle issue resulting in invalid mapping function,"[""Hi ! `map` calls `__getstate__` using `dill` to hash your map function. This is used by the caching mechanism to recover previously computed results. That's why you don't see any `__setstate__` call.\r\n\r\nWhy do you change an attribute of your tokenizer when `__getstate__` is called ?""
 '@lhoestq because if I try to pickle my custom tokenizer (it contains a pure python pretokenization step in an otherwise rust backed tokenizer) I get\r\n\r\n> Exception: Error while attempting to pickle Tokenizer: Custom PreTokenizer cannot be serialized\r\n\r\nSo I remove the Custom PreTokenizer in `__getstate__` and then restore it in `__setstate__` (since it doesn\'t contain any state). This is what my `__getstate__`  / `__setstate__` looks like:\r\n\r\n    def __getstate__(self):\r\n        """"""\r\n        Removes pre_tokenizer since it cannot be pickled\r\n        """"""\r\n        logger.debug(""Copy state dict"")\r\n        out = self.__dict__.copy()\r\n        logger.debug(""Detaching pre_tokenizer"")\r\n        out[\'_tokenizer\'].pre_tokenizer = tokenizers.pre_tokenizers.Sequence([]) \r\n        return out\r\n\r\n    def __setstate__(self, d):\r\n        """"""\r\n        Reinstates pre_tokenizer\r\n        """"""\r\n        logger.debug(""Reattaching pre_tokenizer"")\r\n        self.__dict__ = d\r\n        self.backend_tokenizer.pre_tokenizer = self._pre_tokenizer()\r\n\r\nIf this is the case can you think of another way of avoiding my issue?'
 ""Actually, maybe I need to deep copy `self.__dict__`? That way `self` isn't modified. That was my intention and I thought it was working - I'll double-check after the weekend.""
 'Doing a deep copy results in the warning:\r\n\r\n> 06/20/2021 16:02:15 - WARNING - datasets.fingerprint -   Parameter \'function\'=<function tokenize_function at 0x7f1e95f05d40> of the transform datasets.arrow_dataset.Dataset._map_single couldn\'t be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won\'t be showed.\r\n\r\n\r\n```\r\ndef __getstate__(self):\r\n    """"""\r\n    Removes pre_tokenizer since it cannot be pickled\r\n    """"""\r\n    logger.debug(""Copy state dict"")\r\n    out = copy.deepcopy(self.__dict__)\r\n    logger.debug(""Detaching pre_tokenizer"")\r\n    out[\'_tokenizer\'].pre_tokenizer = tokenizers.pre_tokenizers.Sequence([]) \r\n    return out\r\n```'
 'Looks like there is still an object that is not pickable in your `tokenize_function` function.\r\n\r\nYou can test if an object can be pickled and hashed by using \r\n```python\r\nfrom datasets.fingerprint import Hasher\r\n\r\nHasher.hash(my_object)\r\n```\r\n\r\nUnder the hood it pickles the object to compute its hash, so it calls `__getstate__` when applicable.'
 'I figured it out, the problem is deep copy itself uses pickle (unless you implement `__deepcopy__`). So when I changed `__getstate__` it started throwing an error.\r\n\r\nI\'m sure there\'s a better way of doing this, but in order to return the `__dict__` without the non-pikelable pre-tokeniser and without modifying self I removed the pre-tokenizers, did a deep copy and then re-generated it.\r\n\r\nIt does work -  although I noticed Hasher doesn\'t call `__hash__` if the object being hashed implements it which I feel it should? If it did I could return a hash of the tokenizers.json file instead.\r\n\r\n```\r\n    def __getstate__(self):\r\n        """"""\r\n        Removes pre_tokenizer since it cannot be pickled\r\n        """"""\r\n        logger.debug(""Copy state dict"")\r\n        self.backend_tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Sequence([])\r\n        out = copy.deepcopy(self.__dict__)  #self.__dict__.copy()\r\n        self.backend_tokenizer.pre_tokenizer = self._pre_tokenizer()\r\n\r\n        return out\r\n```\r\n'
 ""I'm glad you figured something out :)\r\n\r\nRegarding hashing: we're not using hashing for the same purpose as the python `__hash__` purpose (which is in general for dictionary lookups). For example it is allowed for python hashing to not return the same hash across sessions, while our hashing must return the same hashes across sessions for the caching to work properly.""]","I trained my own tokenizer, and I needed to use a python custom class. Because of this I have to detach the custom step before saving and reattach after restore. I did this using the standard pickle `__get_state__` / `__set_state__` mechanism. I think it's correct but it fails when I use it inside a function which is mapped to a dataset, i.e. in the manner of run_mlm.py and other huggingface scripts.

The following reproduces the issue - most likely I'm missing something

A simulated tokeniser which can be pickled

```
class CustomTokenizer:
    def __init__(self):
        self.state = ""init""

    def __getstate__(self):
        print(""__getstate__ called"")
        out = self.__dict__.copy()
        self.state = ""pickled""
        return out
    
    def __setstate__(self, d):
        print(""__setstate__ called"")
        self.__dict__ = d
        self.state = ""restored""

tokenizer = CustomTokenizer()
```

Test that it actually works - prints ""__getstate__ called"" and ""__setstate__ called""
```
import pickle
serialized = pickle.dumps(tokenizer)
restored = pickle.loads(serialized)
assert restored.state == ""restored""
```

Simulate a function that tokenises examples, when dataset.map is called, this function 
```
def tokenize_function(examples):
    assert tokenizer.state == ""restored"" # this shouldn't fail but it does
    output = tokenizer(examples)           # this will fail as tokenizer isn't really a tokenizer
    return output
```

Use map to simulate tokenization
```
import glob
from datasets import load_dataset

assert tokenizer.state == ""restored""
train_files = glob.glob('train*.csv')
validation_files = glob.glob('validation*.csv')
datasets = load_dataset(""csv"", data_files=dict(train=train_files, validation=validation_files))

tokenized_datasets = datasets.map(
    tokenize_function,
    batched=True,
)
```

What's happening is I can see that __getstate__ is called but not __setstate__, so the state of `tokenize_function` is invalid at the point that it's actually executed. This doesn't matter as far as I can see for the standard tokenizers as they don't use __getstate__ / __setstate__. I'm not sure if there's another hook I'm supposed to implement as well?

---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-22-a2aef4f74aaa> in <module>
      8 tokenized_datasets = datasets.map(
      9     tokenize_function,
---> 10     batched=True,
     11 )

~/.pyenv/versions/3.7.6/envs/xxx/lib/python3.7/site-packages/datasets/dataset_dict.py in map(self, function, with_indices, input_columns, batched, batch_size, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)
    487                     desc=desc,
    488                 )
--> 489                 for k, dataset in self.items()
    490             }
    491         )

~/.pyenv/versions/3.7.6/envs/xxx/lib/python3.7/site-packages/datasets/dataset_dict.py in <dictcomp>(.0)
    487                     desc=desc,
    488                 )
--> 489                 for k, dataset in self.items()
    490             }
    491         )

~/.pyenv/versions/3.7.6/envs/xxx/lib/python3.7/site-packages/datasets/arrow_dataset.py in map(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)
   1633                 fn_kwargs=fn_kwargs,
   1634                 new_fingerprint=new_fingerprint,
-> 1635                 desc=desc,
   1636             )
   1637         else:

~/.pyenv/versions/3.7.6/envs/xxx/lib/python3.7/site-packages/datasets/arrow_dataset.py in wrapper(*args, **kwargs)
    184         }
    185         # apply actual function
--> 186         out: Union[""Dataset"", ""DatasetDict""] = func(self, *args, **kwargs)
    187         datasets: List[""Dataset""] = list(out.values()) if isinstance(out, dict) else [out]
    188         # re-apply format to the output

~/.pyenv/versions/3.7.6/envs/xxx/lib/python3.7/site-packages/datasets/fingerprint.py in wrapper(*args, **kwargs)
    395             # Call actual function
    396 
--> 397             out = func(self, *args, **kwargs)
    398 
    399             # Update fingerprint of in-place transforms + update in-place history of transforms

~/.pyenv/versions/3.7.6/envs/xxx/lib/python3.7/site-packages/datasets/arrow_dataset.py in _map_single(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, desc)
   1961                                 indices,
   1962                                 check_same_num_examples=len(input_dataset.list_indexes()) > 0,
-> 1963                                 offset=offset,
   1964                             )
   1965                         except NumExamplesMismatch:

~/.pyenv/versions/3.7.6/envs/xxx/lib/python3.7/site-packages/datasets/arrow_dataset.py in apply_function_on_filtered_inputs(inputs, indices, check_same_num_examples, offset)
   1853                 effective_indices = [i + offset for i in indices] if isinstance(indices, list) else indices + offset
   1854             processed_inputs = (
-> 1855                 function(*fn_args, effective_indices, **fn_kwargs) if with_indices else function(*fn_args, **fn_kwargs)
   1856             )
   1857             if update_data is None:

<ipython-input-21-8ee4a8ba5b1b> in tokenize_function(examples)
      1 def tokenize_function(examples):
----> 2     assert tokenizer.state == ""restored""
      3     tokenizer(examples)
      4     return examples


"
https://github.com/huggingface/datasets/issues/2514,Can datasets remove duplicated rows?,"['Hi ! For now this is probably the best option.\r\nWe might add a feature like this in the feature as well.\r\n\r\nDo you know any deduplication method that works on arbitrary big datasets without filling up RAM ?\r\nOtherwise we can have do the deduplication in memory like pandas but I feel like this is going to be limiting for some cases'
 ""Yes, I'd like to work on this feature once I'm done with #2500, but first I have to do some research, and see if the implementation wouldn't be too complex.\r\n\r\nIn the meantime, maybe [this lib](https://github.com/TomScheffers/pyarrow_ops) can help. However, note that this lib operates directly on pyarrow tables and relies only on `hash` to find duplicates (e.g. `-1` and `-2` have the same hash in Python 3, so this lib will treat them as duplicates), which doesn't make much sense.""
 ""> Hi ! For now this is probably the best option.\r\n> We might add a feature like this in the feature as well.\r\n> \r\n> Do you know any deduplication method that works on arbitrary big datasets without filling up RAM ?\r\n> Otherwise we can have do the deduplication in memory like pandas but I feel like this is going to be limiting for some cases\r\n\r\nGreat if this is can be done. Thanks!!\r\n\r\nNot sure if you are asking me. In any case I don't know of any unfortunately :( in practice if data is really large we normally do it with spark (only for info. I understand this is not useful in developing this library..)""]","**Is your feature request related to a problem? Please describe.**
i find myself more and more relying on datasets just to do all the preprocessing. One thing however, for removing duplicated rows, I couldn't find out how and am always converting datasets to pandas to do that..


**Describe the solution you'd like**
have a functionality of "" remove duplicated rows""

**Describe alternatives you've considered**
convert dataset to pandas, remove duplicate, and convert back...


**Additional context**
no"
https://github.com/huggingface/datasets/issues/2513,Corelation should be Correlation,"['Hi @colbym-MM, thanks for reporting. We are fixing it.']",https://github.com/huggingface/datasets/blob/0e87e1d053220e8ecddfa679bcd89a4c7bc5af62/metrics/matthews_correlation/matthews_correlation.py#L66
https://github.com/huggingface/datasets/issues/2512,seqeval metric does not work with a recent version of sklearn: classification_report() got an unexpected keyword argument 'output_dict',"['Sorry, I was using an old version of sequeval']","## Describe the bug
A clear and concise description of what the bug is.

## Steps to reproduce the bug
```python
from datasets import load_dataset, load_metric
seqeval = load_metric(""seqeval"")
seqeval.compute(predictions=[['A']], references=[['A']])
```

## Expected results
The function computes a dict with metrics

## Actual results
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-39-69a57f5cf06f> in <module>
      1 from datasets import load_dataset, load_metric
      2 seqeval = load_metric(""seqeval"")
----> 3 seqeval.compute(predictions=[['A']], references=[['A']])

~/p3/lib/python3.7/site-packages/datasets/metric.py in compute(self, *args, **kwargs)
    396             references = self.data[""references""]
    397             with temp_seed(self.seed):
--> 398                 output = self._compute(predictions=predictions, references=references, **kwargs)
    399 
    400             if self.buf_writer is not None:

~/.cache/huggingface/modules/datasets_modules/metrics/seqeval/81eda1ff004361d4fa48754a446ec69bb7aa9cf4d14c7215f407d1475941c5ff/seqeval.py in _compute(self, predictions, references, suffix)
     95 
     96     def _compute(self, predictions, references, suffix=False):
---> 97         report = classification_report(y_true=references, y_pred=predictions, suffix=suffix, output_dict=True)
     98         report.pop(""macro avg"")
     99         report.pop(""weighted avg"")

TypeError: classification_report() got an unexpected keyword argument 'output_dict'
```

## Environment info
sklearn=0.24
datasets=1.1.3

"
https://github.com/huggingface/datasets/issues/2511,Add C4,"[""Update on this: I'm computing the checksums of the data files. It will be available soon""
 'Added in #2575 :)']","## Adding a Dataset
- **Name:** *C4*
- **Description:** *https://github.com/allenai/allennlp/discussions/5056*
- **Paper:** *https://arxiv.org/abs/1910.10683*
- **Data:** *https://huggingface.co/datasets/allenai/c4*
- **Motivation:** *Used a lot for pretraining*

Instructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).

Should fix https://github.com/huggingface/datasets/issues/1710"
https://github.com/huggingface/datasets/issues/2508,Load Image Classification Dataset from Local ,"['Hi ! Is this folder structure a standard, a bit like imagenet ?\r\nIn this case maybe we can consider having a dataset loader for cifar-like, imagenet-like, squad-like, conll-like etc. datasets ?\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nmy_custom_cifar = load_dataset(""cifar_like"", data_dir=""path/to/data/dir"")\r\n```\r\n\r\nLet me know what you think'
 'Yep that would be sweet - closing for now as we found a workaround. '
 ""@lhoestq I think we'll want a generic `image-folder` dataset (same as 'imagenet-like'). This is like `torchvision.datasets.ImageFolder`, and is something vision folks are used to seeing.""
 ""Opening this back up, since I'm planning on tackling this. Already posted a quick version of it on my account on the hub.\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset('nateraw/image-folder', data_files='PetImages/')\r\n```""]","**Is your feature request related to a problem? Please describe.**
Yes - we would like to load an image classification dataset with datasets without having to write a custom data loader.

**Describe the solution you'd like**

Given a folder structure with images of each class in each folder, the ability to load these folders into a HuggingFace dataset like ""cifar10"".

**Describe alternatives you've considered**

Implement ViT training outside of the HuggingFace Trainer and without datasets (we did this but prefer to stay on the main path)

Write custom data loader logic

**Additional context**

We're training ViT on custom dataset
"
https://github.com/huggingface/datasets/issues/2503,SubjQA wrong boolean values in entries,"[""Hi @arnaudstiegler, thanks for reporting. I'm investigating it.""
 '@arnaudstiegler I have just checked that these mismatches are already present in the original dataset: https://github.com/megagonlabs/SubjQA\r\n\r\nWe are going to contact the dataset owners to report this.'
 'I have:\r\n- opened an issue in their repo: https://github.com/megagonlabs/SubjQA/issues/3\r\n- written an email to all the paper authors'
 'Please [see my response](https://github.com/megagonlabs/SubjQA/issues/3#issuecomment-905160010). There will be a fix in a couple of days.']","## Describe the bug
SubjQA seems to have a boolean that's consistently wrong.

It defines:
- question_subj_level: The subjectiviy level of the question (on a 1 to 5 scale with 1 being the most subjective).
- is_ques_subjective: A boolean subjectivity label derived from question_subj_level (i.e., scores below 4 are considered as subjective)

However, `is_ques_subjective` seems to have wrong values in the entire dataset.

For instance, in the example in the dataset card, we have:
- ""question_subj_level"": 2
- ""is_ques_subjective"": false

However, according to the description, the question should be subjective since the `question_subj_level` is below 4
"
https://github.com/huggingface/datasets/issues/2499, Python Programming Puzzles,"['👀 @TalSchuster'
 ""Thanks @VictorSanh!\r\nThere's also a [notebook](https://aka.ms/python_puzzles) and [demo](https://aka.ms/python_puzzles_study) available now to try out some of the puzzles""]","## Adding a Dataset
- **Name:** Python Programming Puzzles
- **Description:** Programming challenge called programming puzzles, as an objective and comprehensive evaluation of program synthesis
- **Paper:** https://arxiv.org/pdf/2106.05784.pdf
- **Data:** https://github.com/microsoft/PythonProgrammingPuzzles ([Scrolling through the data](https://github.com/microsoft/PythonProgrammingPuzzles/blob/main/problems/README.md))
- **Motivation:** Spans a large range of difficulty, problems, and domains. A useful resource for evaluation as we don't have a clear understanding of the abilities and skills of extremely large LMs.

Note: it's a growing dataset (contributions are welcome), so we'll need careful versioning for this dataset.

Instructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).
"
https://github.com/huggingface/datasets/issues/2498,Improve torch formatting performance,"['That’s interesting thanks, let’s see what we can do. Can you detail your last sentence? I’m not sure I understand it well.'
 'Hi ! I just re-ran a quick benchmark and using `to_numpy()` seems to be faster now:\r\n\r\n```python\r\nimport pyarrow as pa  # I used pyarrow 3.0.0\r\nimport numpy as np\r\n\r\nn, max_length = 1_000, 512\r\nlow, high, size = 0, 2 << 16, (n, max_length)\r\n\r\ntable = pa.Table.from_pydict({\r\n    ""input_ids"": np.random.default_rng(42).integers(low=low, high=high, size=size).tolist()\r\n})\r\n\r\n\r\n%%timeit\r\n_ = table.to_pandas()[""input_ids""].to_numpy()\r\n# 1.44 ms ± 80.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\r\n\r\n%%timeit\r\n_ = table[""input_ids""].to_pandas().to_numpy()\r\n# 461 µs ± 14.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\r\n\r\n%%timeit\r\n_ = table[""input_ids""].to_numpy()\r\n# 317 µs ± 5.06 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\r\n```\r\n\r\nCurrently the conversion from arrow to numpy is done in the NumpyArrowExtractor here:\r\n\r\nhttps://github.com/huggingface/datasets/blob/d6d0ede9486ffad7944642ca9a326e058b676788/src/datasets/formatting/formatting.py#L143-L166\r\n\r\nLet\'s update the NumpyArrowExtractor to call `to_numpy` directly and see how our github benchmarks evolve ?__'
 ""Sounds like a plan @lhoestq  If you create a PR I'll pick it up and try it out right away! ""
 '@lhoestq I can also prepare the PR, just lmk. '
 'I’m not exactly sure how to read the graph but it seems that to_categorical take a lot of time here. Could you share more informations on the features/stats of your datasets so we could maybe design a synthetic datasets that looks more similar for debugging testing?'
 'I created https://github.com/huggingface/datasets/pull/2505 if you want to play with it @vblagoje '
 '> I’m not exactly sure how to read the graph but it seems that to_categorical take a lot of time here. Could you share more informations on the features/stats of your datasets so we could maybe design a synthetic datasets that looks more similar for debugging testing?\r\n\r\n@thomwolf starting from the top, each rectangle represents the cumulative amount of it takes to execute the method call. Therefore, format_batch in torch_formatter.py takes ~20 sec, and the largest portion of that call is taken by to_pandas call and the smaller portion (grey rectangle) by the other method invocation(s) in format_batch (series_to_numpy etc). \r\n\r\nFeatures of the dataset are BERT pre-training model input columns i.e:\r\n```\r\nf = Features({            \r\n ""input_ids"": Sequence(feature=Value(dtype=""int32"")),           \r\n ""attention_mask"": Sequence(feature=Value(dtype=""int8"")),            \r\n ""token_type_ids"": Sequence(feature=Value(dtype=""int8"")),           \r\n ""labels"": Sequence(feature=Value(dtype=""int32"")),           \r\n ""next_sentence_label"": Value(dtype=""int8"")\r\n})\r\n```\r\n\r\nI\'ll work with @lhoestq till we get to the bottom of this one. \r\n '
 ""@lhoestq the proposed branch is faster, but overall training speedup is a few percentage points. I couldn't figure out how to include the GitHub branch into setup.py, so I couldn't start NVidia optimized Docker-based pre-training run.  But on bare metal, there is a slight improvement. I'll do some more performance traces. ""
 'Hi @vblagoje, to install Datasets from @lhoestq PR reference #2505, you can use:\r\n```shell\r\npip install git+ssh://git@github.com/huggingface/datasets.git@refs/pull/2505/head#egg=datasets\r\n```'
 ""Hey @albertvillanova yes thank you, I am aware, I can easily pull it from a terminal command line but then I can't automate docker image builds as dependencies are picked up from setup.py and for some reason setup.py doesn't accept this string format.""
 '@vblagoje in that case, you can add this to your `setup.py`:\r\n```python\r\n    install_requires=[\r\n        ""datasets @ git+ssh://git@github.com/huggingface/datasets.git@refs/pull/2505/head"",\r\n```'
 '@lhoestq @thomwolf @albertvillanova The new approach is definitely faster, dataloader now takes less than 3% cumulative time (pink rectangle two rectangles to the right of tensor.py backward invocation)\r\n\r\n![Screen Shot 2021-06-16 at 3 05 06 PM](https://user-images.githubusercontent.com/458335/122224432-19de4700-ce82-11eb-982f-d45d4bcc1e41.png)\r\n\r\nWhen we drill down into dataloader next invocation we get:\r\n\r\n![Screen Shot 2021-06-16 at 3 09 56 PM](https://user-images.githubusercontent.com/458335/122224976-a1c45100-ce82-11eb-8d40-59194740d616.png)\r\n\r\nAnd finally format_batch:\r\n\r\n![Screen Shot 2021-06-16 at 3 11 07 PM](https://user-images.githubusercontent.com/458335/122225132-cae4e180-ce82-11eb-8a16-967ab7c1c2aa.png)\r\n\r\n\r\nNot sure this could be further improved but this is definitely a decent step forward.\r\n\r\n'
 '> ```python\r\n> datasets @ git+ssh://git@github.com/huggingface/datasets.git@refs/pull/2505/head\r\n> ```\r\n\r\n@albertvillanova how would I replace datasets dependency in https://github.com/huggingface/transformers/blob/master/setup.py as the above approach is not working. '
 '@vblagoje I tested my proposed approach before posting it here and it worked for me. \r\n\r\nIs it not working in your case because of the SSH protocol? In that case you could try the same approach but using HTTPS:\r\n```\r\n""datasets @ git+https://github.com/huggingface/datasets.git@refs/pull/2505/head"",\r\n``` '
 'Also note the blanks before and after the `@`.'
 '@albertvillanova of course it works. Apologies. I needed to change datasets in all deps references , like [here](https://github.com/huggingface/transformers/blob/master/setup.py#L235) for example. ']","**Is your feature request related to a problem? Please describe.**
It would be great, if possible, to further improve read performance of raw encoded datasets and their subsequent conversion to torch tensors. 

A bit more background.  I am working on LM pre-training using HF ecosystem. We use encoded HF Wikipedia and BookCorpus datasets. The training machines are similar to DGX-1 workstations. We use HF trainer torch.distributed training approach on a single machine with 8 GPUs.

The current performance is about 30% slower than NVidia optimized BERT [examples](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling) baseline. Quite a bit of customized code and training loop tricks were used to achieve the baseline performance. It would be great to achieve the same performance while using nothing more than off the shelf HF ecosystem. Perhaps, in the future, with @stas00 work on deepspeed integration, it could even be exceeded. 

**Describe the solution you'd like**
Using profiling tools we've observed that appx. 25% of cumulative run time is spent on data loader next call.
![dataloader_next](https://user-images.githubusercontent.com/458335/121895543-59742a00-ccee-11eb-85fb-f07715e3f1f6.png)

As you can observe most of the data loader next call is spent in HF datasets torch_formatter.py format_batch call. 

Digging a bit deeper into format_batch we can see the following profiler data:
![torch_formatter](https://user-images.githubusercontent.com/458335/121895944-c7b8ec80-ccee-11eb-95d5-5875c5716c30.png)

Once again, a lot of time is spent in pyarrow table conversion to pandas which seems like an intermediary step. Offline @lhoestq told me that this approach was, for some unknown reason, faster than direct to numpy conversion. 

**Describe alternatives you've considered**
I am not familiar with pyarrow and have not yet considered the alternatives to the current approach. 

Most of the online advice around data loader performance improvements revolve around increasing number of workers, using pin memory for copying tensors from host device to gpus but we've already tried these avenues without much performance improvement.  Weights & Biases dashboard for the pre-training task reports CPU utilization of ~ 10%, GPUs are completely saturated (GPU utilization is above 95% on all GPUs), while disk utilization is above 90%. 

"
https://github.com/huggingface/datasets/issues/2484,Implement loading a dataset builder,['#self-assign'],"As discussed with @stas00 and @lhoestq, this would allow things like:

```python
from datasets import load_dataset_builder
dataset_name = ""openwebtext""
builder = load_dataset_builder(dataset_name)
print(builder.cache_dir)
```"
https://github.com/huggingface/datasets/issues/2481,Delete extracted files to save disk space,"[""My suggestion for this would be to have this enabled by default.\r\n\r\nPlus I don't know if there should be a dedicated issue to that is another functionality. But I propose layered building rather than all at once. That is:\r\n\r\n1. uncompress a handful of files via a generator enough to generate one arrow file\r\n2. process arrow file 1\r\n3. delete all the files that went in and aren't needed anymore.\r\n\r\nrinse and repeat.\r\n\r\n1. This way much less disc space will be required - e.g. on JZ we won't be running into inode limitation, also it'd help with the collaborative hub training project\r\n2. The user doesn't need to go and manually clean up all the huge files that were left after pre-processing\r\n3. It would already include deleting temp files this issue is talking about\r\n\r\nI wonder if the new streaming API would be of help, except here the streaming would be into arrow files as the destination, rather than dataloaders.""]","As discussed with @stas00 and @lhoestq, allowing the deletion of extracted files would save a great amount of disk space to typical user."
https://github.com/huggingface/datasets/issues/2480,Set download/extracted paths configurable,"[""For example to be able to send uncompressed and temp  build files to another volume/partition, so that the user gets the minimal disk usage on their primary setup - and ends up with just the downloaded compressed data + arrow files, but outsourcing the huge files and building to another partition. e.g. on JZ there is a special partition for fast data, but it's also volatile, so only temp files should go there.\r\n\r\nThink of it as `TMPDIR` so we need the equivalent for `datasets`.""]","As discussed with @stas00 and @lhoestq, setting these paths configurable may allow to overcome disk space limitation on different partitions/drives.

TODO:
- [x] Set configurable extracted datasets path: #2487
- [x] Set configurable downloaded datasets path: #2488
- [ ] Set configurable ""incomplete"" datasets path?"
https://github.com/huggingface/datasets/issues/2475,Issue in timit_asr database,"['This bug was fixed in #1995. Upgrading datasets to version 1.6 fixes the issue!'
 'Indeed was a fixed bug.\r\nWorks on version 1.8\r\nThanks ']","## Describe the bug
I am trying to load the timit_asr dataset however only the first record is shown (duplicated over all the rows).
I am using the next code line
dataset = load_dataset(“timit_asr”, split=“test”).shuffle().select(range(10))

The above code result with the same sentence duplicated ten times.
It also happens when I use the dataset viewer at Streamlit .

## Steps to reproduce the bug
from datasets import load_dataset
dataset = load_dataset(“timit_asr”, split=“test”).shuffle().select(range(10))
data = dataset.to_pandas()

# Sample code to reproduce the bug
```

## Expected results
table with different row information

## Actual results
Specify the actual results or traceback.

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->

- `datasets` version: 1.4.1 (also occur in the latest version)
- Platform: Linux-4.15.0-143-generic-x86_64-with-Ubuntu-18.04-bionic
- Python version: 3.6.9
- PyTorch version (GPU?): 1.8.1+cu102 (False)
- Tensorflow version (GPU?): 1.15.3 (False)
- Using GPU in script?: No
- Using distributed or parallel set-up in script?: No

- `datasets` version:
- Platform:
- Python version:
- PyArrow version:
"
https://github.com/huggingface/datasets/issues/2474,cache_dir parameter for load_from_disk ?,"[""Hi ! `load_from_disk` doesn't move the data. If you specify a local path to your mounted drive, then the dataset is going to be loaded directly from the arrow file in this directory. The cache files that result from `map` operations are also stored in the same directory by default.\r\n\r\nHowever note than writing data to your google drive actually fills the VM's disk (see https://github.com/huggingface/datasets/issues/643)\r\n\r\nGiven that, I don't think that changing the cache directory changes anything.\r\n\r\nLet me know what you think""
 'Thanks for your answer! I am a little surprised since I just want to read the dataset.\r\n\r\nAfter debugging a bit, I noticed that the VM’s disk fills up when the tables (generator) are converted to a list:\r\n\r\nhttps://github.com/huggingface/datasets/blob/5ba149773d23369617563d752aca922081277ec2/src/datasets/table.py#L850\r\n\r\nIf I try to iterate through the table’s generator e.g.: \r\n\r\n`length = sum(1 for x in tables)`\r\n\r\nthe VM’s disk fills up as well.\r\n\r\nI’m running out of Ideas 😄 '
 ""Indeed reading the data shouldn't increase the VM's disk. Not sure what google colab does under the hood for that to happen""]","**Is your feature request related to a problem? Please describe.**
When using Google Colab big datasets can be an issue, as they won't fit on the VM's disk. Therefore mounting google drive could be a possible solution. Unfortunatly when loading my own dataset by using the _load_from_disk_ function, the data gets cached to the VM's disk:

`
from datasets import load_from_disk

myPreprocessedData = load_from_disk(""/content/gdrive/MyDrive/ASR_data/myPreprocessedData"")

`
I know that chaching on google drive could slow down learning. But at least it would run.

**Describe the solution you'd like**
Add cache_Dir parameter to the load_from_disk function.

**Describe alternatives you've considered**
It looks like you could write a custom loading script for the load_dataset function. But this seems to be much too complex for my use case. Is there perhaps a template here that uses the load_from_disk function?
"
https://github.com/huggingface/datasets/issues/2472,Fix automatic generation of Zenodo DOI,"['I have received a reply from Zenodo support:\r\n> We are currently investigating and fixing this issue related to GitHub releases. As soon as we have solved it we will reach back to you.'
 'Other repo maintainers had the same problem with Zenodo. \r\n\r\nThere is an open issue on their GitHub repo: zenodo/zenodo#2181'
 'I have received the following request from Zenodo support:\r\n> Could you send us the link to the repository as well as the release tag?\r\n\r\nMy reply:\r\n> Sure, here it is:\r\n> - Link to the repository: https://github.com/huggingface/datasets\r\n> - Link to the repository at the release tag: https://github.com/huggingface/datasets/releases/tag/1.8.0\r\n> - Release tag: 1.8.0'
 'Zenodo issue has been fixed. The 1.8.0 release DOI can be found here: https://zenodo.org/record/4946100#.YMd6vKj7RPY']","After the last release of Datasets (1.8.0), the automatic generation of the Zenodo DOI failed: it appears in yellow as ""Received"", instead of in green as ""Published"".

I have contacted Zenodo support to fix this issue.

TODO:
- [x] Check with Zenodo to fix the issue
- [x] Check BibTeX entry is right"
https://github.com/huggingface/datasets/issues/2470,Crash when `num_proc` > dataset length for `map()` on a `datasets.Dataset`.,"['Hi ! It looks like the issue comes from pyarrow. What version of pyarrow are you using ? How did you install it ?'
 ""Thank you for the quick reply! I have `pyarrow==4.0.0`, and I am installing with `pip`. It's not one of my explicit dependencies, so I assume it came along with something else.""
 ""Could you trying reinstalling pyarrow with pip ?\r\nI'm not sure why it would check in your multicurtural-sc directory for source files.""
 'Sure! I tried reinstalling to get latest. pip was mad because it looks like Datasets currently wants <4.0.0 (which is interesting, because apparently I ended up with 4.0.0 already?), but I gave it a shot anyway:\r\n\r\n```bash\r\n$ pip install --upgrade --force-reinstall pyarrow\r\nCollecting pyarrow\r\n  Downloading pyarrow-4.0.1-cp39-cp39-manylinux2014_x86_64.whl (21.9 MB)\r\n     |████████████████████████████████| 21.9 MB 23.8 MB/s\r\nCollecting numpy>=1.16.6\r\n  Using cached numpy-1.20.3-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.4 MB)\r\nInstalling collected packages: numpy, pyarrow\r\n  Attempting uninstall: numpy\r\n    Found existing installation: numpy 1.20.3\r\n    Uninstalling numpy-1.20.3:\r\n      Successfully uninstalled numpy-1.20.3\r\n  Attempting uninstall: pyarrow\r\n    Found existing installation: pyarrow 3.0.0\r\n    Uninstalling pyarrow-3.0.0:\r\n      Successfully uninstalled pyarrow-3.0.0\r\nERROR: pip\'s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\ndatasets 1.8.0 requires pyarrow<4.0.0,>=1.0.0, but you have pyarrow 4.0.1 which is incompatible.\r\nSuccessfully installed numpy-1.20.3 pyarrow-4.0.1\r\n```\r\n\r\nTrying it, the same issue:\r\n\r\n![image](https://user-images.githubusercontent.com/1170062/121730226-3f470b80-caa4-11eb-85a5-684c44c816da.png)\r\n\r\nI tried installing `""pyarrow<4.0.0""`, which gave me 3.0.0. Running, still, same issue.\r\n\r\nI agree it\'s weird that pyarrow is checking the source code directory for its files. (There is no `pyarrow/` directory there.) To me, that makes it seem like an issue with how pyarrow is called.\r\n\r\nOut of curiosity, I tried running this with fewer workers to see when the error arises:\r\n\r\n- 1: ✅\r\n- 2: ✅\r\n- 4: ✅\r\n- 8: ✅\r\n- 10: ✅\r\n- 11: ❌  🤔\r\n- 12: ❌\r\n- 16: ❌\r\n- 32: ❌\r\n\r\nchecking my datasets:\r\n\r\n```python\r\n>>> datasets\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: [\'text\'],\r\n        num_rows: 389290\r\n    })\r\n    validation.sc: Dataset({\r\n        features: [\'text\'],\r\n        num_rows: 10  # 🤔\r\n    })\r\n    validation.wvs: Dataset({\r\n        features: [\'text\'],\r\n        num_rows: 93928\r\n    })\r\n})\r\n```\r\n\r\nNew hypothesis: crash if `num_proc` > length of a dataset? 😅\r\n\r\nIf so, this might be totally my fault, as the caller. Could be a docs fix, or maybe this library could do a check to limit `num_proc` for this case?'
 'Good catch ! Not sure why it could raise such a weird issue from pyarrow though\r\nWe should definitely reduce num_proc to the length of the dataset if needed and log a warning.'
 ""This has been fixed in #2566, thanks @connor-mccarthy !\r\nWe'll make a new release soon that includes the fix ;)""]","## Describe the bug
Crash if when using `num_proc` > 1 (I used 16) for `map()` on a `datasets.Dataset`.

I believe I've had cases where `num_proc` > 1 works before, but now it seems either inconsistent, or depends on my data. I'm not sure whether the issue is on my end, because it's difficult for me to debug! Any tips greatly appreciated, I'm happy to provide more info if it would helps us diagnose.

## Steps to reproduce the bug
```python
# this function will be applied with map()
def tokenize_function(examples):
    return tokenizer(
        examples[""text""],
        padding=PaddingStrategy.DO_NOT_PAD,
        truncation=True,
    )

# data_files is a Dict[str, str] mapping name -> path
datasets = load_dataset(""text"", data_files={...})  

# this is where the error happens if num_proc = 16,
# but is fine if num_proc = 1
tokenized_datasets = datasets.map(
    tokenize_function,
    batched=True,
    num_proc=num_workers,
)
```

## Expected results
The `map()` function succeeds with `num_proc` > 1.

## Actual results
![image](https://user-images.githubusercontent.com/1170062/121404271-a6cc5200-c910-11eb-8e27-5c893bd04042.png)
![image](https://user-images.githubusercontent.com/1170062/121404362-be0b3f80-c910-11eb-9117-658943029aef.png)

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: 1.6.2
- Platform: Linux-5.4.0-73-generic-x86_64-with-glibc2.31
- Python version: 3.9.5
- PyTorch version (GPU?): 1.8.1+cu111 (True)
- Tensorflow version (GPU?): not installed (NA)
- Using GPU in script?: Yes, but I think N/A for this issue
- Using distributed or parallel set-up in script?: Multi-GPU on one machine, but I think also N/A for this issue
"
https://github.com/huggingface/datasets/issues/2459,`Proto_qa` hosting seems to be broken,"['@VictorSanh , I think @mariosasko is already working on it. ']","## Describe the bug
The hosting (on Github) of the `proto_qa` dataset seems broken. I haven't investigated more yet, just flagging it for now. 

@zaidalyafeai if you want to dive into it, I think it's just a matter of changing the links in `proto_qa.py`

## Steps to reproduce the bug
```python
from datasets import load_dataset
dataset = load_dataset(""proto_qa"")
```

## Actual results

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/load.py"", line 751, in load_dataset
    use_auth_token=use_auth_token,
  File ""/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/builder.py"", line 575, in download_and_prepare
    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
  File ""/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/builder.py"", line 630, in _download_and_prepare
    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)
  File ""/home/hf/.cache/huggingface/modules/datasets_modules/datasets/proto_qa/445346efaad5c5f200ecda4aa7f0fb50ff1b55edde3003be424a2112c3e8102e/proto_qa.py"", line 131, in _split_generators
    train_fpath = dl_manager.download(_URLs[self.config.name][""train""])
  File ""/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/utils/download_manager.py"", line 199, in download
    num_proc=download_config.num_proc,
  File ""/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/utils/py_utils.py"", line 195, in map_nested
    return function(data_struct)
  File ""/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/utils/download_manager.py"", line 218, in _download
    return cached_path(url_or_filename, download_config=download_config)
  File ""/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/utils/file_utils.py"", line 291, in cached_path
    use_auth_token=download_config.use_auth_token,
  File ""/home/hf/dev/promptsource/.venv/lib/python3.7/site-packages/datasets/utils/file_utils.py"", line 621, in get_from_cache
    raise FileNotFoundError(""Couldn't find file at {}"".format(url))
FileNotFoundError: Couldn't find file at https://raw.githubusercontent.com/iesl/protoqa-data/master/data/train/protoqa_train.jsonl
```"
https://github.com/huggingface/datasets/issues/2458,Revert default in-memory for small datasets,['cc: @krandiash (pinged in reverted PR).'],"Users are reporting issues and confusion about setting default in-memory to True for small datasets.

We see 2 clear use cases of Datasets:
- the ""canonical"" way, where you can work with very large datasets, as they are memory-mapped and cached (after every transformation)
- some edge cases (speed benchmarks, interactive/exploratory analysis,...), where default in-memory can explicitly be enabled, and no caching will be done

After discussing with @lhoestq we have agreed to:
- revert this feature (implemented in #2182)
- explain in the docs how to optimize speed/performance by setting default in-memory

cc: @stas00 https://github.com/huggingface/datasets/pull/2409#issuecomment-856210552"
https://github.com/huggingface/datasets/issues/2452,MRPC test set differences between torch and tensorflow datasets,['Realized that `tensorflow_datasets` is not provided by Huggingface and should therefore raise the issue there.'],"## Describe the bug
When using `load_dataset(""glue"", ""mrpc"")` to load the MRPC dataset, the test set includes the labels. When using `tensorflow_datasets.load('glue/{}'.format('mrpc'))` to load the dataset the test set does not contain the labels. There should be consistency between torch and tensorflow ways of importing the GLUE datasets.

## Steps to reproduce the bug

Minimal working code  
```python
from datasets import load_dataset
import tensorflow as tf
import tensorflow_datasets

# torch
dataset = load_dataset(""glue"", ""mrpc"")
# tf
data = tensorflow_datasets.load('glue/{}'.format('mrpc'))
data = list(data['test'].as_numpy_iterator())
for i in range(40,50):
  tf_sentence1 = data[i]['sentence1'].decode(""utf-8"") 
  tf_sentence2 = data[i]['sentence2'].decode(""utf-8"") 

  tf_label = data[i]['label']
  
  index = data[i]['idx']
  print('Index {}'.format(index))
  torch_sentence1 = dataset['test']['sentence1'][index]
  torch_sentence2 = dataset['test']['sentence2'][index]

  torch_label = dataset['test']['label'][index]
  print('Tensorflow: \n\tSentence1 {}\n\tSentence2 {}\n\tLabel {}'.format(tf_sentence1, tf_sentence2, tf_label))
  print('Torch: \n\tSentence1 {}\n\tSentence2 {}\n\tLabel {}'.format(torch_sentence1, torch_sentence2, torch_label))
```

Sample output  
```
Index 954
Tensorflow: 
	Sentence1 Sabri Yakou , an Iraqi native who is a legal U.S. resident , appeared before a federal magistrate yesterday on charges of violating U.S. arms-control laws .
	Sentence2 The elder Yakou , an Iraqi native who is a legal U.S. resident , appeared before a federal magistrate Wednesday on charges of violating U.S. arms control laws .
	Label -1
Torch: 
	Sentence1 Sabri Yakou , an Iraqi native who is a legal U.S. resident , appeared before a federal magistrate yesterday on charges of violating U.S. arms-control laws .
	Sentence2 The elder Yakou , an Iraqi native who is a legal U.S. resident , appeared before a federal magistrate Wednesday on charges of violating U.S. arms control laws .
	Label 1
Index 711
Tensorflow: 
	Sentence1 Others keep records sealed for as little as five years or as much as 30 .
	Sentence2 Some states make them available immediately ; others keep them sealed for as much as 30 years .
	Label -1
Torch: 
	Sentence1 Others keep records sealed for as little as five years or as much as 30 .
	Sentence2 Some states make them available immediately ; others keep them sealed for as much as 30 years .
	Label 0
```

## Expected results
I would expect the datasets to be independent of whether I am working with torch or tensorflow.

## Actual results
Test set labels are provided in the `datasets.load_datasets()` for MRPC. However MRPC is the only task where the test set labels are not -1.

## Environment info
- `datasets` version: 1.7.0
- Platform: Linux-5.4.109+-x86_64-with-Ubuntu-18.04-bionic
- Python version: 3.7.10
- PyArrow version: 3.0.0
"
https://github.com/huggingface/datasets/issues/2450,BLUE file not found,"[""Hi ! The `blue` metric doesn't exist, but the `bleu` metric does.\r\nYou can get the full list of metrics [here](https://github.com/huggingface/datasets/tree/master/metrics) or by running\r\n```python\r\nfrom datasets import list_metrics\r\n\r\nprint(list_metrics())\r\n```""
 'Ah, my mistake. Thanks for correcting']","Hi, I'm having the following issue when I try to load the `blue` metric.

```shell
import datasets
metric = datasets.load_metric('blue')
Traceback (most recent call last):
  File ""/home/irfan/environments/Perplexity_Transformers/lib/python3.6/site-packages/datasets/load.py"", line 320, in prepare_module
    local_path = cached_path(file_path, download_config=download_config)
  File ""/home/irfan/environments/Perplexity_Transformers/lib/python3.6/site-packages/datasets/utils/file_utils.py"", line 291, in cached_path
    use_auth_token=download_config.use_auth_token,
  File ""/home/irfan/environments/Perplexity_Transformers/lib/python3.6/site-packages/datasets/utils/file_utils.py"", line 621, in get_from_cache
    raise FileNotFoundError(""Couldn't find file at {}"".format(url))
FileNotFoundError: Couldn't find file at https://raw.githubusercontent.com/huggingface/datasets/1.7.0/metrics/blue/blue.py
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""/home/irfan/environments/Perplexity_Transformers/lib/python3.6/site-packages/datasets/load.py"", line 332, in prepare_module
    local_path = cached_path(file_path, download_config=download_config)
  File ""/home/irfan/environments/Perplexity_Transformers/lib/python3.6/site-packages/datasets/utils/file_utils.py"", line 291, in cached_path
    use_auth_token=download_config.use_auth_token,
  File ""/home/irfan/environments/Perplexity_Transformers/lib/python3.6/site-packages/datasets/utils/file_utils.py"", line 621, in get_from_cache
    raise FileNotFoundError(""Couldn't find file at {}"".format(url))
FileNotFoundError: Couldn't find file at https://raw.githubusercontent.com/huggingface/datasets/master/metrics/blue/blue.py
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""<input>"", line 1, in <module>
  File ""/home/irfan/environments/Perplexity_Transformers/lib/python3.6/site-packages/datasets/load.py"", line 605, in load_metric
    dataset=False,
  File ""/home/irfan/environments/Perplexity_Transformers/lib/python3.6/site-packages/datasets/load.py"", line 343, in prepare_module
    combined_path, github_file_path
FileNotFoundError: Couldn't find file locally at blue/blue.py, or remotely at https://raw.githubusercontent.com/huggingface/datasets/1.7.0/metrics/blue/blue.py.
The file is also not present on the master branch on github.
```
Here is dataset installed version info
```shell
pip freeze | grep datasets
datasets==1.7.0
```
"
https://github.com/huggingface/datasets/issues/2447,"dataset adversarial_qa has no answers in the ""test"" set","[""Hi ! I'm pretty sure that the answers are not made available for the test set on purpose because it is part of the DynaBench benchmark, for which you can submit your predictions on the website.\r\nIn any case we should mention this in the dataset card of this dataset.""
 'Makes sense, but not intuitive for someone searching through the datasets.  Thanks for adding the note to clarify.']","## Describe the bug
When loading the adversarial_qa dataset the 'test' portion has no answers.  Only the 'train' and 'validation' portions do.  This occurs with all four of the configs ('adversarialQA', 'dbidaf', 'dbert', 'droberta')

## Steps to reproduce the bug
```
from   datasets import load_dataset
examples = load_dataset('adversarial_qa', 'adversarialQA', script_version=""master"")['test']
print('Loaded {:,} examples'.format(len(examples)))
has_answers = 0
for e in examples:
    if e['answers']['text']:
        has_answers += 1
print('{:,} have answers'.format(has_answers))
>>> Loaded 3,000 examples
>>> 0 have answers

examples = load_dataset('adversarial_qa', 'adversarialQA', script_version=""master"")['validation']
<...code above...>
>>> Loaded 3,000 examples
>>> 3,000 have answers
```

## Expected results
If 'test' is a valid dataset, it should have answers. Also note that all of the 'train' and 'validation' sets have answers, there are no ""no answer"" questions with this set (not sure if this is correct or not).

## Environment info
- `datasets` version: 1.7.0
- Platform: Linux-5.8.0-53-generic-x86_64-with-glibc2.29
- Python version: 3.8.5
- PyArrow version: 1.0.0

"
https://github.com/huggingface/datasets/issues/2446,`yelp_polarity` is broken,"['```\r\nFile ""/home/sasha/.local/share/virtualenvs/lib-ogGKnCK_/lib/python3.7/site-packages/streamlit/script_runner.py"", line 332, in _run_script\r\n    exec(code, module.__dict__)\r\nFile ""/home/sasha/nlp-viewer/run.py"", line 233, in <module>\r\n    configs = get_confs(option)\r\nFile ""/home/sasha/.local/share/virtualenvs/lib-ogGKnCK_/lib/python3.7/site-packages/streamlit/caching.py"", line 604, in wrapped_func\r\n    return get_or_create_cached_value()\r\nFile ""/home/sasha/.local/share/virtualenvs/lib-ogGKnCK_/lib/python3.7/site-packages/streamlit/caching.py"", line 588, in get_or_create_cached_value\r\n    return_value = func(*args, **kwargs)\r\nFile ""/home/sasha/nlp-viewer/run.py"", line 148, in get_confs\r\n    builder_cls = nlp.load.import_main_class(module_path[0], dataset=True)\r\nFile ""/home/sasha/.local/share/virtualenvs/lib-ogGKnCK_/lib/python3.7/site-packages/datasets/load.py"", line 85, in import_main_class\r\n    module = importlib.import_module(module_path)\r\nFile ""/usr/lib/python3.7/importlib/__init__.py"", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nFile ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import\r\nFile ""<frozen importlib._bootstrap>"", line 983, in _find_and_load\r\nFile ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked\r\nFile ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked\r\nFile ""<frozen importlib._bootstrap_external>"", line 728, in exec_module\r\nFile ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed\r\nFile ""/home/sasha/.cache/huggingface/modules/datasets_modules/datasets/yelp_polarity/a770787b2526bdcbfc29ac2d9beb8e820fbc15a03afd3ebc4fb9d8529de57544/yelp_polarity.py"", line 36, in <module>\r\n    from datasets.tasks import TextClassification\r\n```'
 'Solved by updating the `nlpviewer`']","![image](https://user-images.githubusercontent.com/22514219/120828150-c4a35b00-c58e-11eb-8083-a537cee4dbb3.png)
"
https://github.com/huggingface/datasets/issues/2444,Sentence Boundaries missing in Dataset: xtreme / udpos,"['Hi,\r\n\r\nThis is a known issue. More info on this issue can be found in #2061. If you are looking for an open-source contribution, there are step-by-step instructions in the linked issue that you can follow to fix it.'
 'Closed by #2466.']","I was browsing through annotation guidelines, as suggested by the datasets introduction.

The guidlines saids ""There must be exactly one blank line after every sentence, including the last sentence in the file. Empty sentences are not allowed."" in the [Sentence Boundaries and Comments section](https://universaldependencies.org/format.html#sentence-boundaries-and-comments)

But the sentence boundaries seems not to be represented by huggingface datasets features well. I found out that multiple sentence are concatenated together as a 1D array, without any delimiter.

PAN-x, which is another token classification subset from xtreme do represent the sentence boundary using a 2D array.

You may compare in PAN-x.en and udpos.English in the explorer:
 https://huggingface.co/datasets/viewer/?dataset=xtreme"
https://github.com/huggingface/datasets/issues/2443,Some tests hang on Windows,"[""Hi ! That would be nice indeed to at least have a warning, since we don't handle the max path length limit.\r\nAlso if we could have an error instead of an infinite loop I'm sure windows users will appreciate that""
 ""Unfortunately, I know this problem very well... 😅 \r\n\r\nI remember having proposed to throw an error instead of hanging in an infinite loop #2220: 60c7d1b6b71469599a27147a08100f594e7a3f84, 8c8ab60018b00463edf1eca500e434ff061546fc \r\nbut @lhoestq told me:\r\n> Note that the filelock module comes from this project that hasn't changed in years - while still being used by ten of thousands of projects:\r\nhttps://github.com/benediktschmitt/py-filelock\r\n> \r\n> Unless we have proper tests for this, I wouldn't recommend to change it\r\n\r\nI opened an Issue requesting a warning/error at startup for that case: #2224""
 ""@albertvillanova Thanks for additional info on this issue.\r\n\r\nYes, I think the best option is to throw an error instead of suppressing it in a loop. I've considered 2 more options, but I don't really like them:\r\n1. create a temporary file with a filename longer than 255 characters on import; if this fails, long paths are not enabled and raise a warning. I'm not sure about this approach because I don't like the idea of creating a temporary file on import for this purpose.\r\n2. check if long paths are enabled with [this code](https://stackoverflow.com/a/46546731/14095927). As mentioned in the comment, this code relies on an undocumented function and Win10-specific.""]","Currently, several tests hang on Windows if the max path limit of 260 characters is not disabled. This happens due to the changes introduced by #2223 that cause an infinite loop in `WindowsFileLock` described in #2220.  This can be very tricky to debug, so I think now is a good time to address these issues/PRs. IMO throwing an error is too harsh, but maybe we can emit a warning in the top-level `__init__.py ` on startup if long paths are not enabled.
"
https://github.com/huggingface/datasets/issues/2441,DuplicatedKeysError on personal dataset,"['Hi ! In your dataset script you must be yielding examples like\r\n```python\r\nfor line in file:\r\n    ...\r\n    yield key, {...}\r\n```\r\n\r\nSince `datasets` 1.7.0 we enforce the keys to be unique.\r\nHowever it looks like your examples generator creates duplicate keys: at least two examples have key 0.\r\n\r\nYou can fix that by making sure that your keys are unique.\r\n\r\nFor example if you use a counter to define the key of each example, make sure that your counter is not reset to 0 in during examples generation (between two open files for examples).\r\n\r\nLet me know if you have other questions :)'
 ""Yup, I indeed was generating duplicate keys. Fixed it and now it's working.""]","## Describe the bug
Ever since today, I have been getting a DuplicatedKeysError while trying to load my dataset from my own script.
Error returned when running this line: `dataset = load_dataset('/content/drive/MyDrive/Thesis/Datasets/book_preprocessing/goodreads_maharjan_trimmed_and_nered/goodreadsnered.py')`
Note that my script was working fine with earlier versions of the Datasets library. Cannot say with 100% certainty if I have been doing something wrong with my dataset script this whole time or if this is simply a bug with the new version of datasets.

## Steps to reproduce the bug
I cannot provide code to reproduce the error as I am working with my own dataset. I can however provide my script if requested.

## Expected results
For my data to be loaded.

## Actual results
**DuplicatedKeysError** exception is raised
```
Downloading and preparing dataset good_reads_practice_dataset/main_domain (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/good_reads_practice_dataset/main_domain/1.1.0/64ff7c3fee2693afdddea75002eb6887d4fedc3d812ae3622128c8504ab21655...

---------------------------------------------------------------------------

DuplicatedKeysError                       Traceback (most recent call last)

<ipython-input-6-c342ea0dae9d> in <module>()
----> 1 dataset = load_dataset('/content/drive/MyDrive/Thesis/Datasets/book_preprocessing/goodreads_maharjan_trimmed_and_nered/goodreadsnered.py')

5 frames

/usr/local/lib/python3.7/dist-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, script_version, use_auth_token, task, **config_kwargs)
    749         try_from_hf_gcs=try_from_hf_gcs,
    750         base_path=base_path,
--> 751         use_auth_token=use_auth_token,
    752     )
    753 

/usr/local/lib/python3.7/dist-packages/datasets/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)
    573                     if not downloaded_from_gcs:
    574                         self._download_and_prepare(
--> 575                             dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
    576                         )
    577                     # Sync info

/usr/local/lib/python3.7/dist-packages/datasets/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)
    650             try:
    651                 # Prepare split will record examples associated to the split
--> 652                 self._prepare_split(split_generator, **prepare_split_kwargs)
    653             except OSError as e:
    654                 raise OSError(

/usr/local/lib/python3.7/dist-packages/datasets/builder.py in _prepare_split(self, split_generator)
    990                     writer.write(example, key)
    991             finally:
--> 992                 num_examples, num_bytes = writer.finalize()
    993 
    994         split_generator.split_info.num_examples = num_examples

/usr/local/lib/python3.7/dist-packages/datasets/arrow_writer.py in finalize(self, close_stream)
    407         # In case current_examples < writer_batch_size, but user uses finalize()
    408         if self._check_duplicates:
--> 409             self.check_duplicate_keys()
    410             # Re-intializing to empty list for next batch
    411             self.hkey_record = []

/usr/local/lib/python3.7/dist-packages/datasets/arrow_writer.py in check_duplicate_keys(self)
    347         for hash, key in self.hkey_record:
    348             if hash in tmp_record:
--> 349                 raise DuplicatedKeysError(key)
    350             else:
    351                 tmp_record.add(hash)

DuplicatedKeysError: FAILURE TO GENERATE DATASET !
Found duplicate Key: 0
Keys should be unique and deterministic in nature
```

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: 1.7.0
- Platform: Windows-10-10.0.19041-SP0
- Python version: 3.7.9
- PyArrow version: 3.0.0
"
https://github.com/huggingface/datasets/issues/2440,Remove `extended` field from dataset tagger,"[""The tagger also doesn't insert the value for the `size_categories` field automatically, so this should be fixed too""
 ""Thanks for reporting. Indeed the `extended` tag doesn't exist. Not sure why we had that in the tagger.\r\nThe repo of the tagger is here if someone wants to give this a try: https://github.com/huggingface/datasets-tagging\r\nOtherwise I can probably fix it next week""
 ""I've opened a PR on `datasets-tagging` to fix the issue 🚀 ""
 'thanks ! this is fixed now']","## Describe the bug
While working on #2435 I used the [dataset tagger](https://huggingface.co/datasets/tagging/) to generate the missing tags for the YAML metadata of each README.md file. However, it seems that our CI raises an error when the `extended` field is included:

```
dataset_name = 'arcd'

    @pytest.mark.parametrize(""dataset_name"", get_changed_datasets(repo_path))
    def test_changed_dataset_card(dataset_name):
        card_path = repo_path / ""datasets"" / dataset_name / ""README.md""
        assert card_path.exists()
        error_messages = []
        try:
            ReadMe.from_readme(card_path)
        except Exception as readme_error:
            error_messages.append(f""The following issues have been found in the dataset cards:\nREADME:\n{readme_error}"")
        try:
            DatasetMetadata.from_readme(card_path)
        except Exception as metadata_error:
            error_messages.append(
                f""The following issues have been found in the dataset cards:\nYAML tags:\n{metadata_error}""
            )
    
        if error_messages:
>           raise ValueError(""\n"".join(error_messages))
E           ValueError: The following issues have been found in the dataset cards:
E           YAML tags:
E           __init__() got an unexpected keyword argument 'extended'

tests/test_dataset_cards.py:70: ValueError
```

Consider either removing this tag from the tagger or including it as part of the validation step in the CI.

cc @yjernite "
https://github.com/huggingface/datasets/issues/2434,Extend QuestionAnsweringExtractive template to handle nested columns,['this is also the case for the following datasets and configurations:\r\n\r\n* `mlqa` with config `mlqa-translate-train.ar`\r\n\r\n'],"Currently the `QuestionAnsweringExtractive` task template and `preprare_for_task` only support ""flat"" features. We should extend the functionality to cover QA datasets like:

* `iapp_wiki_qa_squad`
* `parsinlu_reading_comprehension`

where the nested features differ with those from `squad` and trigger an `ArrowNotImplementedError`:

```
---------------------------------------------------------------------------
ArrowNotImplementedError                  Traceback (most recent call last)
<ipython-input-12-50e5b8f69c20> in <module>
----> 1 ds.prepare_for_task(""question-answering-extractive"")[0]

~/git/datasets/src/datasets/arrow_dataset.py in prepare_for_task(self, task)
   1436         # We found a template so now flush `DatasetInfo` to skip the template update in `DatasetInfo.__post_init__`
   1437         dataset.info.task_templates = None
-> 1438         dataset = dataset.cast(features=template.features)
   1439         return dataset
   1440 

~/git/datasets/src/datasets/arrow_dataset.py in cast(self, features, batch_size, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, num_proc)
    977         format = self.format
    978         dataset = self.with_format(""arrow"")
--> 979         dataset = dataset.map(
    980             lambda t: t.cast(schema),
    981             batched=True,

~/git/datasets/src/datasets/arrow_dataset.py in map(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)
   1600 
   1601         if num_proc is None or num_proc == 1:
-> 1602             return self._map_single(
   1603                 function=function,
   1604                 with_indices=with_indices,

~/git/datasets/src/datasets/arrow_dataset.py in wrapper(*args, **kwargs)
    176         }
    177         # apply actual function
--> 178         out: Union[""Dataset"", ""DatasetDict""] = func(self, *args, **kwargs)
    179         datasets: List[""Dataset""] = list(out.values()) if isinstance(out, dict) else [out]
    180         # re-apply format to the output

~/git/datasets/src/datasets/fingerprint.py in wrapper(*args, **kwargs)
    395             # Call actual function
    396 
--> 397             out = func(self, *args, **kwargs)
    398 
    399             # Update fingerprint of in-place transforms + update in-place history of transforms

~/git/datasets/src/datasets/arrow_dataset.py in _map_single(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, desc)
   1940                         )  # Something simpler?
   1941                         try:
-> 1942                             batch = apply_function_on_filtered_inputs(
   1943                                 batch,
   1944                                 indices,

~/git/datasets/src/datasets/arrow_dataset.py in apply_function_on_filtered_inputs(inputs, indices, check_same_num_examples, offset)
   1836                 effective_indices = [i + offset for i in indices] if isinstance(indices, list) else indices + offset
   1837             processed_inputs = (
-> 1838                 function(*fn_args, effective_indices, **fn_kwargs) if with_indices else function(*fn_args, **fn_kwargs)
   1839             )
   1840             if update_data is None:

~/git/datasets/src/datasets/arrow_dataset.py in <lambda>(t)
    978         dataset = self.with_format(""arrow"")
    979         dataset = dataset.map(
--> 980             lambda t: t.cast(schema),
    981             batched=True,
    982             batch_size=batch_size,

~/miniconda3/envs/datasets/lib/python3.8/site-packages/pyarrow/table.pxi in pyarrow.lib.Table.cast()

~/miniconda3/envs/datasets/lib/python3.8/site-packages/pyarrow/table.pxi in pyarrow.lib.ChunkedArray.cast()

~/miniconda3/envs/datasets/lib/python3.8/site-packages/pyarrow/compute.py in cast(arr, target_type, safe)
    241     else:
    242         options = CastOptions.unsafe(target_type)
--> 243     return call_function(""cast"", [arr], options)
    244 
    245 

~/miniconda3/envs/datasets/lib/python3.8/site-packages/pyarrow/_compute.pyx in pyarrow._compute.call_function()

~/miniconda3/envs/datasets/lib/python3.8/site-packages/pyarrow/_compute.pyx in pyarrow._compute.Function.call()

~/miniconda3/envs/datasets/lib/python3.8/site-packages/pyarrow/error.pxi in pyarrow.lib.pyarrow_internal_check_status()

~/miniconda3/envs/datasets/lib/python3.8/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()

ArrowNotImplementedError: Unsupported cast from struct<answer_end: list<item: int32>, answer_start: list<item: int32>, text: list<item: string>> to struct using function cast_struct
```"
https://github.com/huggingface/datasets/issues/2431,DuplicatedKeysError when trying to load adversarial_qa,"['Thanks for reporting !\r\n#2433 fixed the issue, thanks @mariosasko :)\r\n\r\nWe\'ll do a patch release soon of the library.\r\nIn the meantime, you can use the fixed version of adversarial_qa by adding `script_version=""master""` in `load_dataset`']","## Describe the bug
A clear and concise description of what the bug is.

## Steps to reproduce the bug
```python
dataset = load_dataset('adversarial_qa', 'adversarialQA')
```

## Expected results
The dataset should be loaded into memory

## Actual results

>DuplicatedKeysError: FAILURE TO GENERATE DATASET !
>Found duplicate Key: 4d3cb5677211ee32895ca9c66dad04d7152254d4
>Keys should be unique and deterministic in nature
>
>
>During handling of the above exception, another exception occurred:
>
>DuplicatedKeysError                       Traceback (most recent call last)
>
>/usr/local/lib/python3.7/dist-packages/datasets/arrow_writer.py in check_duplicate_keys(self)
>    347         for hash, key in self.hkey_record:
>    348             if hash in tmp_record:
>--> 349                 raise DuplicatedKeysError(key)
>    350             else:
>    351                 tmp_record.add(hash)
>
>DuplicatedKeysError: FAILURE TO GENERATE DATASET !
>Found duplicate Key: 4d3cb5677211ee32895ca9c66dad04d7152254d4
>Keys should be unique and deterministic in nature

## Environment info
- `datasets` version: 1.7.0
- Platform: Linux-5.4.109+-x86_64-with-Ubuntu-18.04-bionic
- Python version: 3.7.10
- PyArrow version: 3.0.0
"
https://github.com/huggingface/datasets/issues/2426,Saving Graph/Structured Data in Datasets,"['It should probably work out of the box to save structured data. If you want to show an example we can help you.'
 'An example of a toy dataset is like:\r\n```json\r\n[\r\n    {\r\n        ""name"": ""mike"",\r\n        ""friends"": [\r\n            ""tom"",\r\n            ""lily""\r\n        ],\r\n        ""articles"": [\r\n            {\r\n                ""title"": ""aaaaa"",\r\n                ""reader"": [\r\n                    ""tom"",\r\n                    ""lucy""\r\n                ]\r\n            }\r\n        ]\r\n    },\r\n    {\r\n        ""name"": ""tom"",\r\n        ""friends"": [\r\n            ""mike"",\r\n            ""bbb""\r\n        ],\r\n        ""articles"": [\r\n            {\r\n                ""title"": ""xxxxx"",\r\n                ""reader"": [\r\n                    ""tom"",\r\n                    ""qqqq""\r\n                ]\r\n            }\r\n        ]\r\n    }\r\n]\r\n```\r\nWe can use the friendship relation to build a directional graph, and a user node can be represented using the articles written by himself. And the relationship between articles can be built when the article has read by the same user.\r\nThis dataset can be used to model the heterogeneous relationship between users and articles, and this graph can be used to build recommendation systems to recommend articles to the user, or potential friends to the user.'
 'Hi,\r\n\r\nyou can do the following to load this data into a `Dataset`:\r\n```python\r\nfrom datasets import Dataset\r\nexamples = [\r\n    {\r\n        ""name"": ""mike"",\r\n        ""friends"": [\r\n            ""tom"",\r\n            ""lily""\r\n        ],\r\n        ""articles"": [\r\n            {\r\n                ""title"": ""aaaaa"",\r\n                ""reader"": [\r\n                    ""tom"",\r\n                    ""lucy""\r\n                ]\r\n            }\r\n        ]\r\n    },\r\n    {\r\n        ""name"": ""tom"",\r\n        ""friends"": [\r\n            ""mike"",\r\n            ""bbb""\r\n        ],\r\n        ""articles"": [\r\n            {\r\n                ""title"": ""xxxxx"",\r\n                ""reader"": [\r\n                    ""tom"",\r\n                    ""qqqq""\r\n                ]\r\n            }\r\n        ]\r\n    }\r\n]\r\n\r\nkeys = examples[0].keys()\r\nvalues = [ex.values() for ex in examples]\r\ndataset = Dataset.from_dict({k: list(v) for k, v in zip(keys, zip(*values))})\r\n```\r\n\r\nLet us know if this works for you.'
 'Thank you so much, and that works! I also have a question that if the dataset is very large, that cannot be loaded into the memory. How to create the Dataset?'
 ""If your dataset doesn't fit in memory, store it in a local file and load it from there. Check out [this chapter](https://huggingface.co/docs/datasets/master/loading_datasets.html#from-local-files) in the docs for more info.""
 'Nice! Thanks for your help.']","Thanks for this amazing library! And my question is I have structured data that is organized with a graph. For example, a dataset with users' friendship relations and user's articles. When I try to save a python dict in the dataset, an error occurred ``did not recognize Python value type when inferring an Arrow data type''.
Although I also know that storing a python dict in pyarrow datasets is not the best practice, but I have no idea about how to save structured data in the Datasets. 

Thank you very much for your help."
https://github.com/huggingface/datasets/issues/2424,load_from_disk and save_to_disk are not compatible with each other,"['Hi,\r\n\r\n`load_dataset` returns an instance of `DatasetDict` if `split` is not specified, so instead of `Dataset.load_from_disk`, use `DatasetDict.load_from_disk` to load the dataset from disk.'
 'Thanks it worked!'
 'Though I see a stream of issues open by people lost between datasets and datasets dicts so maybe there is here something that could be better in terms of UX. Could be better error handling or something else smarter to even avoid said errors but maybe we should think about this. Reopening to use this issue as a discussion place but feel free to open a new open if you prefer @lhoestq @albertvillanova '
 'We should probably improve the error message indeed.\r\n\r\nAlso note that there exists a function `load_from_disk` that can load a Dataset or a DatasetDict. Under the hood it calls either `Dataset.load_from_disk` or `DatasetDict.load_from_disk`:\r\n\r\n\r\n```python\r\nfrom datasets import load_from_disk\r\n\r\ndataset_dict = load_from_disk(""path/to/dataset/dict"")\r\nsingle_dataset = load_from_disk(""path/to/single/dataset"")\r\n```'
 'I just opened #2437 to improve the error message' 'Superseded by #2462 ']","## Describe the bug
load_from_disk and save_to_disk are not compatible. When I use save_to_disk to save a dataset to disk it works perfectly but given the same directory load_from_disk throws an error that it can't find state.json. looks like the load_from_disk only works on one split

## Steps to reproduce the bug
```python
from datasets import load_dataset
dataset = load_dataset(""art"")
dataset.save_to_disk(""mydir"")
d = Dataset.load_from_disk(""mydir"")
```

## Expected results
It is expected that these two functions be the reverse of each other without more manipulation

## Actual results
FileNotFoundError: [Errno 2] No such file or directory: 'mydir/art/state.json'

## Environment info
- `datasets` version: 1.6.2
- Platform: Linux-5.4.0-73-generic-x86_64-with-Ubuntu-18.04-bionic
- Python version: 3.7.10
- PyTorch version (GPU?): 1.8.1+cu102 (True)
- Tensorflow version (GPU?): not installed (NA)
- Using GPU in script?: <fill in>
- Using distributed or parallel set-up in script?: <fill in>

"
https://github.com/huggingface/datasets/issues/2415,Cached dataset not loaded,"[""It actually seems to happen all the time in above configuration:\r\n* the function `filter_by_duration` correctly loads cached processed dataset\r\n* the function `prepare_dataset` is always reexecuted\r\n\r\nI end up solving the issue by saving to disk my dataset at the end but I'm still wondering if it's a bug or limitation here.""
 'Hi ! The hash used for caching `map` results is the fingerprint of the resulting dataset. It is computed using three things:\r\n- the old fingerprint of the dataset\r\n- the hash of the function\r\n- the hash of the other parameters passed to `map`\r\n\r\nYou can compute the hash of your function (or any python object) with\r\n```python\r\nfrom datasets.fingerprint import Hasher\r\n\r\nmy_func = lambda x: x + 1\r\nprint(Hasher.hash(my_func))\r\n```\r\n\r\nIf `prepare_dataset` is always executed, maybe this is because your `processor` has a different hash each time you want to execute it.'
 '> If `prepare_dataset` is always executed, maybe this is because your `processor` has a different hash each time you want to execute it.\r\n\r\nYes I\xa0think that was the issue.\r\n\r\nFor the hash of the function:\r\n* does it consider just the name or the actual code of the function\r\n* does it consider variables that are not passed explicitly as parameters to the functions (such as the processor here)'
 '> does it consider just the name or the actual code of the function\r\n\r\nIt looks at the name and the actual code and all variables such as recursively. It uses `dill` to do so, which is based on `pickle`.\r\nBasically the hash is computed using the pickle bytes of your function (computed using `dill` to support most python objects).\r\n\r\n> does it consider variables that are not passed explicitly as parameters to the functions (such as the processor here)\r\n\r\nYes it does thanks to recursive pickling.'
 ""Thanks for these explanations. I'm closing the issue.""]","## Describe the bug
I have a large dataset (common_voice, english) where I use several map and filter functions.
Sometimes my cached datasets after specific functions are not loaded.
I always use the same arguments, same functions, no seed…

## Steps to reproduce the bug
```python
def filter_by_duration(batch):
    return (
        batch[""duration""] <= 10
        and batch[""duration""] >= 1
        and len(batch[""target_text""]) > 5
    )

def prepare_dataset(batch):
    batch[""input_values""] = processor(
        batch[""speech""], sampling_rate=batch[""sampling_rate""][0]
    ).input_values
    with processor.as_target_processor():
        batch[""labels""] = processor(batch[""target_text""]).input_ids
    return batch

train_dataset = train_dataset.filter(
    filter_by_duration,
    remove_columns=[""duration""],
    num_proc=data_args.preprocessing_num_workers,
)

# PROBLEM HERE -> below function is reexecuted and cache is not loaded
train_dataset = train_dataset.map(
    prepare_dataset,
    remove_columns=train_dataset.column_names,
    batch_size=training_args.per_device_train_batch_size,
    batched=True,
    num_proc=data_args.preprocessing_num_workers,
)

# Later in script
set_caching_enabled(False)
# apply map on trained model to eval/test sets

```

## Expected results
The cached dataset should always be reloaded.

## Actual results
The function is reexecuted.

I have access to cached files `cache-xxxxx.arrow`.
Is there a way I can somehow load manually 2 versions and see how the hash was created for debug purposes (to know if it's an issue with dataset or function)?

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: 1.6.2
- Platform: Linux-5.8.0-45-generic-x86_64-with-glibc2.29
- Python version: 3.8.5
- PyTorch version (GPU?): 1.8.1+cu102 (True)
- Tensorflow version (GPU?): not installed (NA)
- Using GPU in script?: Yes
- Using distributed or parallel set-up in script?: No"
https://github.com/huggingface/datasets/issues/2413,AttributeError: 'DatasetInfo' object has no attribute 'task_templates',"[""Hi ! Can you try using a more up-to-date version ? We added the task_templates in `datasets` 1.7.0.\r\n\r\nIdeally when you're working on new datasets, you should install and use the local version of your fork of `datasets`. Here I think you tried to run the 1.7.0 tests with the 1.6.2 code""]","## Describe the bug
Hello, 
I'm trying to add dataset and contribute, but test keep fail with below cli.
` RUN_SLOW=1 pytest tests/test_dataset_common.py::LocalDatasetTest::test_load_dataset_all_configs_<my_dataset>`

## Steps to reproduce the bug
It seems like a bug when I see an error with the existing dataset, not the dataset I'm trying to add.

` RUN_SLOW=1 pytest tests/test_dataset_common.py::LocalDatasetTest::test_load_dataset_all_configs_<any_dataset>`


## Expected results
All test passed

## Actual results
```
                # check that dataset is not empty
                self.parent.assertListEqual(sorted(dataset_builder.info.splits.keys()), sorted(dataset))
                for split in dataset_builder.info.splits.keys():
                    # check that loaded datset is not empty
                    self.parent.assertTrue(len(dataset[split]) > 0)
    
                # check that we can cast features for each task template
>               task_templates = dataset_builder.info.task_templates
E               AttributeError: 'DatasetInfo' object has no attribute 'task_templates'

tests/test_dataset_common.py:175: AttributeError
```


## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: 1.6.2
- Platform: Darwin-20.4.0-x86_64-i386-64bit
- Python version: 3.7.7
- PyTorch version (GPU?): 1.7.0 (False)
- Tensorflow version (GPU?): 2.3.0 (False)
- Using GPU in script?: No
- Using distributed or parallel set-up in script?: No
"
https://github.com/huggingface/datasets/issues/2412,Docstring mistake: dataset vs. metric,['> I can provide a PR l8er...\r\n\r\nSee #2425 '],"This:

https://github.com/huggingface/datasets/blob/d95b95f8cf3cb0cff5f77a675139b584dcfcf719/src/datasets/load.py#L582

Should better be something like:

`a metric identifier on HuggingFace AWS bucket (list all available metrics and ids with ``datasets.list_metrics()``)`

I can provide a PR l8er..."
https://github.com/huggingface/datasets/issues/2407,.map() function got an unexpected keyword argument 'cache_file_name',"[""Hi @cindyxinyiwang,\r\nDid you try adding `.arrow` after `cache_file_name` argument? Here I think they're expecting something like that only for a cache file:\r\nhttps://github.com/huggingface/datasets/blob/e08362256fb157c0b3038437fc0d7a0bbb50de5c/src/datasets/arrow_dataset.py#L1556-L1558""
 ""Hi ! `cache_file_name` is an argument of the `Dataset.map` method. Can you check that your `dataset` is indeed a `Dataset` object ?\r\n\r\nIf you loaded several splits, then it would actually be a `DatasetDict` (one dataset per split, in a dictionary).\r\nIn this case, since there are several datasets in the dict, the `DatasetDict.map` method requires a `cache_file_names` argument (with an 's'), so that you can provide one file name per split.""
 'I think you are right. I used cache_file_names={data1: name1, data2: name2} and it works. Thank you!']","## Describe the bug

I'm trying to save the result of datasets.map() to a specific file, so that I can easily share it among multiple computers without reprocessing the dataset. However, when I try to pass an argument 'cache_file_name' to the .map() function, it throws an error that "".map() function got an unexpected keyword argument 'cache_file_name'"". 

I believe I'm using the latest dataset 1.6.2. Also seems like the document and the actual code indicates there is an argument 'cache_file_name' for the .map() function.

Here is the code I use
## Steps to reproduce the bug
```datasets = load_from_disk(dataset_path=my_path)

[...]

def tokenize_function(examples):
    return tokenizer(examples[text_column_name])

logger.info(""Mapping dataset to tokenized dataset."")
tokenized_datasets = datasets.map(
    tokenize_function,
    batched=True,
    num_proc=preprocessing_num_workers,
    remove_columns=column_names,
    load_from_cache_file=True,
   cache_file_name=""my_tokenized_file""
)
```

## Actual results
    tokenized_datasets = datasets.map(
TypeError: map() got an unexpected keyword argument 'cache_file_name'

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version:1.6.2
- Platform:Linux-4.18.0-193.28.1.el8_2.x86_64-x86_64-with-glibc2.10
- Python version:3.8.5
- PyArrow version:3.0.0
"
https://github.com/huggingface/datasets/issues/2401,"load_dataset('natural_questions') fails with ""ValueError: External features info don't match the dataset""","['I faced the similar problem. Downgrading datasets to 1.5.0 fixed it.'
 ""Thanks for reporting, I'm looking into it""
 'I just opened #2438 to fix this :)'
 'Hi ! This has been fixed in the 1.8.0 release of `datasets`']","## Describe the bug
load_dataset('natural_questions') throws ValueError

## Steps to reproduce the bug
```python
from datasets import load_dataset
datasets = load_dataset('natural_questions', split='validation[:10]')
```

## Expected results
Call to load_dataset returns data.

## Actual results
```
Using custom data configuration default
Reusing dataset natural_questions (/mnt/d/huggingface/datasets/natural_questions/default/0.0.2/19bc04755018a3ad02ee74f7045cde4ba9b4162cb64450a87030ab786b123b76)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-2-d55ab8a8cc1c> in <module>
----> 1 datasets = load_dataset('natural_questions', split='validation[:10]', cache_dir='/mnt/d/huggingface/datasets')

~/miniconda3/lib/python3.8/site-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, script_version, use_auth_token, **config_kwargs)
    756         keep_in_memory if keep_in_memory is not None else is_small_dataset(builder_instance.info.dataset_size)
    757     )
--> 758     ds = builder_instance.as_dataset(split=split, ignore_verifications=ignore_verifications, in_memory=keep_in_memory)
    759     if save_infos:
    760         builder_instance._save_infos()

~/miniconda3/lib/python3.8/site-packages/datasets/builder.py in as_dataset(self, split, run_post_process, ignore_verifications, in_memory)
    735 
    736         # Create a dataset for each of the given splits
--> 737         datasets = utils.map_nested(
    738             partial(
    739                 self._build_single_dataset,

~/miniconda3/lib/python3.8/site-packages/datasets/utils/py_utils.py in map_nested(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, types)
    193     # Singleton
    194     if not isinstance(data_struct, dict) and not isinstance(data_struct, types):
--> 195         return function(data_struct)
    196 
    197     disable_tqdm = bool(logger.getEffectiveLevel() > INFO)

~/miniconda3/lib/python3.8/site-packages/datasets/builder.py in _build_single_dataset(self, split, run_post_process, ignore_verifications, in_memory)
    762 
    763         # Build base dataset
--> 764         ds = self._as_dataset(
    765             split=split,
    766             in_memory=in_memory,

~/miniconda3/lib/python3.8/site-packages/datasets/builder.py in _as_dataset(self, split, in_memory)
    838             in_memory=in_memory,
    839         )
--> 840         return Dataset(**dataset_kwargs)
    841 
    842     def _post_process(self, dataset: Dataset, resources_paths: Dict[str, str]) -> Optional[Dataset]:

~/miniconda3/lib/python3.8/site-packages/datasets/arrow_dataset.py in __init__(self, arrow_table, info, split, indices_table, fingerprint)
    271         assert self._fingerprint is not None, ""Fingerprint can't be None in a Dataset object""
    272         if self.info.features.type != inferred_features.type:
--> 273             raise ValueError(
    274                 ""External features info don't match the dataset:\nGot\n{}\nwith type\n{}\n\nbut expected something like\n{}\nwith type\n{}"".format(
    275                     self.info.features, self.info.features.type, inferred_features, inferred_features.type

ValueError: External features info don't match the dataset:
Got
{'id': Value(dtype='string', id=None), 'document': {'title': Value(dtype='string', id=None), 'url': Value(dtype='string', id=None), 'html': Value(dtype='string', id=None), 'tokens': Sequence(feature={'token': Value(dtype='string', id=None), 'is_html': Value(dtype='bool', id=None)}, length=-1, id=None)}, 'question': {'text': Value(dtype='string', id=None), 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}, 'annotations': Sequence(feature={'id': Value(dtype='string', id=None), 'long_answer': {'start_token': Value(dtype='int64', id=None), 'end_token': Value(dtype='int64', id=None), 'start_byte': Value(dtype='int64', id=None), 'end_byte': Value(dtype='int64', id=None)}, 'short_answers': Sequence(feature={'start_token': Value(dtype='int64', id=None), 'end_token': Value(dtype='int64', id=None), 'start_byte': Value(dtype='int64', id=None), 'end_byte': Value(dtype='int64', id=None), 'text': Value(dtype='string', id=None)}, length=-1, id=None), 'yes_no_answer': ClassLabel(num_classes=2, names=['NO', 'YES'], names_file=None, id=None)}, length=-1, id=None)}
with type
struct<annotations: struct<id: list<item: string>, long_answer: list<item: struct<start_token: int64, end_token: int64, start_byte: int64, end_byte: int64>>, short_answers: list<item: struct<end_byte: list<item: int64>, end_token: list<item: int64>, start_byte: list<item: int64>, start_token: list<item: int64>, text: list<item: string>>>, yes_no_answer: list<item: int64>>, document: struct<title: string, url: string, html: string, tokens: struct<is_html: list<item: bool>, token: list<item: string>>>, id: string, question: struct<text: string, tokens: list<item: string>>>

but expected something like
{'id': Value(dtype='string', id=None), 'document': {'html': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'tokens': {'is_html': Sequence(feature=Value(dtype='bool', id=None), length=-1, id=None), 'token': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}, 'url': Value(dtype='string', id=None)}, 'question': {'text': Value(dtype='string', id=None), 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}, 'annotations': {'id': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'long_answer': [{'end_byte': Value(dtype='int64', id=None), 'end_token': Value(dtype='int64', id=None), 'start_byte': Value(dtype='int64', id=None), 'start_token': Value(dtype='int64', id=None)}], 'short_answers': [{'end_byte': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'end_token': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'start_byte': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'start_token': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'text': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}], 'yes_no_answer': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}}
with type
struct<annotations: struct<id: list<item: string>, long_answer: list<item: struct<end_byte: int64, end_token: int64, start_byte: int64, start_token: int64>>, short_answers: list<item: struct<end_byte: list<item: int64>, end_token: list<item: int64>, start_byte: list<item: int64>, start_token: list<item: int64>, text: list<item: string>>>, yes_no_answer: list<item: int64>>, document: struct<html: string, title: string, tokens: struct<is_html: list<item: bool>, token: list<item: string>>, url: string>, id: string, question: struct<text: string, tokens: list<item: string>>>
```

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: 1.6.2
- Platform: Linux-5.4.72-microsoft-standard-WSL2-x86_64-with-glibc2.10
- Python version: 3.8.3
- PyTorch version (GPU?): 1.6.0 (False)
- Tensorflow version (GPU?): not installed (NA)
- Using GPU in script?: No
- Using distributed or parallel set-up in script?: No
"
https://github.com/huggingface/datasets/issues/2400,Concatenate several datasets with removed columns is not working.,"[""Hi,\r\n\r\ndid you fill out the env info section manually or by copy-pasting the output of the `datasets-cli env` command?\r\n\r\nThis code should work without issues on 1.6.2 version (I'm working on master (1.6.2.dev0 version) and can't reproduce this error).""
 '@mariosasko you are right I was still on `1.5.0`. ']","## Describe the bug

You can't concatenate datasets when you removed columns before.

## Steps to reproduce the bug
```python
from datasets import load_dataset, concatenate_datasets

wikiann= load_dataset(""wikiann"",""en"")

wikiann[""train""] = wikiann[""train""].remove_columns([""langs"",""spans""])
wikiann[""test""] = wikiann[""test""].remove_columns([""langs"",""spans""])

assert wikiann[""train""].features.type == wikiann[""test""].features.type

concate = concatenate_datasets([wikiann[""train""],wikiann[""test""]])
```

## Expected results
Merged dataset 


## Actual results
```python
ValueError: External features info don't match the dataset:
Got
{'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(num_classes=7, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], names_file=None, id=None), length=-1, id=None), 'langs': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'spans': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}
with type
struct<langs: list<item: string>, ner_tags: list<item: int64>, spans: list<item: string>, tokens: list<item: string>>

but expected something like
{'ner_tags': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}
with type
struct<ner_tags: list<item: int64>, tokens: list<item: string>>
```
## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: ~1.6.2~ 1.5.0
- Platform: macos
- Python version: 3.8.5
- PyArrow version: 3.0.0
"
https://github.com/huggingface/datasets/issues/2396,strange datasets from OSCAR corpus,"[""Hi ! Thanks for reporting\r\ncc @pjox is this an issue from the data ?\r\n\r\nAnyway we should at least mention that OSCAR could contain such contents in the dataset card, you're totally right @jerryIsHere ""
 ""Hi @jerryIsHere , sorry for the late response! Sadly this is normal, the problem comes form fasttext's classifier which we used to create the original corpus. In general the classifier is not really capable of properly recognizing Yue Chineese so the file ends un being just noise from Common Crawl. Some of these problems with OSCAR were already discussed [here](https://arxiv.org/pdf/2103.12028.pdf) but we are working on explicitly documenting the problems by language on our website. In fact, could please you open an issue on [our repo](https://github.com/oscar-corpus/oscar-website/issues) as well so that we can track it?""]","![image](https://user-images.githubusercontent.com/50871412/119260850-4f876b80-bc07-11eb-8894-124302600643.png)
![image](https://user-images.githubusercontent.com/50871412/119260875-675eef80-bc07-11eb-9da4-ee27567054ac.png)
From the [official site ](https://oscar-corpus.com/), the Yue Chinese dataset should have 2.2KB data.
7 training instances is obviously not a right number.
As I can read Yue Chinese, I call tell the last instance is definitely not something that would appear on Common Crawl.
And even if you don't read Yue Chinese, you can tell the first six instance are problematic.
(It is embarrassing, as the 7 training instances look exactly like something from a pornographic novel or flitting messages in a chat of a dating app)
It might not be the problem of the huggingface/datasets implementation, because when I tried to download the dataset from the official site, I found out that the zip file is corrupted.
I will try to inform the host of OSCAR corpus later.
Awy a remake about this dataset in huggingface/datasets is needed, perhaps after the host of the dataset fixes the issue.

> Hi @jerryIsHere , sorry for the late response! Sadly this is normal, the problem comes form fasttext's classifier which we used to create the original corpus. In general the classifier is not really capable of properly recognizing Yue Chineese so the file ends un being just noise from Common Crawl. Some of these problems with OSCAR were already discussed [here](https://arxiv.org/pdf/2103.12028.pdf) but we are working on explicitly documenting the problems by language on our website. In fact, could please you open an issue on [our repo](https://github.com/oscar-corpus/oscar-website/issues) as well so that we can track it?

Thanks a lot, the new post is here:
https://github.com/oscar-corpus/oscar-website/issues/11"
https://github.com/huggingface/datasets/issues/2391,Missing original answers in kilt-TriviaQA,"[""That could be useful indeed! Feel free to open a PR on the dataset card if you already have some code that runs, otherwise we'll take care of it soon :) ""
 ""I can open a PR but there is 2 details to fix:\r\n- the name for the corresponding key (e.g. `original_answer`)\r\n- how to implement it: I’m not sure what happens when you map `lambda x: {'input': ...}`\xa0as it keeps the other keys (e.g. `output`) intact but here since we want to set a nested value (e.g. `x['output']['original_answer']`) I implemented it with a regular function (not lambda), see below\r\n\r\n```py\r\ndef add_original_answer(x, trivia_qa, triviaqa_map):\r\n    i = triviaqa_map[x['id']]\r\n    x['output']['original_answer'] = trivia_qa['validation'][i]['answer']['value']\r\n    return x\r\n```""]","I previously opened an issue at https://github.com/facebookresearch/KILT/issues/42 but from the answer of @fabiopetroni it seems that the problem comes from HF-datasets

## Describe the bug
The `answer` field in kilt-TriviaQA, e.g. `kilt_tasks['train_triviaqa'][0]['output']['answer']` contains a list of alternative answer which are accepted for the question.  
However it'd be nice to know the original answer to the question (the only fields in `output` are `'answer', 'meta', 'provenance'`)

## How to fix
It can be fixed by retrieving the original answer from the original TriviaQA (e.g. `trivia_qa['train'][0]['answer']['value']`), perhaps at the same place as here where one retrieves the questions https://github.com/huggingface/datasets/blob/master/datasets/kilt_tasks/README.md#loading-the-kilt-knowledge-source-and-task-data

cc @yjernite who previously answered to an issue about KILT and TriviaQA :)
"
https://github.com/huggingface/datasets/issues/2387,datasets 1.6 ignores cache,"['Looks like there are multiple issues regarding this (#2386, #2322) and it\'s a WIP #2329. Currently these datasets are being loaded in-memory which is causing this issue. Quoting @mariosasko here for a quick fix:\r\n\r\n> set `keep_in_memory` to `False` when loading a dataset (`sst = load_dataset(""sst"", keep_in_memory=False)`) to prevent it from loading in-memory. Currently, in-memory datasets fail to find cached files due to this check (always False for them)\r\n\r\n'
 ""Hi ! Since `datasets` 1.6.0 we no longer keep small datasets (<250MB) on disk and load them in RAM instead by default. This makes data processing and iterating on data faster. However datasets in RAM currently have no way to reload previous results from the cache (since nothing is written on disk). We are working on making the caching work for datasets in RAM.\r\n\r\nUntil then, I'd recommend passing `keep_in_memory=False` to the calls to `load_dataset` like here:\r\n\r\nhttps://github.com/huggingface/transformers/blob/223943872e8c9c3fc11db3c6e93da07f5177423f/examples/pytorch/language-modeling/run_clm.py#L233\r\n\r\nThis way you say explicitly that you want your dataset to stay on the disk, and it will be able to recover previously computed results from the cache.""
 'gotcha! thanks Quentin'
 ""OK, It doesn't look like we can use the proposed workaround - see https://github.com/huggingface/transformers/issues/11801\r\n\r\nCould you please add an env var for us to be able to turn off this unwanted in our situation behavior? It is really problematic for dev work, when one needs to restart the training very often and needs a quick startup time. Manual editing of standard scripts is not a practical option when one uses examples.\r\n\r\nThis could also be a problem for tests, which will be slower because of lack of cache, albeit usually we use tiny datasets there. I think we want caching for tests.\r\n\r\nThank you.""
 'Hi @stas00, \r\n\r\nYou are right: an env variable is needed to turn off this behavior. I am adding it.\r\n\r\nFor the moment there is a config parameter to turn off this behavior: `datasets.config.MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES = None`\r\n\r\nYou can find this info in the docs:\r\n- in the docstring of the parameter `keep_in_memory` of the function [`load_datasets`](https://huggingface.co/docs/datasets/package_reference/loading_methods.html#datasets.load_dataset):\r\n- in a Note in the docs about [Loading a Dataset](https://huggingface.co/docs/datasets/loading_datasets.html#from-the-huggingface-hub)\r\n\r\n> The default in 🤗Datasets is to memory-map the dataset on drive if its size is larger than datasets.config.MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES (default 250 MiB); otherwise, the dataset is copied in-memory. This behavior can be disabled by setting datasets.config.MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES = None, and in this case the dataset is not loaded in memory.'
 ""Yes, but this still requires one to edit the standard example scripts, so if I'm doing that already I just as well can add `keep_in_memory=False`.\r\n\r\nMay be the low hanging fruit is to add `MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES` env var to match the config, and if the user sets it to 0, then it'll be the same as `keep_in_memory=False` or `datasets.config.MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES=0`?""
 '@stas00, however, for the moment, setting the value to `0` is equivalent to the opposite, i.e. `keep_in_memory=True`. This means the max size until which I load in memory is 0 bytes.\r\n\r\nTell me if this is logical/convenient, or I should change it.'
 'In my PR, to turn off current default bahavior, you should set env variable to one of: `{"""", ""OFF"", ""NO"", ""FALSE""}`.\r\n\r\nFor example:\r\n```\r\nMAX_IN_MEMORY_DATASET_SIZE_IN_BYTES=\r\n```'
 'IMHO, this behaviour is not very intuitive, as 0 is a normal quantity of bytes. So `MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES=0` to me reads as don\'t cache ever.\r\n\r\nAlso ""SIZE_IN_BYTES"" that can take one of `{"""", ""OFF"", ""NO"", ""FALSE""}` is also quite odd.\r\n\r\nI think supporting a very simple `MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES` that can accept any numerical value to match the name of the variable, requires minimal logic and is very straightforward. \r\n\r\nSo if you could adjust this logic - then `MAX_IN_MEMORY_DATASET_SIZE_IN_BYTES=0` is all that\'s needed to not do in-memory datasets.\r\n\r\nDoes it make sense?'
 'I understand your point @stas00, as I am not very convinced with current implementation.\r\n\r\nMy concern is: which numerical value should then pass a user who wants `keep_in_memory=True` by default, independently of dataset size? Currently it is `0` for this case.'
 ""That's a good question, and again the normal bytes can be used for that:\r\n```\r\nMAX_IN_MEMORY_DATASET_SIZE_IN_BYTES=1e12 # (~2**40)\r\n```\r\nSince it's unlikely that anybody will have more than 1TB RAM.\r\n\r\nIt's also silly that it uses BYTES and not MBYTES - that level of refinement doesn't seem to be of a practical use in this context.\r\n\r\nNot sure when it was added and if there are back-compat issues here, but perhaps it could be renamed `MAX_IN_MEMORY_DATASET_SIZE` and support 1M, 1G, 1T, etc. \r\n\r\nBut scientific notation is quite intuitive too, as each 000 zeros is the next M, G, T multiplier. Minus the discrepancy of 1024 vs 1000, which adds up. And it is easy to write down `1e12`, as compared to `1099511627776` (2**40).  (`1.1e12` is more exact).\r\n""
 'Great! Thanks, @stas00.\r\n\r\nI am implementing your suggestion to turn off default value when set to `0`.\r\n\r\nFor the other suggestion (allowing different metric prefixes), I will discuss with @lhoestq to agree on its implementation.'
 'Awesome! Thank you, @albertvillanova!!!\r\n\r\n']","Moving from https://github.com/huggingface/transformers/issues/11801#issuecomment-845546612 

Quoting @VictorSanh:

> 
> I downgraded datasets to `1.5.0` and printed `tokenized_datasets.cache_files` (L335):
> 
> > `{'train': [{'filename': '/home/victor/.cache/huggingface/datasets/openwebtext10k/plain_text/1.0.0/3a8df094c671b4cb63ed0b41f40fb3bd855e9ce2e3765e5df50abcdfb5ec144b/cache-c6aefe81ca4e5152.arrow'}], 'validation': [{'filename': '/home/victor/.cache/huggingface/datasets/openwebtext10k/plain_text/1.0.0/3a8df094c671b4cb63ed0b41f40fb3bd855e9ce2e3765e5df50abcdfb5ec144b/cache-97cf4c813e6469c6.arrow'}]}`
> 
> while the same command with the latest version of datasets (actually starting at `1.6.0`) gives:
> > `{'train': [], 'validation': []}`
> 

I also confirm that downgrading to `datasets==1.5.0` makes things fast again - i.e. cache is used.

to reproduce:
```
USE_TF=0 python  examples/pytorch/language-modeling/run_clm.py \
    --model_name_or_path gpt2 \
    --dataset_name ""stas/openwebtext-10k"" \
    --output_dir output_dir \
    --overwrite_output_dir \
    --do_train \
    --do_eval \
    --max_train_samples 1000 \
    --max_eval_samples 200 \
    --per_device_train_batch_size 4 \
    --per_device_eval_batch_size 4 \
    --num_train_epochs 1 \
    --warmup_steps 8 \
    --block_size 64 \
    --fp16 \
    --report_to none
```

the first time the startup is slow and some 5 tqdm bars. It shouldn't do it on consequent runs. but with `datasets>1.5.0` it rebuilds on every run.

@lhoestq 
"
https://github.com/huggingface/datasets/issues/2386,Accessing Arrow dataset cache_files,['Thanks @bhavitvyamalik for referencing the workaround. Setting `keep_in_memory=False` is working.'],"## Describe the bug
In datasets 1.5.0 the following code snippet would have printed the cache_files:

```
train_data = load_dataset('conll2003', split='train', cache_dir='data')
print(train_data.cache_files[0]['filename'])

```

However, in the newest release (1.6.1), it prints an empty list.

I also tried loading the dataset with `keep_in_memory=True` argument but still `cache_files` is empty.

Was wondering if this is a bug or I need to pass additional arguments so I can access the cache_files.
"
https://github.com/huggingface/datasets/issues/2377,ArrowDataset.save_to_disk produces files that cannot be read using pyarrow.feather,['Hi ! This is because we are actually using the arrow streaming format. We plan to switch to the arrow IPC format.\r\nMore info at #1933 '],"## Describe the bug
A clear and concise description of what the bug is.

## Steps to reproduce the bug
```python
from datasets import load_dataset
from pyarrow import feather

dataset = load_dataset('imdb', split='train')
dataset.save_to_disk('dataset_dir')
table = feather.read_table('dataset_dir/dataset.arrow')
```

## Expected results
I expect that the saved dataset can be read by the official Apache Arrow methods.

## Actual results
```
  File ""/usr/local/lib/python3.7/site-packages/pyarrow/feather.py"", line 236, in read_table
    reader.open(source, use_memory_map=memory_map)
  File ""pyarrow/feather.pxi"", line 67, in pyarrow.lib.FeatherReader.open
  File ""pyarrow/error.pxi"", line 123, in pyarrow.lib.pyarrow_internal_check_status
  File ""pyarrow/error.pxi"", line 85, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: Not a Feather V1 or Arrow IPC file
```

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: datasets-1.6.2
- Platform: Linux
- Python version: 3.7
- PyArrow version: 0.17.1, also 2.0.0
"
https://github.com/huggingface/datasets/issues/2373,Loading dataset from local path,"[""Version below works, checked again in the docs, and data_files should be a path.\r\n```\r\nds = datasets.load_dataset('my_script.py', \r\n                           data_files='/data/dir/corpus.txt', \r\n                           cache_dir='.')\r\n```""]","I'm trying to load a local dataset with the code below

```
ds = datasets.load_dataset('my_script.py', 
                           data_files='corpus.txt', 
                           data_dir='/data/dir', 
                           cache_dir='.')
```
But internally a BuilderConfig is created, which tries to use getmtime on the data_files string, without using data_dir. Is this a bug or am I not using the load_dataset correctly?

https://github.com/huggingface/datasets/blob/bc61954083f74e6460688202e9f77dde2475319c/src/datasets/builder.py#L153"
https://github.com/huggingface/datasets/issues/2363,Trying to use metric.compute but get OSError,"['also, I test the function on some little data ,  get the same message:\r\n\r\n```\r\nPython 3.8.5 (default, Jan 27 2021, 15:41:15)\r\n[GCC 9.3.0] on linux\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n>>> from datasets import load_metric\r\n>>> metric = load_metric(\'accuracy\')\r\n>>> metric.add_batch(predictions=[1, 1, 1, 1], references=[1, 1, 0, 0])\r\n2021-05-15 16:39:17.240991: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n>>> metric.compute()\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/yshuang/.local/lib/python3.8/site-packages/datasets/metric.py"", line 391, in compute\r\n    self._finalize()\r\n  File ""/home/yshuang/.local/lib/python3.8/site-packages/datasets/metric.py"", line 342, in _finalize\r\n    self.writer.finalize()\r\n  File ""/home/yshuang/.local/lib/python3.8/site-packages/datasets/arrow_writer.py"", line 370, in finalize\r\n    self.stream.close()\r\n  File ""pyarrow/io.pxi"", line 132, in pyarrow.lib.NativeFile.close\r\n  File ""pyarrow/error.pxi"", line 112, in pyarrow.lib.check_status\r\nOSError: error closing file\r\n```'
 ""Hi @hyusterr,\r\nIf you look at the example provided in `metrics/accuracy.py`, it only does `metric.compute()` to calculate the accuracy. Here's an example:\r\n```\r\nfrom datasets import load_metric\r\nmetric = load_metric('accuracy')\r\noutput = metric.compute(predictions=[1, 1, 1, 1], references=[1, 1, 0, 0])\r\nprint(output['accuracy'])  # 0.5\r\n```\r\n""
 ""I thought I can use Metric to collect predictions and references, this follows the step from huggingface's sample colab.\r\nBTW, I fix the problem by setting other cache_dir in load_metric, but I'm still wondering about the mechanism.""
 ""I tried this code on a colab notebook and it worked fine (with gpu enabled):\r\n```\r\nfrom datasets import load_metric\r\nmetric = load_metric('accuracy')\r\noutput = metric.add_batch(predictions=[1, 1, 1, 1], references=[1, 1, 0, 0])\r\nfinal_score = metric.compute()\r\nprint(final_score)  # 0.5\r\n```\r\nAlso, in `load_metric`, I saw `cache_dir` is optional and it defaults to `~/.datasets/`""
 'Hi ! By default it caches the predictions and references used to compute the metric in `~/.cache/huggingface/datasets/metrics` (not `~/.datasets/`). Let me update the documentation @bhavitvyamalik .\r\n\r\nThe cache is used to store all the predictions and references passed to `add_batch` for example in order to compute the metric later when `compute` is called.\r\n\r\nI think the issue might come from the cache directory that is used by default. Can you check that you have the right permissions ? Otherwise feel free to set `cache_dir` to another location.']","I want to use metric.compute from load_metric('accuracy') to get training accuracy, but receive OSError. I am wondering what is the mechanism behind the metric calculation, why would it report an OSError?

```python
195     for epoch in range(num_train_epochs):
196         model.train()
197         for step, batch in enumerate(train_loader):
198             # print(batch['input_ids'].shape)
199             outputs = model(**batch)
200
201             loss = outputs.loss
202             loss /= gradient_accumulation_steps
203             accelerator.backward(loss)
204
205             predictions = outputs.logits.argmax(dim=-1)
206             metric.add_batch(
207                 predictions=accelerator.gather(predictions),
208                 references=accelerator.gather(batch['labels'])
209             )
210             progress_bar.set_postfix({'loss': loss.item(), 'train batch acc.': train_metrics})
211
212             if (step + 1) % 50 == 0 or step == len(train_loader) - 1:
213                 train_metrics = metric.compute()
```

the error message is as below:

```
Traceback (most recent call last):
  File ""run_multi.py"", line 273, in <module>
    main()
  File ""/home/yshuang/.local/lib/python3.8/site-packages/click/core.py"", line 829, in __call__
    return self.main(*args, **kwargs)
  File ""/home/yshuang/.local/lib/python3.8/site-packages/click/core.py"", line 782, in main
    rv = self.invoke(ctx)
  File ""/home/yshuang/.local/lib/python3.8/site-packages/click/core.py"", line 1066, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/home/yshuang/.local/lib/python3.8/site-packages/click/core.py"", line 610, in invoke
    return callback(*args, **kwargs)
  File ""run_multi.py"", line 213, in main
    train_metrics = metric.compute()
  File ""/home/yshuang/.local/lib/python3.8/site-packages/datasets/metric.py"", line 391, in compute
    self._finalize()
  File ""/home/yshuang/.local/lib/python3.8/site-packages/datasets/metric.py"", line 342, in _finalize
    self.writer.finalize()
  File ""/home/yshuang/.local/lib/python3.8/site-packages/datasets/arrow_writer.py"", line 370, in finalize
    self.stream.close()
  File ""pyarrow/io.pxi"", line 132, in pyarrow.lib.NativeFile.close
  File ""pyarrow/error.pxi"", line 99, in pyarrow.lib.check_status
OSError: error closing file
```

## Environment info
<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->
- `datasets` version: 1.6.1
- Platform: Linux NAME=""Ubuntu"" VERSION=""20.04.1 LTS (Focal Fossa)""
- Python version: python3.8.5
- PyArrow version: 4.0.0
"
https://github.com/huggingface/datasets/issues/2356,How to Add New Metrics Guide,"[""Hi ! sorry for the late response \r\n\r\nIt would be fantastic to have a guide for adding metrics as well ! Currently we only have this template here:\r\nhttps://github.com/huggingface/datasets/blob/master/templates/new_metric_script.py\r\n\r\nWe can also include test utilities for metrics in the guide.\r\n\r\nWe have a pytest suite with commands that you can use to make sure your metric works as expected.\r\nIt has two useful commands:\r\n\r\n1. This commands tests the code in the `Examples:` desction of the docstring of the metric:\r\n```\r\npytest tests/test_metric_common.py::LocalMetricTest::test_load_metric_<metric_name>\r\n```\r\nThis will run this code for example:\r\n\r\nhttps://github.com/huggingface/datasets/blob/e0787aa2a781cc15a80f7597f56d1f12e23df4c9/metrics/accuracy/accuracy.py#L40-L45\r\n\r\nMoreover this test is meant to be fast so users are free to add patches to the metric to avoid intensive computations.\r\nAnd example of intensive call patch can be found here:\r\n\r\nhttps://github.com/huggingface/datasets/blob/e0787aa2a781cc15a80f7597f56d1f12e23df4c9/tests/test_metric_common.py#L138-L151\r\n\r\n2. This test runs the same thing as 1. except that it doesn't use patches (the real metric is used):\r\n```\r\nRUN_SLOW=1 pytest tests/test_metric_common.py::LocalMetricTest::test_load_metric_<metric_name>\r\n```\r\n\r\nFinally additional metric-specific tests can be added to `test_metric_common.py`.\r\n\r\nVoila :) Feel free to ping me if you have any question or if I can help\r\n""]","**Is your feature request related to a problem? Please describe.**
Currently there is an absolutely fantastic guide for how to contribute a new dataset to the library. However, there isn't one for adding new metrics.

**Describe the solution you'd like**
I'd like for a guide in a similar style to the dataset guide for adding metrics. I believe many of the content in the dataset guide such as setup can be easily copied over with minimal changes. Also, from what I've seen with existing metrics, it shouldn't be as complicated, especially in documentation of the metric, mainly just citation and usage. The most complicated part I see would be in automated tests that run the new metrics, but y'all's test suite seem pretty comprehensive, so it might not be that hard.

**Describe alternatives you've considered**
One alternative would be just not having the metrics be community generated and so would not need a step by step guide. New metrics would just be proposed as issues and the internal team would take care of them. However, I think it makes more sense to have a step by step guide for contributors to follow.

**Additional context**
I'd be happy to help with creating this guide as I am very interested in adding software engineering metrics to the library :nerd_face:, the part I would need guidance on would be testing.

P.S. Love the library and community y'all have built! :hugs: 
"
https://github.com/huggingface/datasets/issues/2350,`FaissIndex.save` throws error on GPU,"['Just in case, this is a workaround that I use in my code and it seems to do the job.\r\n\r\n```python\r\nif use_gpu_index:\r\n    data[""train""]._indexes[""text_emb""].faiss_index = faiss.index_gpu_to_cpu(data[""train""]._indexes[""text_emb""].faiss_index)\r\n```']","## Describe the bug

After training an index with a factory string `OPQ16_128,IVF512,PQ32` on GPU, `.save_faiss_index` throws this error.

```
  File ""index_wikipedia.py"", line 119, in <module>
    data[""train""].save_faiss_index(""text_emb"", index_save_path)
  File ""/home/vlialin/miniconda3/envs/cat/lib/python3.8/site-packages/datasets/search.py"", line 470, in save_faiss_index
    index.save(file)
  File ""/home/vlialin/miniconda3/envs/cat/lib/python3.8/site-packages/datasets/search.py"", line 334, in save
    faiss.write_index(index, str(file))
  File ""/home/vlialin/miniconda3/envs/cat/lib/python3.8/site-packages/faiss/swigfaiss_avx2.py"", line 5654, in write_index
    return _swigfaiss.write_index(*args)
RuntimeError: Error in void faiss::write_index(const faiss::Index*, faiss::IOWriter*) at /root/miniconda3/conda-bld/faiss-pkg_1613235005464/work/faiss/impl/index_write.cpp:453: don't know how to serialize this type of index
```

## Steps to reproduce the bug

Any dataset will do, I just selected a familiar one.

```python
import numpy as np
import datasets
INDEX_STR = ""OPQ16_128,IVF512,PQ32""
INDEX_SAVE_PATH = ""will_not_save.faiss""

data = datasets.load_dataset(""Fraser/news-category-dataset"", split=f""train[:10000]"")

def encode(item):
    return {""text_emb"": np.random.randn(768).astype(np.float32)}

data = data.map(encode)

data.add_faiss_index(column=""text_emb"", string_factory=INDEX_STR, train_size=10_000, device=0)
data.save_faiss_index(""text_emb"", INDEX_SAVE_PATH)
```

## Expected results
Saving the index

## Actual results
Error in void faiss::write_index(const faiss::Index*, faiss::IOWriter*) ... don't know how to serialize this type of index

## Environment info
- `datasets` version: 1.6.2
- Platform: Linux-4.15.0-142-generic-x86_64-with-glibc2.10
- Python version: 3.8.8
- PyTorch version (GPU?): 1.8.1+cu111 (True)
- Tensorflow version (GPU?): 2.2.0 (False)
- Using GPU in script?: Yes
- Using distributed or parallel set-up in script?: No


I will be proposing a fix in a couple of minutes"
https://github.com/huggingface/datasets/issues/2347,Add an API to access the language and pretty name of a dataset,"['Hi ! With @bhavitvyamalik we discussed about having something like\r\n```python\r\nfrom datasets import load_dataset_card\r\n\r\ndataset_card = load_dataset_card(""squad"")\r\nprint(dataset_card.metadata.pretty_name)\r\n# Stanford Question Answering Dataset (SQuAD)\r\nprint(dataset_card.metadata.languages)\r\n# [""en""]\r\n\r\n```\r\nWhat do you think ?\r\n\r\nI don\'t know if you already have a way to load the model tags in `transformers` but we can agree on the API to have something consistent.\r\n\r\nAlso note that the pretty name would only be used to show users something prettier than a dataset id, but in the end the source of truth will stay the dataset id (here `squad`).'
 'That works for me!'
 ""maybe use the hub-backed dataset_info method? (so there's only one parser of README.md metadata)?""
 'What dataset_info method are you talking about @julien-c ? In `huggingface_hub` I can only see `model_info`.'
 'hmm the equivalent method in `datasets` (which could go into `huggingface_hub` at some point)']",It would be super nice to have an API to get some metadata of the dataset from the name and args passed to `load_dataset`. This way we could programmatically infer the language and the name of a dataset when creating model cards automatically in the Transformers examples scripts.
https://github.com/huggingface/datasets/issues/2345,[Question] How to move and reuse preprocessed dataset? ,"['@lhoestq @LysandreJik'
 ""<s>Hi :) Can you share with us the code you used ?</s>\r\n\r\nEDIT: from https://github.com/huggingface/transformers/issues/11665#issuecomment-838348291 I understand you're using the run_clm.py script. Can you share your logs ?\r\n""
 'Also note that for the caching to work, you must reuse the exact same parameters as in the first run. Did you change any parameter ? The `preprocessing_num_workers` should also stay the same'
 '> Also note that for the caching to work, you must reuse the exact same parameters as in the first run. Did you change any parameter ? The `preprocessing_num_workers` should also stay the same\r\n\r\nI only changed the `preprocessing_num_workers` maybe it is the problem~ I will try again~']","Hi, I am training a gpt-2 from scratch using run_clm.py.

I want to move and reuse the preprocessed dataset (It take 2 hour to preprocess),

I tried to :

copy path_to_cache_dir/datasets to new_cache_dir/datasets
set export HF_DATASETS_CACHE=""new_cache_dir/""
but the program still re-preprocess the whole dataset without loading cache.

I also tried to torch.save(lm_datasets, fw), but the saved file is only 14M.

What is the proper way to do this?"
https://github.com/huggingface/datasets/issues/2344,Is there a way to join multiple datasets in one?,"[""Hi ! We don't have `join`/`merge` on a certain column as in pandas.\r\nMaybe you can just use the [concatenate_datasets](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=concatenate#datasets.concatenate_datasets) function.\r\n""]","**Is your feature request related to a problem? Please describe.**
I need to join 2 datasets, one that is in the hub and another I've created from my files. Is there an easy way to join these 2? 

**Describe the solution you'd like**
Id like to join them with a merge or join method, just like pandas dataframes. 

**Additional context**
If you want to extend an existing dataset with more data, for example for training a language model, you need that functionality. I've not found it in the documentation."
https://github.com/huggingface/datasets/issues/2337,NonMatchingChecksumError for web_of_science dataset,"['I\'ve raised a PR for this. Should work with `dataset = load_dataset(""web_of_science"", ""WOS11967"", ignore_verifications=True)`once it gets merged into the main branch. Thanks for reporting this! ']","NonMatchingChecksumError when trying to download the web_of_science dataset. 

>NonMatchingChecksumError: Checksums didn't match for dataset source files:
['https://data.mendeley.com/datasets/9rw3vkcfy4/6/files/c9ea673d-5542-44c0-ab7b-f1311f7d61df/WebOfScience.zip?dl=1']

Setting `ignore_verfications=True` results in OSError.

>OSError: Cannot find data file. 
Original error:
[Errno 20] Not a directory: '/root/.cache/huggingface/datasets/downloads/37ab2c42f50d553c1d0ea432baca3e9e11fedea4aeec63a81e6b7e25dd10d4e7/WOS5736/X.txt'

```python
dataset = load_dataset('web_of_science', 'WOS5736')
```
There are 3 data instances and they all don't work. 'WOS5736', 'WOS11967', 'WOS46985'

datasets 1.6.2
python 3.7.10
Ubuntu 18.04.5 LTS"
https://github.com/huggingface/datasets/issues/2330,Allow passing `desc` to `tqdm` in `Dataset.map()`,"['Hi @lhoestq,\r\nShould we change `desc` in [pbar](https://github.com/huggingface/datasets/blob/81fcf88172ed5e3026ef68aed4c0ec6980372333/src/datasets/arrow_dataset.py#L1860) to something meaningful?'
 'I think the user could pass the `desc` parameter to `map` so that it can be displayed in the tqdm progress bar, as suggested by @cccntu.\r\n\r\nWhen there\'s no multiprocessing, the `desc` of the progress bar could be the `desc` passed by the user.\r\nIn multiprocessing, we were already using a `desc` equal to `""#"" + str(rank)`.\r\nWe can change it to be `(desc or """") + ""#"" + str(rank)` instead.\r\n\r\nIn the end, since both `desc` and `rank` could be None, we can have:\r\n```python\r\npbar_desc = (desc or """") + ""#"" + str(rank) if rank is not None else desc\r\n```\r\n\r\nFinally let\'s remember that if we add `desc` as a new parameter to `map`, we should add it to the `ignore_kwargs` list of the `@fingerprint_transform` decorator of `Dataset._map_single` since we don\'t want this parameter to affect the fingerprint of the resulting dataset.']","It's normal to have many `map()` calls, and some of them can take a few minutes,
it would be nice to have a description on the progress bar.

Alternative solution:
Print the description before/after the `map()` call."
https://github.com/huggingface/datasets/issues/2327,A syntax error in example,"['cc @beurkinger but I think this has been fixed internally and will soon be updated right ?'
 'This issue has been fixed.']","![image](https://user-images.githubusercontent.com/6883957/117315905-b47a5c00-aeba-11eb-91eb-b2a4a0212a56.png)

Sorry to report with an image, I can't find the template source code of this snippet."
https://github.com/huggingface/datasets/issues/2323,"load_dataset(""timit_asr"") gives back duplicates of just one sample text","['Upgrading datasets to version 1.6 fixes the issue'
 'This bug was fixed in #1995. Upgrading the `datasets` should work! '
 'Thanks @ekeleshian for having reported.\r\n\r\nI am closing this issue once that you updated `datasets`. Feel free to reopen it if the problem persists.']","## Describe the bug
When you look up on key [""train""] and then ['text'], you get back a list  with just one sentence duplicated 4620 times. Namely, the sentence ""Would such an act of refusal be useful?"". Similarly when you look up ['test'] and then ['text'], the list is one sentence repeated ""The bungalow was pleasantly situated near the shore."" 1680 times. 

I tried to work around the issue by downgrading to datasets version 1.3.0, inspired by [this post](https://www.gitmemory.com/issue/huggingface/datasets/2052/798904836) and removing the entire huggingface directory from ~/.cache, but I still get the same issue.  

## Steps to reproduce the bug
```python
from datasets import load_dataset
timit = load_dataset(""timit_asr"")
print(timit['train']['text'])
print(timit['test']['text'])
```

## Expected Result
Rows of diverse text, like how it is shown in the [wav2vec2.0 tutorial](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_tuning_Wav2Vec2_for_English_ASR.ipynb)
<img width=""485"" alt=""Screen Shot 2021-05-05 at 9 09 57 AM"" src=""https://user-images.githubusercontent.com/33647474/117146094-d9b77f00-ad81-11eb-8306-f281850c127a.png"">


## Actual results
Rows of repeated text.
<img width=""319"" alt=""Screen Shot 2021-05-05 at 9 11 53 AM"" src=""https://user-images.githubusercontent.com/33647474/117146231-f8b61100-ad81-11eb-834a-fc10410b0c9c.png"">


## Versions
- Datasets: 1.3.0
- Python: 3.9.1
- Platform: macOS-11.2.1-x86_64-i386-64bit}
"
https://github.com/huggingface/datasets/issues/2322,Calls to map are not cached.,"['I tried upgrading to `datasets==1.6.2` and downgrading to `1.6.0`. Both versions produce the same output.\r\n\r\nDowngrading to `1.5.0` works and produces the following output for me:\r\n\r\n```bash\r\nDownloading: 9.20kB [00:00, 3.94MB/s]                   \r\nDownloading: 5.99kB [00:00, 3.29MB/s]                   \r\nNo config specified, defaulting to: sst/default\r\nDownloading and preparing dataset sst/default (download: 6.83 MiB, generated: 3.73 MiB, post-processed: Unknown size, total: 10.56 MiB) to /home/johannes/.cache/huggingface/datasets/sst/default/1.0.0/a16a45566b63b2c3179e6c1d0f8edadde56e45570ee8cf99394fbb738491d34b...\r\n                                    Dataset sst downloaded and prepared to /home/johannes/.cache/huggingface/datasets/sst/default/1.0.0/a16a45566b63b2c3179e6c1d0f8edadde56e45570ee8cf99394fbb738491d34b. Subsequent calls will reuse this data.\r\nexecuted [0, 1]\r\n#0:   0%|          | 0/5 [00:00<?, ?ba/s]\r\n#1:   0%|          | 0/5 [00:00<?, ?ba/s]\r\nexecuted [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\r\nexecuted [4272, 4273, 4274, 4275, 4276, 4277, 4278, 4279, 4280, 4281]\r\nexecuted [1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009]\r\nexecuted [5272, 5273, 5274, 5275, 5276, 5277, 5278, 5279, 5280, 5281]\r\nexecuted [2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009]\r\nexecuted [6272, 6273, 6274, 6275, 6276, 6277, 6278, 6279, 6280, 6281]\r\nexecuted [3000, 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, 3009]\r\nexecuted [7272, 7273, 7274, 7275, 7276, 7277, 7278, 7279, 7280, 7281]\r\nexecuted [4000, 4001, 4002, 4003, 4004, 4005, 4006, 4007, 4008, 4009]\r\n#0: 100%|██████████| 5/5 [00:00<00:00, 94.83ba/s]\r\nexecuted [8272, 8273, 8274, 8275, 8276, 8277, 8278, 8279, 8280, 8281]\r\n#1: 100%|██████████| 5/5 [00:00<00:00, 92.75ba/s]\r\nexecuted [0, 1]\r\n#0:   0%|          | 0/1 [00:00<?, ?ba/s]\r\n#1:   0%|          | 0/1 [00:00<?, ?ba/s]\r\nexecuted [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\r\nexecuted [551, 552, 553, 554, 555, 556, 557, 558, 559, 560]\r\n#0: 100%|██████████| 1/1 [00:00<00:00, 118.81ba/s]\r\n#1: 100%|██████████| 1/1 [00:00<00:00, 123.06ba/s]\r\nexecuted [0, 1]\r\n#0:   0%|          | 0/2 [00:00<?, ?ba/s]\r\n#1:   0%|          | 0/2 [00:00<?, ?ba/s]\r\nexecuted [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\r\nexecuted [1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114]\r\nexecuted [1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009]\r\n#0: 100%|██████████| 2/2 [00:00<00:00, 119.42ba/s]\r\nexecuted [2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114]\r\n#1: 100%|██████████| 2/2 [00:00<00:00, 123.33ba/s]\r\n\r\n\r\n\r\n ############################## \r\n\r\n\r\n\r\nexecuted [0, 1]\r\nLoading cached processed dataset at /home/johannes/.cache/huggingface/datasets/sst/default/1.0.0/a16a45566b63b2c3179e6c1d0f8edadde56e45570ee8cf99394fbb738491d34b/cache-6079777aa097c8f8.arrow\r\nLoading cached processed dataset at /home/johannes/.cache/huggingface/datasets/sst/default/1.0.0/a16a45566b63b2c3179e6c1d0f8edadde56e45570ee8cf99394fbb738491d34b/cache-2dc05c46f68eda6e.arrow\r\nexecuted [0, 1]\r\nLoading cached processed dataset at /home/johannes/.cache/huggingface/datasets/sst/default/1.0.0/a16a45566b63b2c3179e6c1d0f8edadde56e45570ee8cf99394fbb738491d34b/cache-1ca347e7430b98f1.arrow\r\nLoading cached processed dataset at /home/johannes/.cache/huggingface/datasets/sst/default/1.0.0/a16a45566b63b2c3179e6c1d0f8edadde56e45570ee8cf99394fbb738491d34b/cache-c0f1a73ce3ba40cd.arrow\r\nexecuted [0, 1]\r\nLoading cached processed dataset at /home/johannes/.cache/huggingface/datasets/sst/default/1.0.0/a16a45566b63b2c3179e6c1d0f8edadde56e45570ee8cf99394fbb738491d34b/cache-832a1407bf1ac5b7.arrow\r\nLoading cached processed dataset at /home/johannes/.cache/huggingface/datasets/sst/default/1.0.0/a16a45566b63b2c3179e6c1d0f8edadde56e45570ee8cf99394fbb738491d34b/cache-036316a259b773c4.arrow\r\n- Datasets: 1.5.0\r\n- Python: 3.8.3 (default, May 19 2020, 18:47:26) \r\n[GCC 7.3.0]\r\n- Platform: Linux-5.4.0-72-generic-x86_64-with-glibc2.10\r\n```'
 'Hi,\r\n\r\nset `keep_in_memory` to False when loading a dataset (`sst = load_dataset(""sst"", keep_in_memory=False)`) to prevent it from loading in-memory. Currently, in-memory datasets fail to find cached files due to this check (always False for them):\r\n\r\nhttps://github.com/huggingface/datasets/blob/241a0b4a3a868778ee91e767ad406f9da7610df2/src/datasets/arrow_dataset.py#L1718\r\n\r\n@albertvillanova It seems like this behavior was overlooked in #2182.\r\n\r\n'
 'Hi @villmow, thanks for reporting. \r\n\r\nAs @mariosasko has pointed out, we did not consider this case when introducing the feature of automatic in-memory for small datasets. This needs to be fixed.'
 ""Hi ! Currently a dataset that is in memory doesn't know doesn't know in which directory it has to read/write cache files.\r\nOn the other hand, a dataset that loaded from the disk (via memory mapping) uses the directory from which the dataset is located to read/write cache files.\r\n\r\nBecause of that, currently in-memory datasets simply don't use caching.\r\n\r\nMaybe a Dataset object could have a `cache_dir` that is set to the directory where the arrow files are created during `load_dataset` ?""
 'Fixed once reverted the default in-memory feature:\r\nClosed by #2460 (to close issue #2458).'
 'Please @villmow, feel free to update to `Datasets` latest version (1.8).']","## Describe the bug
Somehow caching does not work for me anymore. Am I doing something wrong, or is there anything that I missed?

## Steps to reproduce the bug
```python

import datasets
datasets.set_caching_enabled(True)
sst = datasets.load_dataset(""sst"")

def foo(samples, i):
    print(""executed"", i[:10])
    return samples

# first call
x = sst.map(foo, batched=True, with_indices=True,  num_proc=2)

print('\n'*3, ""#"" * 30, '\n'*3)

# second call
y = sst.map(foo, batched=True, with_indices=True, num_proc=2)

# print version
import sys
import platform
print(f""""""
- Datasets: {datasets.__version__}
- Python: {sys.version}
- Platform: {platform.platform()}
"""""")
```

## Actual results
This code prints the following output for me:
```bash
No config specified, defaulting to: sst/default
Reusing dataset sst (/home/johannes/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff)
#0:   0%|          | 0/5 [00:00<?, ?ba/s]
#1:   0%|          | 0/5 [00:00<?, ?ba/s]
executed [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
executed [4272, 4273, 4274, 4275, 4276, 4277, 4278, 4279, 4280, 4281]
executed [1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009]
executed [5272, 5273, 5274, 5275, 5276, 5277, 5278, 5279, 5280, 5281]
executed [2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009]
executed [6272, 6273, 6274, 6275, 6276, 6277, 6278, 6279, 6280, 6281]
executed [3000, 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, 3009]
executed [7272, 7273, 7274, 7275, 7276, 7277, 7278, 7279, 7280, 7281]
executed [4000, 4001, 4002, 4003, 4004, 4005, 4006, 4007, 4008, 4009]
#0: 100%|██████████| 5/5 [00:00<00:00, 59.85ba/s]
executed [8272, 8273, 8274, 8275, 8276, 8277, 8278, 8279, 8280, 8281]
#1: 100%|██████████| 5/5 [00:00<00:00, 60.85ba/s]
#0:   0%|          | 0/1 [00:00<?, ?ba/s]
#1:   0%|          | 0/1 [00:00<?, ?ba/s]executed [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
#0: 100%|██████████| 1/1 [00:00<00:00, 69.32ba/s]
executed [551, 552, 553, 554, 555, 556, 557, 558, 559, 560]
#1: 100%|██████████| 1/1 [00:00<00:00, 70.93ba/s]
#0:   0%|          | 0/2 [00:00<?, ?ba/s]
#1:   0%|          | 0/2 [00:00<?, ?ba/s]executed [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
executed [1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009]
#0: 100%|██████████| 2/2 [00:00<00:00, 63.25ba/s]
executed [1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114]
executed [2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114]
#1: 100%|██████████| 2/2 [00:00<00:00, 57.69ba/s]



 ############################## 



#0:   0%|          | 0/5 [00:00<?, ?ba/s]
#1:   0%|          | 0/5 [00:00<?, ?ba/s]
executed [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
executed [4272, 4273, 4274, 4275, 4276, 4277, 4278, 4279, 4280, 4281]
executed [1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009]
executed [5272, 5273, 5274, 5275, 5276, 5277, 5278, 5279, 5280, 5281]
executed [2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009]
executed [6272, 6273, 6274, 6275, 6276, 6277, 6278, 6279, 6280, 6281]
executed [3000, 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, 3009]
executed [4000, 4001, 4002, 4003, 4004, 4005, 4006, 4007, 4008, 4009]
#0: 100%|██████████| 5/5 [00:00<00:00, 58.10ba/s]
executed [7272, 7273, 7274, 7275, 7276, 7277, 7278, 7279, 7280, 7281]
executed [8272, 8273, 8274, 8275, 8276, 8277, 8278, 8279, 8280, 8281]
#1: 100%|██████████| 5/5 [00:00<00:00, 57.19ba/s]
#0:   0%|          | 0/1 [00:00<?, ?ba/s]
#1:   0%|          | 0/1 [00:00<?, ?ba/s]
executed [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
#0: 100%|██████████| 1/1 [00:00<00:00, 60.10ba/s]
executed [551, 552, 553, 554, 555, 556, 557, 558, 559, 560]
#1: 100%|██████████| 1/1 [00:00<00:00, 53.82ba/s]
#0:   0%|          | 0/2 [00:00<?, ?ba/s]
#1:   0%|          | 0/2 [00:00<?, ?ba/s]
executed [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
executed [1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009]
executed [1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114]
#0: 100%|██████████| 2/2 [00:00<00:00, 72.76ba/s]
executed [2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114]
#1: 100%|██████████| 2/2 [00:00<00:00, 71.55ba/s]

- Datasets: 1.6.1
- Python: 3.8.3 (default, May 19 2020, 18:47:26) 
[GCC 7.3.0]
- Platform: Linux-5.4.0-72-generic-x86_64-with-glibc2.10
```

## Expected results
Caching should work.

"
https://github.com/huggingface/datasets/issues/2319,UnicodeDecodeError for OSCAR (Afrikaans),"['Thanks for reporting, @sgraaf.\r\n\r\nI am going to have a look at it. \r\n\r\nI guess the expected codec is ""UTF-8"". Normally, when no explicitly codec is passed, Python uses one which is platform-dependent. For Linux machines, the default codec is `utf_8`, which is OK. However for Windows machine, the default codec is `cp1252`, which causes the problem.'
 'Awesome, thank you. 😃 '
 '@sgraaf, I have just merged the fix in the master branch.\r\n\r\nYou can either:\r\n- install `datasets` from source code\r\n- wait until we make the next release of `datasets`\r\n- set the `utf-8` codec as your default instead of `cp1252`. This can be done by activating the Python [UTF-8 mode](https://www.python.org/dev/peps/pep-0540) either by passing the command-line option `-X utf8` or by setting the environment variable `PYTHONUTF8=1`.']","## Describe the bug
When loading the [OSCAR dataset](https://huggingface.co/datasets/oscar) (specifically `unshuffled_deduplicated_af`), I encounter a `UnicodeDecodeError`.

## Steps to reproduce the bug
```python
from datasets import load_dataset
dataset = load_dataset(""oscar"", ""unshuffled_deduplicated_af"")
```

## Expected results
Anything but an error, really.

## Actual results
```python
>>> from datasets import load_dataset
>>> dataset = load_dataset(""oscar"", ""unshuffled_deduplicated_af"")
Downloading: 14.7kB [00:00, 4.91MB/s]
Downloading: 3.07MB [00:00, 32.6MB/s]
Downloading and preparing dataset oscar/unshuffled_deduplicated_af (download: 62.93 MiB, generated: 163.38 MiB, post-processed: Unknown size, total: 226.32 MiB) to C:\Users\sgraaf\.cache\huggingface\datasets\oscar\unshuffled_deduplicated_af\1.0.0\bd4f96df5b4512007ef9fd17bbc1ecde459fa53d2fc0049cf99392ba2efcc464...
Downloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 81.0/81.0 [00:00<00:00, 40.5kB/s]
Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 66.0M/66.0M [00:18<00:00, 3.50MB/s]
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\sgraaf\AppData\Local\Programs\Python\Python39\lib\site-packages\datasets\load.py"", line 745, in load_dataset
    builder_instance.download_and_prepare(
  File ""C:\Users\sgraaf\AppData\Local\Programs\Python\Python39\lib\site-packages\datasets\builder.py"", line 574, in download_and_prepare
    self._download_and_prepare(
  File ""C:\Users\sgraaf\AppData\Local\Programs\Python\Python39\lib\site-packages\datasets\builder.py"", line 652, in _download_and_prepare
    self._prepare_split(split_generator, **prepare_split_kwargs)
  File ""C:\Users\sgraaf\AppData\Local\Programs\Python\Python39\lib\site-packages\datasets\builder.py"", line 979, in _prepare_split
    for key, record in utils.tqdm(
  File ""C:\Users\sgraaf\AppData\Local\Programs\Python\Python39\lib\site-packages\tqdm\std.py"", line 1133, in __iter__
    for obj in iterable:
  File ""C:\Users\sgraaf\.cache\huggingface\modules\datasets_modules\datasets\oscar\bd4f96df5b4512007ef9fd17bbc1ecde459fa53d2fc0049cf99392ba2efcc464\oscar.py"", line 359, in _generate_examples
    for line in f:
  File ""C:\Users\sgraaf\AppData\Local\Programs\Python\Python39\lib\encodings\cp1252.py"", line 23, in decode
    return codecs.charmap_decode(input,self.errors,decoding_table)[0]
UnicodeDecodeError: 'charmap' codec can't decode byte 0x9d in position 7454: character maps to <undefined>
```

## Versions
Paste the output of the following code:
```python
import datasets
import sys
import platform

print(f""""""
- Datasets: {datasets.__version__}
- Python: {sys.version}
- Platform: {platform.platform()}
"""""")
```
- Datasets: 1.6.2
- Python: 3.9.4 (tags/v3.9.4:1f2e308, Apr  6 2021, 13:40:21) [MSC v.1928 64 bit (AMD64)]
- Platform: Windows-10-10.0.19041-SP0"
https://github.com/huggingface/datasets/issues/2318,"[api request] API to obtain ""dataset_module"" dynamic path?","['Hi @richardliaw, \r\n\r\nFirst, thanks for the compliments.\r\n\r\nIn relation with your request, currently, the dynamic modules path is obtained this way:\r\n```python\r\nfrom datasets.load import init_dynamic_modules, MODULE_NAME_FOR_DYNAMIC_MODULES\r\n\r\ndynamic_modules_path = init_dynamic_modules(MODULE_NAME_FOR_DYNAMIC_MODULES)\r\n```\r\n\r\nLet me know if it is OK for you this way. \r\n\r\nI could set `MODULE_NAME_FOR_DYNAMIC_MODULES` as default value, so that you could instead obtain the path with:\r\n```\r\ndynamic_modules_path = datasets.load.init_dynamic_modules()\r\n```'
 'Hi @albertvillanova, the default value proposal seems great :) Looking forward to this!'
 'I like the idea as well ! thanks @albertvillanova '
 'Hi @richardliaw, the feature is on the master branch and will be included in the next release in a couple of weeks.'
 'awesome work @albertvillanova !']","**Is your feature request related to a problem? Please describe.**
A clear and concise description of what the problem is.

This is an awesome library. 

It seems like the dynamic module path in this library has broken some of hyperparameter tuning functionality: https://discuss.huggingface.co/t/using-hyperparameter-search-in-trainer/785/34

This is because Ray will spawn new processes, and each process will load modules by path. However, we need to explicitly inform Ray to load the right modules, or else it will error upon import. 

I'd like an API to obtain the dynamic paths. This will allow us to support this functionality in this awesome library while being future proof.

**Describe the solution you'd like**
A clear and concise description of what you want to happen.

`datasets.get_dynamic_paths -> List[str]` will be sufficient for my use case.

By offering this API, we will be able to address the following issues (by patching the ray integration sufficiently):

https://github.com/huggingface/blog/issues/106
https://github.com/huggingface/transformers/issues/11565
https://discuss.huggingface.co/t/using-hyperparameter-search-in-trainer/785/34
https://discuss.huggingface.co/t/using-hyperparameter-search-in-trainer/785/35

"
https://github.com/huggingface/datasets/issues/2316,Incorrect version specification for pyarrow,['Fixed by #2317.'],"## Describe the bug
The pyarrow dependency is incorrectly specified in setup.py file, in [this line](https://github.com/huggingface/datasets/blob/3a3e5a4da20bfcd75f8b6a6869b240af8feccc12/setup.py#L77).
Also as a snippet:
```python
 ""pyarrow>=1.0.0<4.0.0"",
```

## Steps to reproduce the bug
```bash
 pip install ""pyarrow>=1.0.0<4.0.0""
```

## Expected results
It is expected to get a pyarrow version between 1.0.0 (inclusive) and 4.0.0 (exclusive).

## Actual results
pip ignores the specified versions since there is a missing comma between the lower and upper limits. Therefore, pip installs the latest pyarrow version from PYPI, which is 4.0.0.
This is especially problematic since ""conda env export"" fails due to incorrect version specification. Here is the conda error as well:
```bash
conda env export
InvalidVersionSpec: Invalid version '1.0.0<4.0.0': invalid character(s)
```


## Fix suggestion
Put a comma between the version limits which means replacing the line in setup.py file with the following:
```python
 ""pyarrow>=1.0.0,<4.0.0"",
```

## Versions
Paste the output of the following code:
```python
- Datasets: 1.6.2
- Python: 3.7.10 (default, Feb 26 2021, 18:47:35) 
[GCC 7.3.0]
- Platform: Linux-5.4.0-42-generic-x86_64-with-debian-buster-sid
```
"
https://github.com/huggingface/datasets/issues/2308,Add COCO evaluation metrics,"[""Hi @NielsRogge, \r\nI'd like to contribute these metrics to datasets. Let's start with `CocoEvaluator` first? Currently how are are you sending the ground truths and predictions in coco_evaluator?\r\n""
 ""Great!\r\n\r\nHere's a notebook that illustrates how I'm using `CocoEvaluator`: https://drive.google.com/file/d/1VV92IlaUiuPOORXULIuAdtNbBWCTCnaj/view?usp=sharing\r\n\r\nThe evaluation is near the end of the notebook.\r\n\r\n""
 ""I went through the code you've [mentioned](https://github.com/facebookresearch/detr/blob/a54b77800eb8e64e3ad0d8237789fcbf2f8350c5/datasets/coco_eval.py) and I think there are 2 options on how we can go ahead:\r\n\r\n1) Implement how DETR people have done this (they're relying very heavily on the official implementation and they're focussing on torch dataset here. I feel ours should be something generic instead of pytorch specific.\r\n2) Do this [implementation](https://github.com/cocodataset/cocoapi/blob/ed842bffd41f6ff38707c4f0968d2cfd91088688/PythonAPI/pycocoEvalDemo.ipynb) where user can convert its output and ground truth annotation to pre-defined format and then feed it into our function to calculate metrics (looks very similar to you wanted above)\r\n\r\nIn my opinion, 2nd option looks very clean but I'm still figuring out how's it transforming the box co-ordinates of `coco_gt` which you've passed to `CocoEvaluator` (ground truth for evaluation). Since your model output was already converted to COCO api, I faced little problems there.""
 ""Ok, thanks for the update.\r\n\r\nIndeed, the metrics API of Datasets is framework agnostic, so we can't rely on a PyTorch-only implementation.\r\n\r\n[This file](https://github.com/cocodataset/cocoapi/blob/ed842bffd41f6ff38707c4f0968d2cfd91088688/PythonAPI/pycocotools/cocoeval.py) is probably want we need to implement.\r\n\r\n""]","I'm currently working on adding Facebook AI's DETR model (end-to-end object detection with Transformers) to HuggingFace Transformers. The model is working fine, but regarding evaluation, I'm currently relying on external `CocoEvaluator` and `PanopticEvaluator` objects which are defined in the original repository ([here](https://github.com/facebookresearch/detr/blob/a54b77800eb8e64e3ad0d8237789fcbf2f8350c5/datasets/coco_eval.py#L22) and [here](https://github.com/facebookresearch/detr/blob/a54b77800eb8e64e3ad0d8237789fcbf2f8350c5/datasets/panoptic_eval.py#L13) respectively). 

Running these in a notebook gives you nice summaries like this:
![image](https://user-images.githubusercontent.com/48327001/116878842-326f0680-ac20-11eb-9061-d6da02193694.png)

It would be great if we could import these metrics from the Datasets library, something like this:

```
import datasets

metric = datasets.load_metric('coco')

for model_input, gold_references in evaluation_dataset:
    model_predictions = model(model_inputs)
    metric.add_batch(predictions=model_predictions, references=gold_references)

final_score = metric.compute()
```

I think this would be great for object detection and semantic/panoptic segmentation in general, not just for DETR. Reproducing results of object detection papers would be way easier.

However, object detection and panoptic segmentation evaluation is a bit more complex than accuracy (it's more like a summary of metrics at different thresholds rather than a single one). I'm not sure how to proceed here, but happy to help making this possible.



"
https://github.com/huggingface/datasets/issues/2301,Unable to setup dev env on Windows,"['Hi @gchhablani, \r\n\r\nThere are some 3rd-party dependencies that require to build code in C. In this case, it is the library `python-Levenshtein`.\r\n\r\nOn Windows, in order to be able to build C code, you need to install at least `Microsoft C++ Build Tools` version 14. You can find more info here: https://visualstudio.microsoft.com/visual-cpp-build-tools/'
 'Hi @albertvillanova \r\n\r\nSorry for such a trivial issue ;-; \r\n\r\nThanks a lot.']","Hi

I tried installing the `"".[dev]""` version on Windows 10 after cloning.

Here is the error I'm facing:

```bat
(env) C:\testing\datasets>pip install -e "".[dev]""
Obtaining file:///C:/testing/datasets
Requirement already satisfied: numpy>=1.17 in c:\programdata\anaconda3\envs\env\lib\site-packages (from datasets==1.5.0.dev0) (1.19.5)
Collecting pyarrow>=0.17.1
  Using cached pyarrow-4.0.0-cp37-cp37m-win_amd64.whl (13.3 MB)
Requirement already satisfied: dill in c:\programdata\anaconda3\envs\env\lib\site-packages (from datasets==1.5.0.dev0) (0.3.1.1)
Collecting pandas
  Using cached pandas-1.2.4-cp37-cp37m-win_amd64.whl (9.1 MB)
Requirement already satisfied: requests>=2.19.0 in c:\programdata\anaconda3\envs\env\lib\site-packages (from datasets==1.5.0.dev0) (2.25.1)
Requirement already satisfied: tqdm<4.50.0,>=4.27 in c:\programdata\anaconda3\envs\env\lib\site-packages (from datasets==1.5.0.dev0) (4.49.0)
Requirement already satisfied: xxhash in c:\programdata\anaconda3\envs\env\lib\site-packages (from datasets==1.5.0.dev0) (2.0.2)
Collecting multiprocess
  Using cached multiprocess-0.70.11.1-py37-none-any.whl (108 kB)
Requirement already satisfied: fsspec in c:\programdata\anaconda3\envs\env\lib\site-packages (from datasets==1.5.0.dev0) (2021.4.0)
Collecting huggingface_hub<0.1.0
  Using cached huggingface_hub-0.0.8-py3-none-any.whl (34 kB)
Requirement already satisfied: importlib_metadata in c:\programdata\anaconda3\envs\env\lib\site-packages (from datasets==1.5.0.dev0) (4.0.1)
Requirement already satisfied: absl-py in c:\programdata\anaconda3\envs\env\lib\site-packages (from datasets==1.5.0.dev0) (0.12.0)
Requirement already satisfied: pytest in c:\programdata\anaconda3\envs\env\lib\site-packages (from datasets==1.5.0.dev0) (6.2.3)
Collecting pytest-xdist
  Using cached pytest_xdist-2.2.1-py3-none-any.whl (37 kB)
Collecting apache-beam>=2.24.0
  Using cached apache_beam-2.29.0-cp37-cp37m-win_amd64.whl (3.7 MB)
Collecting elasticsearch
  Using cached elasticsearch-7.12.1-py2.py3-none-any.whl (339 kB)
Requirement already satisfied: boto3==1.16.43 in c:\programdata\anaconda3\envs\env\lib\site-packages (from datasets==1.5.0.dev0) (1.16.43)
Requirement already satisfied: botocore==1.19.43 in c:\programdata\anaconda3\envs\env\lib\site-packages (from datasets==1.5.0.dev0) (1.19.43)
Collecting moto[s3]==1.3.16
  Using cached moto-1.3.16-py2.py3-none-any.whl (879 kB)
Collecting rarfile>=4.0
  Using cached rarfile-4.0-py3-none-any.whl (28 kB)
Collecting tensorflow>=2.3
  Using cached tensorflow-2.4.1-cp37-cp37m-win_amd64.whl (370.7 MB)
Requirement already satisfied: torch in c:\programdata\anaconda3\envs\env\lib\site-packages (from datasets==1.5.0.dev0) (1.8.1)
Requirement already satisfied: transformers in c:\programdata\anaconda3\envs\env\lib\site-packages (from datasets==1.5.0.dev0) (4.5.1)
Collecting bs4
  Using cached bs4-0.0.1-py3-none-any.whl
Collecting conllu
  Using cached conllu-4.4-py2.py3-none-any.whl (15 kB)
Collecting langdetect
  Using cached langdetect-1.0.8-py3-none-any.whl
Collecting lxml
  Using cached lxml-4.6.3-cp37-cp37m-win_amd64.whl (3.5 MB)
Collecting mwparserfromhell
  Using cached mwparserfromhell-0.6-cp37-cp37m-win_amd64.whl (101 kB)
Collecting nltk
  Using cached nltk-3.6.2-py3-none-any.whl (1.5 MB)
Collecting openpyxl
  Using cached openpyxl-3.0.7-py2.py3-none-any.whl (243 kB)
Collecting py7zr
  Using cached py7zr-0.15.2-py3-none-any.whl (66 kB)
Collecting tldextract
  Using cached tldextract-3.1.0-py2.py3-none-any.whl (87 kB)
Collecting zstandard
  Using cached zstandard-0.15.2-cp37-cp37m-win_amd64.whl (582 kB)
Collecting bert_score>=0.3.6
  Using cached bert_score-0.3.9-py3-none-any.whl (59 kB)
Collecting rouge_score
  Using cached rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)
Collecting sacrebleu
  Using cached sacrebleu-1.5.1-py3-none-any.whl (54 kB)
Requirement already satisfied: scipy in c:\programdata\anaconda3\envs\env\lib\site-packages (from datasets==1.5.0.dev0) (1.6.3)
Collecting seqeval
  Using cached seqeval-1.2.2-py3-none-any.whl
Collecting sklearn
  Using cached sklearn-0.0-py2.py3-none-any.whl
Collecting jiwer
  Using cached jiwer-2.2.0-py3-none-any.whl (13 kB)
Requirement already satisfied: toml>=0.10.1 in c:\programdata\anaconda3\envs\env\lib\site-packages (from datasets==1.5.0.dev0) (0.10.2)
Requirement already satisfied: requests_file>=1.5.1 in c:\programdata\anaconda3\envs\env\lib\site-packages (from datasets==1.5.0.dev0) (1.5.1)
Requirement already satisfied: texttable>=1.6.3 in c:\programdata\anaconda3\envs\env\lib\site-packages (from datasets==1.5.0.dev0) (1.6.3)
Requirement already satisfied: s3fs>=0.4.2 in c:\programdata\anaconda3\envs\env\lib\site-packages (from datasets==1.5.0.dev0) (0.4.2)
Requirement already satisfied: Werkzeug>=1.0.1 in c:\programdata\anaconda3\envs\env\lib\site-packages (from datasets==1.5.0.dev0) (1.0.1)
Collecting black
  Using cached black-21.4b2-py3-none-any.whl (130 kB)
Collecting isort
  Using cached isort-5.8.0-py3-none-any.whl (103 kB)
Collecting flake8==3.7.9
  Using cached flake8-3.7.9-py2.py3-none-any.whl (69 kB)
Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\programdata\anaconda3\envs\env\lib\site-packages (from boto3==1.16.43->datasets==1.5.0.dev0) (0.10.0)
Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in c:\programdata\anaconda3\envs\env\lib\site-packages (from boto3==1.16.43->datasets==1.5.0.dev0) (0.3.7)
Requirement already satisfied: urllib3<1.27,>=1.25.4 in c:\programdata\anaconda3\envs\env\lib\site-packages (from botocore==1.19.43->datasets==1.5.0.dev0) (1.26.4)
Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\programdata\anaconda3\envs\env\lib\site-packages (from botocore==1.19.43->datasets==1.5.0.dev0) (2.8.1)
Collecting entrypoints<0.4.0,>=0.3.0
  Using cached entrypoints-0.3-py2.py3-none-any.whl (11 kB)
Collecting pyflakes<2.2.0,>=2.1.0
  Using cached pyflakes-2.1.1-py2.py3-none-any.whl (59 kB)
Collecting pycodestyle<2.6.0,>=2.5.0
  Using cached pycodestyle-2.5.0-py2.py3-none-any.whl (51 kB)
Collecting mccabe<0.7.0,>=0.6.0
  Using cached mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)
Requirement already satisfied: jsondiff>=1.1.2 in c:\programdata\anaconda3\envs\env\lib\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (1.3.0)
Requirement already satisfied: pytz in c:\programdata\anaconda3\envs\env\lib\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (2021.1)
Requirement already satisfied: mock in c:\programdata\anaconda3\envs\env\lib\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (4.0.3)
Requirement already satisfied: MarkupSafe<2.0 in c:\programdata\anaconda3\envs\env\lib\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (1.1.1)
Requirement already satisfied: python-jose[cryptography]<4.0.0,>=3.1.0 in c:\programdata\anaconda3\envs\env\lib\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (3.2.0)
Requirement already satisfied: aws-xray-sdk!=0.96,>=0.93 in c:\programdata\anaconda3\envs\env\lib\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (2.8.0)
Requirement already satisfied: cryptography>=2.3.0 in c:\programdata\anaconda3\envs\env\lib\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (3.4.7)
Requirement already satisfied: more-itertools in c:\programdata\anaconda3\envs\env\lib\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (8.7.0)
Requirement already satisfied: PyYAML>=5.1 in c:\programdata\anaconda3\envs\env\lib\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (5.4.1)
Requirement already satisfied: boto>=2.36.0 in c:\programdata\anaconda3\envs\env\lib\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (2.49.0)
Requirement already satisfied: idna<3,>=2.5 in c:\programdata\anaconda3\envs\env\lib\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (2.10)
Requirement already satisfied: sshpubkeys>=3.1.0 in c:\programdata\anaconda3\envs\env\lib\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (3.3.1)
Requirement already satisfied: responses>=0.9.0 in c:\programdata\anaconda3\envs\env\lib\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (0.13.3)
Requirement already satisfied: xmltodict in c:\programdata\anaconda3\envs\env\lib\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (0.12.0)
Requirement already satisfied: setuptools in c:\programdata\anaconda3\envs\env\lib\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (52.0.0.post20210125)
Requirement already satisfied: Jinja2>=2.10.1 in c:\programdata\anaconda3\envs\env\lib\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (2.11.3)
Requirement already satisfied: zipp in c:\programdata\anaconda3\envs\env\lib\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (3.4.1)
Requirement already satisfied: six>1.9 in c:\programdata\anaconda3\envs\env\lib\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (1.15.0)
Requirement already satisfied: ecdsa<0.15 in c:\programdata\anaconda3\envs\env\lib\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (0.14.1)
Requirement already satisfied: docker>=2.5.1 in c:\programdata\anaconda3\envs\env\lib\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (5.0.0)
Requirement already satisfied: cfn-lint>=0.4.0 in c:\programdata\anaconda3\envs\env\lib\site-packages (from moto[s3]==1.3.16->datasets==1.5.0.dev0) (0.49.0)
Requirement already satisfied: grpcio<2,>=1.29.0 in c:\programdata\anaconda3\envs\env\lib\site-packages (from apache-beam>=2.24.0->datasets==1.5.0.dev0) (1.32.0)
Collecting hdfs<3.0.0,>=2.1.0
  Using cached hdfs-2.6.0-py3-none-any.whl (33 kB)
Collecting pyarrow>=0.17.1
  Using cached pyarrow-3.0.0-cp37-cp37m-win_amd64.whl (12.6 MB)
Collecting fastavro<2,>=0.21.4
  Using cached fastavro-1.4.0-cp37-cp37m-win_amd64.whl (394 kB)
Requirement already satisfied: httplib2<0.18.0,>=0.8 in c:\programdata\anaconda3\envs\env\lib\site-packages (from apache-beam>=2.24.0->datasets==1.5.0.dev0) (0.17.4)
Collecting pymongo<4.0.0,>=3.8.0
  Using cached pymongo-3.11.3-cp37-cp37m-win_amd64.whl (382 kB)
Collecting crcmod<2.0,>=1.7
  Using cached crcmod-1.7-py3-none-any.whl
Collecting avro-python3!=1.9.2,<1.10.0,>=1.8.1
  Using cached avro_python3-1.9.2.1-py3-none-any.whl
Requirement already satisfied: typing-extensions<3.8.0,>=3.7.0 in c:\programdata\anaconda3\envs\env\lib\site-packages (from apache-beam>=2.24.0->datasets==1.5.0.dev0) (3.7.4.3)
Requirement already satisfied: future<1.0.0,>=0.18.2 in c:\programdata\anaconda3\envs\env\lib\site-packages (from apache-beam>=2.24.0->datasets==1.5.0.dev0) (0.18.2)
Collecting oauth2client<5,>=2.0.1
  Using cached oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)
Collecting pydot<2,>=1.2.0
  Using cached pydot-1.4.2-py2.py3-none-any.whl (21 kB)
Requirement already satisfied: protobuf<4,>=3.12.2 in c:\programdata\anaconda3\envs\env\lib\site-packages (from apache-beam>=2.24.0->datasets==1.5.0.dev0) (3.15.8)
Requirement already satisfied: wrapt in c:\programdata\anaconda3\envs\env\lib\site-packages (from aws-xray-sdk!=0.96,>=0.93->moto[s3]==1.3.16->datasets==1.5.0.dev0) (1.12.1)
Collecting matplotlib
  Using cached matplotlib-3.4.1-cp37-cp37m-win_amd64.whl (7.1 MB)
Requirement already satisfied: junit-xml~=1.9 in c:\programdata\anaconda3\envs\env\lib\site-packages (from cfn-lint>=0.4.0->moto[s3]==1.3.16->datasets==1.5.0.dev0) (1.9)
Requirement already satisfied: jsonpatch in c:\programdata\anaconda3\envs\env\lib\site-packages (from cfn-lint>=0.4.0->moto[s3]==1.3.16->datasets==1.5.0.dev0) (1.32)
Requirement already satisfied: jsonschema~=3.0 in c:\programdata\anaconda3\envs\env\lib\site-packages (from cfn-lint>=0.4.0->moto[s3]==1.3.16->datasets==1.5.0.dev0) (3.2.0)
Requirement already satisfied: networkx~=2.4 in c:\programdata\anaconda3\envs\env\lib\site-packages (from cfn-lint>=0.4.0->moto[s3]==1.3.16->datasets==1.5.0.dev0) (2.5.1)
Requirement already satisfied: aws-sam-translator>=1.35.0 in c:\programdata\anaconda3\envs\env\lib\site-packages (from cfn-lint>=0.4.0->moto[s3]==1.3.16->datasets==1.5.0.dev0) (1.35.0)
Requirement already satisfied: cffi>=1.12 in c:\programdata\anaconda3\envs\env\lib\site-packages (from cryptography>=2.3.0->moto[s3]==1.3.16->datasets==1.5.0.dev0) (1.14.5)
Requirement already satisfied: pycparser in c:\programdata\anaconda3\envs\env\lib\site-packages (from cffi>=1.12->cryptography>=2.3.0->moto[s3]==1.3.16->datasets==1.5.0.dev0) (2.20)
Requirement already satisfied: pywin32==227 in c:\programdata\anaconda3\envs\env\lib\site-packages (from docker>=2.5.1->moto[s3]==1.3.16->datasets==1.5.0.dev0) (227)
Requirement already satisfied: websocket-client>=0.32.0 in c:\programdata\anaconda3\envs\env\lib\site-packages (from docker>=2.5.1->moto[s3]==1.3.16->datasets==1.5.0.dev0) (0.58.0)
Requirement already satisfied: docopt in c:\programdata\anaconda3\envs\env\lib\site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam>=2.24.0->datasets==1.5.0.dev0) (0.6.2)
Requirement already satisfied: filelock in c:\programdata\anaconda3\envs\env\lib\site-packages (from huggingface_hub<0.1.0->datasets==1.5.0.dev0) (3.0.12)
Requirement already satisfied: pyrsistent>=0.14.0 in c:\programdata\anaconda3\envs\env\lib\site-packages (from jsonschema~=3.0->cfn-lint>=0.4.0->moto[s3]==1.3.16->datasets==1.5.0.dev0) (0.17.3)
Requirement already satisfied: attrs>=17.4.0 in c:\programdata\anaconda3\envs\env\lib\site-packages (from jsonschema~=3.0->cfn-lint>=0.4.0->moto[s3]==1.3.16->datasets==1.5.0.dev0) (20.3.0)
Requirement already satisfied: decorator<5,>=4.3 in c:\programdata\anaconda3\envs\env\lib\site-packages (from networkx~=2.4->cfn-lint>=0.4.0->moto[s3]==1.3.16->datasets==1.5.0.dev0) (4.4.2)
Requirement already satisfied: rsa>=3.1.4 in c:\programdata\anaconda3\envs\env\lib\site-packages (from oauth2client<5,>=2.0.1->apache-beam>=2.24.0->datasets==1.5.0.dev0) (4.7.2)
Requirement already satisfied: pyasn1-modules>=0.0.5 in c:\programdata\anaconda3\envs\env\lib\site-packages (from oauth2client<5,>=2.0.1->apache-beam>=2.24.0->datasets==1.5.0.dev0) (0.2.8)
Requirement already satisfied: pyasn1>=0.1.7 in c:\programdata\anaconda3\envs\env\lib\site-packages (from oauth2client<5,>=2.0.1->apache-beam>=2.24.0->datasets==1.5.0.dev0) (0.4.8)
Requirement already satisfied: pyparsing>=2.1.4 in c:\programdata\anaconda3\envs\env\lib\site-packages (from pydot<2,>=1.2.0->apache-beam>=2.24.0->datasets==1.5.0.dev0) (2.4.7)
Requirement already satisfied: certifi>=2017.4.17 in c:\programdata\anaconda3\envs\env\lib\site-packages (from requests>=2.19.0->datasets==1.5.0.dev0) (2020.12.5)
Requirement already satisfied: chardet<5,>=3.0.2 in c:\programdata\anaconda3\envs\env\lib\site-packages (from requests>=2.19.0->datasets==1.5.0.dev0) (4.0.0)
Collecting keras-preprocessing~=1.1.2
  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)
Requirement already satisfied: termcolor~=1.1.0 in c:\programdata\anaconda3\envs\env\lib\site-packages (from tensorflow>=2.3->datasets==1.5.0.dev0) (1.1.0)
Requirement already satisfied: tensorboard~=2.4 in c:\programdata\anaconda3\envs\env\lib\site-packages (from tensorflow>=2.3->datasets==1.5.0.dev0) (2.5.0)
Requirement already satisfied: wheel~=0.35 in c:\programdata\anaconda3\envs\env\lib\site-packages (from tensorflow>=2.3->datasets==1.5.0.dev0) (0.36.2)
Collecting opt-einsum~=3.3.0
  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)
Collecting gast==0.3.3
  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)
Collecting google-pasta~=0.2
  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)
Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in c:\programdata\anaconda3\envs\env\lib\site-packages (from tensorflow>=2.3->datasets==1.5.0.dev0) (2.4.0)
Collecting astunparse~=1.6.3
  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)
Collecting flatbuffers~=1.12.0
  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)
Collecting h5py~=2.10.0
  Using cached h5py-2.10.0-cp37-cp37m-win_amd64.whl (2.5 MB)
Requirement already satisfied: markdown>=2.6.8 in c:\programdata\anaconda3\envs\env\lib\site-packages (from tensorboard~=2.4->tensorflow>=2.3->datasets==1.5.0.dev0) (3.3.4)
Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\programdata\anaconda3\envs\env\lib\site-packages (from tensorboard~=2.4->tensorflow>=2.3->datasets==1.5.0.dev0) (1.8.0)
Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\programdata\anaconda3\envs\env\lib\site-packages (from tensorboard~=2.4->tensorflow>=2.3->datasets==1.5.0.dev0) (0.4.4)
Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\programdata\anaconda3\envs\env\lib\site-packages (from tensorboard~=2.4->tensorflow>=2.3->datasets==1.5.0.dev0) (0.6.0)
Requirement already satisfied: google-auth<2,>=1.6.3 in c:\programdata\anaconda3\envs\env\lib\site-packages (from tensorboard~=2.4->tensorflow>=2.3->datasets==1.5.0.dev0) (1.30.0)
Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\programdata\anaconda3\envs\env\lib\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.3->datasets==1.5.0.dev0) (4.2.2)
Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\programdata\anaconda3\envs\env\lib\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.3->datasets==1.5.0.dev0) (1.3.0)
Requirement already satisfied: oauthlib>=3.0.0 in c:\programdata\anaconda3\envs\env\lib\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.3->datasets==1.5.0.dev0) (3.1.0)
Requirement already satisfied: regex!=2019.12.17 in c:\programdata\anaconda3\envs\env\lib\site-packages (from transformers->datasets==1.5.0.dev0) (2021.4.4)
Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\programdata\anaconda3\envs\env\lib\site-packages (from transformers->datasets==1.5.0.dev0) (0.10.2)
Requirement already satisfied: sacremoses in c:\programdata\anaconda3\envs\env\lib\site-packages (from transformers->datasets==1.5.0.dev0) (0.0.45)
Requirement already satisfied: packaging in c:\programdata\anaconda3\envs\env\lib\site-packages (from transformers->datasets==1.5.0.dev0) (20.9)
Collecting pathspec<1,>=0.8.1
  Using cached pathspec-0.8.1-py2.py3-none-any.whl (28 kB)
Requirement already satisfied: click>=7.1.2 in c:\programdata\anaconda3\envs\env\lib\site-packages (from black->datasets==1.5.0.dev0) (7.1.2)
Collecting appdirs
  Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)
Collecting mypy-extensions>=0.4.3
  Using cached mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)
Requirement already satisfied: typed-ast>=1.4.2 in c:\programdata\anaconda3\envs\env\lib\site-packages (from black->datasets==1.5.0.dev0) (1.4.3)
Collecting beautifulsoup4
  Using cached beautifulsoup4-4.9.3-py3-none-any.whl (115 kB)
Requirement already satisfied: soupsieve>1.2 in c:\programdata\anaconda3\envs\env\lib\site-packages (from beautifulsoup4->bs4->datasets==1.5.0.dev0) (2.2.1)
Collecting python-Levenshtein
  Using cached python-Levenshtein-0.12.2.tar.gz (50 kB)
Requirement already satisfied: jsonpointer>=1.9 in c:\programdata\anaconda3\envs\env\lib\site-packages (from jsonpatch->cfn-lint>=0.4.0->moto[s3]==1.3.16->datasets==1.5.0.dev0) (2.1)
Requirement already satisfied: pillow>=6.2.0 in c:\programdata\anaconda3\envs\env\lib\site-packages (from matplotlib->bert_score>=0.3.6->datasets==1.5.0.dev0) (8.2.0)
Requirement already satisfied: cycler>=0.10 in c:\programdata\anaconda3\envs\env\lib\site-packages (from matplotlib->bert_score>=0.3.6->datasets==1.5.0.dev0) (0.10.0)
Requirement already satisfied: kiwisolver>=1.0.1 in c:\programdata\anaconda3\envs\env\lib\site-packages (from matplotlib->bert_score>=0.3.6->datasets==1.5.0.dev0) (1.3.1)
Collecting multiprocess
  Using cached multiprocess-0.70.11-py3-none-any.whl (98 kB)
  Using cached multiprocess-0.70.10.zip (2.4 MB)
  Using cached multiprocess-0.70.9-py3-none-any.whl
Requirement already satisfied: joblib in c:\programdata\anaconda3\envs\env\lib\site-packages (from nltk->datasets==1.5.0.dev0) (1.0.1)
Collecting et-xmlfile
  Using cached et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)
Requirement already satisfied: pyzstd<0.15.0,>=0.14.4 in c:\programdata\anaconda3\envs\env\lib\site-packages (from py7zr->datasets==1.5.0.dev0) (0.14.4)
Collecting pyppmd<0.13.0,>=0.12.1
  Using cached pyppmd-0.12.1-cp37-cp37m-win_amd64.whl (32 kB)
Collecting pycryptodome>=3.6.6
  Using cached pycryptodome-3.10.1-cp35-abi3-win_amd64.whl (1.6 MB)
Collecting bcj-cffi<0.6.0,>=0.5.1
  Using cached bcj_cffi-0.5.1-cp37-cp37m-win_amd64.whl (21 kB)
Collecting multivolumefile<0.3.0,>=0.2.0
  Using cached multivolumefile-0.2.3-py3-none-any.whl (17 kB)
Requirement already satisfied: iniconfig in c:\programdata\anaconda3\envs\env\lib\site-packages (from pytest->datasets==1.5.0.dev0) (1.1.1)
Requirement already satisfied: py>=1.8.2 in c:\programdata\anaconda3\envs\env\lib\site-packages (from pytest->datasets==1.5.0.dev0) (1.10.0)
Requirement already satisfied: pluggy<1.0.0a1,>=0.12 in c:\programdata\anaconda3\envs\env\lib\site-packages (from pytest->datasets==1.5.0.dev0) (0.13.1)
Requirement already satisfied: atomicwrites>=1.0 in c:\programdata\anaconda3\envs\env\lib\site-packages (from pytest->datasets==1.5.0.dev0) (1.4.0)
Requirement already satisfied: colorama in c:\programdata\anaconda3\envs\env\lib\site-packages (from pytest->datasets==1.5.0.dev0) (0.4.4)
Collecting pytest-forked
  Using cached pytest_forked-1.3.0-py2.py3-none-any.whl (4.7 kB)
Collecting execnet>=1.1
  Using cached execnet-1.8.0-py2.py3-none-any.whl (39 kB)
Requirement already satisfied: apipkg>=1.4 in c:\programdata\anaconda3\envs\env\lib\site-packages (from execnet>=1.1->pytest-xdist->datasets==1.5.0.dev0) (1.5)
Collecting portalocker==2.0.0
  Using cached portalocker-2.0.0-py2.py3-none-any.whl (11 kB)
Requirement already satisfied: scikit-learn>=0.21.3 in c:\programdata\anaconda3\envs\env\lib\site-packages (from seqeval->datasets==1.5.0.dev0) (0.24.2)
Requirement already satisfied: threadpoolctl>=2.0.0 in c:\programdata\anaconda3\envs\env\lib\site-packages (from scikit-learn>=0.21.3->seqeval->datasets==1.5.0.dev0) (2.1.0)
Building wheels for collected packages: python-Levenshtein
  Building wheel for python-Levenshtein (setup.py) ... error
  ERROR: Command errored out with exit status 1:
   command: 'C:\ProgramData\Anaconda3\envs\env\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'C:\\Users\\VKC~1\\AppData\\Local\\Temp\\pip-install-ynt_dbm4\\python-levenshtein_c02e7e6f9def4629a475349654670ae9\\setup.py'""'""'; __file__='""'""'C:\\Users\\VKC~1\\AppData\\Local\\Temp\\pip-install-ynt_dbm4\\python-levenshtein_c02e7e6f9def4629a475349654670ae9\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d 'C:\Users\VKC~1\AppData\Local\Temp\pip-wheel-8jh7fm18'
       cwd: C:\Users\VKC~1\AppData\Local\Temp\pip-install-ynt_dbm4\python-levenshtein_c02e7e6f9def4629a475349654670ae9\
  Complete output (27 lines):
  running bdist_wheel
  running build
  running build_py
  creating build
  creating build\lib.win-amd64-3.7
  creating build\lib.win-amd64-3.7\Levenshtein
  copying Levenshtein\StringMatcher.py -> build\lib.win-amd64-3.7\Levenshtein
  copying Levenshtein\__init__.py -> build\lib.win-amd64-3.7\Levenshtein
  running egg_info
  writing python_Levenshtein.egg-info\PKG-INFO
  writing dependency_links to python_Levenshtein.egg-info\dependency_links.txt
  writing entry points to python_Levenshtein.egg-info\entry_points.txt
  writing namespace_packages to python_Levenshtein.egg-info\namespace_packages.txt
  writing requirements to python_Levenshtein.egg-info\requires.txt
  writing top-level names to python_Levenshtein.egg-info\top_level.txt
  reading manifest file 'python_Levenshtein.egg-info\SOURCES.txt'
  reading manifest template 'MANIFEST.in'
  warning: no previously-included files matching '*pyc' found anywhere in distribution
  warning: no previously-included files matching '*so' found anywhere in distribution
  warning: no previously-included files matching '.project' found anywhere in distribution
  warning: no previously-included files matching '.pydevproject' found anywhere in distribution
  writing manifest file 'python_Levenshtein.egg-info\SOURCES.txt'
  copying Levenshtein\_levenshtein.c -> build\lib.win-amd64-3.7\Levenshtein
  copying Levenshtein\_levenshtein.h -> build\lib.win-amd64-3.7\Levenshtein
  running build_ext
  building 'Levenshtein._levenshtein' extension
  error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/
  ----------------------------------------
  ERROR: Failed building wheel for python-Levenshtein
  Running setup.py clean for python-Levenshtein
Failed to build python-Levenshtein
Installing collected packages: python-Levenshtein, pytest-forked, pyppmd, pymongo, pyflakes, pydot, pycryptodome, pycodestyle, pyarrow, portalocker, pathspec, pandas, opt-einsum, oauth2client, nltk, mypy-extensions, multivolumefile, multiprocess, moto, mccabe, matplotlib, keras-preprocessing, huggingface-hub, hdfs, h5py, google-pasta, gast, flatbuffers, fastavro, execnet, et-xmlfile, entrypoints, crcmod, beautifulsoup4, bcj-cffi, avro-python3, astunparse, appdirs, zstandard, tldextract, tensorflow, sklearn, seqeval, sacrebleu, rouge-score, rarfile, pytest-xdist, py7zr, openpyxl, mwparserfromhell, lxml, langdetect, jiwer, isort, flake8, elasticsearch, datasets, conllu, bs4, black, bert-score, apache-beam
    Running setup.py install for python-Levenshtein ... error
    ERROR: Command errored out with exit status 1:
     command: 'C:\ProgramData\Anaconda3\envs\env\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'C:\\Users\\VKC~1\\AppData\\Local\\Temp\\pip-install-ynt_dbm4\\python-levenshtein_c02e7e6f9def4629a475349654670ae9\\setup.py'""'""'; __file__='""'""'C:\\Users\\VKC~1\\AppData\\Local\\Temp\\pip-install-ynt_dbm4\\python-levenshtein_c02e7e6f9def4629a475349654670ae9\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record 'C:\Users\VKC~1\AppData\Local\Temp\pip-record-v7l7zitb\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\ProgramData\Anaconda3\envs\env\Include\python-Levenshtein'
         cwd: C:\Users\VKC~1\AppData\Local\Temp\pip-install-ynt_dbm4\python-levenshtein_c02e7e6f9def4629a475349654670ae9\
    Complete output (27 lines):
    running install
    running build
    running build_py
    creating build
    creating build\lib.win-amd64-3.7
    creating build\lib.win-amd64-3.7\Levenshtein
    copying Levenshtein\StringMatcher.py -> build\lib.win-amd64-3.7\Levenshtein
    copying Levenshtein\__init__.py -> build\lib.win-amd64-3.7\Levenshtein
    running egg_info
    writing python_Levenshtein.egg-info\PKG-INFO
    writing dependency_links to python_Levenshtein.egg-info\dependency_links.txt
    writing entry points to python_Levenshtein.egg-info\entry_points.txt
    writing namespace_packages to python_Levenshtein.egg-info\namespace_packages.txt
    writing requirements to python_Levenshtein.egg-info\requires.txt
    writing top-level names to python_Levenshtein.egg-info\top_level.txt
    reading manifest file 'python_Levenshtein.egg-info\SOURCES.txt'
    reading manifest template 'MANIFEST.in'
    warning: no previously-included files matching '*pyc' found anywhere in distribution
    warning: no previously-included files matching '*so' found anywhere in distribution
    warning: no previously-included files matching '.project' found anywhere in distribution
    warning: no previously-included files matching '.pydevproject' found anywhere in distribution
    writing manifest file 'python_Levenshtein.egg-info\SOURCES.txt'
    copying Levenshtein\_levenshtein.c -> build\lib.win-amd64-3.7\Levenshtein
    copying Levenshtein\_levenshtein.h -> build\lib.win-amd64-3.7\Levenshtein
    running build_ext
    building 'Levenshtein._levenshtein' extension
    error: Microsoft Visual C++ 14.0 or greater is required. Get it with ""Microsoft C++ Build Tools"": https://visualstudio.microsoft.com/visual-cpp-build-tools/
    ----------------------------------------
ERROR: Command errored out with exit status 1: 'C:\ProgramData\Anaconda3\envs\env\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'C:\\Users\\VKC~1\\AppData\\Local\\Temp\\pip-install-ynt_dbm4\\python-levenshtein_c02e7e6f9def4629a475349654670ae9\\setup.py'""'""'; __file__='""'""'C:\\Users\\VKC~1\\AppData\\Local\\Temp\\pip-install-ynt_dbm4\\python-levenshtein_c02e7e6f9def4629a475349654670ae9\\setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record 'C:\Users\VKC~1\AppData\Local\Temp\pip-record-v7l7zitb\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\ProgramData\Anaconda3\envs\env\Include\python-Levenshtein' Check the logs for full command output.
```

Here are conda and python versions:

```bat
(env) C:\testing\datasets>conda --version
conda 4.9.2

(env) C:\testing\datasets>python --version
Python 3.7.10
```

Please help me out. Thanks."
https://github.com/huggingface/datasets/issues/2300,Add VoxPopuli,"[""I'm happy to take this on:) One question: The original unlabelled data is stored unsegmented (see e.g. https://github.com/facebookresearch/voxpopuli/blob/main/voxpopuli/get_unlabelled_data.py#L30), but segmenting the audio in the dataset would require a dependency on something like soundfile or torchaudio. An alternative could be to provide the segments start and end times as a Sequence and then it's up to the user to perform the segmentation on-the-fly if they wish?""
 'Hey @jfainberg,\r\n\r\nThis sounds great! I think adding a dependency would not be a big problem, however automatically segmenting the data probably means that it would take a very long time to do:\r\n\r\n```python\r\ndataset = load_dataset(""voxpopuli"", ""french"")\r\n```\r\n\r\n=> so as a start I think your option 2 is the way to go!']","## Adding a Dataset
- **Name:** Voxpopuli
- **Description:** VoxPopuli is raw data is collected from 2009-2020 European Parliament event recordings
- **Paper:** https://arxiv.org/abs/2101.00390
- **Data:** https://github.com/facebookresearch/voxpopuli
- **Motivation:** biggest unlabeled speech dataset

**Note**: Since the dataset is so huge, we should only add the config `10k` in the beginning.

Instructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).
"
https://github.com/huggingface/datasets/issues/2294,Slow #0 when using map to tokenize.,"['Hi ! Have you tried other values for `preprocessing_num_workers` ? Is it always process 0 that is slower ?\r\nThere are no difference between process 0 and the others except that it processes the first shard of the dataset.'
 'Hi, I have found the reason of it. Before using the map function to tokenize the data, I concatenate the wikipedia and bookcorpus first, like this:\r\n```if args.dataset_name1 is not None:\r\n        dataset1 = load_dataset(args.dataset_name1, args.dataset_config_name1, split=""train"")\r\n        dataset1 = dataset1.remove_columns(\'title\')\r\n        if args.dataset_name2 is not None:\r\n            dataset2 = load_dataset(args.dataset_name2, args.dataset_config_name2,split=""train"")\r\n            assert dataset1.features.type == dataset2.features.type, str(dataset1.features.type)+\';\'+str(dataset2.features.type)\r\n           datasets12 = concatenate_datasets([dataset1, dataset2], split=\'train\')\r\n```\r\nWhen I just use one datasets, e.g. wikipedia, the problem seems no longer exist:\r\n![image](https://user-images.githubusercontent.com/31714566/116967059-13d24380-ace4-11eb-8d14-b7b9c9a275cc.png)\r\n\r\nBookcorpus has more row numbers than Wikipedia, however, it takes much more time to process each batch of wiki than that of bookcorpus. When we first concatenate two datasets and then use _map_ to process the concatenated datasets,  e.g. `num_proc=5`, process 0 has to process all of the wikipedia data, causing the problem that #0 takes a longer time to finish the job. \r\n\r\nThe problem is caused by the different characteristic of different datasets. One solution might be using _map_ first to process two datasets seperately, then concatenate the tokenized and processed datasets before input to the `Dataloader`.\r\n\r\n'
 'That makes sense ! You can indeed use `map` on both datasets separately and then concatenate.\r\nAnother option is to concatenate, then shuffle, and then `map`.']","Hi, _datasets_ is really amazing! I am following [run_mlm_no_trainer.py](url) to pre-train BERT, and it uses `tokenized_datasets = raw_datasets.map(
            tokenize_function,
            batched=True,
            num_proc=args.preprocessing_num_workers,
            remove_columns=column_names,
            load_from_cache_file=not args.overwrite_cache,
        )` to tokenize by multiprocessing. However, I have found that when `num_proc`>1，the process _#0_ is much slower than others.
It looks like this:
![image](https://user-images.githubusercontent.com/31714566/116665555-81246280-a9cc-11eb-8a37-6e608ab310d0.png)
It takes more than 12 hours for #0, while others just about half an hour. Could anyone tell me it is normal or not, and is there any methods to speed up it?
"
https://github.com/huggingface/datasets/issues/2288,Load_dataset for local CSV files,"['Hi,\r\n\r\nthis is not a standard CSV file (requires additional preprocessing) so I wouldn\'t label this as s bug. You could parse the examples with the regex module or the string API to extract the data, but the following approach is probably the easiest (once you load the data):\r\n```python\r\nimport ast\r\n# load the dataset and copy the features\r\ndef process(ex):\r\n    return {""tokens"": ast.literal_eval(ex[""tokens""]), ""labels"": ast.literal_eval(ex[""labels""])}\r\ndataset = dataset.map(process, features=new_features)\r\n```\r\n'
 'Hi,\r\n\r\nThanks for the reply.\r\nI  have already used  ```ast.literal_eval``` to evaluate the string into list, but I was getting another error:\r\n```\r\nArrowInvalid: Could not convert X with type str: tried to convert to int\r\n```\r\nWhy this happens ? Should labels be mapped to their ids and use int instead of str ?'
 'Yes, just map the labels to their ids.']","The method load_dataset fails to correctly load a dataset from csv. 

Moreover, I am working on a token-classification task ( POS tagging) , where each row in my CSV contains two columns each of them having a list of strings.
row example:
```tokens  |  labels
['I' , 'am', 'John']  |  ['PRON', 'AUX', 'PROPN' ] 
```
The method, loads each list as a string:  (i.g ""['I' , 'am', 'John']"").
To solve this issue, I copied the Datasets.Features, created Sequence types ( instead of Value)  and tried to cast the features type
```
new_features['tokens'] = Sequence(feature=Value(dtype='string', id=None))
new_features['labels'] = Sequence(feature=ClassLabel(num_classes=len(tag2idx), names=list(unique_tags)))
dataset = dataset.cast(new_features)
```
but I got the following error 
```
ArrowNotImplementedError: Unsupported cast from string to list using function cast_list
```
Moreover, I tried to set feature parameter in load_dataset method, to my new_features, but this fails as well.
How can this be solved ?"
https://github.com/huggingface/datasets/issues/2285,Help understanding how to build a dataset for language modeling as with the old TextDataset,"['\r\nI received an answer for this question on the HuggingFace Datasets forum by @lhoestq\r\n\r\nHi !\r\n\r\nIf you want to tokenize line by line, you can use this:\r\n\r\n```\r\nmax_seq_length = 512\r\nnum_proc = 4\r\n\r\ndef tokenize_function(examples):\r\n# Remove empty lines\r\nexamples[""text""] = [line for line in examples[""text""] if len(line) > 0 and not line.isspace()]\r\nreturn tokenizer(\r\n    examples[""text""],\r\n    truncation=True,\r\n    max_length=max_seq_length,\r\n)\r\n\r\ntokenized_dataset = dataset.map(\r\ntokenize_function,\r\nbatched=True,\r\nnum_proc=num_proc,\r\nremove_columns=[""text""],\r\n)\r\n```\r\n\r\nThough the TextDataset was doing a different processing by concatenating all the texts and building blocks of size 512. If you need this behavior, then you must apply an additional map function after the tokenization:\r\n\r\n```\r\n# Main data processing function that will concatenate all texts from\r\n# our dataset and generate chunks of max_seq_length.\r\ndef group_texts(examples):\r\n# Concatenate all texts.\r\nconcatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\r\ntotal_length = len(concatenated_examples[list(examples.keys())[0]])\r\n# We drop the small remainder, we could add padding if the model supported it instead of this drop,\r\n# you can customize this part to your needs.\r\ntotal_length = (total_length // max_seq_length) * max_seq_length\r\n# Split by chunks of max_len.\r\nresult = {\r\n    k: [t[i : i + max_seq_length] for i in range(0, total_length, max_seq_length)]\r\n    for k, t in concatenated_examples.items()\r\n}\r\nreturn result\r\n\r\n# Note that with `batched=True`, this map processes 1,000 texts together,\r\n# so group_texts throws away a remainder for each of those groups of 1,000 texts.\r\n# You can adjust that batch_size here but a higher value might be slower to preprocess.\r\n\r\ntokenized_dataset = tokenized_dataset.map(\r\ngroup_texts,\r\nbatched=True,\r\nnum_proc=num_proc,\r\n)\r\n```\r\n\r\nThis code comes from the processing of the run_mlm.py example script of transformers\r\n\r\n'
 'Resolved']","Hello,

I am trying to load a custom dataset that I will then use for language modeling. The dataset consists of a text file that has a whole document in each line, meaning that each line overpasses the normal 512 tokens limit of most tokenizers.

I would like to understand what is the process to build a text dataset that tokenizes each line, having previously split the documents in the dataset into lines of a ""tokenizable"" size, as the old TextDataset class would do, where you only had to do the following, and a tokenized dataset without text loss would be available to pass to a DataCollator:

```
model_checkpoint = 'distilbert-base-uncased'

from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

from transformers import TextDataset

dataset = TextDataset(
    tokenizer=tokenizer,
    file_path=""path/to/text_file.txt"",
    block_size=512,
)
```

For now, what I have is the following, which, of course, throws an error because each line is longer than the maximum block size in the tokenizer:

```
import datasets
dataset = datasets.load_dataset('path/to/text_file.txt')

model_checkpoint = 'distilbert-base-uncased'
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

def tokenize_function(examples):
    return tokenizer(examples[""text""])

tokenized_datasets = dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=[""text""])

tokenized_datasets
```

So what would be the ""standard"" way of creating a dataset in the way it was done before?

Thank you very much for the help :))"
https://github.com/huggingface/datasets/issues/2279,Compatibility with Ubuntu 18 and GLIBC 2.27?,"['From the trace this seems like an error in the tokenizer library instead.\r\n\r\nDo you mind opening an issue at https://github.com/huggingface/tokenizers instead?'
 'Hi @tginart, thanks for reporting.\r\n\r\nI think this issue is already open at `tokenizers` library: https://github.com/huggingface/tokenizers/issues/685']","## Describe the bug
For use on Ubuntu systems, it seems that datasets requires GLIBC 2.29. However, Ubuntu 18 runs with GLIBC 2.27 and it seems [non-trivial to upgrade GLIBC to 2.29 for Ubuntu 18 users](https://www.digitalocean.com/community/questions/how-install-glibc-2-29-or-higher-in-ubuntu-18-04). 

I'm not sure if there is anything that can be done about this, but I'd like to confirm that using huggingface/datasets requires either an upgrade to Ubuntu 19/20 or a hand-rolled install of a higher version of GLIBC.

## Steps to reproduce the bug
1. clone the transformers repo
2. move to examples/pytorch/language-modeling
3. run example command:
```python run_clm.py     --model_name_or_path gpt2     --dataset_name wikitext     --dataset_config_name wikitext-2-raw-v1     --do_train     --do_eval     --output_dir /tmp/test-clm```


## Expected results
As described in the transformers repo.

## Actual results
```Traceback (most recent call last):
  File ""run_clm.py"", line 34, in <module>
    from transformers import (
  File ""/home/tginart/anaconda3/envs/huggingface/lib/python3.7/site-packages/transformers/__init__.py"", line 2487, in __getattr__
    return super().__getattr__(name)
  File ""/home/tginart/anaconda3/envs/huggingface/lib/python3.7/site-packages/transformers/file_utils.py"", line 1699, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File ""/home/tginart/anaconda3/envs/huggingface/lib/python3.7/site-packages/transformers/__init__.py"", line 2481, in _get_module
    return importlib.import_module(""."" + module_name, self.__name__)
  File ""/home/tginart/anaconda3/envs/huggingface/lib/python3.7/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""/home/tginart/anaconda3/envs/huggingface/lib/python3.7/site-packages/transformers/models/__init__.py"", line 19, in <module>
    from . import (
  File ""/home/tginart/anaconda3/envs/huggingface/lib/python3.7/site-packages/transformers/models/layoutlm/__init__.py"", line 23, in <module>
    from .tokenization_layoutlm import LayoutLMTokenizer
  File ""/home/tginart/anaconda3/envs/huggingface/lib/python3.7/site-packages/transformers/models/layoutlm/tokenization_layoutlm.py"", line 19, in <module>
    from ..bert.tokenization_bert import BertTokenizer
  File ""/home/tginart/anaconda3/envs/huggingface/lib/python3.7/site-packages/transformers/models/bert/tokenization_bert.py"", line 23, in <module>
    from ...tokenization_utils import PreTrainedTokenizer, _is_control, _is_punctuation, _is_whitespace
  File ""/home/tginart/anaconda3/envs/huggingface/lib/python3.7/site-packages/transformers/tokenization_utils.py"", line 26, in <module>
    from .tokenization_utils_base import (
  File ""/home/tginart/anaconda3/envs/huggingface/lib/python3.7/site-packages/transformers/tokenization_utils_base.py"", line 68, in <module>
    from tokenizers import AddedToken
  File ""/home/tginart/anaconda3/envs/huggingface/lib/python3.7/site-packages/tokenizers/__init__.py"", line 79, in <module>
    from .tokenizers import (
ImportError: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /home/tginart/anaconda3/envs/huggingface/lib/python3.7/site-packages/tokenizers/tokenizers.cpython-37m-x86_64-linux-gnu.so)
```

## Versions
Paste the output of the following code:
```
- Datasets: 1.6.1
- Python: 3.7.10 (default, Feb 26 2021, 18:47:35) 
[GCC 7.3.0]
- Platform: Linux-4.15.0-128-generic-x86_64-with-debian-buster-sid

```
"
https://github.com/huggingface/datasets/issues/2278,Loss result inGptNeoForCasual,"[""Hi ! I think you might have to ask on the `transformers` repo on or the forum at https://discuss.huggingface.co/\r\n\r\nClosing since it's not related to this library""]","Is there any way you give the "" loss"" and ""logits"" results in the gpt neo api? "
https://github.com/huggingface/datasets/issues/2276,concatenate_datasets loads all the data into memory,"['Therefore, when I try to concatenate larger datasets (5x 35GB data sets) I also get an out of memory error, since over 90GB of swap space was used at the time of the crash:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nMemoryError                               Traceback (most recent call last)\r\n<ipython-input-6-9766d77530b9> in <module>\r\n     20         print(file_name)\r\n     21         cv_batch = load_from_disk(file_name)\r\n---> 22         cv_sampled_train = concatenate_datasets([cv_sampled_train, cv_batch])\r\n     23 \r\n     24 print(""Saving to disk!"")\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py in concatenate_datasets(dsets, info, split, axis)\r\n   2891 \r\n   2892     # Concatenate tables\r\n-> 2893     table = concat_tables([dset._data for dset in dsets if len(dset._data) > 0], axis=axis)\r\n   2894     table = update_metadata_with_features(table, None)\r\n   2895 \r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\datasets\\table.py in concat_tables(tables, axis)\r\n    837     if len(tables) == 1:\r\n    838         return tables[0]\r\n--> 839     return ConcatenationTable.from_tables(tables, axis=axis)\r\n    840 \r\n    841 \r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\datasets\\table.py in from_tables(cls, tables, axis)\r\n    697             return result\r\n    698 \r\n--> 699         blocks = to_blocks(tables[0])\r\n    700         for table in tables[1:]:\r\n    701             table_blocks = to_blocks(table)\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\datasets\\table.py in to_blocks(table)\r\n    669                 return [[InMemoryTable(table)]]\r\n    670             elif isinstance(table, ConcatenationTable):\r\n--> 671                 return copy.deepcopy(table.blocks)\r\n    672             else:\r\n    673                 return [[table]]\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n    144     copier = _deepcopy_dispatch.get(cls)\r\n    145     if copier is not None:\r\n--> 146         y = copier(x, memo)\r\n    147     else:\r\n    148         if issubclass(cls, type):\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\copy.py in _deepcopy_list(x, memo, deepcopy)\r\n    203     append = y.append\r\n    204     for a in x:\r\n--> 205         append(deepcopy(a, memo))\r\n    206     return y\r\n    207 d[list] = _deepcopy_list\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n    144     copier = _deepcopy_dispatch.get(cls)\r\n    145     if copier is not None:\r\n--> 146         y = copier(x, memo)\r\n    147     else:\r\n    148         if issubclass(cls, type):\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\copy.py in _deepcopy_list(x, memo, deepcopy)\r\n    203     append = y.append\r\n    204     for a in x:\r\n--> 205         append(deepcopy(a, memo))\r\n    206     return y\r\n    207 d[list] = _deepcopy_list\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n    151             copier = getattr(x, ""__deepcopy__"", None)\r\n    152             if copier is not None:\r\n--> 153                 y = copier(memo)\r\n    154             else:\r\n    155                 reductor = dispatch_table.get(cls)\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\datasets\\table.py in __deepcopy__(self, memo)\r\n    143         # by adding it to the memo, self.table won\'t be copied\r\n    144         memo[id(self.table)] = self.table\r\n--> 145         return _deepcopy(self, memo)\r\n    146 \r\n    147     def __getstate__(self):\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\datasets\\table.py in _deepcopy(x, memo)\r\n     62     memo[id(x)] = result\r\n     63     for k, v in x.__dict__.items():\r\n---> 64         setattr(result, k, copy.deepcopy(v, memo))\r\n     65     return result\r\n     66 \r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n    144     copier = _deepcopy_dispatch.get(cls)\r\n    145     if copier is not None:\r\n--> 146         y = copier(x, memo)\r\n    147     else:\r\n    148         if issubclass(cls, type):\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\copy.py in _deepcopy_list(x, memo, deepcopy)\r\n    203     append = y.append\r\n    204     for a in x:\r\n--> 205         append(deepcopy(a, memo))\r\n    206     return y\r\n    207 d[list] = _deepcopy_list\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n    170                     y = x\r\n    171                 else:\r\n--> 172                     y = _reconstruct(x, memo, *rv)\r\n    173 \r\n    174     # If is its own copy, don\'t memoize.\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n    262     if deep and args:\r\n    263         args = (deepcopy(arg, memo) for arg in args)\r\n--> 264     y = func(*args)\r\n    265     if deep:\r\n    266         memo[id(x)] = y\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\copy.py in <genexpr>(.0)\r\n    261     deep = memo is not None\r\n    262     if deep and args:\r\n--> 263         args = (deepcopy(arg, memo) for arg in args)\r\n    264     y = func(*args)\r\n    265     if deep:\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n    144     copier = _deepcopy_dispatch.get(cls)\r\n    145     if copier is not None:\r\n--> 146         y = copier(x, memo)\r\n    147     else:\r\n    148         if issubclass(cls, type):\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\copy.py in _deepcopy_list(x, memo, deepcopy)\r\n    203     append = y.append\r\n    204     for a in x:\r\n--> 205         append(deepcopy(a, memo))\r\n    206     return y\r\n    207 d[list] = _deepcopy_list\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n    170                     y = x\r\n    171                 else:\r\n--> 172                     y = _reconstruct(x, memo, *rv)\r\n    173 \r\n    174     # If is its own copy, don\'t memoize.\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n    262     if deep and args:\r\n    263         args = (deepcopy(arg, memo) for arg in args)\r\n--> 264     y = func(*args)\r\n    265     if deep:\r\n    266         memo[id(x)] = y\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\copy.py in <genexpr>(.0)\r\n    261     deep = memo is not None\r\n    262     if deep and args:\r\n--> 263         args = (deepcopy(arg, memo) for arg in args)\r\n    264     y = func(*args)\r\n    265     if deep:\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n    144     copier = _deepcopy_dispatch.get(cls)\r\n    145     if copier is not None:\r\n--> 146         y = copier(x, memo)\r\n    147     else:\r\n    148         if issubclass(cls, type):\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\copy.py in _deepcopy_tuple(x, memo, deepcopy)\r\n    208 \r\n    209 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):\r\n--> 210     y = [deepcopy(a, memo) for a in x]\r\n    211     # We\'re not going to put the tuple in the memo, but it\'s still important we\r\n    212     # check for it, in case the tuple contains recursive mutable structures.\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\copy.py in <listcomp>(.0)\r\n    208 \r\n    209 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):\r\n--> 210     y = [deepcopy(a, memo) for a in x]\r\n    211     # We\'re not going to put the tuple in the memo, but it\'s still important we\r\n    212     # check for it, in case the tuple contains recursive mutable structures.\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n    144     copier = _deepcopy_dispatch.get(cls)\r\n    145     if copier is not None:\r\n--> 146         y = copier(x, memo)\r\n    147     else:\r\n    148         if issubclass(cls, type):\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\copy.py in _deepcopy_list(x, memo, deepcopy)\r\n    203     append = y.append\r\n    204     for a in x:\r\n--> 205         append(deepcopy(a, memo))\r\n    206     return y\r\n    207 d[list] = _deepcopy_list\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n    144     copier = _deepcopy_dispatch.get(cls)\r\n    145     if copier is not None:\r\n--> 146         y = copier(x, memo)\r\n    147     else:\r\n    148         if issubclass(cls, type):\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\copy.py in _deepcopy_tuple(x, memo, deepcopy)\r\n    208 \r\n    209 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):\r\n--> 210     y = [deepcopy(a, memo) for a in x]\r\n    211     # We\'re not going to put the tuple in the memo, but it\'s still important we\r\n    212     # check for it, in case the tuple contains recursive mutable structures.\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\copy.py in <listcomp>(.0)\r\n    208 \r\n    209 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):\r\n--> 210     y = [deepcopy(a, memo) for a in x]\r\n    211     # We\'re not going to put the tuple in the memo, but it\'s still important we\r\n    212     # check for it, in case the tuple contains recursive mutable structures.\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n    144     copier = _deepcopy_dispatch.get(cls)\r\n    145     if copier is not None:\r\n--> 146         y = copier(x, memo)\r\n    147     else:\r\n    148         if issubclass(cls, type):\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\copy.py in _deepcopy_list(x, memo, deepcopy)\r\n    203     append = y.append\r\n    204     for a in x:\r\n--> 205         append(deepcopy(a, memo))\r\n    206     return y\r\n    207 d[list] = _deepcopy_list\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\copy.py in deepcopy(x, memo, _nil)\r\n    159                     reductor = getattr(x, ""__reduce_ex__"", None)\r\n    160                     if reductor is not None:\r\n--> 161                         rv = reductor(4)\r\n    162                     else:\r\n    163                         reductor = getattr(x, ""__reduce__"", None)\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyarrow\\io.pxi in pyarrow.lib.Buffer.__reduce_ex__()\r\n\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyarrow\\io.pxi in pyarrow.lib.Buffer.to_pybytes()\r\n\r\nMemoryError: \r\n\r\n```'
 'Hi ! this looks like an important issue. Let me try to reproduce this.\r\nCc @samsontmr this might be related to the memory issue you have in #2134 '
 '@lhoestq Just went to open a similar issue.\r\n\r\nIt seems like deep copying (tested on master) the dataset object writes the table\'s record batches (`dset._data._batches`) into RAM.\r\n\r\nTo find the bug, I modified the `_deepcopy` function in `table.py` as follows:\r\n```python\r\ndef _deepcopy(x, memo: dict):\r\n    """"""deepcopy a regular class instance""""""\r\n    import psutil # pip install this package\r\n    import time\r\n    cls = x.__class__\r\n    result = cls.__new__(cls)\r\n    memo[id(x)] = result\r\n    for k, v in x.__dict__.items():\r\n        print(""=""* 50)\r\n        print(""Current memory:"", psutil.virtual_memory().percent)\r\n        print(f""Saving object {k} with value {v}"")\r\n        setattr(result, k, copy.deepcopy(v, memo))\r\n        time.sleep(5)\r\n        print(""Memory after copy:"", psutil.virtual_memory().percent)\r\n    return result\r\n```\r\nTest script:\r\n```python\r\nimport copy\r\nfrom datasets import load_dataset\r\nbk = load_dataset(""bookcorpus"", split=""train"")\r\nbk_copy = copy.deepcopy(bk)\r\n```'
 ""Thanks for the insights @mariosasko ! I'm working on a fix.\r\nSince this is a big issue I'll make a patch release as soon as this is fixed""
 'Hi @samsontmr @TaskManager91 the fix is on the master branch, feel free to install `datasets` from source and let us know if you still have issues'
 'We just released `datasets` 1.6.2 that includes the fix :)'
 'thanks it works like a charm! :)']","## Describe the bug
When I try to concatenate 2 datasets (10GB each) , the entire data is loaded into memory instead of being written directly to disk.

Interestingly, this happens when trying to save the new dataset to disk or concatenating it again.

![image](https://user-images.githubusercontent.com/7063207/116420321-2b21b480-a83e-11eb-9006-8f6ca729fb6f.png)


## Steps to reproduce the bug
```python
from datasets import concatenate_datasets, load_from_disk

test_sampled_pro = load_from_disk(""test_sampled_pro"")
val_sampled_pro = load_from_disk(""val_sampled_pro"")

big_set = concatenate_datasets([test_sampled_pro, val_sampled_pro])

# Loaded to memory
big_set.save_to_disk(""big_set"")

# Loaded to memory
big_set = concatenate_datasets([big_set, val_sampled_pro])
```

## Expected results
The data should be loaded into memory in batches and then saved directly to disk.

## Actual results
The entire data set is loaded into the memory and then saved to the hard disk.

## Versions
Paste the output of the following code:
```python
- Datasets: 1.6.1
- Python: 3.8.8 (default, Apr 13 2021, 19:58:26) 
[GCC 7.3.0]
- Platform: Linux-5.4.72-microsoft-standard-WSL2-x86_64-with-glibc2.10
```
"
https://github.com/huggingface/datasets/issues/2275,SNLI dataset has labels of -1 ,"['Hi @puzzler10, \r\nThose examples where `gold_label` field was empty, -1 label was alloted to it. In order to remove it you can filter the samples from train/val/test splits. Here\'s how you can drop those rows from the dataset:\r\n`dataset = load_dataset(""snli"")`\r\n`dataset_test_filter = dataset[\'test\'].filter(lambda example: example[\'label\'] != -1)`\r\n\r\nI agree it should have been mentioned in the documentation. I\'ll raise a PR regarding the same. Thanks for pointing out!']","There are a number of rows with a label of -1 in the SNLI dataset. The dataset descriptions [here](https://nlp.stanford.edu/projects/snli/) and [here](https://github.com/huggingface/datasets/tree/master/datasets/snli) don't list  -1 as a label possibility, and neither does the dataset viewer. As examples, see index 107 or 124 of the test set.

It isn't clear what these labels mean. I found a [line of code](https://github.com/huggingface/datasets/blob/80e59ef178d3bb2090d091bc32315c655eb0633d/datasets/snli/snli.py#L94) that seems to put them in but it seems still unclear why they are there. The current workaround is to just drop the rows from any model being trained. 

Perhaps the documentation should be updated."
https://github.com/huggingface/datasets/issues/2272,Bug in Dataset.class_encode_column,"[""This has been fixed in this commit: https://github.com/huggingface/datasets/pull/2254/commits/88676c930216cd4cc31741b99827b477d2b46cb6\r\n\r\nIt was introduced in #2246 : using map with `input_columns` doesn't return the other columns anymore""]","## Describe the bug

All the rest of the columns except the one passed to `Dataset.class_encode_column` are discarded.

## Expected results

All the original columns should be kept.

This needs regression tests.
"
https://github.com/huggingface/datasets/issues/2271,Synchronize table metadata with features,['See PR #2274 '],"**Is your feature request related to a problem? Please describe.**

As pointed out in this [comment](https://github.com/huggingface/datasets/pull/2145#discussion_r621326767):
> Metadata stored in the schema is just a redundant information regarding the feature types.
It is used when calling Dataset.from_file to know which feature types to use.
These metadata are stored in the schema of the pyarrow table by using `update_metadata_with_features`.
However this something that's almost never tested properly.

**Describe the solution you'd like**

We should find a way to always make sure that the metadata (in `self.data.schema.metadata`) are synced with the actual feature types (in `self.info.features`)."
https://github.com/huggingface/datasets/issues/2267,DatasetDict save load Failing test in 1.6 not in 1.5,"[""Thanks for reporting ! We're looking into it""
 ""I'm not able to reproduce this, do you think you can provide a code that creates a DatasetDict that has this issue when saving and reloading ?""
 'Hi, I just ran into a similar error. Here is the minimal code to reproduce:\r\n```python\r\nfrom datasets import load_dataset, DatasetDict\r\nds = load_dataset(\'super_glue\', \'multirc\')\r\n\r\nds.save_to_disk(\'tempds\')\r\n\r\nds = DatasetDict.load_from_disk(\'tempds\')\r\n\r\n```\r\n\r\n```bash\r\nReusing dataset super_glue (/home/idahl/.cache/huggingface/datasets/super_glue/multirc/1.0.2/2fb163bca9085c1deb906aff20f00c242227ff704a4e8c9cfdfe820be3abfc83)\r\nTraceback (most recent call last):\r\n  File ""/home/idahl/eval-util-expl/multirc/tmp.py"", line 7, in <module>\r\n    ds = DatasetDict.load_from_disk(\'tempds\')\r\n  File ""/home/idahl/miniconda3/envs/eval-util-expl/lib/python3.9/site-packages/datasets/dataset_dict.py"", line 710, in load_from_disk\r\n    dataset_dict[k] = Dataset.load_from_disk(dataset_dict_split_path, fs, keep_in_memory=keep_in_memory)\r\n  File ""/home/idahl/miniconda3/envs/eval-util-expl/lib/python3.9/site-packages/datasets/arrow_dataset.py"", line 687, in load_from_disk\r\n    return Dataset(\r\n  File ""/home/idahl/miniconda3/envs/eval-util-expl/lib/python3.9/site-packages/datasets/arrow_dataset.py"", line 274, in __init__\r\n    raise ValueError(\r\nValueError: External features info don\'t match the dataset:\r\nGot\r\n{\'answer\': Value(dtype=\'string\', id=None), \'idx\': {\'answer\': Value(dtype=\'int32\', id=None), \'paragraph\': Value(dtype=\'int32\', id=None), \'question\': Value(dtype=\'int32\', id=None)}, \'label\': ClassLabel(num_classes=2, names=[\'False\', \'True\'], names_file=None, id=None), \'paragraph\': Value(dtype=\'string\', id=None), \'question\': Value(dtype=\'string\', id=None)}\r\nwith type\r\nstruct<answer: string, idx: struct<answer: int32, paragraph: int32, question: int32>, label: int64, paragraph: string, question: string>\r\n\r\nbut expected something like\r\n{\'answer\': Value(dtype=\'string\', id=None), \'idx\': {\'paragraph\': Value(dtype=\'int32\', id=None), \'question\': Value(dtype=\'int32\', id=None), \'answer\': Value(dtype=\'int32\', id=None)}, \'label\': Value(dtype=\'int64\', id=None), \'paragraph\': Value(dtype=\'string\', id=None), \'question\': Value(dtype=\'string\', id=None)}\r\nwith type\r\nstruct<answer: string, idx: struct<paragraph: int32, question: int32, answer: int32>, label: int64, paragraph: string, question: string>\r\n\r\n```\r\n\r\nThe non-matching part seems to be\r\n`\'label\': ClassLabel(num_classes=2, names=[\'False\', \'True\'], names_file=None, id=None),`\r\nvs \r\n`\'label\': Value(dtype=\'int64\', id=None),`\r\n\r\nAnd the order in the `<struct...` being different, which might cause the [features.type != inferred_features.type](https://github.com/huggingface/datasets/blob/master/src/datasets/arrow_dataset.py#L274) condition to become true and raise this ValueError.\r\n\r\n\r\nI am using datasets version 1.6.2.\r\n\r\nEdit: can confirm, this works without error in version 1.5.0'
 ""My current workaround is to remove the idx feature:\r\n\r\n```\r\n\r\nfrom datasets import load_dataset, DatasetDict, Value\r\nds = load_dataset('super_glue', 'multirc')\r\nds = ds.remove_columns('idx')\r\n\r\nds.save_to_disk('tempds')\r\n\r\nds = DatasetDict.load_from_disk('tempds')\r\n\r\n```\r\n\r\nworks.""
 ""It looks like this issue comes from the order of the fields in the 'idx' struct that is different for some reason.\r\nI'm looking into it. Note that as a workaround you can also flatten the nested features with `ds = ds.flatten()`""
 ""I just pushed a fix on `master`. We'll do a new release soon !\r\n\r\nThanks for reporting""]","## Describe the bug

We have a test that saves a DatasetDict to disk and then loads it from disk. In 1.6 there is an incompatibility in the schema.




Downgrading to `>1.6` -- fixes the problem.

## Steps to reproduce the bug
```python

### Load a dataset dict from jsonl 

path = '/test/foo'

ds_dict.save_to_disk(path)

ds_from_disk = DatasetDict.load_from_disk(path).  ## <-- this is where I see the error on 1.6
```

## Expected results

Upgrading to 1.6 shouldn't break that test. We should be able to serialize to and from disk.

## Actual results
```
        # Infer features if None
        inferred_features = Features.from_arrow_schema(arrow_table.schema)
        if self.info.features is None:
            self.info.features = inferred_features
    
        # Infer fingerprint if None
    
        if self._fingerprint is None:
            self._fingerprint = generate_fingerprint(self)
    
        # Sanity checks
    
        assert self.features is not None, ""Features can't be None in a Dataset object""
        assert self._fingerprint is not None, ""Fingerprint can't be None in a Dataset object""
        if self.info.features.type != inferred_features.type:
>           raise ValueError(
                ""External features info don't match the dataset:\nGot\n{}\nwith type\n{}\n\nbut expected something like\n{}\nwith type\n{}"".format(
                    self.info.features, self.info.features.type, inferred_features, inferred_features.type
                )
            )
E           ValueError: External features info don't match the dataset:
E           Got
E           {'_input_hash': Value(dtype='int64', id=None), '_task_hash': Value(dtype='int64', id=None), '_view_id': Value(dtype='string', id=None), 'answer': Value(dtype='string', id=None), 'encoding__ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'encoding__offsets': Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None), 'encoding__overflowing': Sequence(feature=Value(dtype='null', id=None), length=-1, id=None), 'encoding__tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'encoding__words': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'ner_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'ner_labels': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'relations': [{'child': Value(dtype='int64', id=None), 'child_span': {'end': Value(dtype='int64', id=None), 'label': Value(dtype='string', id=None), 'start': Value(dtype='int64', id=None), 'token_end': Value(dtype='int64', id=None), 'token_start': Value(dtype='int64', id=None)}, 'color': Value(dtype='string', id=None), 'head': Value(dtype='int64', id=None), 'head_span': {'end': Value(dtype='int64', id=None), 'label': Value(dtype='string', id=None), 'start': Value(dtype='int64', id=None), 'token_end': Value(dtype='int64', id=None), 'token_start': Value(dtype='int64', id=None)}, 'label': Value(dtype='string', id=None)}], 'spans': [{'end': Value(dtype='int64', id=None), 'label': Value(dtype='string', id=None), 'start': Value(dtype='int64', id=None), 'text': Value(dtype='string', id=None), 'token_end': Value(dtype='int64', id=None), 'token_start': Value(dtype='int64', id=None), 'type': Value(dtype='string', id=None)}], 'text': Value(dtype='string', id=None), 'tokens': [{'disabled': Value(dtype='bool', id=None), 'end': Value(dtype='int64', id=None), 'id': Value(dtype='int64', id=None), 'start': Value(dtype='int64', id=None), 'text': Value(dtype='string', id=None), 'ws': Value(dtype='bool', id=None)}]}
E           with type
E           struct<_input_hash: int64, _task_hash: int64, _view_id: string, answer: string, encoding__ids: list<item: int64>, encoding__offsets: list<item: list<item: int64>>, encoding__overflowing: list<item: null>, encoding__tokens: list<item: string>, encoding__words: list<item: int64>, ner_ids: list<item: int64>, ner_labels: list<item: string>, relations: list<item: struct<child: int64, child_span: struct<end: int64, label: string, start: int64, token_end: int64, token_start: int64>, color: string, head: int64, head_span: struct<end: int64, label: string, start: int64, token_end: int64, token_start: int64>, label: string>>, spans: list<item: struct<end: int64, label: string, start: int64, text: string, token_end: int64, token_start: int64, type: string>>, text: string, tokens: list<item: struct<disabled: bool, end: int64, id: int64, start: int64, text: string, ws: bool>>>
E           
E           but expected something like
E           {'_input_hash': Value(dtype='int64', id=None), '_task_hash': Value(dtype='int64', id=None), '_view_id': Value(dtype='string', id=None), 'answer': Value(dtype='string', id=None), 'encoding__ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'encoding__offsets': Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None), 'encoding__overflowing': Sequence(feature=Value(dtype='null', id=None), length=-1, id=None), 'encoding__tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'encoding__words': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'ner_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'ner_labels': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'relations': [{'head': Value(dtype='int64', id=None), 'child': Value(dtype='int64', id=None), 'head_span': {'start': Value(dtype='int64', id=None), 'end': Value(dtype='int64', id=None), 'token_start': Value(dtype='int64', id=None), 'token_end': Value(dtype='int64', id=None), 'label': Value(dtype='string', id=None)}, 'child_span': {'start': Value(dtype='int64', id=None), 'end': Value(dtype='int64', id=None), 'token_start': Value(dtype='int64', id=None), 'token_end': Value(dtype='int64', id=None), 'label': Value(dtype='string', id=None)}, 'color': Value(dtype='string', id=None), 'label': Value(dtype='string', id=None)}], 'spans': [{'text': Value(dtype='string', id=None), 'start': Value(dtype='int64', id=None), 'token_start': Value(dtype='int64', id=None), 'token_end': Value(dtype='int64', id=None), 'end': Value(dtype='int64', id=None), 'type': Value(dtype='string', id=None), 'label': Value(dtype='string', id=None)}], 'text': Value(dtype='string', id=None), 'tokens': [{'text': Value(dtype='string', id=None), 'start': Value(dtype='int64', id=None), 'end': Value(dtype='int64', id=None), 'id': Value(dtype='int64', id=None), 'ws': Value(dtype='bool', id=None), 'disabled': Value(dtype='bool', id=None)}]}
E           with type
E           struct<_input_hash: int64, _task_hash: int64, _view_id: string, answer: string, encoding__ids: list<item: int64>, encoding__offsets: list<item: list<item: int64>>, encoding__overflowing: list<item: null>, encoding__tokens: list<item: string>, encoding__words: list<item: int64>, ner_ids: list<item: int64>, ner_labels: list<item: string>, relations: list<item: struct<head: int64, child: int64, head_span: struct<start: int64, end: int64, token_start: int64, token_end: int64, label: string>, child_span: struct<start: int64, end: int64, token_start: int64, token_end: int64, label: string>, color: string, label: string>>, spans: list<item: struct<text: string, start: int64, token_start: int64, token_end: int64, end: int64, type: string, label: string>>, text: string, tokens: list<item: struct<text: string, start: int64, end: int64, id: int64, ws: bool, disabled: bool>>>

../../../../../.virtualenvs/tf_ner_rel_lib/lib/python3.8/site-packages/datasets/arrow_dataset.py:274: ValueError
```
## Versions
- Datasets: 1.6.1
- Python: 3.8.5 (default, Jan 26 2021, 10:01:04) 
[Clang 12.0.0 (clang-1200.0.32.2)]
- Platform: macOS-10.15.7-x86_64-i386-64bit

```
"
https://github.com/huggingface/datasets/issues/2262,NewsPH NLI dataset script fails to access test data.,"['Thanks @bhavitvyamalik for the fix !\r\nThe fix will be available in the next release.\r\nIt\'s already available on the `master` branch. For now you can either install `datasets` from source or use `script_version=""master""` in `load_dataset` to use the fixed version of this dataset.']","In Newsph-NLI Dataset (#1192), it fails to access test data.

According to the script below, the download manager will download the train data when trying to download the test data. 

https://github.com/huggingface/datasets/blob/2a2dd6316af2cc7fdf24e4779312e8ee0c7ed98b/datasets/newsph_nli/newsph_nli.py#L71

If you download it according to the script above, you can see that train and test receive the same data as shown below.
```python
>>> from datasets import load_dataset
>>> newsph_nli = load_dataset(path=""./datasets/newsph_nli.py"")
>>> newsph_nli
DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label'],
        num_rows: 420000
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label'],
        num_rows: 420000
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label'],
        num_rows: 90000
    })
})
>>> newsph_nli[""train""][0]
{'hypothesis': 'Ito ang dineklara ni Atty. Romulo Macalintal, abogado ni Robredo, kaugnay ng pagsisimula ng preliminary conference ngayong hapon sa Presidential Electoral Tribunal (PET).',
 'label': 1,
 'premise': '""Hindi ko ugali ang mamulitika; mas gusto kong tahimik na magtrabaho. Pero sasabihin ko ito ngayon: ang tapang, lakas, at diskarte, hindi nadadaan sa mapanirang salita. Ang kailangan ng taumbayan ay tapang sa gawa,"" ayon kay Robredo sa inilabas nitong statement.'}
>>> newsph_nli[""test""][0]
{'hypothesis': 'Ito ang dineklara ni Atty. Romulo Macalintal, abogado ni Robredo, kaugnay ng pagsisimula ng preliminary conference ngayong hapon sa Presidential Electoral Tribunal (PET).',
 'label': 1,
 'premise': '""Hindi ko ugali ang mamulitika; mas gusto kong tahimik na magtrabaho. Pero sasabihin ko ito ngayon: ang tapang, lakas, at diskarte, hindi nadadaan sa mapanirang salita. Ang kailangan ng taumbayan ay tapang sa gawa,"" ayon kay Robredo sa inilabas nitong statement.'}
```

In local, I modified the code of the source as below and got the correct result.
```python
71    test_path = os.path.join(download_path, ""test.csv"") 
```
```python
>>> from datasets import load_dataset
>>> newsph_nli = load_dataset(path=""./datasets/newsph_nli.py"")
>>> newsph_nli
DatasetDict({
    train: Dataset({
        features: ['premise', 'hypothesis', 'label'],
        num_rows: 420000
    })
    test: Dataset({
        features: ['premise', 'hypothesis', 'label'],
        num_rows: 9000
    })
    validation: Dataset({
        features: ['premise', 'hypothesis', 'label'],
        num_rows: 90000
    })
})
>>> newsph_nli[""train""][0]
{'hypothesis': 'Ito ang dineklara ni Atty. Romulo Macalintal, abogado ni Robredo, kaugnay ng pagsisimula ng preliminary conference ngayong hapon sa Presidential Electoral Tribunal (PET).',
 'label': 1,
 'premise': '""Hindi ko ugali ang mamulitika; mas gusto kong tahimik na magtrabaho. Pero sasabihin ko ito ngayon: ang tapang, lakas, at diskarte, hindi nadadaan sa mapanirang salita. Ang kailangan ng taumbayan ay tapang sa gawa,"" ayon kay Robredo sa inilabas nitong statement.'}
>>> newsph_nli[""test""][0]
{'hypothesis': '-- JAI (@JaiPaller) September 13, 2019',
 'label': 1,
 'premise': 'Pinag-iingat ng Konsulado ng Pilipinas sa Dubai ang publiko, partikular ang mga donor, laban sa mga scam na gumagamit ng mga charitable organization.'}
```

I don't have experience with open source pull requests, so I suggest that you reflect them in the source.

Thank you for reading :)"
https://github.com/huggingface/datasets/issues/2256,Running `datase.map` with `num_proc > 1` uses a lot of memory,"[""Thanks for reporting ! We are working on this and we'll do a patch release very soon.""
 'We did a patch release to fix this issue.\r\nIt should be fixed in the new version 1.6.1\r\n\r\nThanks again for reporting and for the details :)']","## Describe the bug
Running `datase.map` with `num_proc > 1`  leads to a tremendous memory usage that requires swapping on disk and it becomes very slow.

## Steps to reproduce the bug
```python
from datasets import load_dataset

dstc8_datset = load_dataset(""roskoN/dstc8-reddit-corpus"", keep_in_memory=False)


def _prepare_sample(batch):
    return {""input_ids"": list(), ""attention_mask"": list()}


for split_name, dataset_split in list(dstc8_datset.items()):
    print(f""Processing {split_name}"")
    encoded_dataset_split = dataset_split.map(
        function=_prepare_sample,
        batched=True,
        num_proc=4,
        remove_columns=dataset_split.column_names,
        batch_size=10,
        writer_batch_size=10,
        keep_in_memory=False,
    )
    print(encoded_dataset_split)

    path = f""./data/encoded_{split_name}""

    encoded_dataset_split.save_to_disk(path)
```

## Expected results
Memory usage should stay within reasonable boundaries.


## Actual results
This is htop-output from running the provided script.

![image](https://user-images.githubusercontent.com/8143425/115954836-66954980-a4f3-11eb-8340-0153bdc3a475.png)

## Versions
```
- Datasets: 1.6.0
- Python: 3.8.8 (default, Apr 13 2021, 19:58:26)
[GCC 7.3.0]
- Platform: Linux-4.19.128-microsoft-standard-x86_64-with-glibc2.10
```
Running on WSL2
"
https://github.com/huggingface/datasets/issues/2252,Slow dataloading with big datasets issue persists,"[""Hi ! Sorry to hear that. This may come from another issue then.\r\n\r\nFirst can we check if this latency comes from the dataset itself ?\r\nYou can try to load your dataset and benchmark the speed of querying random examples inside it ?\r\n```python\r\nimport time\r\nimport numpy as np\r\n\r\nfrom datasets import load_from_disk\r\n\r\ndataset = load_from_disk(...) # or from load_dataset...\r\n\r\n_start = time.time()\r\nn = 100\r\nfor i in np.random.default_rng(42).integers(0, len(dataset), size=n):\r\n    _ = dataset[i]\r\nprint(time.time() - _start)\r\n```\r\n\r\nIf we see a significant speed difference between your two datasets then it would mean that there's an issue somewhere""
 ""Hi @lhoestq, here is the result. I additionally measured time to `load_from_disk`:\r\n* 60GB\r\n```\r\nloading took:  22.618776321411133\r\nramdom indexing 100 times took: 0.10214924812316895\r\n```\r\n\r\n* 600GB\r\n```\r\nloading took:  1176.1764674186707\r\nramdom indexing 100 times took: 2.853600025177002\r\n```\r\n\r\nHmm.. I double checked that it's version 1.6.0. The difference seems quite big, could it be related to the running environment? \r\n""
 ""I'm surprised by the speed change. Can you give more details about your dataset ?\r\nThe speed depends on the number of batches in the arrow tables and the distribution of the lengths of the batches.\r\nYou can access the batches by doing `dataset.data.to_batches()` (use only for debugging) (it doesn't bring data in memory).\r\n\r\nAlso can you explain what parameters you used if you used `map` calls ?\r\nAlso if you have some code that reproduces the issue I'd be happy to investigate it.""
 ""Also if you could give us more info about your env like your OS, version of pyarrow and if you're using an HDD or a SSD""
 'Here are some details of my 600GB dataset. This is a dataset AFTER the `map` function and once I load this dataset, I do not use `map` anymore in the training. Regarding the distribution of the lengths, it is almost uniform (90% is 512 tokens, and 10% is randomly shorter than that -- typical setting for language modeling).\r\n```\r\nlen(batches):\r\n492763\r\n\r\nbatches[0]: \r\npyarrow.RecordBatch\r\nattention_mask: list<item: uint8>\r\n  child 0, item: uint8\r\ninput_ids: list<item: int16>\r\n  child 0, item: int16\r\nspecial_tokens_mask: list<item: uint8>\r\n  child 0, item: uint8\r\ntoken_type_ids: list<item: uint8>\r\n  child 0, item: uint8\r\n```\r\n\r\nHere the some parameters to `map` function just in case it is relevant:\r\n```\r\nnum_proc=1    # as multi processing is slower in my case\r\nload_from_cache_file=False\r\n```\r\n'
 'Regarding the environment, I am running the code on a cloud server. Here are some info:\r\n```\r\nUbuntu 18.04.5 LTS   # cat /etc/issue\r\npyarrow                 3.0.0  # pip list | grep pyarrow\r\n```\r\nThe data is stored in SSD and it is mounted to the machine via Network File System.\r\n\r\nIf you could point me to some of the commands to check the details of the environment, I would be happy to provide relevant information @lhoestq !'
 'I am not sure how I could provide you with the reproducible code, since the problem only arises when the data is big. For the moment, I would share the part that I think is relevant. Feel free to ask me for more info.\r\n\r\n```python\r\nclass MyModel(pytorch_lightning.LightningModule)\r\n    def setup(self, stage):\r\n        self.dataset = datasets.load_from_disk(path)\r\n        self.dataset.set_format(""torch"")\r\n\r\n    def train_dataloader(self):\r\n        collate_fn = transformers.DataCollatorForLanguageModeling(\r\n                tokenizer=transformers.ElectraTokenizerFast.from_pretrained(tok_path)\r\n        )\r\n        dataloader = torch.utils.DataLoader(\r\n                self.dataset,\r\n                batch_size=32,\r\n                collate_fn=collate_fn,\r\n                num_workers=8,\r\n                pin_memory=True,\r\n       )\r\n```'
 ""Hi ! Sorry for the delay I haven't had a chance to take a look at this yet. Are you still experiencing this issue ?\r\nI'm asking because the latest patch release 1.6.2 fixed a few memory issues that could have lead to slow downs""
 'Hi! I just ran the same code with different datasets (one is 60 GB and another 600 GB), and the latter runs much slower. ETA differs by 10x.'
 '@lhoestq and @hwijeen\r\n\r\nDespite upgrading to datasets 1.6.2, still experiencing extremely slow (2h00) loading for a 300Gb local dataset shard size 1.1Gb on local HDD (40Mb/s read speed). This corresponds almost exactly to total data divided by reading speed implying that it reads the entire dataset at each load.\r\n\r\nStack details:\r\n=========\r\n\r\n> GCC version: Could not collect\r\n> Clang version: Could not collect\r\n> CMake version: Could not collect\r\n> \r\n> Python version: 3.7 (64-bit runtime)\r\n> Is CUDA available: True\r\n> CUDA runtime version: 10.2.89\r\n> GPU models and configuration: GPU 0: GeForce GTX 1050\r\n> Nvidia driver version: 457.63\r\n> cuDNN version: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.2\\bin\\cudnn64_7.dll\r\n> HIP runtime version: N/A\r\n> MIOpen runtime version: N/A\r\n> \r\n> Versions of relevant libraries:\r\n> [pip3] datasets==1.6.2\r\n> [pip3] transformers==4.5.1\r\n> [pip3] numpy==1.19.1\r\n> [pip3] numpydoc==1.1.0\r\n> [pip3] pytorch-metric-learning==0.9.98\r\n> [pip3] torch==1.8.1\r\n> [pip3] torchaudio==0.8.1\r\n> [pip3] torchvision==0.2.2\r\n> [conda] blas                      2.16                        mkl    conda-forge\r\n> [conda] cudatoolkit               10.2.89              hb195166_8    conda-forge\r\n> [conda] libblas                   3.8.0                    16_mkl    conda-forge\r\n> [conda] libcblas                  3.8.0                    16_mkl    conda-forge\r\n> [conda] liblapack                 3.8.0                    16_mkl    conda-forge\r\n> [conda] liblapacke                3.8.0                    16_mkl    conda-forge\r\n> [conda] mkl                       2020.1                      216\r\n> [conda] numpy                     1.19.1           py37hae9e721_0    conda-forge\r\n> [conda] numpydoc                  1.1.0                      py_1    conda-forge\r\n> [conda] pytorch                   1.8.1           py3.7_cuda10.2_cudnn7_0    pytorch\r\n> [conda] pytorch-metric-learning   0.9.98             pyh39e3cac_0    metric-learning\r\n> [conda] torchaudio                0.8.1                      py37    pytorch\r\n> [conda] torchvision               0.2.2                      py_3    pytorch'
 'Hi @BenoitDalFerro how do your load your dataset ?'
 ""Hi @lhoestq thanks for the quick turn-around, actually the plain vanilla way, without an particular knack or fashion, I tried to look into the documentation for some alternative but couldn't find any\r\n\r\n> dataset = load_from_disk(dataset_path=os.path.join(datasets_dir,dataset_dir))""
 'I’m facing the same issue when loading a 900GB dataset (stored via `save_to_disk`): `load_from_disk(path_to_dir)` takes 1.5 hours and htop consistently shows high IO rates > 120 M/s.'
 '@tsproisl same here, smells like ~~teen spirit~~ intended generator inadvertently ending up iterator\r\n\r\n@lhoestq perhaps solution to detect bug location in code is to track its signature via HD read usage monitoring, option is to add tracking decorator on top each function and sequentially close all hatches from top to bottom, suggest PySmart https://pypi.org/project/pySMART/ a Smartmontools implementation'
 'I wasn\'t able to reproduce this on a toy dataset of around 300GB:\r\n\r\n```python\r\nimport datasets as ds\r\n\r\ns = ds.load_dataset(""squad"", split=""train"")\r\ns4000 = ds.concatenate_datasets([s] * 4000)\r\nprint(ds.utils.size_str(s4000.data.nbytes))  # \'295.48 GiB\'\r\n\r\ns4000.save_to_disk(""tmp/squad_4000"")\r\n```\r\n\r\n```python\r\nimport psutil\r\nimport time\r\nfrom datasets import load_from_disk\r\n\r\ndisk = ""disk0""  # You may have to change your disk here\r\niocnt1 = psutil.disk_io_counters(perdisk=True)[disk]\r\ntime1 = time.time()\r\n\r\ns4000_reloaded = load_from_disk(""tmp/squad_4000"")\r\n\r\ntime2 = time.time()\r\niocnt2 = psutil.disk_io_counters(perdisk=True)[disk]\r\n\r\nprint(f""Blocks read {iocnt2.read_count - iocnt1.read_count}"")  # Blocks read 18\r\nprint(f""Elapsed time: {time2 - time1:.02f}s"")  # Elapsed time: 14.60s\r\n```\r\n\r\nCould you run this on your side and tell me if how much time it takes ? Please run this when your machine is idle so that other processes don\'t interfere.\r\n\r\nI got these results on my macbook pro on datasets 1.6.2'
 '@lhoestq thanks, test running as we speak, bear with me'
 ""Just tried on google colab and got ~1min for a 15GB dataset (only 200 times SQuAD), while it should be instantaneous. The time is spent reading the Apache Arrow table from the memory mapped file. This might come a virtual disk management issue. I'm trying to see if I can still speed it up on colab.""
 ""@lhoestq what is Google Colab's HD read speed, is it possible to introspect incl. make like SSD or HDD ?""
 ""@lhoestq Thank you! The issue is getting more interesting. The second script is still running, but it's definitely taking much longer than 15 seconds.""
 'Okay, here’s the ouput:\r\nBlocks read 158396\r\nElapsed time: 529.10s\r\n\r\nAlso using datasets 1.6.2. Do you have any ideas, how to pinpoint the problem?'
 '@lhoestq, @tsproisl mmmh still writing on my side about 1h to go, thinking on it are your large datasets all monoblock unsharded ? mine is 335 times 1.18Gb shards.'
 'The 529.10s was a bit too optimistic. I cancelled the reading process once before running it completely, therefore the harddrive cache probably did its work.\r\n\r\nHere are three consecutive runs\r\nFirst run (freshly written to disk):\r\nBlocks read 309702\r\nElapsed time: 1267.74s\r\nSecond run (immediately after):\r\nBlocks read 113944\r\nElapsed time: 417.55s\r\nThird run (immediately after):\r\nBlocks read 42518\r\nElapsed time: 199.19s\r\n'
 '@lhoestq \r\nFirst test\r\n> elapsed time: 11219.05s\r\n\r\nSecond test running bear with me, for Windows users slight trick to modify original ""disk0"" string:\r\n\r\nFirst find physical unit relevant key in dictionnary\r\n```\r\nimport psutil\r\npsutil.disk_io_counters(perdisk=True)\r\n```\r\n\r\n> {\'PhysicalDrive0\': sdiskio(read_count=18453286, write_count=4075333, read_bytes=479546467840, write_bytes=161590275072, read_time=20659, write_time=2464),\r\n>  \'PhysicalDrive1\': sdiskio(read_count=1495778, write_count=388781, read_bytes=548628622336, write_bytes=318234849280, read_time=426066, write_time=19085)}\r\n\r\nIn my case it\'s _PhysicalDrive1_\r\n\r\nThen insert relevant key\'s string as _disk_ variable\r\n\r\n```\r\npsutil.disk_io_counters()\r\ndisk = \'PhysicalDrive1\'  # You may have to change your disk here\r\niocnt1 = psutil.disk_io_counters(perdisk=True)[disk]\r\ntime1 = time.time()\r\ns4000_reloaded = load_from_disk(""your path here"")\r\ntime2 = time.time()\r\niocnt2 = psutil.disk_io_counters(perdisk=True)[disk]\r\nprint(f""Blocks read {iocnt2.read_count - iocnt1.read_count}"")  # Blocks read 18\r\nprint(f""Elapsed time: {time2 - time1:.02f}s"")  # Elapsed time: 14.60s\r\n```'
 '@lhoestq\r\nSecond test\r\n\r\n> Blocks read 1265609\r\n> Elapsed time: 11216.55s'
 '@lhoestq any luck ?'
 ""Unfortunately no. Thanks for running the benchmark though, it shows that you machine does a lot of read operations. This is not expected: in other machines it does almost no read operations which enables a very fast loading.\r\n\r\nI did some tests on google colab and have the same issue. The first time the dataset arrow file is memory mapped takes always a lot of time (time seems linear with respect to the dataset size). Reloading the dataset is then instantaneous since the arrow file has already been memory mapped.\r\n\r\nI also tried using the Arrow IPC file format (see #1933) instead of the current streaming format that we use but it didn't help.\r\n\r\nMemory mapping is handled by the OS and depends on the disk you're using, so I'm not sure we can do much about it. I'll continue to investigate anyway, because I still don't know why in some cases it would go through the entire file (high `Blocks read ` as in your tests) and in other cases it would do almost no reading.""
 ""@lhoestq thanks for the effort, let's stay in touch""
 'Just want to say that I am seeing the same issue. Dataset size if 268GB and it takes **3 hours** to load `load_from_disk`, using dataset version `1.9.0`. Filesystem underneath is `Lustre` '
 'Hi @lhoestq, confirmed Windows issue, exact same code running on Linux OS total loading time about 3 minutes.'
 ""Hmm that's different from what I got. I was on Ubuntu when reporting the initial issue.""]","Hi,

I reported too slow data fetching when data is large(#2210) a couple of weeks ago, and @lhoestq referred me to the fix (#2122).
However, the problem seems to persist. Here is the profiled results:


1) Running with 60GB
```
Action                             	|  Mean duration (s)	|Num calls      	|  Total time (s) 	|  Percentage %   	|
------------------------------------------------------------------------------------------------------------------------------------
Total                              	|  -              	|_              	|  517.96         	|  100 %          	|
------------------------------------------------------------------------------------------------------------------------------------
model_backward                     	|  0.26144        	|100            	|  26.144         	|  5.0475         	|
model_forward                      	|  0.11123        	|100            	|  11.123         	|  2.1474         	|
get_train_batch                    	|  0.097121       	|100            	|  9.7121         	|  1.8751         	|
```


3) Running with 600GB, datasets==1.6.0
```
Action                             	|  Mean duration (s)	|Num calls      	|  Total time (s) 	|  Percentage %   	|
------------------------------------------------------------------------------------------------------------------------------------
Total                              	|  -              	|_              	|  4563.2         	|  100 %          	|
------------------------------------------------------------------------------------------------------------------------------------
get_train_batch                    	|  5.1279         	|100            	|  512.79         	|  11.237         	|
model_backward                     	|  4.8394         	|100            	|  483.94         	|  10.605         	|
model_forward                      	|  0.12162        	|100            	|  12.162         	|  0.26652        	|
```

I see that `get_train_batch` lags when data is large. Could this be related to different issues?
I would be happy to provide necessary information to investigate."
https://github.com/huggingface/datasets/issues/2250,some issue in loading local txt file as Dataset for run_mlm.py,"['Hi,\r\n\r\n1. try\r\n    ```python\r\n    dataset = load_dataset(""text"", data_files={""train"": [""a1.txt"", ""b1.txt""], ""test"": [""c1.txt""]})\r\n    ```\r\n    instead.\r\n\r\n    Sadly, I can\'t reproduce the error on my machine. If the above code doesn\'t resolve the issue, try to update the library to the \r\n    newest version (`pip install datasets --upgrade`).\r\n\r\n2. https://github.com/huggingface/transformers/blob/3ed5e97ba04ce9b24b4a7161ea74572598a4c480/examples/pytorch/language-modeling/run_mlm.py#L258-L259\r\nThis is the original code. You\'ll have to modify the example source to work with multiple train files. To make it easier, let\'s say ""|"" will act as a delimiter between files:\r\n    ```python\r\n        if data_args.train_file is not None:\r\n            data_files[""train""] = data_args.train_file.split(""|"")  # + .split(""|"")\r\n    ```\r\n    Then call the script as follows (**dataset_name must be None**):\r\n    ```bash\r\n    python run_mlm.py [... other args] --train_file a1.txt|b1.txt\r\n    ```'
 'i meet the same error with datasets 1.11.0, is there any insight about this?']","![image](https://user-images.githubusercontent.com/14968123/115773877-18cef300-a3c6-11eb-8e58-a9cbfd1001ec.png)

first of all, I tried to load 3 .txt files as a dataset (sure that the directory and permission is OK.), I face with the below error.

> FileNotFoundError: [Errno 2] No such file or directory: 'c'

by removing one of the training .txt files It's fixed and although if I put all file as training it's ok
![image](https://user-images.githubusercontent.com/14968123/115774207-867b1f00-a3c6-11eb-953b-905cfb112d25.png)
![image](https://user-images.githubusercontent.com/14968123/115774264-9b57b280-a3c6-11eb-9f36-7b109f0e5a31.png)


after this, my question is how could I use this defined Dataset for run_mlm.py for from scratch pretraining.
by using  --train_file path_to_train_file just can use one .txt , .csv or, .json file. I tried to set my defined Dataset as --dataset_name but the below issue occurs.


> Traceback (most recent call last):
  File ""/usr/local/lib/python3.7/dist-packages/datasets/load.py"", line 336, in prepare_module
    local_path = cached_path(file_path, download_config=download_config)
  File ""/usr/local/lib/python3.7/dist-packages/datasets/utils/file_utils.py"", line 291, in cached_path
    use_auth_token=download_config.use_auth_token,
  File ""/usr/local/lib/python3.7/dist-packages/datasets/utils/file_utils.py"", line 621, in get_from_cache
    raise FileNotFoundError(""Couldn't find file at {}"".format(url))
FileNotFoundError: Couldn't find file at https://raw.githubusercontent.com/huggingface/datasets/master/datasets/dataset/dataset.py

> During handling of the above exception, another exception occurred:

> Traceback (most recent call last):
  File ""run_mlm.py"", line 486, in <module>
    main()
  File ""run_mlm.py"", line 242, in main
    datasets = load_dataset(data_args.dataset_name, data_args.dataset_config_name, cache_dir=model_args.cache_dir)
  File ""/usr/local/lib/python3.7/dist-packages/datasets/load.py"", line 719, in load_dataset
    use_auth_token=use_auth_token,
  File ""/usr/local/lib/python3.7/dist-packages/datasets/load.py"", line 347, in prepare_module
    combined_path, github_file_path
FileNotFoundError: Couldn't find file locally at dataset/dataset.py, or remotely at https://raw.githubusercontent.com/huggingface/datasets/1.6.0/datasets/dataset/dataset.py.
The file is also not present on the master branch on github.
"
https://github.com/huggingface/datasets/issues/2243,Map is slow and processes batches one after another,"['Hi @villmow, thanks for reporting.\r\n\r\nCould you please try with the Datasets version 1.6? We released it yesterday and it fixes some issues about the processing speed. You can see the fix implemented by @lhoestq here: #2122.\r\n\r\nOnce you update Datasets, please confirm if the problem persists.'
 'Hi @albertvillanova, thanks for the reply. I just tried the new version and the problem still persists. \r\n\r\nDo I need to rebuild the saved dataset (which I load from disk) with the 1.6.0 version of datasets? My script loads this dataset and creates new datasets from it. I tried it without rebuilding.\r\n\r\nSee this short video of what happens. It does not create all processes at the same time:\r\n\r\nhttps://user-images.githubusercontent.com/2743060/115720139-0da3a500-a37d-11eb-833a-9bbacc70868d.mp4\r\n\r\n'
 ""There can be a bit of delay between the creations of the processes but this delay should be the same for both your `map` calls. We should look into this.\r\nAlso if you hav some code that reproduces this issue on google colab that'd be really useful !\r\n\r\nRegarding the speed differences:\r\nThis looks like a similar issue as https://github.com/huggingface/datasets/issues/1992 who is experiencing the same speed differences between processes.\r\nThis is a known bug that we are investigating. As of now I've never managed to reproduce it on my machine so it's pretty hard for me to find where this issue comes from.\r\n""
 'Upgrade to 1.6.1 solved my problem somehow. I did not change any of my code, but now it starts all processes around the same time.'
 ""Nice ! I'm glad this works now.\r\nClosing for now, but feel free to re-open if you experience this issue again.""]","## Describe the bug

I have a somewhat unclear bug to me, where I can't figure out what the problem is. The code works as expected on a small subset of my dataset (2000 samples) on my local machine, but when I execute the same code with a larger dataset (1.4 million samples) this problem occurs. Thats why I can't give exact steps to reproduce, I'm sorry. 

I process a large dataset in a two step process. I first call map on a dataset I load from disk and create a new dataset from it. This works like expected and `map` uses all workers I started it with. Then I process the dataset created by the first step, again with `map`, which is really slow and starting only one or two process at a time. Number of processes is the same for both steps.

pseudo code:
```python
ds = datasets.load_from_disk(""path"")
new_dataset = ds.map(work, batched=True, ...)  # fast uses all processes
final_dataset = new_dataset.map(work2, batched=True, ...)  # slow starts one process after another
```

## Expected results
Second stage should be as fast as the first stage.

## Versions
Paste the output of the following code:
- Datasets: 1.5.0
- Python: 3.8.8 (default, Feb 24 2021, 21:46:12)
- Platform: Linux-5.4.0-60-generic-x86_64-with-glibc2.10    

Do you guys have any idea? Thanks a lot!"
https://github.com/huggingface/datasets/issues/2242,"Link to datasets viwer on Quick Tour page returns ""502 Bad Gateway""",['This should be fixed now!\r\n\r\ncc @srush '],"Link to datasets viwer (https://huggingface.co/datasets/viewer/)  on Quick Tour page  (https://huggingface.co/docs/datasets/quicktour.html) returns ""502 Bad Gateway""

The same error with https://huggingface.co/datasets/viewer/?dataset=glue&config=mrpc "
https://github.com/huggingface/datasets/issues/2239,Error loading wikihow dataset,"[""Hi @odellus, thanks for reporting.\r\n\r\nThe `wikihow` dataset has 2 versions:\r\n- `all`: Consisting of the concatenation of all paragraphs as the articles and the bold lines as the reference summaries.\r\n- `sep`: Consisting of each paragraph and its summary.\r\n\r\nTherefore, in order to load it, you have to specify which version you would like, for example:\r\n```python\r\ndataset = load_dataset('wikihow', 'all')\r\n```\r\n\r\nPlease, tell me if this solves your problem.""
 ""Good call out. I did try that and that's when it told me to download the\ndataset. Don't believe I have tried it with local files. Will try first\nthing in the morning and get back to you.\n\nOn Mon, Apr 19, 2021, 11:17 PM Albert Villanova del Moral <\n***@***.***> wrote:\n\n> Hi @odellus <https://github.com/odellus>, thanks for reporting.\n>\n> The wikihow dataset has 2 versions:\n>\n>    - all: Consisting of the concatenation of all paragraphs as the\n>    articles and the bold lines as the reference summaries.\n>    - sep: Consisting of each paragraph and its summary.\n>\n> Therefore, in order to load it, you have to specify which version you\n> would like, for example:\n>\n> dataset = load_dataset('wikihow', 'all')\n>\n> Please, tell me if this solves your problem.\n>\n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/huggingface/datasets/issues/2239#issuecomment-823004146>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ABDYI3HVRTBI2QT3BOG262DTJUL57ANCNFSM43GV5BZQ>\n> .\n>\n""
 ""Hi @odellus, yes you are right.\r\n\r\nDue to the server where the `wikihow` dataset is hosted, the dataset can't be downloaded automatically by `huggingface` and you have to download it manually as you did.\r\n\r\nNevertheless, you have to specify which dataset version you would like to load anyway:\r\n```python\r\ndataset = load_dataset('wikihow', 'all', data_dir='./wikihow')\r\n```\r\nor\r\n```python\r\ndataset = load_dataset('wikihow', 'sep', data_dir='./wikihow')\r\n```\r\nI find that the instructions given by `huggingface` are not clear enough: I am going to fix this.\r\nPlease tell me if this eventually works for you.""
 'That was it. Thank you Albert!']","## Describe the bug

When attempting to load wikihow into a dataset with
```python
from datasets import load_dataset
dataset = load_dataset('wikihow', data_dir='./wikihow')
```
I get the message:
```
AttributeError: 'BuilderConfig' object has no attribute 'filename'
```
at the end of a [full stack trace](https://gist.github.com/odellus/602c3b2de52f541d353b1022f320ffc2).

## Steps to reproduce the bug

I have followed the instructions for creating a wikihow dataset. The [wikihow dataset site](https://huggingface.co/datasets/wikihow) says to use 
```python
from datasets import load_dataset
dataset = load_dataset('wikihow')
```
to load the dataset. I do so and I get the message
```
AssertionError: The dataset wikihow with config all requires manual data.
 Please follow the manual download instructions:   You need to manually download two wikihow files. An overview of which files to download can be seen at https://github.com/mahnazkoupaee/WikiHow-Dataset.
  You need to download the following two files manually:
    1) https://ucsb.app.box.com/s/ap23l8gafpezf4tq3wapr6u8241zz358 and save the file under <path/to/folder>/wikihowAll.csv
    2) https://ucsb.app.box.com/s/7yq601ijl1lzvlfu4rjdbbxforzd2oag and save the file under <path/to/folder>/wikihowSep.csv

  The <path/to/folder> can e.g. be ""~/manual_wikihow_data"".

  Wikihow can then be loaded using the following command `datasets.load_dataset(""wikihow"", data_dir=""<path/to/folder>"")`.
  .
 Manual data can be loaded with `datasets.load_dataset(wikihow, data_dir='<path/to/manual/data>')
```

So I create a directory `./wikihow` and download `wikihowAll.csv` and `wikihowSep.csv` into the new directory.

Then I run 
```python
from datasets import load_dataset
dataset = load_dataset('wikihow', data_dir='./wikihow')
```

that's when I get the [stack trace](https://gist.github.com/odellus/602c3b2de52f541d353b1022f320ffc2)

## Expected results
I expected it to load the downloaded files into a dataset.

## Actual results
```python
Using custom data configuration default-data_dir=.%2Fwikihow
Downloading and preparing dataset wikihow/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/azureuser/.cache/huggingface/datasets/wikihow/default-data_dir=.%2Fwikihow/0.0.0/58f42f8f0e4d459811a0f69aaab35870093830ccd58006769e7e1eb3e0e686c2...                                                    ---------------------------------------------------------------------------
AttributeError
Traceback (most recent call last)
<ipython-input-9-5e4d40142f30> in <module>
----> 1 dataset = load_dataset('wikihow',data_dir='./wikihow')
~/.local/lib/python3.6/site-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, script_version, use_auth_token, **config_kwargs)
745         try_from_hf_gcs=try_from_hf_gcs,
746         base_path=base_path,--> 
747         use_auth_token=use_auth_token,
748     )
749 
~/.local/lib/python3.6/site-packages/datasets/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)
577                     if not downloaded_from_gcs:
578                         self._download_and_prepare(                                                             -->
579                             dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs        
580                         )                                                                                          
581                     # Sync info
~/.local/lib/python3.6/site-packages/datasets/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)
632         split_dict = SplitDict(dataset_name=self.name)
633         split_generators_kwargs = self._make_split_generators_kwargs(prepare_split_kwargs)                      -->
634         split_generators = self._split_generators(dl_manager, **split_generators_kwargs)                            
635                                                                                                                     
636         # Checksums verification
~/.cache/huggingface/modules/datasets_modules/datasets/wikihow/58f42f8f0e4d459811a0f69aaab35870093830ccd58006769e7e1eb3e0e686c2/wikihow.py in _split_generators(self, dl_manager)
132
133         path_to_manual_file = os.path.join(
--> 134             os.path.abspath(os.path.expanduser(dl_manager.manual_dir)), self.config.filename                        
135         )                                                                                                           
136
AttributeError: 'BuilderConfig' object has no attribute 'filename'
```
## Versions
Paste the output of the following code:
```python
import datasets
import sys
import platform

print(f""""""
- Datasets: {datasets.__version__}
- Python: {sys.version}
- Platform: {platform.platform()}
"""""")
```
```
- Datasets: 1.5.0
- Python: 3.6.9 (default, Jan 26 2021, 15:33:00)                                      [GCC 8.4.0]
- Platform: Linux-5.4.0-1046-azure-x86_64-with-Ubuntu-18.04-bionic
```"
https://github.com/huggingface/datasets/issues/2237,Update Dataset.dataset_size after transformed with map,['@albertvillanova I would like to take this up. It would be great if you could point me as to how the dataset size is calculated in HF. Thanks!'],"After loading a dataset, if we transform it by using `.map` its `dataset_size` attirbute is not updated."
https://github.com/huggingface/datasets/issues/2230,Keys yielded while generating dataset are not being checked,"[""Hi ! Indeed there's no verification on the uniqueness nor the types of the keys.\r\nDo you already have some ideas of what you would like to implement and how ?""
 ""Hey @lhoestq, thank you so much for the opportunity.\r\nAlthough I haven't had much experience with the HF Datasets code, after a careful look at how the `ArrowWriter` functions, I think we can implement this as follows:\r\n\r\n1.  First, we would have to update the `ArrowWriter.write()` function here:\r\nhttps://github.com/huggingface/datasets/blob/fcd3c3c8e3b1d9a2f3686a496082e21f06591380/src/datasets/arrow_writer.py#L296\r\nso that it accepts an additional argument `key` which would be appended along with the example here after hashing.\r\n\r\n2.  Then, we would need to create a `Hasher` class which will take the key as its input and return a hash for it (We might need to use some hash salt which can be passed to the ArrowWriter.writer() with value equal to the `split_name` for differentiating between same keys of different splits)\r\n\r\n    We can use the `hashlib.md5` function for hashing which will conert each key to its byte code before hashing (depending on the data type of the key) **Thus, the `key` type will be verified here**.\r\n\r\n3.  Now, we would have to edit this\r\nhttps://github.com/huggingface/datasets/blob/fcd3c3c8e3b1d9a2f3686a496082e21f06591380/src/datasets/arrow_writer.py#L257\r\n so that it iterates over each `(hash, example)` pair (sorted according to hash). We can then simply **check whether each hash is different from the previous hash** (since they will be sorted)\r\n\r\nHowever, since I'm not very familiar with how the data is being written on disk in the form of a table, I might need some guidance for Step 3. \r\nPlease let me know your thought on this. Thanks!""
 ""Interesting !\r\nWe keep the dataset sorted in the order examples are generated by the builder (we expect the dataset builders to generate examples in deterministic order). Therefore I don't think we should shuffle the examples with the hashing. Let me know what you think.\r\nOther that that, I really like the idea of checking for keys duplicates in `write_examples_on_file` :)\r\n\r\nThis looks like a great plan ! Feel free to open a PR and ping me if you have questions or if I can help\r\n""
 ""@lhoestq I'm glad you liked the idea!\r\nI think that since the keys will be unique and deterministic in the nature themselves, so even if we shuffle the examples according to the hash, a deterministic order would still be maintained (as the keys will always have the same hash, whenever the dataset is generated). \r\nAnd since, we are not dealing with time series data (which would require the data to be in original order), I don't think the order of examples would matter much, as long as the order is deterministic and constant for all users.\r\n\r\nI think that this is also what was originally envisioned as mentioned in the documentation here:\r\nhttps://github.com/huggingface/datasets/blob/6775661b19d2ec339784f3d84553a3996a1d86c3/src/datasets/builder.py#L973\r\n\r\nAlso, if we avoid this, we would need to keep track of all the hashed keys in some place and compare each individual key with all others. This can cause some major overhead as each dataset consists of tens of thousands of examples.\r\nLet me know your thoughts in it! I would be opening a PR soon :)""
 ""When users load their own data, they expect the order to stay the same. I think that shuffling the data can make things inconvenient.\r\n\r\n> I think that this is also what was originally envisioned as mentioned in the documentation here:\r\n\r\nThis part was originally developed by tensorflow datasets, and tensorflow datasets indeed does the shuffling. However in this library this is probably not what we want in the general case. But if @albertvillanova and @thomwolf you have opinions on this please let us know.\r\n\r\n> Also, if we avoid this, we would need to keep track of all the hashed keys in some place and compare each individual key with all others. This can cause some major overhead as each dataset consists of tens of thousands of examples.\r\n\r\nMaybe we cam simply keep track of the hashes of of each batch being written ? The size of the batch when the data are save in arrow is 10 000 examples. This would only ensure that we don't have duplicates in each batch, but there might still be duplicates across batches. For 10 000 examples the hashes can just be stored as a python `set`.\r\n\r\nOtherwise if we want full deduplication, we need an extra tool that allows to temporarily save and query hashes that may need to use disk space rather than memory.""
 'Yes I think we want to keep the original order by default and only shuffle when the user ask for it (for instance by calling `dataset.shuffle()`). That’s how I had it in mind originally.'
 ""Hey @lhoestq, I just had a more in-depth look at the original TFDS code about why the keys and hash were used in the first place.\r\n\r\nIn my opinion, the only use that the `hash(key)` serves is that it allows us to shuffle the examples in a deterministic order (as each example will always yield the same key and thus, the same hash on every system) so that the same dataset is generated for each user, irrespective of the order the examples are yielded by the dataset builder on different user systems.\r\n\r\nOtherwise, if we are not shuffling, then while yielding and writing the data, after getting the key and hashing it for an example, I can't quite see the use of the hash or the key. The hash will simply be generated for each example but not actually used anywhere?\r\n\r\n@lhoestq @thomwolf It would be great if you could explain a bit more about the usage of keys. Thanks!\r\n""
 ""In `datasets` the keys are currently ignored.\r\nFor shuffling we don't use the keys. Instead we shuffle an array of indices. Since both the original order of the dataset and the indices shuffling are deterministic, then `dataset.shuffle` is deterministic as well.\r\nWe can use it to:\r\n1. detect duplicates\r\n2. verify that the generation order is indeed deterministic\r\n3. maybe more ?""
 ""Thanks a lot @lhoestq. I think I understand what we need to do now. The keys can indeed be used for detecting duplicates in generated examples as well as ensuring the order.\r\n\r\n> Maybe we cam simply keep track of the hashes of of each batch being written ? The size of the batch when the data are save in arrow is 10 000 examples. This would only ensure that we don't have duplicates in each batch,\r\n\r\nI think that checking for duplicates in every batch independently would be sufficient as the probability of collisions using something like `MD5` is very low. I would be opening a draft PR soon. It would be great to have your guidance. Thanks!""]","The keys used in the dataset generation script to ensure the same order is generated on every user's end should be checked for their types (i.e either `str` or `int`) as well as whether they are unique or not.
Currently, the keys are not being checked for any of these, as evident from `xnli' dataset generation:
https://github.com/huggingface/datasets/blob/56346791aed417306d054d89bd693d6b7eab17f7/datasets/xnli/xnli.py#L196
Even after having a tuple as key, the dataset is generated without any warning.

Also, as tested in the case of `anli` dataset (I tweeked the dataset script to use `1` as a key for every example):
```
>>> import datasets
>>> nik = datasets.load_dataset('anli')
Downloading and preparing dataset anli/plain_text (download: 17.76 MiB, generated: 73.55 MiB, post-processed: Unknown size, total: 91.31 MiB) to C:\Users\nikhil\.cache\huggingface\datasets\anli\plain_text\0.1.0\43fa2c99c10bf8478f1fa0860f7b122c6b277c4c41306255b7641257cf4e3299...
0 examples [00:00, ? examples/s]1        {'uid': '0fd0abfb-659e-4453-b196-c3a64d2d8267', 'premise': 'The Parma trolleybus system (Italian: ""Rete filoviaria di Parma"" ) forms part of the public transport network of the city and ""comune"" of Parma, in the region of Emilia-Romagna, northern Italy. In operation since 1953, the system presently comprises four urban routes.', 'hypothesis': 'The trolleybus system has over 2 urban routes', 'label': 'entailment', 'reason': ''}
2021-04-16 12:38:14.483968: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
1 examples [00:01,  1.87s/ examples]1    {'uid': '7ed72ff4-40b7-4f8a-b1b9-6c612aa62c84', 'premise': 'Alexandra Lendon Bastedo (9 March 1946 – 12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series ""The Champions"". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate.', 'hypothesis': ""Sharron Macready was a popular character through the 1980's."", 'label': 'neutral', 'reason': ''}
1        {'uid': '5d2930a3-62ac-485d-94d7-4e36cbbcd7b5', 'premise': 'Alexandra Lendon Bastedo (9 March 1946 – 12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series ""The Champions"". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate.', 'hypothesis': ""Bastedo didn't keep any pets because of her views on animal rights."", 'label': 'neutral', 'reason': ''}
1        {'uid': '324db753-ddc9-4a85-a825-f09e2e5aebdd', 'premise': 'Alexandra Lendon Bastedo (9 March 1946 – 12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series ""The Champions"". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate.', 'hypothesis': 'Alexandra Bastedo was named by her mother.', 'label': 'neutral', 'reason': ''}
1        {'uid': '4874f429-da0e-406a-90c7-22240ff3ddf8', 'premise': 'Alexandra Lendon Bastedo (9 March 1946 – 12 January 2014) was a British actress, best known for her role as secret agent Sharron Macready in the 1968 British espionage/science fiction adventure series ""The Champions"". She has been cited as a sex symbol of the 1960s and 1970s. Bastedo was a vegetarian and animal welfare advocate.', 'hypothesis': 'Bastedo cared for all the animals that inhabit the earth.', 'label': 'neutral', 'reason': ''}
```
Here also, the dataset was generated successfuly even hough it had same keys without any warning.

The reason appears to stem from here:
https://github.com/huggingface/datasets/blob/56346791aed417306d054d89bd693d6b7eab17f7/src/datasets/builder.py#L988
Here, although it has access to every key, but it is not being checked and the example is written directly:
https://github.com/huggingface/datasets/blob/56346791aed417306d054d89bd693d6b7eab17f7/src/datasets/builder.py#L992

I would like to take this issue if you allow me. Thank You!"
https://github.com/huggingface/datasets/issues/2229,`xnli` dataset creating a tuple key while yielding instead of `str` or `int`,"['Hi ! Sure sounds good. Also if you find other datasets that use tuples instead of str/int, you can also fix them !\r\nthanks :)'
 '@lhoestq I have sent a PR for fixing the issue. Would be great if you could have a look! Thanks!']","When using  `ds = datasets.load_dataset('xnli', 'ar')`, the dataset generation script uses the following section of code in the egging, which yields a tuple key instead of the specified `str` or `int` key:
https://github.com/huggingface/datasets/blob/56346791aed417306d054d89bd693d6b7eab17f7/datasets/xnli/xnli.py#L196

Since, community datasets in Tensorflow Datasets also use HF datasets, this causes a Tuple key error while loading HF's `xnli` dataset. 
I'm up for sending a fix for this, I think we can simply use `file_idx + ""_"" + row_idx` as a unique key instead of a tuple."
https://github.com/huggingface/datasets/issues/2226,Batched map fails when removing all columns,"['I found the problem. I called `set_format` on some columns before. This makes it crash. Here is a complete example to reproduce:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\nsst = load_dataset(""sst"")\r\nsst.set_format(""torch"", columns=[""label""], output_all_columns=True)\r\nds = sst[""train""]\r\n\r\n# crashes\r\nds.map(\r\n    lambda x: {""a"": list(range(20))},\r\n    remove_columns=ds.column_names,\r\n    load_from_cache_file=False,\r\n    num_proc=1,\r\n    batched=True,\r\n)\r\n```'
 'Thanks for reporting and for providing this code to reproduce the issue, this is really helpful !'
 ""I merged a fix, it should work on `master` now :)\r\nWe'll do a new release soon !""]","Hi @lhoestq ,

I'm hijacking this issue, because I'm currently trying to do the approach you recommend:

> Currently the optimal setup for single-column computations is probably to do something like
> 
> ```python
> result = dataset.map(f, input_columns=""my_col"", remove_columns=dataset.column_names)
> ```

Here is my code: (see edit, in which I added a simplified version

```
This is the error:
```bash
pyarrow.lib.ArrowInvalid: Column 1 named tokens expected length 8964 but got length 1000
```
I wonder why this error occurs, when I delete every column? Can you give me a hint?

### Edit:
I preprocessed my dataset before (using map with the features argument) and saved it to disk. May this be part of the error?  I can iterate over the
complete dataset and print every sample before calling map. There seems to be no other problem with the dataset.

I tried to simplify the code that crashes:

```python
# works
log.debug(dataset.column_names)
log.debug(dataset)
for i, sample in enumerate(dataset):
    log.debug(i, sample)

# crashes
counted_dataset = dataset.map(
    lambda x: {""a"": list(range(20))},
    input_columns=column,
    remove_columns=dataset.column_names,
    load_from_cache_file=False,
    num_proc=num_workers,
    batched=True,
)
```

```
pyarrow.lib.ArrowInvalid: Column 1 named tokens expected length 20 but got length 1000
```

Edit2: 

May this be a problem with a schema I set when preprocessing the dataset before? I tried to add the `features` argument to the function and then I get a new error:

```python
# crashes
counted_dataset = dataset.map(
    lambda x: {""a"": list(range(20))},
    input_columns=column,
    remove_columns=dataset.column_names,
    load_from_cache_file=False,
    num_proc=num_workers,
    batched=True,
    features=datasets.Features(
        {
              ""a"": datasets.Sequence(datasets.Value(""int32""))
         }
    )
)
```

```
 File ""env/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 1704, in _map_single
    writer.write_batch(batch)
  File ""env/lib/python3.8/site-packages/datasets/arrow_writer.py"", line 312, in write_batch
    col_type = schema.field(col).type if schema is not None else None
  File ""pyarrow/types.pxi"", line 1341, in pyarrow.lib.Schema.field
KeyError: 'Column tokens does not exist in schema'
```

_Originally posted by @villmow in https://github.com/huggingface/datasets/issues/2193#issuecomment-820230874_"
https://github.com/huggingface/datasets/issues/2218,Duplicates in the LAMA dataset,"[""Hi,\r\n\r\ncurrently the datasets API doesn't have a dedicated function to remove duplicate rows, but since the LAMA dataset is not too big (it fits in RAM), we can leverage pandas to help us remove duplicates:\r\n```python\r\n>>> from datasets import load_dataset, Dataset\r\n>>> dataset = load_dataset('lama', split='train')\r\n>>> dataset = Dataset.from_pandas(dataset.to_pandas().drop_duplicates(subset=...)) # specify a subset of the columns to consider in a list or use all of the columns if None\r\n```\r\n\r\nNote that the same can be achieved with the `Dataset.filter` method but this would requrie some extra work (filter function, speed?).""
 ""Oh, seems like my question wasn't specified well. I'm _not_ asking how to remove duplicates, but whether duplicates should be removed if I want to do the evaluation on the LAMA dataset as it was proposed in the original paper/repository? In other words, will I get the same result if evaluate on the de-duplicated dataset loaded from HF's `datasets` as the results I'd get if I use the original data format and data processing script in https://github.com/facebookresearch/LAMA? ""
 'So it looks like the person who added LAMA to the library chose to have one item per piece of evidence rather than one per relation - and in this case, there are duplicate pieces of evidence for the target relation\r\n\r\nIf I understand correctly, to reproduce reported results, you would have to aggregate predictions for the several pieces of evidence provided for each relation (each unique `uuid`), but the original authors will know better \r\n\r\ncc @fabiopetroni ']","I observed duplicates in the LAMA probing dataset, see a minimal code below. 

```
>>> import datasets
>>> dataset = datasets.load_dataset('lama')
No config specified, defaulting to: lama/trex
Reusing dataset lama (/home/anam/.cache/huggingface/datasets/lama/trex/1.1.0/97deffae13eca0a18e77dfb3960bb31741e973586f5c1fe1ec0d6b5eece7bddc)
>>> train_dataset = dataset['train']
>>> train_dataset[0]
{'description': 'language or languages a person has learned from early childhood', 'label': 'native language', 'masked_sentence': 'Louis Jules Trochu ([lwi ʒyl tʁɔʃy]; 12 March 1815 – 7 October 1896) was a [MASK] military leader and politician.', 'obj_label': 'French', 'obj_surface': 'French', 'obj_uri': 'Q150', 'predicate_id': 'P103', 'sub_label': 'Louis Jules Trochu', 'sub_surface': 'Louis Jules Trochu', 'sub_uri': 'Q441235', 'template': 'The native language of [X] is [Y] .', 'template_negated': '[X] is not owned by [Y] .', 'type': 'N-1', 'uuid': '40b2ed1c-0961-482e-844e-32596b6117c8'}
>>> train_dataset[1]
{'description': 'language or languages a person has learned from early childhood', 'label': 'native language', 'masked_sentence': 'Louis Jules Trochu ([lwi ʒyl tʁɔʃy]; 12 March 1815 – 7 October 1896) was a [MASK] military leader and politician.', 'obj_label': 'French', 'obj_surface': 'French', 'obj_uri': 'Q150', 'predicate_id': 'P103', 'sub_label': 'Louis Jules Trochu', 'sub_surface': 'Louis Jules Trochu', 'sub_uri': 'Q441235', 'template': 'The native language of [X] is [Y] .', 'template_negated': '[X] is not owned by [Y] .', 'type': 'N-1', 'uuid': '40b2ed1c-0961-482e-844e-32596b6117c8'}
```

I checked the original data available at https://dl.fbaipublicfiles.com/LAMA/data.zip. This particular duplicated comes from:
```
{""uuid"": ""40b2ed1c-0961-482e-844e-32596b6117c8"", ""obj_uri"": ""Q150"", ""obj_label"": ""French"", ""sub_uri"": ""Q441235"", ""sub_label"": ""Louis Jules Trochu"", ""predicate_id"": ""P103"", ""evidences"": [{""sub_surface"": ""Louis Jules Trochu"", ""obj_surface"": ""French"", ""masked_sentence"": ""Louis Jules Trochu ([lwi \u0292yl t\u0281\u0254\u0283y]; 12 March 1815 \u2013 7 October 1896) was a [MASK] military leader and politician.""}, {""sub_surface"": ""Louis Jules Trochu"", ""obj_surface"": ""French"", ""masked_sentence"": ""Louis Jules Trochu ([lwi \u0292yl t\u0281\u0254\u0283y]; 12 March 1815 \u2013 7 October 1896) was a [MASK] military leader and politician.""}]}
``` 

What is the best way to deal with these duplicates if I want to use `datasets` to probe with LAMA?  "
https://github.com/huggingface/datasets/issues/2214,load_metric error: module 'datasets.utils.file_utils' has no attribute 'add_start_docstrings',"['Hi @nsaphra, thanks for reporting.\r\n\r\nThis issue was fixed in `datasets` version 1.3.0. Could you please update `datasets` and tell me if the problem persists?\r\n```shell\r\npip install -U datasets\r\n```'
 'There might be a bug in the conda version of `datasets` 1.2.1 where the datasets/metric scripts are downloaded from `master` instead of the `1.2.1` repo.\r\n\r\nYou can try setting the env var `HF_SCRIPTS_VERSION=""1.2.1""` as a workaround. Let me know if that helps.'
 ""I just faced the same issue. I was using 1.2.1 from conda and received the same AttributeError complaining about 'add_start_docstrings'. Uninstalling the conda installed datasets and then installing the latest datasets (version 1.5.0) using pip install solved the issue for me. I don't like mixing up conda and pip installs in the same environments but this will have to do for now, until 1.5.0 is made available through conda.""
 'Yep, seems to have fixed things! The conda package could really do with an update. Thanks!']","I'm having the same problem as [Notebooks issue 10](https://github.com/huggingface/notebooks/issues/10) on datasets 1.2.1, and it seems to be an issue with the datasets package.

```python
>>> from datasets import load_metric
>>> metric = load_metric(""glue"", ""sst2"")
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/ext3/miniconda3/lib/python3.8/site-packages/datasets-1.2.1-py3.8.egg/datasets/load.py"", line 502, in load_metric
  File ""/ext3/miniconda3/lib/python3.8/site-packages/datasets-1.2.1-py3.8.egg/datasets/load.py"", line 66, in import_main_class
  File ""/ext3/miniconda3/lib/python3.8/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1014, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 975, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 671, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 783, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""/home/ns4008/.cache/huggingface/modules/datasets_modules/metrics/glue/e4606ab9804a36bcd5a9cebb2cb65bb14b6ac78ee9e6d5981fa679a495dd55de/glue.py"", line 105, in <module>
    @datasets.utils.file_utils.add_start_docstrings(_DESCRIPTION, _KWARGS_DESCRIPTION)
AttributeError: module 'datasets.utils.file_utils' has no attribute 'add_start_docstrings'
```"
https://github.com/huggingface/datasets/issues/2212,"Can't reach ""https://storage.googleapis.com/illuin/fquad/train.json.zip"" when trying to load fquad dataset","[""Hi ! Apparently the data are not available from this url anymore. We'll replace it with the new url when it's available""
 'I saw this on their website when we request to download the dataset:\r\n![image](https://user-images.githubusercontent.com/19718818/114879600-fa458680-9e1e-11eb-9e05-f0963d68ff0f.png)\r\n\r\nCan we still request them link for the dataset and make a PR? @lhoestq @yjernite '
 ""I've contacted Martin (first author of the fquad paper) regarding a possible new url. Hopefully we can get one soon !""
 'They now made a website to force people who want to use the dataset for commercial purposes to seek a commercial license from them ...']","I'm trying to load the [fquad dataset](https://huggingface.co/datasets/fquad) by running: 

```Python
fquad = load_dataset(""fquad"")
```

which produces the following error:

```
Using custom data configuration default

Downloading and preparing dataset fquad/default (download: 3.14 MiB, generated: 6.62 MiB, post-processed: Unknown size, total: 9.76 MiB) to /root/.cache/huggingface/datasets/fquad/default/0.1.0/778dc2c85813d05ddd0c17087294d5f8f24820752340958070876b677af9f061...

---------------------------------------------------------------------------

ConnectionError                           Traceback (most recent call last)

<ipython-input-48-a2721797e23b> in <module>()
----> 1 fquad = load_dataset(""fquad"")

11 frames

/usr/local/lib/python3.7/dist-packages/datasets/utils/file_utils.py in get_from_cache(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, use_auth_token)
    614             raise FileNotFoundError(""Couldn't find file at {}"".format(url))
    615         _raise_if_offline_mode_is_enabled(f""Tried to reach {url}"")
--> 616         raise ConnectionError(""Couldn't reach {}"".format(url))
    617 
    618     # Try a second time

ConnectionError: Couldn't reach https://storage.googleapis.com/illuin/fquad/train.json.zip
```

Does anyone know why that is and how to fix it? "
https://github.com/huggingface/datasets/issues/2211,Getting checksum error when trying to load lc_quad dataset,"[""Hi,\r\n\r\nI've already opened a PR with the fix. If you are in a hurry, just build the project from source and run:\r\n```bash\r\ndatasets-cli test datasets/lc_quad --save_infos --all_configs --ignore_verifications\r\n```\r\n\r\n""
 ""Ah sorry, I tried searching but couldn't find any related PR. \r\n\r\nThank you! ""]","I'm having issues loading the [lc_quad](https://huggingface.co/datasets/fquad) dataset by running:

```Python
lc_quad = load_dataset(""lc_quad"")
```

which is giving me the following error:

``` 
Using custom data configuration default

Downloading and preparing dataset lc_quad/default (download: 3.69 MiB, generated: 19.77 MiB, post-processed: Unknown size, total: 23.46 MiB) to /root/.cache/huggingface/datasets/lc_quad/default/2.0.0/5a98fe174603f5dec6df07edf1c2b4d2317210d2ad61f5a393839bca4d64e5a7...

---------------------------------------------------------------------------

NonMatchingChecksumError                  Traceback (most recent call last)

<ipython-input-42-404ace83f73c> in <module>()
----> 1 lc_quad = load_dataset(""lc_quad"")

3 frames

/usr/local/lib/python3.7/dist-packages/datasets/utils/info_utils.py in verify_checksums(expected_checksums, recorded_checksums, verification_name)
     37     if len(bad_urls) > 0:
     38         error_msg = ""Checksums didn't match"" + for_verification_name + "":\n""
---> 39         raise NonMatchingChecksumError(error_msg + str(bad_urls))
     40     logger.info(""All the checksums matched successfully"" + for_verification_name)
     41 

NonMatchingChecksumError: Checksums didn't match for dataset source files:
['https://github.com/AskNowQA/LC-QuAD2.0/archive/master.zip']
```

Does anyone know why this could be and how I fix it? "
https://github.com/huggingface/datasets/issues/2210,dataloading slow when using HUGE dataset,"[""Hi ! Yes this is an issue with `datasets<=1.5.0`\r\nThis issue has been fixed by #2122 , we'll do a new release soon :)\r\nFor now you can test it on the `master` branch.""
 'Hi, thank you for your answer. I did not realize that my issue stems from the same problem. ']","Hi,

When I use datasets with 600GB data, the dataloading speed increases significantly. 
I am experimenting with two datasets, and one is about 60GB and the other 600GB.
Simply speaking, my code uses `datasets.set_format(""torch"")` function and let pytorch-lightning handle ddp training.
When looking at the pytorch-lightning supported profile of two different runs, I see that fetching a batch(`get_train_batch`) consumes an unreasonable amount of time when data is large. What could be the cause?

* 60GB data
```
Action                             	|  Mean duration (s)	|Num calls      	|  Total time (s) 	|  Percentage %   	|
------------------------------------------------------------------------------------------------------------------------------------
Total                              	|  -              	|_              	|  200.33         	|  100 %          	|
------------------------------------------------------------------------------------------------------------------------------------
run_training_epoch                 	|  71.994         	|1              	|  71.994         	|  35.937         	|
run_training_batch                 	|  0.64373        	|100            	|  64.373         	|  32.133         	|
optimizer_step_and_closure_0       	|  0.64322        	|100            	|  64.322         	|  32.108         	|
training_step_and_backward         	|  0.61004        	|100            	|  61.004         	|  30.452         	|
model_backward                     	|  0.37552        	|100            	|  37.552         	|  18.745         	|
model_forward                      	|  0.22813        	|100            	|  22.813         	|  11.387         	|
training_step                      	|  0.22759        	|100            	|  22.759         	|  11.361         	|
get_train_batch                    	|  0.066385       	|100            	|  6.6385         	|  3.3138         	|
```

* 600GB data
```
Action                             	|  Mean duration (s)	|Num calls      	|  Total time (s) 	|  Percentage %   	|
------------------------------------------------------------------------------------------------------------------------------------
Total                              	|  -              	|_              	|  3285.6         	|  100 %          	|
------------------------------------------------------------------------------------------------------------------------------------
run_training_epoch                 	|  1397.9         	|1              	|  1397.9         	|  42.546         	|
run_training_batch                 	|  7.2596         	|100            	|  725.96         	|  22.095         	|
optimizer_step_and_closure_0       	|  7.2589         	|100            	|  725.89         	|  22.093         	|
training_step_and_backward         	|  7.223          	|100            	|  722.3          	|  21.984         	|
model_backward                     	|  6.9662         	|100            	|  696.62         	|  21.202         	|
get_train_batch                    	|  6.322          	|100            	|  632.2          	|  19.241         	|
model_forward                      	|  0.24902        	|100            	|  24.902         	|  0.75789        	|
training_step                      	|  0.2485         	|100            	|  24.85          	|  0.75633        	|
```
"
https://github.com/huggingface/datasets/issues/2207,making labels consistent across the datasets,"[""Hi ! The ClassLabel feature type encodes the labels as integers.\r\nThe integer corresponds to the index of the label name in the `names` list of the ClassLabel.\r\nHere that means that the labels are 'entailment' (0), 'neutral' (1), 'contradiction' (2).\r\n\r\nYou can get the label names back by using `a.features['label'].int2str(i)`.\r\n""]","Hi
For accessing the labels one can type 
```
>>> a.features['label']
ClassLabel(num_classes=3, names=['entailment', 'neutral', 'contradiction'], names_file=None, id=None)
```
The labels however are not consistent with the actual labels sometimes, for instance in case of XNLI, the actual labels are 0,1,2, but if one try to access as above they are entailment, neutral,contradiction,
it would be great to have the labels consistent.

thanks 
"
https://github.com/huggingface/datasets/issues/2206,Got pyarrow error when loading a dataset while adding special tokens into the tokenizer,"['Hi,\r\n\r\nthe output of the tokenizers is treated specially in the lib to optimize the dataset size (see the code [here](https://github.com/huggingface/datasets/blob/master/src/datasets/arrow_writer.py#L138-L141)). It looks like that one of the values in a dictionary returned by the tokenizer is out of the assumed range.\r\nCan you please provide a minimal reproducible example for more help?'
 'Hi @yana-xuyan, thanks for reporting.\r\n\r\nAs clearly @mariosasko explained, `datasets` performs some optimizations in order to reduce the size of the dataset cache files. And one of them is storing the field `special_tokens_mask` as `int8`, which means that this field can only contain integers between `-128` to `127`. As your message error states, one of the values of this field is `50259`, and therefore it cannot be stored as an `int8`.\r\n\r\nMaybe we could implement a way to disable this optimization and allow using any integer value; although the size of the cache files would be much larger.']","I added five more special tokens into the GPT2 tokenizer. But after that, when I try to pre-process the data using my previous code, I got an error shown below:

Traceback (most recent call last):
  File ""/home/xuyan/anaconda3/envs/convqa/lib/python3.7/site-packages/datasets/arrow_dataset.py"", line 1687, in _map_single
    writer.write(example)
  File ""/home/xuyan/anaconda3/envs/convqa/lib/python3.7/site-packages/datasets/arrow_writer.py"", line 296, in write
    self.write_on_file()
  File ""/home/xuyan/anaconda3/envs/convqa/lib/python3.7/site-packages/datasets/arrow_writer.py"", line 270, in write_on_file
    pa_array = pa.array(typed_sequence)
  File ""pyarrow/array.pxi"", line 222, in pyarrow.lib.array
  File ""pyarrow/array.pxi"", line 110, in pyarrow.lib._handle_arrow_array_protocol
  File ""/home/xuyan/anaconda3/envs/convqa/lib/python3.7/site-packages/datasets/arrow_writer.py"", line 108, in __arrow_array__
    out = out.cast(pa.list_(self.optimized_int_type))
  File ""pyarrow/array.pxi"", line 810, in pyarrow.lib.Array.cast
  File ""/home/xuyan/anaconda3/envs/convqa/lib/python3.7/site-packages/pyarrow/compute.py"", line 281, in cast
    return call_function(""cast"", [arr], options)
  File ""pyarrow/_compute.pyx"", line 465, in pyarrow._compute.call_function
  File ""pyarrow/_compute.pyx"", line 294, in pyarrow._compute.Function.call
  File ""pyarrow/error.pxi"", line 122, in pyarrow.lib.pyarrow_internal_check_status
  File ""pyarrow/error.pxi"", line 84, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: Integer value 50259 not in range: -128 to 127

Do you have any idea about it?"
https://github.com/huggingface/datasets/issues/2200,_prepare_split will overwrite DatasetBuilder.info.features,"[""Hi ! This might be related to #2153 \r\n\r\nYou're right the ArrowWriter should be initialized with `features=self.info.features` ! Good catch\r\nI'm opening a PR to fix this and also to figure out how it was not caught in the tests\r\n\r\nEDIT: opened #2201""
 ""> Hi ! This might be related to #2153\r\n> \r\n> You're right the ArrowWriter should be initialized with `features=self.info.features` ! Good catch\r\n> I'm opening a PR to fix this and also to figure out how it was not caught in the tests\r\n> \r\n> EDIT: opened #2201\r\n\r\nGlad to hear that! Thank you for your fix, I'm new to huggingface, it's a fantastic project 😁""]","Hi, here is my issue:
I initialized a Csv datasetbuilder with specific features:
```
def get_dataset_features(data_args):
    features = {}
    if data_args.text_features:
        features.update({text_feature: hf_features.Value(""string"") for text_feature in data_args.text_features.strip().split("","")})
    if data_args.num_features:
        features.update({text_feature: hf_features.Value(""float32"") for text_feature in data_args.num_features.strip().split("","")})
    if data_args.label_classes:
        features[""label""] = hf_features.ClassLabel(names=data_args.label_classes.strip().split("",""))
    else:
        features[""label""] = hf_features.Value(""float32"")
    return hf_features.Features(features)

datasets = load_dataset(extension,
                                data_files=data_files,
                                sep=data_args.delimiter,
                                header=data_args.header,
                                column_names=data_args.column_names.split("","") if data_args.column_names else None,
                                features=get_dataset_features(data_args=data_args))
```
The `features` is printout as below before `builder_instance.as_dataset` is called:
```
{'label': ClassLabel(num_classes=2, names=['unacceptable', 'acceptable'], names_file=None, id=None), 'notated': Value(dtype='string', id=None), 'sentence': Value(dtype='string', id=None), 'src_code': Value(dtype='string', id=None)}
````

But after the `builder_instance.as_dataset` is called for Csv dataset builder, the `features` is changed to:
```
{'label': Value(dtype='int64', id=None), 'notated': Value(dtype='string', id=None), 'sentence': Value(dtype='string', id=None), 'src_code': Value(dtype='string', id=None)}
```

After digged into the code, I releazed that in `ArrowBasedBuilder._prepare_split`, the DatasetBuilder's info's features will be overwrited by `ArrowWriter`'s `_features`. 
But `ArrowWriter` is initailized without passing `features`.
So my concern is:
It's this overwrite must be done, or, should it be an option to pass features in `_prepare_split` function?"
https://github.com/huggingface/datasets/issues/2196,`load_dataset` caches two arrow files?,"['Hi ! Files that starts with `cache-*` are cached computation files, i.e. they are the cached results of map/filter/cast/etc. operations. For example if you used `map` on your dataset to transform it, then the resulting dataset is going to be stored and cached in a `cache-*` file. These files are used to avoid having to load the dataset in RAM, even after many transforms'
 ""Thanks @lhoestq! Hmm.. that's strange because I specifically turned off auto caching, and saved mapped result, using `save_to_disk`, to another location. At this location, the following file is created:`355G\tcache-ed205e500a7dc44c.arrow`\r\n\r\nTo my observation, both `load_dataset` and `map` creates `cache-*` files, and I wonder what the `cache-*` file from `load_dataset` is for (as I believe the same information is stored in `json-train.arrow`.""
 'This is a wrong report -- `cache-*` files are created only my `map`, not by `load_dataset`. ']","Hi,

I am using datasets to load large json file of 587G.
I checked the cached folder and found that there are two arrow files created:
* `cache-ed205e500a7dc44c.arrow` - 355G
*  `json-train.arrow` - 582G

Why is the first file created?
If I delete it, would I still be able to `load_from_disk`?"
https://github.com/huggingface/datasets/issues/2195,KeyError: '_indices_files' in `arrow_dataset.py`,"['Thanks for reporting @samsontmr.\r\n\r\nIt seems a backward compatibility issue...'
 ""Thanks @samsontmr this should be fixed on master now\r\n\r\nFeel free to reopen if you're still having issues""]","After pulling the latest master, I'm getting a crash when `load_from_disk` tries to load my local dataset.

Trace:
```
Traceback (most recent call last):
  File ""load_data.py"", line 11, in <module>
    dataset = load_from_disk(SRC)
  File ""/opt/conda/envs/py38/lib/python3.8/site-packages/datasets/load.py"", line 784, in load_from_disk
    return DatasetDict.load_from_disk(dataset_path, fs, keep_in_memory=keep_in_memory)
  File ""/opt/conda/envs/py38/lib/python3.8/site-packages/datasets/dataset_dict.py"", line 692, in load_from_disk
    dataset_dict[k] = Dataset.load_from_disk(dataset_dict_split_path, fs, keep_in_memory=keep_in_memory)
  File ""/opt/conda/envs/py38/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 634, in load_from_disk
    if state[""_indices_files""]:
KeyError: '_indices_files'
```

I believe this is the line causing the error since there may not be a `_indices_files` key in the older versions:
https://github.com/huggingface/datasets/blob/b70141e3c5149430951773aaa0155555c5fb3e76/src/datasets/arrow_dataset.py#L634

May I suggest using `state.get()` instead of directly indexing the dictionary?

@lhoestq "
https://github.com/huggingface/datasets/issues/2194,py3.7: TypeError: can't pickle _LazyModule objects,"[""\r\nThis wasn't a `datasets` problem, but `transformers`' and it was solved here https://github.com/huggingface/transformers/pull/11168\r\n""]","While this works fine with py3.8, under py3.7, with a totally new conda env and transformers install:

```
git clone https://github.com/huggingface/transformers
cd transformers
pip install -e .[testing]

export BS=1; rm -rf /tmp/test-clm; PYTHONPATH=src USE_TF=0 CUDA_VISIBLE_DEVICES=0 python \
examples/language-modeling/run_clm.py --model_name_or_path distilgpt2 --dataset_name wikitext \
--dataset_config_name wikitext-2-raw-v1 --do_train --max_train_samples 1 \
--per_device_train_batch_size $BS --output_dir /tmp/test-clm --block_size 128 --logging_steps 1  \
--fp16
```

```
Traceback (most recent call last):
  File ""examples/language-modeling/run_clm.py"", line 453, in <module>
    main()
  File ""examples/language-modeling/run_clm.py"", line 336, in main
    load_from_cache_file=not data_args.overwrite_cache,
  File ""/home/stas/anaconda3/lib/python3.7/site-packages/datasets/dataset_dict.py"", line 303, in map
    for k, dataset in self.items()
  File ""/home/stas/anaconda3/lib/python3.7/site-packages/datasets/dataset_dict.py"", line 303, in <dictcomp>
    for k, dataset in self.items()
  File ""/home/stas/anaconda3/lib/python3.7/site-packages/datasets/arrow_dataset.py"", line 1259, in map
    update_data=update_data,
  File ""/home/stas/anaconda3/lib/python3.7/site-packages/datasets/arrow_dataset.py"", line 157, in wrapper
    out: Union[""Dataset"", ""DatasetDict""] = func(self, *args, **kwargs)
  File ""/home/stas/anaconda3/lib/python3.7/site-packages/datasets/fingerprint.py"", line 158, in wrapper
    self._fingerprint, transform, kwargs_for_fingerprint
  File ""/home/stas/anaconda3/lib/python3.7/site-packages/datasets/fingerprint.py"", line 105, in update_fingerprint
    hasher.update(transform_args[key])
  File ""/home/stas/anaconda3/lib/python3.7/site-packages/datasets/fingerprint.py"", line 57, in update
    self.m.update(self.hash(value).encode(""utf-8""))
  File ""/home/stas/anaconda3/lib/python3.7/site-packages/datasets/fingerprint.py"", line 53, in hash
    return cls.hash_default(value)
  File ""/home/stas/anaconda3/lib/python3.7/site-packages/datasets/fingerprint.py"", line 46, in hash_default
    return cls.hash_bytes(dumps(value))
  File ""/home/stas/anaconda3/lib/python3.7/site-packages/datasets/utils/py_utils.py"", line 389, in dumps
    dump(obj, file)
  File ""/home/stas/anaconda3/lib/python3.7/site-packages/datasets/utils/py_utils.py"", line 361, in dump
    Pickler(file, recurse=True).dump(obj)
  File ""/home/stas/anaconda3/lib/python3.7/site-packages/dill/_dill.py"", line 454, in dump
    StockPickler.dump(self, obj)
  File ""/home/stas/anaconda3/lib/python3.7/pickle.py"", line 437, in dump
    self.save(obj)
  File ""/home/stas/anaconda3/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/home/stas/anaconda3/lib/python3.7/site-packages/datasets/utils/py_utils.py"", line 556, in save_function
    obj=obj,
  File ""/home/stas/anaconda3/lib/python3.7/pickle.py"", line 638, in save_reduce
    save(args)
  File ""/home/stas/anaconda3/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/home/stas/anaconda3/lib/python3.7/pickle.py"", line 789, in save_tuple
    save(element)
  File ""/home/stas/anaconda3/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/home/stas/anaconda3/lib/python3.7/site-packages/dill/_dill.py"", line 941, in save_module_dict
    StockPickler.save_dict(pickler, obj)
  File ""/home/stas/anaconda3/lib/python3.7/pickle.py"", line 859, in save_dict
    self._batch_setitems(obj.items())
  File ""/home/stas/anaconda3/lib/python3.7/pickle.py"", line 885, in _batch_setitems
    save(v)
  File ""/home/stas/anaconda3/lib/python3.7/pickle.py"", line 524, in save
    rv = reduce(self.proto)
TypeError: can't pickle _LazyModule objects
```
```
$ python --version
Python 3.7.4

$ python -m torch.utils.collect_env
Collecting environment information...
PyTorch version: 1.8.0.dev20210110+cu110
Is debug build: False
CUDA used to build PyTorch: 11.0
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.2 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: 10.0.0-4ubuntu1 
CMake version: version 3.16.3
```

Thanks."
https://github.com/huggingface/datasets/issues/2193,Filtering/mapping on one column is very slow,"[""Hi ! Yes we are working on making `filter` significantly faster. You can look at related PRs here: #2060 #2178 \r\n\r\nI think you can expect to have the fast version of `filter` available next week.\r\n\r\nWe'll make it only select one column, and we'll also make the overall filtering operation way faster by avoiding many arrow<->python conversions especially during writing.\r\n\r\nI'll let you know how it goes !""
 ""@lhoestq Thanks for the response— it's great to hear that we'll be getting a much faster `filter` method soon. However, my use case does also involve using `map` over a single column in order to pre-compute roughly uniformly sized batches, and right now that is also very slow. Is there any plan to make `map` faster for single column operations?\r\n\r\nIf that's not a priority for the maintainers right now, I could try my hand at adding the feature, but I can't guarantee I would do a good job given my lack of familiarity with pyarrow.""
 'Currently the optimal setup for single-column computations is probably to do something like\r\n```python\r\nresult = dataset.map(f, input_columns=""my_col"", remove_columns=dataset.column_names)\r\n```\r\nThis has two advantages:\r\n- input_columns=""my_col"" allows to only read the column ""my_col""\r\n- remove_columns=dataset.column_names makes `map` only keep the output of your function `f`, and it drops the other columns of the dataset instead of keeping them.\r\n\r\nLet me know if it improves speed on your side.\r\n\r\nYou can also get more speed by using `batched=True` and setting `num_proc=` for multiprocessing'
 'Hi @lhoestq ,\r\n\r\nI\'m hijacking this issue, because I\'m currently trying to do the approach you recommend:\r\n\r\n> Currently the optimal setup for single-column computations is probably to do something like\r\n> \r\n> ```python\r\n> result = dataset.map(f, input_columns=""my_col"", remove_columns=dataset.column_names)\r\n> ```\r\n\r\nHere is my code: (see edit, in which I added a simplified version\r\n\r\n```\r\nThis is the error:\r\n```bash\r\npyarrow.lib.ArrowInvalid: Column 1 named tokens expected length 8964 but got length 1000\r\n```\r\nI wonder why this error occurs, when I delete every column? Can you give me a hint?\r\n\r\n### Edit:\r\nI preprocessed my dataset before (using map with the features argument) and saved it to disk. May this be part of the error?  I can iterate over the\r\ncomplete dataset and print every sample before calling map. There seems to be no other problem with the dataset.\r\n\r\nI tried to simplify the code that crashes:\r\n\r\n```python\r\n# works\r\nlog.debug(dataset.column_names)\r\nlog.debug(dataset)\r\nfor i, sample in enumerate(dataset):\r\n    log.debug(i, sample)\r\n\r\n# crashes\r\ncounted_dataset = dataset.map(\r\n    lambda x: {""a"": list(range(20))},\r\n    input_columns=column,\r\n    remove_columns=dataset.column_names,\r\n    load_from_cache_file=False,\r\n    num_proc=num_workers,\r\n    batched=True,\r\n)\r\n```\r\n\r\n```\r\npyarrow.lib.ArrowInvalid: Column 1 named tokens expected length 20 but got length 1000\r\n```\r\n\r\nEdit2: \r\n\r\nMay this be a problem with a schema I set when preprocessing the dataset before? I tried to add the `features` argument to the function and then I get a new error:\r\n\r\n```python\r\n# crashes\r\ncounted_dataset = dataset.map(\r\n    lambda x: {""a"": list(range(20))},\r\n    input_columns=column,\r\n    remove_columns=dataset.column_names,\r\n    load_from_cache_file=False,\r\n    num_proc=num_workers,\r\n    batched=True,\r\n    features=datasets.Features(\r\n        {\r\n              ""a"": datasets.Sequence(datasets.Value(""int32""))\r\n         }\r\n    )\r\n)\r\n```\r\n\r\n```\r\n File ""env/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 1704, in _map_single\r\n    writer.write_batch(batch)\r\n  File ""env/lib/python3.8/site-packages/datasets/arrow_writer.py"", line 312, in write_batch\r\n    col_type = schema.field(col).type if schema is not None else None\r\n  File ""pyarrow/types.pxi"", line 1341, in pyarrow.lib.Schema.field\r\nKeyError: \'Column tokens does not exist in schema\'\r\n```'
 'Hi ! Can you open a separate issue for that ?\r\nAlso if you could provide a google colab or a sample code to reproduce this issue that would be helpful.\r\nOn my side I was not able to reproduce this error.'
 ""@lhoestq Sorry I'm just responding now. I'm currently using your recommendation for the map on a single column, and I've gotten it to be fast enough to sort of work for my use case by just setting `num_proc=10`, although it's still quite slow. It's clear that it is still loading the entirety of each row into memory and then discarding everything except the selected column, instead of exploiting the columnar data format to only load the selected column.\r\n\r\nMy code is like this:\r\n```\r\n self.dataset = self.dataset.sort('num_tokens')\r\n batch_dataset = self.dataset.map(\r\n\tcompute_uniform_sized_batches,\r\n\tbatched=True, batch_size=10_000, num_proc=10, input_columns=['num_tokens'],\r\n\tremove_columns=get_columns_all_equal(self.dataset),\r\n\twith_indices=True,\r\n\tfn_kwargs=dict(max_size=tokens_per_batch)\r\n)\r\nself.batches = {\r\n\tname: list(zip(split['start'], split['length']))\r\n\tfor name, split in batch_dataset.items()\r\n}\r\n```\r\nI find that the processes with higher IDs take significantly longer to complete, presumably because the dataset is sorted by article length and they're loading the entire article text into memory, instead of just the 'num_tokens' column.\r\n\r\nI should note that my batching procedure would work best if I just used `batch_size=None` and loaded the whole column into memory at once, but I found that this was intolerably slow and gave me no progress information, so I'm using the less than ideal `batch_size=10_000`.""
 ""Hi @norabelrose ! I'm glad you managed to make this work on your side.\r\nRegarding memory usage, you can try to drop the columns that you don't want to use for your `map` for now.\r\n\r\nIn the future we'll try to find a way to not load unnecessary columns in memory in `map`. Currently the way it works is that it gets the batch as a python dict, then it updates it using the output of your mapping function, and finally it removes columns from `remove_columns`. Therefore for a moment some columns are loaded in memory even if you remove them or don't use them for your mapping function.\r\n\r\nIt would be nice to have a way to optimize memory for cases such as yours !""
 '@lhoestq After looking through the source code, it looks like the following solution has at least some chance of working:\r\n- refactor `Dataset.map()` so that the `input_columns` parameter is implemented by using the `self.formatted_as()` context manager with `columns=input_columns`\r\n- change `Dataset._getitem()` so that it passes `self._data.drop(drop_columns)` to the `query_table()` function whenever `format_columns` is non-None and `output_all_columns` is False, instead of `self._data` itself'
 ""Looks like a great direction :)\r\nNote that `query_table` doesn't bring data into memory. Only `format_table` does.\r\nAlso the dataset may already have a format with `columns=` already defined so we would need to define the formatted `input_dataset` like:\r\n```python\r\n# before the `map` main for loop\r\ninput_columns = input_columns if input_columns is not None else self.column_names\r\nif not self._output_all_columns:\r\n    columns = [col for col in input_columns if self._format_columns is None or col in self._format_columns]\r\n    input_dataset = self.with_format(\r\n        type=self._format_type,\r\n        columns=columns\r\n    )\r\nelse:\r\n    # in this case we could find a way to filter both format_columns and unformatted columns eventually\r\n    input_dataset = self\r\n# then input_dataset can be used in the main for loop of `map`\r\n```\r\n\r\nEDIT: oh and regarding streaming format versus file format for arrow, we plan to start using the file format #1933 at one point (though I'm not sure if it would improve performance)""
 ""Good to know about `query_table` not bringing anything into memory. I was under the impression that it did because a while back I looked at my `map` operation in pdb and it looked like it was spending forever in line 93 of formatting.py, `return pa.concat_tables(....)`, although that was before the `fast_slice` interpolation search was implemented, so it may have had more to do with the slow ChunkedArray slice implementation than anything else.\r\n\r\nIf `query_table` is I/O free then the fix may be as simple as just adding this to line 1779 of arrow_dataset.py:\r\n```python\r\n# Only load the columns we actually need\r\nif input_columns:\r\n  stack.enter_context(self.formatted_as(\r\n    self._format_type,\r\n    columns=input_columns,\r\n    output_all_columns=False,\r\n    **self._format_kwargs\r\n  ))\r\n```\r\nIt's not clear to me why the `[col for col in input_columns if self._format_columns is None or col in self._format_columns]` check would be necessary— it seems like either `input_columns` should simply temporarily override the `_format_columns` within the `map` operation, or we should throw an error if there are any conflicts. Currently it doesn't look like this case is checked for at all within `map`, but maybe I'm just missing it.""
 ""`query_table` simply slices/concatenates parts of the table. The actual data inside the table is not brought in memory.\r\nAlso I'm more in favor of declaring `input_dataset = self.with_format(...)` since `formatted_as` may update the dataset fingerprint of `self`, which is not expected when someone runs `map`.\r\n\r\n> It's not clear to me why the [col for col in input_columns if self._format_columns is None or col in self._format_columns] check would be necessary— it seems like either input_columns should simply temporarily override the _format_columns within the map operation, or we should throw an error if there are any conflicts. Currently it doesn't look like this case is checked for at all within map, but maybe I'm just missing it.\r\n\r\nActually yes we can just use input_columns. And we do need to add a check to make sure there are not conflicts or this could lead to confusing errors.""
 'That sounds good to me! I just submitted a PR (#2246) implementing your approach. I also changed how `_query_table` handles Iterable keys since it still seemed like `pa.concat_tables` was taking a long time to create the table for each batch. Now my whole `map()` operation takes 1 min 46 seconds where it used to take somewhere on the order of 10 minutes.']","I'm currently using the `wikipedia` dataset— I'm tokenizing the articles with the `tokenizers` library using `map()` and also adding a new `num_tokens` column to the dataset as part of that map operation.

I want to be able to _filter_ the dataset based on this `num_tokens` column, but even when I specify `input_columns=['num_tokens']`, it seems that the entirety of each row is loaded into memory, which makes the operation take much longer than it should. Indeed, `filter` currently just calls `map`, and I found that in `_map_single` on lines 1690-1704 of `arrow_dataset.py`, the method is just grabbing slices of _all the rows_ of the dataset and then passing only the specified columns to the map function. It seems that, when the user passes a value for `input_columns`, the `map` function should create a temporary pyarrow table by selecting just those columns, and then get slices from that table. Or something like that— I'm not very familiar with the pyarrow API.

I know that in the meantime I can sort of get around this by simply only returning the rows that match my filter criterion from the tokenizing function I pass to `map()`, but I actually _also_ want to map on just the `num_tokens` column in order to compute batches with a roughly uniform number of tokens per batch. I would also ideally like to be able to change my minimum and maximum article lengths without having to re-tokenize the entire dataset.

PS: This is definitely not a ""dataset request."" I'm realizing that I don't actually know how to remove labels from my own issues on other people's repos, if that is even possible."
https://github.com/huggingface/datasets/issues/2190,News_commentary Dataset Translation Pairs are of Incorrect Language Specified Pairs,"['Hi @anassalamah,\r\n\r\nCould you please try with this:\r\n```python\r\ntrain_ds = load_dataset(""news_commentary"", lang1=""ar"", lang2=""en"", split=\'train[:98%]\')\r\nval_ds = load_dataset(""news_commentary"", lang1=""ar"", lang2=""en"", split=\'train[98%:]\')\r\n```'
 ""Hello @albertvillanova, \r\n\r\nThanks for the suggestion. I didn't know you could do that. however, it didn't resolve the issue\r\n\r\n![image](https://user-images.githubusercontent.com/8571003/114169966-ec819400-993a-11eb-8a67-930f9a9b2290.png)\r\n""]","I used load_dataset to load the news_commentary dataset for ""ar-en"" translation pairs but found translations from Arabic to Hindi.  

```
train_ds = load_dataset(""news_commentary"", ""ar-en"", split='train[:98%]')
val_ds = load_dataset(""news_commentary"", ""ar-en"", split='train[98%:]')

# filtering out examples that are not ar-en translations but ar-hi
val_ds = val_ds.filter(lambda example, indice: indice not in chain(range(1312,1327) ,range(1384,1399), range(1030,1042)), with_indices=True)
```

* I'm fairly new to using datasets so I might be doing something wrong"
https://github.com/huggingface/datasets/issues/2189,save_to_disk doesn't work when we use concatenate_datasets function before creating the final dataset_object.,"[""Hi ! We refactored save_to_disk in #2025 so this doesn't happen.\r\nFeel free to try it on master for now\r\nWe'll do a new release soon""]","As you can see, it saves the entire dataset.

@lhoestq 

You can  check by going through the following example,

```
from datasets import load_from_disk,concatenate_datasets

loaded_data=load_from_disk('/home/gsir059/HNSW-ori/my_knowledge_dataset')
n=20
kb_list=[loaded_data.shard(n, i, contiguous=True) for i in range(n)]
final_dataset=concatenate_datasets([kb_list[1],kb_list[2]])
final_dataset.save_to_disk('/home/gsir059/haha/k.arrow')
```"
https://github.com/huggingface/datasets/issues/2188,Duplicate data in Timit dataset,"['Hi ! Thanks for reporting\r\nIf I recall correctly this has been recently fixed #1995\r\nCan you try to upgrade your local version of `datasets` ?\r\n```\r\npip install --upgrade datasets\r\n```'
 'Hi Ihoestq,\r\n\r\nThank you. It works after upgrading the datasets\r\n']","I ran a simple code to list all texts in Timit dataset and the texts were all the same.
Is this dataset corrupted?
**Code:**
timit = load_dataset(""timit_asr"")
print(*timit['train']['text'], sep='\n')
**Result:**
Would such an act of refusal be useful?
Would such an act of refusal be useful?
Would such an act of refusal be useful?
Would such an act of refusal be useful?
...
...
Would such an act of refusal be useful?"
https://github.com/huggingface/datasets/issues/2187,Question (potential issue?) related to datasets caching,"['An educated guess: does this refer to the fact that depending on the custom column names in the dataset files (csv in this case), there is a dataset loader being created? and this dataset loader - using the ""custom data configuration"" is used among all jobs running using this particular csv files? (thinking out loud here...)\r\n\r\nIf this is the case, it may be ok for my use case (have to think about it more), still a bit surprising given that datasets caching is disabled (or so I hope) by the lines I pasted above. '
 ""Hi ! Currently disabling the caching means that all the dataset transform like `map`, `filter` etc. ignore the cache: it doesn't write nor read processed cache files.\r\nHowever `load_dataset` reuses datasets that have already been prepared: it does reload prepared dataset files.\r\n\r\nIndeed from the documentation:\r\n> datasets.set_caching_enabled(boolean: bool)\r\n\r\n> When applying transforms on a dataset, the data are stored in cache files. The caching mechanism allows to reload an existing cache file if it’s already been computed.\r\n> Reloading a dataset is possible since the cache files are named using the dataset fingerprint, which is updated after each transform.\r\n> If disabled, the library will no longer reload cached datasets files when applying transforms to the datasets. More precisely, if the caching is disabled:\r\n> - cache files are always recreated\r\n> - cache files are written to a temporary directory that is deleted when session closes\r\n> - cache files are named using a random hash instead of the dataset fingerprint - use datasets.Dataset.save_to_disk() to save a transformed dataset or it will be deleted when session closes\r\n> - caching doesn’t affect datasets.load_dataset(). If you want to regenerate a dataset from scratch you should use the download_mode parameter in datasets.load_dataset().""
 'Thank you for the clarification. \r\n\r\nThis is a bit confusing. On one hand, it says that cache files are always recreated and written to a temporary directory that is removed; on the other hand the last bullet point makes me think that since the default according to the docs for `download_mode (Optional datasets.GenerateMode) – select the download/generate mode - Default to REUSE_DATASET_IF_EXISTS` => it almost sounds that it could reload prepared dataset files. Where are these files stored? I guess not in the temporary directory that is removed... \r\n\r\nI find this type of api design error-prone. When I see as a programmer `datasets.set_caching_enabled(False)` I expect no reuse of anything in the cache. '
 'It would be nice if the documentation elaborated on all the possible values for `download_mode` and/or a link to `datasets.GenerateMode`. \r\nThis info here:\r\n```\r\n    """"""`Enum` for how to treat pre-existing downloads and data.\r\n    The default mode is `REUSE_DATASET_IF_EXISTS`, which will reuse both\r\n    raw downloads and the prepared dataset if they exist.\r\n    The generations modes:\r\n    |                                    | Downloads | Dataset |\r\n    | -----------------------------------|-----------|---------|\r\n    | `REUSE_DATASET_IF_EXISTS` (default)| Reuse     | Reuse   |\r\n    | `REUSE_CACHE_IF_EXISTS`            | Reuse     | Fresh   |\r\n    | `FORCE_REDOWNLOAD`                 | Fresh     | Fresh   |\r\n```'
 ""I have another question. Assuming that I understood correctly and there is reuse of datasets files when caching is disabled (!), I'm guessing there is a directory that is created based on some information on the dataset file. I'm interested in the situation where I'm loading a (custom) dataset from local disk. What information is used to create the directory/filenames where the files are stored?\r\n\r\nI'm concerned about the following scenario: if I have a file, let's say `train.csv` at path `the_path`, run once, the dataset is prepared, some models are run, etc. Now let's say there is an issue and I recreate `train.csv` at the same path `the_path`. Is there enough information in the temporary name/hash to *not* reload the *old* prepared dataset (e.g., timestamp of the file)? Or is it going to reload the *old* prepared file? ""
 'Thanks for the feedback, we\'ll work in improving this aspect of the documentation.\r\n\r\n> Where are these files stored? I guess not in the temporary directory that is removed...\r\n\r\nWe\'re using the Arrow file format to load datasets. Therefore each time you load a dataset, it is prepared as an arrow file on your disk. By default the file is located in the ~/.cache/huggingface/datasets/<dataset_name>/<config_id>/<version> directory.\r\n\r\n> What information is used to create the directory/filenames where the files are stored?\r\n\r\nThe config_id contains a hash that takes into account:\r\n- the dataset loader used and its source code (e.g. the ""csv"" loader)\r\n- the arguments passed to the loader (e.g. the csv delimiter)\r\n- metadata of the local data files if any (e.g. their timestamps)\r\n\r\n> I\'m concerned about the following scenario: if I have a file, let\'s say train.csv at path the_path, run once, the dataset is prepared, some models are run, etc. Now let\'s say there is an issue and I recreate train.csv at the same path the_path. Is there enough information in the temporary name/hash to not reload the old prepared dataset (e.g., timestamp of the file)? Or is it going to reload the old prepared file?\r\n\r\nYes the timestamp of the local csv file is taken into account. If you edit your csv file, the config_id will change and loading the dataset will create a new arrow file.'
 'Thank you for all your clarifications, really helpful! \r\n\r\nIf you have the bandwidth, please do revisit the api wrt cache disabling. Anywhere in the computer stack  (hardware included) where you disable the cache, one assumes there is no caching that happens. '
 'That makes total sense indeed !\r\nI think we can do the change'
 ""I have another question about caching, this time in the case where FORCE_REDOWNLOAD is used to load the dataset, the datasets cache is one directory as defined by HF_HOME and there are multiple concurrent jobs running in a cluster using the same local dataset (i.e., same local files in the cluster). Does anything in the naming convention and/or file access/locking that you're using prevent race conditions between the concurrent jobs on the caching of the local dataset they all use?\r\n\r\nI noticed some errors (can provide more details if helpful) in load_dataset/prepare_split that lead to my question above. \r\n\r\nLet me know if my question is clear, I can elaborate more if needed @lhoestq  Thank you!""
 ""I got another error that convinces me there is a race condition (one of the test files had zero samples at prediction time). I think it comes down to the fact that the `config_id` above (used in the naming for the cache) has no information on who's touching the data. If I have 2 concurrent jobs, both loading the same dataset and forcing redownload, they may step on each other foot/caching of the dataset. ""
 'We\'re using a locking mechanism to prevent two processes from writing at the same time. The locking is based on the `filelock` module.\r\nAlso directories that are being written use a suffix "".incomplete"" so that reading is not possible on a dataset being written.\r\n\r\nDo you think you could provide a simple code to reproduce the race condition you experienced ?'
 ""I can provide details about the code I'm running (it's really-really close to some official samples from the huggingface transformers examples, I can point to the exact sample file, I kept a record of that). I can also describe in which conditions this race occurs (I'm convinced it has to do with forcing the redownloading of the dataset, I've been running hundreds of experiments before and didn't have a problem before I forced the redownload). I also can provide samples of the different stack errors I get and some details about the level of concurrency of jobs I was running. I can also try to imagine how the race manifests (I'm fairly sure that it's a combo of one job cleaning up and another job being in the middle of the run).\r\n\r\nHowever, I have to cleanup all this to make sure I'm no spilling any info I shouldn't be spilling. I'll try to do it by the end of the week, if you think all this is helpful. \r\n\r\nFor now, I have a workaround. Don't use forcing redownloading. And to be ultra careful (although I don't think this is a problem), I run a series of jobs that will prepare the datasets and I know there is no concurrency wrt the dataset. Once that's done (and I believe even having multiple jobs loading the datasets at the same time doesn't create problems, as long as REUSE_DATASET_IF_EXISTS is the policy for loading the dataset, so the filelock mechanism you're using is working in that scenario), the prepared datasets will be reused, no race possible in any way. \r\n\r\nThanks for all the details you provided, it helped me understand the underlying implementation and coming up with workarounds when I ran into issues. ""]","I thought I had disabled datasets caching in my code, as follows:
```
from datasets import set_caching_enabled
...
def main():

    # disable caching in datasets
    set_caching_enabled(False)
```
However, in my log files I see messages like the following:

```
04/07/2021 18:34:42 - WARNING - datasets.builder -   Using custom data configuration default-888a87931cbc5877
04/07/2021 18:34:42 - WARNING - datasets.builder -   Reusing dataset csv (xxxx/cache-transformers/datasets/csv/default-888a87931cbc5877/0.0.0/965b6429be0fc05f975b608ce64e1fa941cc8fb4f30629b523d2390f3c0e1a93
```
Can you please let me know what this reusing dataset csv means? I wouldn't expect any reusing with the datasets caching disabled. Thank you!"
https://github.com/huggingface/datasets/issues/2185,.map() and distributed training,"['Hi, one workaround would be to save the mapped(tokenized in your case) file using `save_to_disk`, and having each process load this file using `load_from_disk`. This is what I am doing, and in this case, I turn off the ability to automatically load from the cache.\r\n\r\nAlso, multiprocessing the map function seems to be slower at the moment (#1992), hope this helps you.'
 ""Thanks @hwijeen for the workaround, feels a bit prototypical but it works! (it seems files are written twice then though)\r\n\r\n(I haven't observed slowness using multiprocessed map function but I could be wrong)""
 ""To my understanding, files are written twice anyhow(one after load_dataset, another aftet map). It's just that you now have it at the location where you can see, whereas it was secretlely saved at caching folder(.cache/huggingface/datasets by default)! Correct me if I'm wrong!""
 ""Slowness in multiprocessing has been observed in certain environments but not others. We're investigating ;)""
 ""So to answer my initial question, I was just doing something stupid as I was not re-giving the `preprocessing_num_workers` arguments when launching the distributed training (and it was then set to `None`). I initially thought the hash was computed only with the `tokenize_function` but it's all arguments. Thanks @lhoestq for clarifying!""]","Hi,
I have a question regarding distributed training and the `.map` call on a dataset.

I have a local dataset ""my_custom_dataset"" that I am loading with `datasets = load_from_disk(dataset_path=my_path)`.
`dataset` is then tokenized:
```python
datasets = load_from_disk(dataset_path=my_path)

[...]

def tokenize_function(examples):
    return tokenizer(examples[text_column_name])

logger.info(""Mapping dataset to tokenized dataset."")
tokenized_datasets = datasets.map(
    tokenize_function,
    batched=True,
    num_proc=preprocessing_num_workers,
    remove_columns=column_names,
    load_from_cache_file=True,
)
```
I am using 31 workers (`preprocessing_num_workers=31`) and thus it creates 31 `cache*.arrow` files in `my_path/train` (there is only a train split).
When I relaunch the script, the map is tokenization is skipped in favor of loading the 31 previously cached files, and that's perfect.

Everything so far was done by launching a **single process script**.
I now launch the same training script in **distributed mode** (`pytorch -m torch.distributed.launch --nproc_per_node 2`). However, once it reaches the map call, it re-does the tokenization... instead of loading the 31 cached files. 

I tried adding the `cache_file_name` argument: `cache_file_name={""train"": my_path/one_of_the_arrow_file}`, but I can't give the 31 cached files, so it probably isn't the right way to do it.

**My question: what is the best way to load cached files if they were pre-processed and dumped in multiple arrow files?** It seems automatically handled for single processes but fails on distributed training.

- I am following the same structure as the examples of transformers (more specifically [run_clm.py](https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_clm.py) in my case)
- I am using 1.5.0 version of datasets if that matters."
https://github.com/huggingface/datasets/issues/2181,Error when loading a HUGE json file (pyarrow.lib.ArrowInvalid: straddling object straddles two block boundaries),"['Hi ! Can you try to increase the block size ? For example\r\n```python\r\nblock_size_10MB = 10<<20\r\nload_dataset(""json"", ..., block_size=block_size_10MB)\r\n```\r\nThe block size corresponds to how much bytes to process at a time from the input stream.\r\nThis will determine multi-threading granularity as well as the size of individual chunks in the dataset.\r\n\r\nYou can also try with bigger block sizes if needed'
 ""Hi @lhoestq! Thank you for your prompt reply.\r\nI have experimented with (10<<20, 10<<28, 10<<30, 10<<33, 10<<34), since my machine has 192G of memory, but it's either the above-mentioned error or processed killed because of OOM.\r\n\r\nCould you give me a bit of background on why block size needs to be exactly calibrated?\r\nTo my understanding, small block sized should run just fine despite its slowness..\r\n\r\n\r\n""
 ""We're using the JSON loader of pyarrow. It parses the file chunk by chunk to load the dataset.\r\nThis issue happens when there's no delimiter in one chunk of data. For json line, the delimiter is the end of line.\r\nSo with a big value for chunk_size this should have worked unless you have one extremely long line in your file.\r\n\r\nAlso what version of pyarrow are you using ?\r\n\r\nFInally I wonder if it could be an issue on pyarrow's side when using big json files. (I haven't tested big json files like yours)""
 'I\'m using `pyarrow==3.0.0` with `datasets==1.5.0`.\r\n\r\nYour point totally makes sense. I will check if my jsonl file contains an extremely long file and let you know. \r\n\r\nHere are some different error messages that I got when tweaking `block_size`. I also suspect that this is related to the pyarrow... but I guess it would be wonderful if datasesets could give a clear guide on how to play with large datasets! (I am suddenly experiencing various issue when working with large datasets.. e.g. #1992 )\r\n```python\r\n    return paj.ReadOptions(use_threads=self.use_threads, block_size=self.block_size)\r\n  File ""pyarrow/_json.pyx"", line 56, in pyarrow._json.ReadOptions.__init__\r\n  File ""pyarrow/_json.pyx"", line 81, in pyarrow._json.ReadOptions.block_size.__set__\r\nOverflowError: value too large to convert to int32_t\r\n```\r\n\r\n```python\r\n\r\nline 83, in _generate_tables\r\n    parse_options=self.config.pa_parse_options,\r\n  File ""pyarrow/_json.pyx"", line 247, in pyarrow._json.read_json\r\n  File ""pyarrow/error.pxi"", line 122, in pyarrow.lib.pyarrow_internal_check_status\r\n  File ""pyarrow/error.pxi"", line 84, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Exceeded maximum rows\r\n```'
 'I am getting the same error. When I tweak the block_size, I also find:\r\n`OverflowError: value too large to convert to int32_t`\r\nand \r\n`pyarrow.lib.ArrowInvalid: Exceeded maximum rows`\r\n'
 'I made more tests. I used a smaller dataset and I was getting the same error, which means that it was not necessarily linked to the dataset size. To make both my smaller and larger datasets work, I got rid of lists with the json file. I had the following data format:\r\n```python\r\n[\r\n  {\'key\': ""a"", \'value\': [\'one\', \'two\', \'three\']},\r\n  {\'key\': ""b"", \'value\': [\'four\', \'five\', \'six\']}\r\n]\r\n```\r\nI changed to:\r\n\r\n```python\r\n  {\'key\': ""a"", \'value\': \'one\\ntwo\\nthree\'},\r\n  {\'key\': ""b"", \'value\': \'four\\nfive\\nsix\']}\r\n```\r\nand that worked!\r\n\r\nI used the following to reformat my json file:\r\n```python\r\nwith open(file_name, ""w"", encoding=""utf-8"") as f:\r\n    for item in list_:\r\n        f.write(json.dumps(item) + ""\\n"")\r\n```\r\nThis works with `block_size_10MB = 10 << 20` or without specifying `block_size`.'
 'Thanks @hwijeen for reporting and thanks @jpilaul for pointing this out.\r\n\r\nIndeed, those are different JSON-like formats:\r\n- the first one is the **standard JSON** format: all the file content is JSON-valid, thus all content is either a JSON object (between curly brackets `{...}`) or a JSON array (between square brackets `[...]`)\r\n- the second one is called **JSON Lines**: the entire file content is not JSON-valid, but only every line (newline-delimited) is JSON-valid\r\n\r\nCurrently PyArrow only supports **JSON Lines** format: \r\n- https://arrow.apache.org/docs/python/generated/pyarrow.json.read_json.html\r\n  > Currently only the line-delimited JSON format is supported.\r\n- https://arrow.apache.org/docs/python/json.html\r\n  > Arrow supports reading columnar data from line-delimited JSON files.'
 'Thanks @albertvillanova for your explanation, it is helpful to know (maybe add to docs?)!\r\nHowever, the problem I described above happened when I was dealing with jsonl files 😿\r\nAlthough I did not thoroughly inspect, I suspect the cause was the one extremely long document in my case.'
 'I see... I guess there is another problem going one then, related to the size.']","Hi, thanks for the great library. I have used the brilliant library for a couple of small projects, and now using it for a fairly big project.
When loading a huge json file of 500GB, pyarrow complains as follows:
```
Traceback (most recent call last):
  File ""/home/user/.pyenv/versions/3.7.9/lib/python3.7/site-packages/datasets/builder.py"", line 531, in incomplete_dir
    yield tmp_dir
  File ""/home/user/.pyenv/versions/3.7.9/lib/python3.7/site-packages/datasets/builder.py"", line 573, in download_and_prepare
    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
  File ""/home/user/.pyenv/versions/3.7.9/lib/python3.7/site-packages/datasets/builder.py"", line 650, in _download_and_prepare
    self._prepare_split(split_generator, **prepare_split_kwargs)
  File ""/home/user/.pyenv/versions/3.7.9/lib/python3.7/site-packages/datasets/builder.py"", line 1027, in _prepare_split
    for key, table in utils.tqdm(generator, unit="" tables"", leave=False, disable=not_verbose):
  File ""/home/user/.pyenv/versions/3.7.9/lib/python3.7/site-packages/tqdm/std.py"", line 1133, in __iter__
    for obj in iterable:
  File ""/app/.cache/huggingface/modules/datasets_modules/datasets/json/9498524fd296a6cca99c66d6c5be507d1c0991f5a814e535b507f4a66096a641/json.py"", line 83, in _generate_tables
    parse_options=self.config.pa_parse_options,
  File ""pyarrow/_json.pyx"", line 247, in pyarrow._json.read_json
  File ""pyarrow/error.pxi"", line 122, in pyarrow.lib.pyarrow_internal_check_status
  File ""pyarrow/error.pxi"", line 84, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: straddling object straddles two block boundaries (try to increase block size?)
```
When using only a small portion of the sample file, say first 100 lines, it works perfectly well..

I see that it is the error from pyarrow, but could you give me a hint or possible solutions?
#369 describes the same error and #372 claims to have fixed the issue, but I have no clue why I am still getting this one. Thanks in advance!"
https://github.com/huggingface/datasets/issues/2176,Converting a Value to a ClassLabel,"['Hi @nelson-liu!\r\nHere is what I do to convert a string to class label:\r\n\r\n```python\r\nfrom datasets import load_dataset, features\r\n\r\n\r\ndset = load_dataset(...)\r\ncol_name = ""the string column name""\r\n\r\nclass_names  = dset.unique(col_name)\r\nclass_feature = features.ClassLabel(names=sorted(class_names))\r\ndset = dset.map(lambda str_value: {col_name: class_feature.str2int(str_value)}, input_columns=col_name)\r\n\r\ndset = dset.cast(features.Features({\r\n    ...\r\n    col_name: class_feature\r\n})\r\n```\r\n']","Hi!

In the docs for `cast`, it's noted that `For non-trivial conversion, e.g. string <-> ClassLabel you should use map() to update the Dataset.`

Would it be possible to have an example  that demonstrates such a string <-> ClassLabel conversion using `map`? Thanks!"
https://github.com/huggingface/datasets/issues/2175,dataset.search_batch() function outputs all -1 indices sometime.,"[""Actually, I found the answer [here](https://github.com/facebookresearch/faiss/wiki/FAQ#what-does-it-mean-when-a-search-returns--1-ids). \r\n\r\nSo we have to do some modifications to the code for instances where the index doesn't retrieve any IDs.""
 '@lhoestq  @patrickvonplaten \r\n\r\nI also found another short bug in the retrieval part.   Especially, when retrieving documents. If Faiss returns the -1 as the index, the retriever will always use the last element in the dataset.\r\n\r\nplease check [def get_doc_dicts function](https://github.com/huggingface/transformers/blob/master/src/transformers/models/rag/retrieval_rag.py#L222)\r\n\r\n\r\nDoes the use of the HNSW guarantee to retrieve valid indexes always? \r\n\r\n'
 ""Hi !\r\nNo it happens sometimes to return -1, especially if your dataset is small.\r\nIf your dataset is big enough it shouldn't happen in my experience.\r\n\r\nIdeally we should ignore all the -1 that are returned. It should be possible to change that in RAG's code ""
 ""I also checked with some indexes it returns more -1s. Specially with IVF\nwhen nprobr is very low. It doesn't happen when using HNSW though. But at\nthe moment if it happens, dataset will always return the last element.\nMaybe we should change it to repeat the most last valid retrieved doc id.\nWhat do you think?\n\nOn Wed, Apr 7, 2021, 21:09 Quentin Lhoest ***@***.***> wrote:\n\n> Hi !\n> No it happens sometimes to return -1, especially if your dataset is small.\n> If your dataset is big enough it shouldn't happen.\n>\n> Ideally we should ignore all the -1 that are returned. It should be\n> possible to change that in RAG's code\n>\n> —\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/huggingface/datasets/issues/2175#issuecomment-814746509>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AEA4FGTENOTLBEZTXEO2RS3THQOMPANCNFSM42PRVYDA>\n> .\n>\n""
 'That would be an easy way to workaround this issue. Feel free to open a PR on `transformers` and ping me ! :)'
 'Sure.  Will push everything together with RAG end to end. :) thanks a lot.\n\nOn Wed, Apr 7, 2021, 21:16 Quentin Lhoest ***@***.***> wrote:\n\n> That would be an easy way to workaround this issue. Feel free to open a PR\n> on transformers and ping me ! :)\n>\n> —\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/huggingface/datasets/issues/2175#issuecomment-814752589>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AEA4FGWLROCGARKN7WOJYSTTHQPH5ANCNFSM42PRVYDA>\n> .\n>\n']","I am working with RAG and playing around with different faiss indexes. At the moment I use **index = faiss.index_factory(768, ""IVF65536_HNSW32,Flat"")**.

During the retrieval phase exactly in [this line of retrieval_rag.py](https://github.com/huggingface/transformers/blob/master/src/transformers/models/rag/retrieval_rag.py#L231) an error issue when all retrieved indices are -1.  Please refer to the screenshot of a PID worker. 

![image](https://user-images.githubusercontent.com/16892570/113782387-37a67600-9786-11eb-9c29-acad661a9648.png)


Here, my retrieve batch size is 2 and n_docs is 5. I can solve this by working around np. stack, but I want to ask, why we get an output index of -1. Do you have any idea :) ?

Is this a problem of the index, where the faiss can't find any similar vector?
Is there documentation on the output index being -1?

@lhoestq 
 "
https://github.com/huggingface/datasets/issues/2170,Wikipedia historic dumps are deleted but hf/datasets hardcodes dump date,"[""It seems that this can be fixed from user's end by including a `date` argument, like this:\r\n\r\n`dataset = datasets.load_dataset('wikipedia', '20200501.en', date='20210420')`\r\n\r\nYou can get available dates from [here](https://dumps.wikimedia.org/enwiki/).\r\n\r\nThis is not a proper fix however as all the files will still have '20200501' in their file names.""]","Wikimedia does not keep all historical dumps. For example, as of today https://dumps.wikimedia.org/kowiki/ only provides

```
20201220/                                          02-Feb-2021 01:36                   -
20210101/                                          21-Feb-2021 01:26                   -
20210120/                                          02-Mar-2021 01:25                   -
20210201/                                          21-Mar-2021 01:26                   -
20210220/                                          02-Apr-2021 01:26                   -
20210301/                                          03-Mar-2021 08:10                   -
20210320/                                          21-Mar-2021 18:13                   -
20210401/                                          03-Apr-2021 10:08                   -
latest/                                            03-Apr-2021 10:08                   -
```

However, the wikipedia dataset provided in the library, only supports the following configs, none of which are applicable anymore when disregarding the cached datasets:

```
ValueError: BuilderConfig 20210401.ko not found. Available: ['20200501.aa', '20200501.ab', '20200501.ace', '20200501.ady', '20200501.af', '20200501.ak', '20200501.als', '20200501.am', '20200501.an', '20200501.ang', '20200501.ar', '20200501.arc', '20200501.arz', '20200501.as', '20200501.ast', '20200501.atj', '20200501.av', '20200501.ay', '20200501.az', '20200501.azb', '20200501.ba', '20200501.bar', '20200501.bat-smg', '20200501.bcl', '20200501.be', '20200501.be-x-old', '20200501.bg', '20200501.bh', '20200501.bi', '20200501.bjn', '20200501.bm', '20200501.bn', '20200501.bo', '20200501.bpy', '20200501.br', '20200501.bs', '20200501.bug', '20200501.bxr', '20200501.ca', '20200501.cbk-zam', '20200501.cdo', '20200501.ce', '20200501.ceb', '20200501.ch', '20200501.cho', '20200501.chr', '20200501.chy', '20200501.ckb', '20200501.co', '20200501.cr', '20200501.crh', '20200501.cs', '20200501.csb', '20200501.cu', '20200501.cv', '20200501.cy', '20200501.da', '20200501.de', '20200501.din', '20200501.diq', '20200501.dsb', '20200501.dty', '20200501.dv', '20200501.dz', '20200501.ee', '20200501.el', '20200501.eml', '20200501.en', '20200501.eo', '20200501.es', '20200501.et', '20200501.eu', '20200501.ext', '20200501.fa', '20200501.ff', '20200501.fi', '20200501.fiu-vro', '20200501.fj', '20200501.fo', '20200501.fr', '20200501.frp', '20200501.frr', '20200501.fur', '20200501.fy', '20200501.ga', '20200501.gag', '20200501.gan', '20200501.gd', '20200501.gl', '20200501.glk', '20200501.gn', '20200501.gom', '20200501.gor', '20200501.got', '20200501.gu', '20200501.gv', '20200501.ha', '20200501.hak', '20200501.haw', '20200501.he', '20200501.hi', '20200501.hif', '20200501.ho', '20200501.hr', '20200501.hsb', '20200501.ht', '20200501.hu', '20200501.hy', '20200501.ia', '20200501.id', '20200501.ie', '20200501.ig', '20200501.ii', '20200501.ik', '20200501.ilo', '20200501.inh', '20200501.io', '20200501.is', '20200501.it', '20200501.iu', '20200501.ja', '20200501.jam', '20200501.jbo', '20200501.jv', '20200501.ka', '20200501.kaa', '20200501.kab', '20200501.kbd', '20200501.kbp', '20200501.kg', '20200501.ki', '20200501.kj', '20200501.kk', '20200501.kl', '20200501.km', '20200501.kn', '20200501.ko', '20200501.koi', '20200501.krc', '20200501.ks', '20200501.ksh', '20200501.ku', '20200501.kv', '20200501.kw', '20200501.ky', '20200501.la', '20200501.lad', '20200501.lb', '20200501.lbe', '20200501.lez', '20200501.lfn', '20200501.lg', '20200501.li', '20200501.lij', '20200501.lmo', '20200501.ln', '20200501.lo', '20200501.lrc', '20200501.lt', '20200501.ltg', '20200501.lv', '20200501.mai', '20200501.map-bms', '20200501.mdf', '20200501.mg', '20200501.mh', '20200501.mhr', '20200501.mi', '20200501.min', '20200501.mk', '20200501.ml', '20200501.mn', '20200501.mr', '20200501.mrj', '20200501.ms', '20200501.mt', '20200501.mus', '20200501.mwl', '20200501.my', '20200501.myv', '20200501.mzn', '20200501.na', '20200501.nah', '20200501.nap', '20200501.nds', '20200501.nds-nl', '20200501.ne', '20200501.new', '20200501.ng', '20200501.nl', '20200501.nn', '20200501.no', '20200501.nov', '20200501.nrm', '20200501.nso', '20200501.nv', '20200501.ny', '20200501.oc', '20200501.olo', '20200501.om', '20200501.or', '20200501.os', '20200501.pa', '20200501.pag', '20200501.pam', '20200501.pap', '20200501.pcd', '20200501.pdc', '20200501.pfl', '20200501.pi', '20200501.pih', '20200501.pl', '20200501.pms', '20200501.pnb', '20200501.pnt', '20200501.ps', '20200501.pt', '20200501.qu', '20200501.rm', '20200501.rmy', '20200501.rn', '20200501.ro', '20200501.roa-rup', '20200501.roa-tara', '20200501.ru', '20200501.rue', '20200501.rw', '20200501.sa', '20200501.sah', '20200501.sat', '20200501.sc', '20200501.scn', '20200501.sco', '20200501.sd', '20200501.se', '20200501.sg', '20200501.sh', '20200501.si', '20200501.simple', '20200501.sk', '20200501.sl', '20200501.sm', '20200501.sn', '20200501.so', '20200501.sq', '20200501.sr', '20200501.srn', '20200501.ss', '20200501.st', '20200501.stq', '20200501.su', '20200501.sv', '20200501.sw', '20200501.szl', '20200501.ta', '20200501.tcy', '20200501.te', '20200501.tet', '20200501.tg', '20200501.th', '20200501.ti', '20200501.tk', '20200501.tl', '20200501.tn', '20200501.to', '20200501.tpi', '20200501.tr', '20200501.ts', '20200501.tt', '20200501.tum', '20200501.tw', '20200501.ty', '20200501.tyv', '20200501.udm', '20200501.ug', '20200501.uk', '20200501.ur', '20200501.uz', '20200501.ve', '20200501.vec', '20200501.vep', '20200501.vi', '20200501.vls', '20200501.vo', '20200501.wa', '20200501.war', '20200501.wo', '20200501.wuu', '20200501.xal', '20200501.xh', '20200501.xmf', '20200501.yi', '20200501.yo', '20200501.za', '20200501.zea', '20200501.zh', '20200501.zh-classical', '20200501.zh-min-nan', '20200501.zh-yue', '20200501.zu']
```

The cached datasets:

```
% aws s3 --no-sign-request --endpoint-url https://storage.googleapis.com ls s3://huggingface-nlp/cache/datasets/wikipedia/
                           PRE 20200501.de/
                           PRE 20200501.en/
                           PRE 20200501.fr/
                           PRE 20200501.frr/
                           PRE 20200501.it/
                           PRE 20200501.simple/
```"
https://github.com/huggingface/datasets/issues/2166,Regarding Test Sets for the GEM datasets,"[""Hi @vyraun ! The test references for CommonGen are not publicly available: you can reach out to the original dataset authors if you would like to ask for them, but we will not be releasing them as part of GEM (March 31st was the release date for the test set inputs, references are incidentally released for some of the test sets but shouldn't really be used for benchmark submissions)\r\n\r\ncc @sebastiangehrmann""
 'Oh okay, thanks @yjernite ! ']","@yjernite Hi, are the test sets for the GEM datasets scheduled to be [added soon](https://gem-benchmark.com/shared_task)? 

e.g.

```
from datasets import load_dataset
DATASET_NAME=""common_gen""
data = load_dataset(""gem"", DATASET_NAME)
```

The test set doesn't have the target or references.

```
data['test'][0]
{'concept_set_id': 0, 'concepts': ['drill', 'field', 'run', 'team'], 'gem_id': 'common_gen-test-0', 'gem_parent_id': 'common_gen-test-0', 'references': [], 'target': ''}
```

"
https://github.com/huggingface/datasets/issues/2165,How to convert datasets.arrow_dataset.Dataset to torch.utils.data.Dataset,"['Hi,\r\n\r\na HF dataset can be converted to a Torch Dataset with a simple wrapper as follows:\r\n```python\r\nfrom torch.utils.data import Dataset\r\n \r\nclass HFDataset(Dataset):\r\n    def __init__(self, dset):\r\n        self.dset = dset\r\n\r\n    def __getitem__(self, idx):\r\n        return self.dset[idx]\r\n\r\n    def __len__(self):\r\n        return len(self.dset)\r\n\r\ntrain_ds = HFDataset(train_ds)\r\n```\r\n@lhoestq Since the Arrow Dataset already provides `__getitem__` and `__len__`, I think we could use the [virtual subclass](https://docs.python.org/3/library/abc.html#abc.ABCMeta.register) mechanism from the `abc` module to elegantly solve this issue. This mechanism would allow the Arrow Dataset to be used in place of the Torch Dataset because the `isinstance(instance of Arrow Dataset, TorchDataset)` check would return True (DeepSpeed has this check [here](https://github.com/microsoft/DeepSpeed/blob/ab5534fc4c0f8ca21ada321f9730d723aa31288b/deepspeed/runtime/engine.py#L823)).\r\n\r\nAnd it requires a minimal change in the `arrow_dataset.py` file:\r\n```python\r\nif config.TORCH_AVAILABLE:\r\n    from torch.utils.data import Dataset as TorchDataset\r\n    TorchDataset.register(Dataset)\r\n```'
 'Interesting ! Thanks for sharing this @mariosasko . I like the idea\r\nThis looks like something we should add IMO'
 ""@mariosasko \r\nThx for your code!\r\nIt perfectly works with a small modification for HF NLP dataset:\r\n```\r\noriginal_ds = nlp.load_dataset('scientific_papers', 'arxiv')\r\ntrain_ds = HFDataset(train_ds['train']) # needs splitting\r\n```""
 ""@lhoestq Sadly, from Python 3.7 onwards `torch.utils.data.Dataset` doesn't support the virtual subclass mechanism due to `typing.Generic` type no longer having `abc.ABCMeta` as its metaclass.\r\n\r\nWith that in mind, another option is to remove a direct type check (`isinstance(dataset, torch.utils.data.Dataset)`) in `deepspeed.initalize` and to rewrite the checks in a manner similar to `torch.utils.data.DataLoader` ([link](https://github.com/pytorch/pytorch/blob/b80c6f863f2327c712c478f67c248b94d66b65ac/torch/utils/data/dataloader.py#L197-L239)). This is exactly why the `DataLoader` works with arbitrary objects that provide `__getitem__` and `__len__` (and in our case, the `ArrowDataset`). By doing so, their code wouldn't be any stricter in comparison to the `DataLoader`.\r\n\r\nSo if you agree, I can open an issue in their repo and fix this if they like the idea.""
 'That makes sense ! Feel free to open an issue on their repo and discuss this idea'
 '@y-rokutan Hi, now if you install `deepspeed` from master (this feature will be available in the next official release), the code should work without subclassing. Let us know if you still have any issues.'
 ""Worth mentioning that any function that expects a `torch..Dataset` (like `torch..DataLoader`) will fail a mypy-esque typecheck if a `datasets.Dataset` is passed, even though it implements the interface correctly (I think). The virtual subclass idea was a good one- I wonder if there's another workaround given the Generic issue. What we're really talking about is something similar to the structural subtyping semantics that `typing.Protocol` defines. If `torch..DataLoader` accepted anything that supports `__getitem__` and `__len__` methods this would be much easier. Not sure if there's a way to do this without the wrapper from the perspective of `datasets`.""]","Hi,

I'm trying to pretraine deep-speed model using HF arxiv dataset like:
```
train_ds = nlp.load_dataset('scientific_papers', 'arxiv')
train_ds.set_format(
        type=""torch"",
        columns=[""input_ids"", ""attention_mask"", ""global_attention_mask"", ""labels""],
    )
engine, _, _, _ = deepspeed.initialize(
    args=args,
    model=model,
    model_parameters=[p for p in model.parameters() if p.requires_grad],
    training_data=train_ds)
```
but deepspeed.initialize accepts torch.utils.data.Dataset only. How can I convert HF-style dataset to torch-style dataset?
"
https://github.com/huggingface/datasets/issues/2162,visualization for cc100 is broken ,"['This looks like an issue with the cc100 dataset itself but not sure\r\nDid you try loading cc100 on your machine ?'
 'Hi\nloading works fine, but the viewer only is broken\nthanks\n\nOn Wed, Apr 7, 2021 at 12:17 PM Quentin Lhoest ***@***.***>\nwrote:\n\n> This looks like an issue with the cc100 dataset itself but not sure\n> Did you try loading cc100 on your machine ?\n>\n> —\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/huggingface/datasets/issues/2162#issuecomment-814793809>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AS37NMRUO33JSOYGT6RETWLTHQWNLANCNFSM42IUOR6Q>\n> .\n>\n']","Hi
visualization through dataset viewer for cc100 is broken
https://huggingface.co/datasets/viewer/

thanks a lot
"
https://github.com/huggingface/datasets/issues/2161,any possibility to download part of large datasets only?,"['Not yet but it’s on the short/mid-term roadmap (requested by many indeed).'
 'oh, great, really awesome feature to have, thank you very much for the great, fabulous work'
 ""We'll work on dataset streaming soon. This should allow you to only load the examples you need ;)""
 ""thanks a lot Quentin, this would be really really a great feature to have\n\nOn Wed, Apr 7, 2021 at 12:14 PM Quentin Lhoest ***@***.***>\nwrote:\n\n> We'll work on dataset streaming soon. This should allow you to only load\n> the examples you need ;)\n>\n> —\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/huggingface/datasets/issues/2161#issuecomment-814791922>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AS37NMROD62QAKIJMAKWISTTHQWBVANCNFSM42IUI5JQ>\n> .\n>\n""
 'Is streaming completed? On the 1.8.0 docs it is mentioned (https://huggingface.co/docs/datasets/dataset_streaming.html), but when following the example I get the following error:\r\n\r\n```\r\n>>> dataset2 = load_dataset(""amazon_us_reviews"", ""Pet_Products_v1_00"", split=\'train\', streaming=True)\r\n\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-21-1eedab26cff1> in <module>()\r\n----> 1 en_dataset = load_dataset(\'oscar\', ""unshuffled_deduplicated_en"", split=\'train\', streaming=True)\r\n\r\n3 frames\r\n/usr/local/lib/python3.7/dist-packages/datasets/builder.py in _create_builder_config(self, name, custom_features, **config_kwargs)\r\n    339                 if value is not None:\r\n    340                     if not hasattr(builder_config, key):\r\n--> 341                         raise ValueError(f""BuilderConfig {builder_config} doesn\'t have a \'{key}\' key."")\r\n    342                     setattr(builder_config, key, value)\r\n    343 \r\n\r\nValueError: BuilderConfig OscarConfig(name=\'unshuffled_deduplicated_en\', version=1.0.0, data_dir=None, data_files=None, description=\'Unshuffled and deduplicated, English OSCAR dataset\') doesn\'t have a \'streaming\' key.\r\n```\r\n\r\nUPDATE: Managed to get streaming working by building from source and installing the additional `datasets[streaming]` package:\r\n\r\n```\r\n!pip install git+https://github.com/huggingface/datasets.git\r\n!pip install datasets[streaming]\r\n```'
 ""Hi ! Streaming is available on `master` only right now. We'll make a new release 1.9.0 on Monday :)""]","Hi
Some of the datasets I need like cc100 are very large, and then I wonder if I can download first X samples of the shuffled/unshuffled data without going through first downloading the whole data then sampling? thanks"
https://github.com/huggingface/datasets/issues/2160,data_args.preprocessing_num_workers almost freezes ,"['Hi.\r\nI cannot always reproduce this issue, and on later runs I did not see it so far. Sometimes also I set 8 processes but I see less being showed, is this normal, here only 5 are shown for 8 being set, thanks\r\n\r\n```\r\n#3:  11%|███████████████▊                                                                                                                                  | 172/1583 [00:46<06:21,  3.70ba/s]\r\n#4:   9%|█████████████▏                                                                                                                                    | 143/1583 [00:46<07:46,  3.09ba/s]\r\n#7:   6%|█████████                                                                                                                                          | 98/1583 [00:45<11:34,  2.14ba/s]\r\n#5:   8%|███████████▍                                                                                                                                      | 124/1583 [00:46<09:03,  2.68ba/s]\r\n#6:   7%|██████████▏  \r\n```'
 'closing since I cannot reproduce it again, thanks ']","Hi @lhoestq 

I am running this code from huggingface transformers https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py 

to speed up tokenization, since I am running on multiple datasets, I am using data_args.preprocessing_num_workers = 4 with opus100 corpus but this moves on till a point and then this freezes almost for sometime during  tokenization steps and then this is back again, overall to me taking more time than normal case, I appreciate your advice on how I can use this option properly to speed up.

thanks"
https://github.com/huggingface/datasets/issues/2159,adding ccnet dataset,"['closing since I think this is cc100, just the name has been changed. thanks ']","## Adding a Dataset
- **Name:**  ccnet

- **Description:** 
Common Crawl

- **Paper:** 
https://arxiv.org/abs/1911.00359

- **Data:** 
https://github.com/facebookresearch/cc_net

- **Motivation:**
this is one of the most comprehensive clean monolingual datasets across a variety of languages. Quite important for cross-lingual reseach

Instructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).

thanks"
https://github.com/huggingface/datasets/issues/2158,"viewer ""fake_news_english"" error","[""Thanks for reporting !\r\nThe viewer doesn't have all the dependencies of the datasets. We may add openpyxl to be able to show this dataset properly""]","When I visit the [Huggingface - viewer](https://huggingface.co/datasets/viewer/) web site, under the dataset ""fake_news_english"" I've got this error:

> ImportError: To be able to use this dataset, you need to install the following dependencies['openpyxl'] using 'pip install # noqa: requires this pandas optional dependency for reading xlsx files' for instance'

as well as the error Traceback.

"
https://github.com/huggingface/datasets/issues/2153,load_dataset ignoring features,"['Hi ! Thanks for reporting. I opened a PR to fix this issue: #2201'
 'Nice question which helped me a lot! I have wasted a lot of time to the `DatasetDict` creation from a csv file. Hope the document of this module add some simple examples.'
 ""Hi :) We're indeed working on tutorials that we will add to the docs !""]","First of all, I'm sorry if it is a repeated issue or the changes are already in master, I searched and I didn't find anything. 

I'm using datasets 1.5.0

![image](https://user-images.githubusercontent.com/37592763/113114369-8f376580-920b-11eb-900d-94365b59f04b.png)

As you can see, when I load the dataset, the ClassLabels are ignored, I have to cast the dataset in order to make it work.

Code to reproduce:

```python
import datasets
data_location = ""/data/prueba_multiclase""
features = datasets.Features(
        {""texto"": datasets.Value(""string""), ""label"": datasets.features.ClassLabel(names=[""false"", ""true""])}
    )
dataset = datasets.load_dataset(
        ""csv"", data_files=data_location, delimiter=""\t"", features=features
    )
```

Dataset I used:


[prueba_multiclase.zip](https://github.com/huggingface/datasets/files/6235022/prueba_multiclase.zip) (it has to be unzipped)


Thank you! ❤️ 
"
https://github.com/huggingface/datasets/issues/2149,Telugu subset missing for xtreme tatoeba dataset,['Good catch ! Thanks for reporting\r\n\r\nI just opened #2180 to fix this'],"from nlp import load_dataset
train_dataset = load_dataset('xtreme', 'tatoeba.tel')['validation']
ValueError: BuilderConfig tatoeba.tel not found.

but language tel is actually included in xtreme:
https://github.com/google-research/xtreme/blob/master/utils_preprocess.py
def tatoeba_preprocess(args):
  lang3_dict = {
    'afr':'af', 'ara':'ar', 'bul':'bg', 'ben':'bn',
    'deu':'de', 'ell':'el', 'spa':'es', 'est':'et',
    'eus':'eu', 'pes':'fa', 'fin':'fi', 'fra':'fr',
    'heb':'he', 'hin':'hi', 'hun':'hu', 'ind':'id',
    'ita':'it', 'jpn':'ja', 'jav':'jv', 'kat':'ka',
    'kaz':'kk', 'kor':'ko', 'mal':'ml', 'mar':'mr',
    'nld':'nl', 'por':'pt', 'rus':'ru', 'swh':'sw',
    'tam':'ta', **_'tel':'te'_**, 'tha':'th', 'tgl':'tl', <----here
    'tur':'tr', 'urd':'ur', 'vie':'vi', 'cmn':'zh',
    'eng':'en',
  }"
https://github.com/huggingface/datasets/issues/2148,Add configurable options to `seqeval` metric,"['Hi @marrodion. \r\n\r\nThanks for pointing this out. It would be great to incorporate this metric-specific enhancement.\r\n\r\nAnother possibility would be to require the user to input the scheme as a string `mode=""strict"", scheme=""IOB2""` and then dynamically import the corresponding module using Python `importlib`:\r\n```python\r\nif scheme:\r\n    scheme = importlib.import_module(f""seqeval.scheme.{scheme}"")\r\n```\r\n\r\nFeel free to create a Pull Request to make this contribution.']","Right now `load_metric(""seqeval"")` only works in the default mode of evaluation (equivalent to conll evaluation).

However, seqeval library [supports](https://github.com/chakki-works/seqeval#support-features) different evaluation schemes (IOB1, IOB2, etc.), which can be plugged in just by supporting additional kwargs in `Seqeval._compute`
https://github.com/huggingface/datasets/blob/85cf7ff920c90ca2e12bedca12b36d2a043c3da2/metrics/seqeval/seqeval.py#L109

Things that would be relevant are, for example, supporting `mode=""strict"", scheme=IOB2` to count only full entity match as a true positive and omit partial matches.

The only problem I see is that the spirit of `metrics` seems to not require additional imports from user. `seqeval` only supports schemes as objects, without any string aliases. 

It can be solved naively with mapping like `{""IOB2"": seqeval.scheme.IOB2}`. Or just left as is and require user to explicitly import scheme from `seqeval` if he wants to configure it past the default implementation.

If that makes sense, I am happy to implement the change."
https://github.com/huggingface/datasets/issues/2146,Dataset file size on disk is very large with 3D Array,"['Hi ! In the arrow file we store all the integers as uint8.\r\nSo your arrow file should weigh around `height x width x n_channels x n_images` bytes.\r\n\r\nWhat feature type do your TFDS dataset have ?\r\n\r\nIf it uses a `tfds.features.Image` type, then what is stored is the encoded data (as png or jpg for example). Since these encodings are made for compression, the resulting tfrecord is smaller that the arrow file.\r\n\r\nWe are working on adding a similar feature in `datasets`: the ability to store the encoded data instead of the raw integers for images, but also for audio data. This way, arrow files will have similar sizes as tfrecords for images.'
 ""Thanks for the prompt response. You're right about the encoding, I have the `tfds.features.Image` feature type you mentioned.\r\nHowever, as described in the `dataset_info.json`, my dataset is made of 1479 (224x224x3) images. 1479 x 224 x 224 x 3 = 222630912 bytes which is far from the actual size 520803408 bytes. \r\n\r\nAnyway I look forward to the Image feature type in `datasets`. ""
 '@lhoestq I changed the data structure so I have a  2D Array feature type instead of a 3D Array by grouping the two last dimensions ( a 224x672 2D Array instead of a 224x224x3 3D Array). The file size is now  223973964 bytes, nearly half the previous size! Which is around of what I would expect.\r\nI found similar behavior in existing `datasets` collection, when comparing black and white vs color image, for example MNIST vs CIFAR. '
 'Interesting !\r\nThis may be because of the offsets that are stored with the array data.\r\n\r\nCurrently the offsets are stored even if the `shape` of the arrays is fixed. This was needed because of some issues with pyarrow a few months ago. I think these issues have been addressed now, so we can probably try to remove them to make the file lighter.\r\n\r\nIdeally in your case the floats data should be 220 MB for both Array2D and Array3D'
 'Yeah for sure, can you be a bit more specific about where the offset is stored in the code base ? And any reference to pyarrow issues if you have some. I would be very interested in contributing to `datasets` by trying to fix this issue. '
 'Pyarrow has two types of lists: variable length lists and fixed size lists.\r\nCurrently we store the ArrayXD data as variable length lists. They take more disk space because they must store both actual data and offsets.\r\nIn the `datasets` code this is done here:\r\n\r\nhttps://github.com/huggingface/nlp/blob/dbac87c8a083f806467f5afc4ec9b401a7e4c15c/src/datasets/features.py#L346-L352\r\n\r\nTo use a fixed length list, one should use the `list_size` argument of `pyarrow.list_()`.\r\nI believe this would work directly modulo some changes in the numpy conversion here:\r\n\r\nhttps://github.com/huggingface/nlp/blob/dbac87c8a083f806467f5afc4ec9b401a7e4c15c/src/datasets/features.py#L381-L395']","Hi, 

I have created my own dataset using the provided dataset loading script. It is an image dataset where images are stored as 3D Array with dtype=uint8. 

The actual size on disk is surprisingly large. It takes 520 MB. Here is some info from `dataset_info.json`. 

`{
    ""description"": """",
    ""citation"": """",
    ""homepage"": """",
    ""license"": """",
    ""features"": {
        ""image"": {
            ""shape"": [224, 224, 3],
            ""dtype"": ""uint8"",
            ""id"": null,
            ""_type"": ""Array3D"",
        }
    },
    ""post_processed"": null,
    ""supervised_keys"": null,
    ""builder_name"": ""shot_type_image_dataset"",
    ""config_name"": ""default"",
    ""version"": {
        ""version_str"": ""0.0.0"",
        ""description"": null,
        ""major"": 0,
        ""minor"": 0,
        ""patch"": 0,
    },
    ""splits"": {
        ""train"": {
            ""name"": ""train"",
            ""num_bytes"": 520803408,
            ""num_examples"": 1479,
            ""dataset_name"": ""shot_type_image_dataset"",
        }
    },
    ""download_checksums"": {
        """": {
            ""num_bytes"": 16940447118,
            ""checksum"": ""5854035705efe08b0ed8f3cf3da7b4d29cba9055c2d2d702c79785350d72ee03"",
        }
    },
    ""download_size"": 16940447118,
    ""post_processing_size"": null,
    ""dataset_size"": 520803408,
    ""size_in_bytes"": 17461250526,
}`

I have created the same dataset with tensorflow_dataset and it takes only 125MB on disk.

I am wondering, is it normal behavior ? I understand `Datasets` uses Arrow for serialization wheres tf uses TF Records.

This might be a problem for large dataset. 

Thanks for your help. 
"
https://github.com/huggingface/datasets/issues/2144,Loading wikipedia 20200501.en throws pyarrow related error,"[""That's how I loaded the dataset\r\n```python\r\nfrom datasets import load_dataset\r\nds = load_dataset('wikipedia', '20200501.en', cache_dir='/usr/local/workspace/NAS_NLP/cache')\r\n```""
 'Hi ! It looks like the arrow file in the folder\r\n`/usr/local/workspace/NAS_NLP/cache/wikipedia/20200501.en/1.0.0/50aa706aa417bb77d910ad61211cc672c0ef3e0f224225a5e0a18277ade8b931` is corrupted.\r\n\r\nCan you take a look and check that it\'s 18.3GB ?\r\n\r\nIf not, then maybe you need to redownload it:\r\n```python\r\nfrom datasets import load_dataset\r\nds = load_dataset(\'wikipedia\', \'20200501.en\', cache_dir=\'/usr/local/workspace/NAS_NLP/cache\', download_mode=""force_redownload"")\r\n```'
 '> Hi ! It looks like the arrow file in the folder\r\n> `/usr/local/workspace/NAS_NLP/cache/wikipedia/20200501.en/1.0.0/50aa706aa417bb77d910ad61211cc672c0ef3e0f224225a5e0a18277ade8b931` is corrupted.\r\n> \r\n> Can you take a look and check that it\'s 18.3GB ?\r\n> \r\n> If not, then maybe you need to redownload it:\r\n> \r\n> ```python\r\n> from datasets import load_dataset\r\n> ds = load_dataset(\'wikipedia\', \'20200501.en\', cache_dir=\'/usr/local/workspace/NAS_NLP/cache\', download_mode=""force_redownload"")\r\n> ```\r\n\r\nHi Ihoestq, thanks for the reply! Actually i think my issue is i couldn\'t download the dataset beyond 10.7G. It feels like the whole dataset is split into different volumes and after the first one was downloaded it crashed before proceeding to the next one. I did try \'force_redownload\' mode but still got the same issue.'
 'I just tried on my side and got no issues.\r\nWhen downloading the dataset again, did it crash at 10.7GB as well ?'
 '> I just tried on my side and got no issues.\r\n> When downloading the dataset again, did it crash at 10.7GB as well ?\r\n\r\nYes i have tried it multiple times on different machines. I am wondering if you could share the screenshot of your dependency versions and i will try to make them the same as yours?'
 'I tried using `datasets` from `master` on macos with python 3.7.2\r\nI also have `requests==2.23.0` and `tqdm==4.45.0`.']","**Problem description**
I am getting the following error when trying to load wikipedia/20200501.en dataset.

**Error log**
Downloading and preparing dataset wikipedia/20200501.en (download: 16.99 GiB, generated: 17.07 GiB, post-processed: Unknown size, total: 34.06 GiB) to /usr/local/workspace/NAS_NLP/cache/wikipedia/20200501.en/1.0.0/50aa706aa417bb77d910ad61211cc672c0ef3e0f224225a5e0a18277ade8b931...
Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14.6k/14.6k [00:00<00:00, 5.41MB/s]
Downloading:  59%|███████████████████████████████████████████████████████████████████████████████████████▊                                                              | 10.7G/18.3G [11:30<08:08, 15.5MB/s]
Dataset wikipedia downloaded and prepared to /usr/local/workspace/NAS_NLP/cache/wikipedia/20200501.en/1.0.0/50aa706aa417bb77d910ad61211cc672c0ef3e0f224225a5e0a18277ade8b931. Subsequent calls will reuse this data.
Traceback (most recent call last):
  File ""load_wiki.py"", line 2, in <module>
    ds = load_dataset('wikipedia', '20200501.en', cache_dir='/usr/local/workspace/NAS_NLP/cache')
  File ""/usr/local/lib/python3.6/dist-packages/datasets/load.py"", line 751, in load_dataset
    ds = builder_instance.as_dataset(split=split, ignore_verifications=ignore_verifications, in_memory=keep_in_memory)
  File ""/usr/local/lib/python3.6/dist-packages/datasets/builder.py"", line 746, in as_dataset
    map_tuple=True,
  File ""/usr/local/lib/python3.6/dist-packages/datasets/utils/py_utils.py"", line 204, in map_nested
    _single_map_nested((function, obj, types, None, True)) for obj in tqdm(iterable, disable=disable_tqdm)
  File ""/usr/local/lib/python3.6/dist-packages/datasets/utils/py_utils.py"", line 204, in <listcomp>
    _single_map_nested((function, obj, types, None, True)) for obj in tqdm(iterable, disable=disable_tqdm)
  File ""/usr/local/lib/python3.6/dist-packages/datasets/utils/py_utils.py"", line 142, in _single_map_nested
    return function(data_struct)
  File ""/usr/local/lib/python3.6/dist-packages/datasets/builder.py"", line 763, in _build_single_dataset
    in_memory=in_memory,
  File ""/usr/local/lib/python3.6/dist-packages/datasets/builder.py"", line 835, in _as_dataset
    in_memory=in_memory,
  File ""/usr/local/lib/python3.6/dist-packages/datasets/arrow_reader.py"", line 215, in read
    return self.read_files(files=files, original_instructions=instructions, in_memory=in_memory)
  File ""/usr/local/lib/python3.6/dist-packages/datasets/arrow_reader.py"", line 236, in read_files
    pa_table = self._read_files(files, in_memory=in_memory)
  File ""/usr/local/lib/python3.6/dist-packages/datasets/arrow_reader.py"", line 171, in _read_files
    pa_table: pa.Table = self._get_dataset_from_filename(f_dict, in_memory=in_memory)
  File ""/usr/local/lib/python3.6/dist-packages/datasets/arrow_reader.py"", line 302, in _get_dataset_from_filename
    pa_table = ArrowReader.read_table(filename, in_memory=in_memory)
  File ""/usr/local/lib/python3.6/dist-packages/datasets/arrow_reader.py"", line 324, in read_table
    pa_table = f.read_all()
  File ""pyarrow/ipc.pxi"", line 544, in pyarrow.lib.RecordBatchReader.read_all
  File ""pyarrow/error.pxi"", line 99, in pyarrow.lib.check_status
OSError: Expected to be able to read 9176784 bytes for message body, got 4918712

**Detailed version info**
datasets==1.5.0
  - dataclasses [required: Any, installed: 0.8]
  - dill [required: Any, installed: 0.3.3]
  - fsspec [required: Any, installed: 0.8.7]
    - importlib-metadata [required: Any, installed: 1.7.0]
      - zipp [required: >=0.5, installed: 3.1.0]
  - huggingface-hub [required: <0.1.0, installed: 0.0.7]
    - filelock [required: Any, installed: 3.0.12]
    - importlib-metadata [required: Any, installed: 1.7.0]
      - zipp [required: >=0.5, installed: 3.1.0]
    - requests [required: Any, installed: 2.24.0]
      - certifi [required: >=2017.4.17, installed: 2020.6.20]
      - chardet [required: >=3.0.2,<4, installed: 3.0.4]
      - idna [required: >=2.5,<3, installed: 2.6]
      - urllib3 [required: >=1.21.1,<1.26,!=1.25.1,!=1.25.0, installed: 1.25.10]
    - tqdm [required: Any, installed: 4.49.0]
  - importlib-metadata [required: Any, installed: 1.7.0]
    - zipp [required: >=0.5, installed: 3.1.0]
  - multiprocess [required: Any, installed: 0.70.11.1]
    - dill [required: >=0.3.3, installed: 0.3.3]
  - numpy [required: >=1.17, installed: 1.17.0]
  - pandas [required: Any, installed: 1.1.5]
    - numpy [required: >=1.15.4, installed: 1.17.0]
    - python-dateutil [required: >=2.7.3, installed: 2.8.0]
      - six [required: >=1.5, installed: 1.15.0]
    - pytz [required: >=2017.2, installed: 2020.1]
  - pyarrow [required: >=0.17.1, installed: 3.0.0]
    - numpy [required: >=1.16.6, installed: 1.17.0]
  - requests [required: >=2.19.0, installed: 2.24.0]
    - certifi [required: >=2017.4.17, installed: 2020.6.20]
    - chardet [required: >=3.0.2,<4, installed: 3.0.4]
    - idna [required: >=2.5,<3, installed: 2.6]
    - urllib3 [required: >=1.21.1,<1.26,!=1.25.1,!=1.25.0, installed: 1.25.10]
  - tqdm [required: >=4.27,<4.50.0, installed: 4.49.0]
  - xxhash [required: Any, installed: 2.0.0]
"
https://github.com/huggingface/datasets/issues/2139,TypeError when using save_to_disk in a dataset loaded with ReadInstruction split,"['Hi !\r\nI think this has been fixed recently on `master`.\r\nCan you try again by installing `datasets` from `master` ?\r\n```\r\npip install git+https://github.com/huggingface/datasets.git\r\n```'
 'Hi!\r\n\r\nUsing that version of the code solves the issue. Thanks!']","Hi,

Loading a dataset with `load_dataset` using a split defined via `ReadInstruction` and then saving it to disk results in the following error: `TypeError: Object of type ReadInstruction is not JSON serializable`.

Here is the minimal reproducible example:

```python
from datasets import load_dataset
from datasets import ReadInstruction

data_1 = load_dataset(
    ""wikiann"",
    ""en"",
    split=""validation"",
)

data_1.save_to_disk(""temporary_path_1"")

print(""Save with regular split works."")

data_2 = load_dataset(
    ""wikiann"",
    ""en"",
    split=ReadInstruction(""validation"", to=50, unit=""%""),
)

data_2.save_to_disk(""temporary_path_2"")
```

and the corresponding output:

```
Reusing dataset wikiann (/xxxxx/.cache/huggingface/datasets/wikiann/en/1.1.0/0b11a6fb31eea02f38ca17610657bfba3206100685283014daceb8da291c3be9)
Save with regular split works.
Reusing dataset wikiann (/xxxxx/.cache/huggingface/datasets/wikiann/en/1.1.0/0b11a6fb31eea02f38ca17610657bfba3206100685283014daceb8da291c3be9)
Traceback (most recent call last):
  File ""bug.py"", line 20, in <module>
    data_2.save_to_disk(""temporary_path_2"")
  File ""/xxxxx/lib/python3.7/site-packages/datasets/arrow_dataset.py"", line 645, in save_to_disk
    json.dump(state, state_file, indent=2, sort_keys=True)
  File ""/usr/lib/python3.7/json/__init__.py"", line 179, in dump
    for chunk in iterable:
  File ""/usr/lib/python3.7/json/encoder.py"", line 431, in _iterencode
    yield from _iterencode_dict(o, _current_indent_level)
  File ""/usr/lib/python3.7/json/encoder.py"", line 405, in _iterencode_dict
    yield from chunks
  File ""/usr/lib/python3.7/json/encoder.py"", line 438, in _iterencode
    o = _default(o)
  File ""/usr/lib/python3.7/json/encoder.py"", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type ReadInstruction is not JSON serializable
```

Let me know if there is some misuse from my end.

Thanks in advance.
 "
https://github.com/huggingface/datasets/issues/2135,en language data from MLQA dataset is missing,"[""Hi ! Indeed only the languages of the `translate-train` data are included...\r\nI can't find a link to download the english train set on https://github.com/facebookresearch/MLQA though, do you know where we can download it ?""
 'Hi @lhoestq \r\nthank you very much for coming back to me, now I see, you are right, in the link you sent I see split of  {split}-context-{context_language}-question-{question_language}.json with context_language=question_language=en, TFDS most probably has extracted english ones from these files as en language files, but translate-train/test do not have en indeed. thanks a lot for the great explanations'
 'I close the ticket, since I do not see any en existing, they have trained on ""SQuAD V1.1"" instead. Thanks. ']","Hi
I need mlqa-translate-train.en dataset, but it is missing from the MLQA dataset. could you have a look please? @lhoestq  thank you for your help to fix this issue. "
https://github.com/huggingface/datasets/issues/2134,Saving large in-memory datasets with save_to_disk crashes because of pickling,"['Hi !\r\nIndeed `save_to_disk` doesn\'t call pickle anymore. Though the `OverflowError` can still appear for in-memory datasets bigger than 4GB. This happens when doing this for example:\r\n```python\r\nimport pyarrow as pa\r\nimport pickle\r\n\r\narr = pa.array([0] * ((4 * 8 << 30) // 64))\r\ntable = pa.Table.from_arrays([a], names=[""foo""])\r\npickle.dumps(table)  # fails with an OverflowError\r\npickle.dumps(table, 4)  # works !\r\n```\r\nWe\'ll do the change to use `protocol=4`.\r\n\r\nMoreover I\'ve also seen other users complain about this error\r\n```\r\nstruct.error: \'I\' format requires 0 <= number <= 4294967295\r\n```\r\n\r\nIt looks like something related to the 4GB limit as well but I\'m not able to reproduce on my side.\r\nDo you think you can provide a script that reproduces the issue ?\r\nHow big is your dataset ? (number of bytes, number of rows)\r\n\r\n'
 'Hi!\r\nSo I\'ve managed to created a minimum working (well technically crashing) example for the multiprocessing case, I create a huge list of zeros, like in your example, and then I try to .map(None, num_proc=2) over it, which then crashes, here\'s the code:\r\n\r\n```python\r\nfrom datasets import  Dataset\r\n\r\nif __name__ == \'__main__\':\r\n    ton_of_zeroes = [0] * ((12 * 8 << 30) // 64)\r\n    large_dataset = Dataset.from_dict({\'col\': ton_of_zeroes})\r\n    print(""Start"")\r\n    large_dataset.map(function=None, num_proc=2)\r\n    print(""Done - should not print"")\r\n```\r\n\r\nThe amount of zeros could probably be reduced, I haven\'t tried to minimize it to find the breaking point, I just increased it from your code (which by quick glance I assumed tried to allocate over 4 GiB)\r\n\r\nRunning this results in the following traceback:\r\n\r\n```\r\nParameter \'indices\'=[        0         1         2 ... 805306365 805306366 805306367] of the transform datasets.arrow_dataset.Dataset.select couldn\'t be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won\'t be showed.\r\nTraceback (most recent call last):\r\n  File ""./crash_multiproc_pickle.py"", line 7, in <module>\r\n    large_dataset.map(function=None, num_proc=2)\r\n  File ""/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/datasets/arrow_dataset.py"", line 1485, in map\r\n    transformed_shards = [r.get() for r in results]\r\n  File ""/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/datasets/arrow_dataset.py"", line 1485, in <listcomp>\r\n    transformed_shards = [r.get() for r in results]\r\n  File ""/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/multiprocess/pool.py"", line 657, in get\r\n    raise self._value\r\n  File ""/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/multiprocess/pool.py"", line 431, in _handle_tasks\r\n    put(task)\r\n  File ""/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/multiprocess/connection.py"", line 209, in send\r\n    self._send_bytes(_ForkingPickler.dumps(obj))\r\n  File ""/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/multiprocess/reduction.py"", line 54, in dumps\r\n    cls(buf, protocol, *args, **kwds).dump(obj)\r\n  File ""/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/dill/_dill.py"", line 454, in dump\r\n    StockPickler.dump(self, obj)\r\n  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 437, in dump\r\n    self.save(obj)\r\n  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 789, in save_tuple\r\n    save(element)\r\n  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File ""/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/dill/_dill.py"", line 941, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 885, in _batch_setitems\r\n    save(v)\r\n  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 662, in save_reduce\r\n    save(state)\r\n  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File ""/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/dill/_dill.py"", line 941, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 859, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 885, in _batch_setitems\r\n    save(v)\r\n  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 638, in save_reduce\r\n    save(args)\r\n  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 774, in save_tuple\r\n    save(element)\r\n  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 819, in save_list\r\n    self._batch_appends(obj)\r\n  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 846, in _batch_appends\r\n    save(tmp[0])\r\n  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 638, in save_reduce\r\n    save(args)\r\n  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 774, in save_tuple\r\n    save(element)\r\n  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 819, in save_list\r\n    self._batch_appends(obj)\r\n  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 846, in _batch_appends\r\n    save(tmp[0])\r\n  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 638, in save_reduce\r\n    save(args)\r\n  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 774, in save_tuple\r\n    save(element)\r\n  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 789, in save_tuple\r\n    save(element)\r\n  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 819, in save_list\r\n    self._batch_appends(obj)\r\n  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 843, in _batch_appends\r\n    save(x)\r\n  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 549, in save\r\n    self.save_reduce(obj=obj, *rv)\r\n  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 638, in save_reduce\r\n    save(args)\r\n  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 774, in save_tuple\r\n    save(element)\r\n  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save\r\n    f(self, obj) # Call unbound method with explicit self\r\n  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 732, in save_bytes\r\n    self._write_large_bytes(BINBYTES + pack(""<I"", n), obj)\r\nstruct.error: \'I\' format requires 0 <= number <= 4294967295\r\n```\r\n\r\nMy datasets usually have hundreds of thousands to low millions of rows, with each row containing a list of 10 strings and list of vectors of different length (the strings tokenized), which in the worst case have 10\\*512\\*8 = 40960 bytes (but usually it is much smaller, as the vectors tend to be shorter. I need these groups of text lines to create training data for the Inverse Cloze Task.\r\n\r\nAnyway I don\'t think my particular dataset is relevant, as the tiny script I created also manages to crash.\r\nBut I think the issue is the same as the save_to_disk, from the traceback it seems that in multiprocessing, it tries to use dill to return the result of the map workers, which tries to pickle the data and can\'t do it, probably because it\'s again using the older pickle protocol. That\'s my guess anyway.'
 'I just merged a fix #2150 that allows to pickle tables bigger than 4GiB\r\nFeel free to try it on the `master` branch !'
 'awesome! I started getting this error as well when I tried to tokenize with a longer sequence length'
 '@prokopCerny does this fix work for you? I found that with the latest master, my container with 500GB RAM starts crashing when I try to map a large dataset using `num_proc`.\r\n\r\n@lhoestq would it be possible to implement some logic to keep the individual cache files small (say below 100mb)? I find this helps with loading large datasets, but the ""hack"" I was using (increasing `num_proc` to a large number) doesn\'t work anymore with the latest master; my container crashes even with `num_proc=200` now'
 'Closing since the original issue was fixed in #2150 \r\nFeel free to reopen if you are still experiencing it.\r\nFor the other problems, please open separate issues']","Using Datasets 1.5.0 on Python 3.7.
Recently I've been working on medium to large size datasets (pretokenized raw text sizes from few gigabytes to low tens of gigabytes), and have found out that several preprocessing steps are massively faster when done in memory, and I have the ability to requisition a lot of RAM, so I decided to do these steps completely out of the datasets library.

 So my workflow is to do several .map() on datasets object, then for the operation which is faster in memory to extract the necessary columns from the dataset and then drop it whole, do the transformation in memory, and then create a fresh Dataset object using .from_dict() or other method. 

When I then try to call save_to_disk(path) on the dataset, it crashes because of pickling, which appears to be because of using old pickle protocol which doesn't support large files (over 4 GiB).
```
Traceback (most recent call last):
  File ""./tokenize_and_chunkify_in_memory.py"", line 80, in <module>
    main()
  File ""./tokenize_and_chunkify_in_memory.py"", line 75, in main
    tokenize_and_chunkify(config)
  File ""./tokenize_and_chunkify_in_memory.py"", line 60, in tokenize_and_chunkify
    contexts_dataset.save_to_disk(chunked_path)
  File ""/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/datasets/arrow_dataset.py"", line 457, in save_to_disk
    self = pickle.loads(pickle.dumps(self))
OverflowError: cannot serialize a bytes object larger than 4 GiB
```
From what I've seen this issue may be possibly fixed, as the line `self = pickle.loads(pickle.dumps(self))` does not appear to be present in the current state of the repository.

To save these datasets to disk, I've resorted to calling .map() over them with `function=None` and specifying the .arrow cache file, and then creating a new dataset using the .from_file() method, which I can then safely save to disk.

Additional issue when working with these large in-memory datasets is when using multiprocessing, is again to do with pickling. I've tried to speed up the mapping with function=None by specifying num_proc to the available cpu count, and I again get issues with transferring the dataset, with the following traceback. I am not sure if I should open a separate issue for that.
```
Traceback (most recent call last):
  File ""./tokenize_and_chunkify_in_memory.py"", line 94, in <module>
    main()
  File ""./tokenize_and_chunkify_in_memory.py"", line 89, in main
    tokenize_and_chunkify(config)
  File ""./tokenize_and_chunkify_in_memory.py"", line 67, in tokenize_and_chunkify
    contexts_dataset.map(function=None, cache_file_name=str(output_dir_path / ""tmp.arrow""), writer_batch_size=50000, num_proc=config.threads)
  File ""/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/datasets/arrow_dataset.py"", line 1485, in map
    transformed_shards = [r.get() for r in results]
  File ""/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/datasets/arrow_dataset.py"", line 1485, in <listcomp>
    transformed_shards = [r.get() for r in results]
  File ""/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/multiprocess/pool.py"", line 657, in get
    raise self._value
  File ""/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/multiprocess/pool.py"", line 431, in _handle_tasks
    put(task)
  File ""/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/multiprocess/connection.py"", line 209, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File ""/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/multiprocess/reduction.py"", line 54, in dumps
    cls(buf, protocol, *args, **kwds).dump(obj)
  File ""/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/dill/_dill.py"", line 454, in dump
    StockPickler.dump(self, obj)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 437, in dump
    self.save(obj)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 789, in save_tuple
    save(element)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/dill/_dill.py"", line 941, in save_module_dict
    StockPickler.save_dict(pickler, obj)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 859, in save_dict
    self._batch_setitems(obj.items())
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 885, in _batch_setitems
    save(v)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 549, in save
    self.save_reduce(obj=obj, *rv)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 662, in save_reduce
    save(state)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/dill/_dill.py"", line 941, in save_module_dict
    StockPickler.save_dict(pickler, obj)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 859, in save_dict
    self._batch_setitems(obj.items())
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 885, in _batch_setitems
    save(v)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 549, in save
    self.save_reduce(obj=obj, *rv)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 638, in save_reduce
    save(args)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 774, in save_tuple
    save(element)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 819, in save_list
    self._batch_appends(obj)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 843, in _batch_appends
    save(x)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 549, in save
    self.save_reduce(obj=obj, *rv)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 638, in save_reduce
    save(args)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 774, in save_tuple
    save(element)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 819, in save_list
    self._batch_appends(obj)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 846, in _batch_appends
    save(tmp[0])
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 549, in save
    self.save_reduce(obj=obj, *rv)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 638, in save_reduce
    save(args)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 774, in save_tuple
    save(element)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 789, in save_tuple
    save(element)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 819, in save_list
    self._batch_appends(obj)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 846, in _batch_appends
    save(tmp[0])
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 789, in save_tuple
    save(element)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 819, in save_list
    self._batch_appends(obj)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 846, in _batch_appends
    save(tmp[0])
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 789, in save_tuple
    save(element)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 819, in save_list
    self._batch_appends(obj)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 843, in _batch_appends
    save(x)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 549, in save
    self.save_reduce(obj=obj, *rv)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 638, in save_reduce
    save(args)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 774, in save_tuple
    save(element)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 732, in save_bytes
    self._write_large_bytes(BINBYTES + pack(""<I"", n), obj)
struct.error: 'I' format requires 0 <= number <= 4294967295Traceback (most recent call last):
  File ""./tokenize_and_chunkify_in_memory.py"", line 94, in <module>
    main()
  File ""./tokenize_and_chunkify_in_memory.py"", line 89, in main
    tokenize_and_chunkify(config)
  File ""./tokenize_and_chunkify_in_memory.py"", line 67, in tokenize_and_chunkify
    contexts_dataset.map(function=None, cache_file_name=str(output_dir_path / ""tmp.arrow""), writer_batch_size=50000, num_proc=config.threads)
  File ""/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/datasets/arrow_dataset.py"", line 1485, in map
    transformed_shards = [r.get() for r in results]
  File ""/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/datasets/arrow_dataset.py"", line 1485, in <listcomp>
    transformed_shards = [r.get() for r in results]
  File ""/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/multiprocess/pool.py"", line 657, in get
    raise self._value
  File ""/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/multiprocess/pool.py"", line 431, in _handle_tasks
    put(task)
  File ""/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/multiprocess/connection.py"", line 209, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File ""/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/multiprocess/reduction.py"", line 54, in dumps
    cls(buf, protocol, *args, **kwds).dump(obj)
  File ""/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/dill/_dill.py"", line 454, in dump
    StockPickler.dump(self, obj)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 437, in dump
    self.save(obj)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 789, in save_tuple
    save(element)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/dill/_dill.py"", line 941, in save_module_dict
    StockPickler.save_dict(pickler, obj)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 859, in save_dict
    self._batch_setitems(obj.items())
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 885, in _batch_setitems
    save(v)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 549, in save
    self.save_reduce(obj=obj, *rv)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 662, in save_reduce
    save(state)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/home/cernypro/dev/envs/huggingface_gpu/lib/python3.7/site-packages/dill/_dill.py"", line 941, in save_module_dict
    StockPickler.save_dict(pickler, obj)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 859, in save_dict
    self._batch_setitems(obj.items())
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 885, in _batch_setitems
    save(v)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 549, in save
    self.save_reduce(obj=obj, *rv)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 638, in save_reduce
    save(args)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 774, in save_tuple
    save(element)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 819, in save_list
    self._batch_appends(obj)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 843, in _batch_appends
    save(x)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 549, in save
    self.save_reduce(obj=obj, *rv)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 638, in save_reduce
    save(args)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 774, in save_tuple
    save(element)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 819, in save_list
    self._batch_appends(obj)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 846, in _batch_appends
    save(tmp[0])
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 549, in save
    self.save_reduce(obj=obj, *rv)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 638, in save_reduce
    save(args)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 774, in save_tuple
    save(element)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 789, in save_tuple
    save(element)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 819, in save_list
    self._batch_appends(obj)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 846, in _batch_appends
    save(tmp[0])
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 789, in save_tuple
    save(element)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 819, in save_list
    self._batch_appends(obj)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 846, in _batch_appends
    save(tmp[0])
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 789, in save_tuple
    save(element)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 819, in save_list
    self._batch_appends(obj)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 843, in _batch_appends
    save(x)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 549, in save
    self.save_reduce(obj=obj, *rv)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 638, in save_reduce
    save(args)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 774, in save_tuple
    save(element)
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/mnt/appl/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/pickle.py"", line 732, in save_bytes
    self._write_large_bytes(BINBYTES + pack(""<I"", n), obj)
struct.error: 'I' format requires 0 <= number <= 4294967295
```"
https://github.com/huggingface/datasets/issues/2133,bug in mlqa dataset ,"['If you print those questions, you get readable texts:\r\n```python\r\n>>> questions = [\r\n...     ""\\u0645\\u062a\\u0649 \\u0628\\u062f\\u0627\\u062a \\u0627\\u0644\\u0645\\u062c\\u0644\\u0629 \\u0627\\u0644\\u0645\\u062f\\u0631\\u0633\\u064a\\u0629 \\u0641\\u064a \\u0646\\u0648\\u062a\\u0631\\u062f\\u0627\\u0645 \\u0628\\u0627\\u0644\\u0646\\u0634\\u0631?"",\r\n...     ""\\u0643\\u0645 \\u0645\\u0631\\u0629 \\u064a\\u062a\\u0645 \\u0646\\u0634\\u0631\\u0647\\u0627 \\u0641\\u064a \\u0646\\u0648\\u062a\\u0631\\u062f\\u0627\\u0645?"",\r\n...     ""\\u0645\\u0627 \\u0647\\u064a \\u0627\\u0644\\u0648\\u0631\\u0642\\u0629 \\u0627\\u0644\\u064a\\u0648\\u0645\\u064a\\u0629 \\u0644\\u0644\\u0637\\u0644\\u0627\\u0628 \\u0641\\u064a \\u0646\\u0648\\u062a\\u0631\\u062f\\u0627\\u0645?"",\r\n...     ""\\u0643\\u0645 \\u0639\\u062f\\u062f \\u0627\\u0644\\u0627\\u0648\\u0631\\u0627\\u0642 \\u0627\\u0644\\u0627\\u062e\\u0628\\u0627\\u0631\\u064a\\u0629 \\u0644\\u0644\\u0637\\u0644\\u0627\\u0628 \\u0627\\u0644\\u062a\\u064a \\u0648\\u062c\\u062f\\u062a \\u0641\\u064a \\u0646\\u0648\\u062a\\u0631\\u062f\\u0627\\u0645?"",\r\n...     ""\\u0641\\u064a \\u0627\\u064a \\u0633\\u0646\\u0629 \\u0628\\u062f\\u0627\\u062a \\u0648\\u0631\\u0642\\u0629 \\u0627\\u0644\\u0637\\u0627\\u0644\\u0628 \\u0627\\u0644\\u062d\\u0633 \\u0627\\u0644\\u0633\\u0644\\u064a\\u0645 \\u0628\\u0627\\u0644\\u0646\\u0634\\u0631 \\u0641\\u064a \\u0646\\u0648\\u062a\\u0631\\u062f\\u0627\\u0645?""\r\n... ]\r\n>>> print(questions)\r\n[\'متى بدات المجلة المدرسية في نوتردام بالنشر?\', \'كم مرة يتم نشرها في نوتردام?\', \'ما هي الورقة اليومية للطلاب في نوتردام?\', \'كم عدد الاوراق الاخبارية للطلاب التي وجدت في نوتردام?\', \'في اي سنة بدات ورقة الطالب الحس السليم بالنشر في نوتردام?\']\r\n```\r\nI don\'t think we can change this'
 'Hi @dorost1234.\r\n\r\nIn Python 3, strings are sequences of Unicode _code points_. Unicode is a specification that maps all characters (and emoji symbols) with its unique representation in terms of code points. That is what you see: Unicode code points (represented by a \\u escaped sequence of 16-bit hex values).\r\n\r\nCharacters are usually represented (on screen and papers) with a graphical element called _glyph_. That is what you would like to see: glyphs. But Python does not care about glyphs: that is the job of the GUI or the terminal; glyphs are what you get with the `print` function (if your terminal is properly configured to display those glyphs).\r\n\r\nYou have more detailed information about Unicode in the Python documentation: https://docs.python.org/3/howto/unicode.html'
 'thank you so much for the insightful comments. ']","Hi 
Looking into MLQA dataset for langauge ""ar"":

```
 ""question"": [
    ""\u0645\u062a\u0649 \u0628\u062f\u0627\u062a \u0627\u0644\u0645\u062c\u0644\u0629 \u0627\u0644\u0645\u062f\u0631\u0633\u064a\u0629 \u0641\u064a \u0646\u0648\u062a\u0631\u062f\u0627\u0645 \u0628\u0627\u0644\u0646\u0634\u0631?"",
    ""\u0643\u0645 \u0645\u0631\u0629 \u064a\u062a\u0645 \u0646\u0634\u0631\u0647\u0627 \u0641\u064a \u0646\u0648\u062a\u0631\u062f\u0627\u0645?"",
    ""\u0645\u0627 \u0647\u064a \u0627\u0644\u0648\u0631\u0642\u0629 \u0627\u0644\u064a\u0648\u0645\u064a\u0629 \u0644\u0644\u0637\u0644\u0627\u0628 \u0641\u064a \u0646\u0648\u062a\u0631\u062f\u0627\u0645?"",
    ""\u0643\u0645 \u0639\u062f\u062f \u0627\u0644\u0627\u0648\u0631\u0627\u0642 \u0627\u0644\u0627\u062e\u0628\u0627\u0631\u064a\u0629 \u0644\u0644\u0637\u0644\u0627\u0628 \u0627\u0644\u062a\u064a \u0648\u062c\u062f\u062a \u0641\u064a \u0646\u0648\u062a\u0631\u062f\u0627\u0645?"",
    ""\u0641\u064a \u0627\u064a \u0633\u0646\u0629 \u0628\u062f\u0627\u062a \u0648\u0631\u0642\u0629 \u0627\u0644\u0637\u0627\u0644\u0628 \u0627\u0644\u062d\u0633 \u0627\u0644\u0633\u0644\u064a\u0645 \u0628\u0627\u0644\u0646\u0634\u0631 \u0641\u064a \u0646\u0648\u062a\u0631\u062f\u0627\u0645?""
  ]
```

the questions are in the wrong format, and not readable, could you please have a look? thanks @lhoestq 
"
https://github.com/huggingface/datasets/issues/2132,TydiQA dataset is mixed and is not split per language ,"['You can filter the languages this way:\r\n```python\r\ntydiqa_en = tydiqa_dataset.filter(lambda x: x[""language""] == ""english"")\r\n```\r\n\r\nOtherwise maybe we can have one configuration per language ?\r\nWhat do you think of this for example ?\r\n\r\n```python\r\nload_dataset(""tydiqa"", ""primary_task.en"")\r\n```'
 'Hi\nthank you very much for the great response, this will be really wonderful\nto have one configuration per language, as one need the dataset in majority\nof case per language for cross-lingual evaluations.\nThis becomes also then more close to TFDS format, which is separated per\nlanguage https://www.tensorflow.org/datasets/catalog/tydi_qa which will be\nreally awesome to have.\nthanks\n\nOn Mon, Mar 29, 2021 at 6:17 PM Quentin Lhoest ***@***.***>\nwrote:\n\n> You can filter the languages this way:\n>\n> tydiqa_en = tydiqa_dataset.filter(lambda x: x[""language""] == ""english"")\n>\n> Otherwise maybe we can have one configuration per language ?\n> What do you think of this for example ?\n>\n> load_dataset(""tydiqa"", ""primary_task.en"")\n>\n> —\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/huggingface/datasets/issues/2132#issuecomment-809516799>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AS37NMXPW2PWSQ2RHG73O7TTGCY4LANCNFSM4Z7ER7IA>\n> .\n>\n'
 '@lhoestq  I greatly appreciate any updates on this. thanks a lot']","Hi @lhoestq 
Currently TydiQA is mixed and user can only access the whole training set of all languages:
https://www.tensorflow.org/datasets/catalog/tydi_qa

for using this dataset, one need to train/evaluate in each separate language, and having them mixed, makes it hard to use this dataset. This is much convenient for user to have  them split and I appreciate your help on this. 

Meanwhile, till hopefully this is split per language, I greatly appreciate telling me how I can preprocess and get data per language. thanks a lot "
https://github.com/huggingface/datasets/issues/2131,When training with Multi-Node Multi-GPU the worker 2 has TypeError: 'NoneType' object,"['Hi ! Thanks for reporting\r\nI was able to reproduce this issue. This was caused by missing split infos if a worker reloads the cache of the other worker.\r\n\r\nI just opened https://github.com/huggingface/datasets/pull/2137 to fix this issue'
 'The PR got merged :)\r\nFeel free to try it out on the `master` branch'
 'Sorry for the late reply.  \r\nNow everything just works well XD']","version: 1.5.0
met a very strange error, I am training large scale language model, and need train on 2 machines(workers).
And sometimes I will get this error `TypeError: 'NoneType' object is not iterable`
This is traceback
```

71 |   | Traceback (most recent call last):
-- | -- | --
72 |   | File ""run_gpt.py"", line 316, in <module>
73 |   | main()
74 |   | File ""run_gpt.py"", line 222, in main
75 |   | delimiter=""\t"", column_names=[""input_ids"", ""attention_mask"", ""chinese_ref""])
76 |   | File ""/data/miniconda3/lib/python3.7/site-packages/datasets/load.py"", line 747, in load_dataset
77 |   | use_auth_token=use_auth_token,
78 |   | File ""/data/miniconda3/lib/python3.7/site-packages/datasets/builder.py"", line 513, in download_and_prepare
79 |   | self.download_post_processing_resources(dl_manager)
80 |   | File ""/data/miniconda3/lib/python3.7/site-packages/datasets/builder.py"", line 673, in download_post_processing_resources
81 |   | for split in self.info.splits:
82 |   | TypeError: 'NoneType' object is not iterable
83 |   | WARNING:datasets.builder:Reusing dataset csv (/usr/local/app/.cache/huggingface/datasets/csv/default-1c257ebd48e225e7/0.0.0/2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2)
84 |   | Traceback (most recent call last):
85 |   | File ""/data/miniconda3/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
86 |   | ""__main__"", mod_spec)
87 |   | File ""/data/miniconda3/lib/python3.7/runpy.py"", line 85, in _run_code
88 |   | exec(code, run_globals)
89 |   | File ""/data/miniconda3/lib/python3.7/site-packages/torch/distributed/launch.py"", line 340, in <module>
90 |   | main()
91 |   | File ""/data/miniconda3/lib/python3.7/site-packages/torch/distributed/launch.py"", line 326, in main
92 |   | sigkill_handler(signal.SIGTERM, None)  # not coming back
93 |   | File ""/data/miniconda3/lib/python3.7/site-packages/torch/distributed/launch.py"", line 301, in sigkill_handler
94 |   | raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)

```
On worker 1 it loads the dataset well, however on worker 2 will get this error. 
And I will meet this error from time to time, sometimes it just goes well."
https://github.com/huggingface/datasets/issues/2130,wikiann dataset is missing columns ,"['Here please find TFDS format of this dataset: https://www.tensorflow.org/datasets/catalog/wikiann\r\nwhere there is a span column, this is really necessary to be able to use the data, and I appreciate your help @lhoestq '
 'Hi !\r\nApparently you can get the spans from the NER tags using `tags_to_spans` defined here:\r\n\r\nhttps://github.com/tensorflow/datasets/blob/c7096bd38e86ed240b8b2c11ecab9893715a7d55/tensorflow_datasets/text/wikiann/wikiann.py#L81-L126\r\n\r\nIt would be nice to include the `spans` field in this dataset as in TFDS. This could be a good first issue for new contributors !\r\n\r\nThe objective is to use `tags_to_spans` in the `_generate_examples` method [here](https://github.com/huggingface/nlp/blob/c98e4b8f23e3770c401c6d9326e243e1ffd599ec/datasets/wikiann/wikiann.py#L292-L316) to create he `spans` for each example.'
 'Hi @lhoestq \r\nthank you very much for the help, it would be very nice to have it included, here is the full code, one need to also convert tags to string first:\r\n\r\n```\r\nimport datasets \r\nfrom datasets import load_dataset\r\n\r\ndef tags_to_spans(tags):\r\n  """"""Convert tags to spans.""""""\r\n  spans = set()\r\n  span_start = 0\r\n  span_end = 0\r\n  active_conll_tag = None\r\n  for index, string_tag in enumerate(tags):\r\n    # Actual BIO tag.\r\n    bio_tag = string_tag[0]\r\n    assert bio_tag in [""B"", ""I"", ""O""], ""Invalid Tag""\r\n    conll_tag = string_tag[2:]\r\n    if bio_tag == ""O"":\r\n      # The span has ended.\r\n      if active_conll_tag:\r\n        spans.add((active_conll_tag, (span_start, span_end)))\r\n      active_conll_tag = None\r\n      # We don\'t care about tags we are\r\n      # told to ignore, so we do nothing.\r\n      continue\r\n    elif bio_tag == ""B"":\r\n      # We are entering a new span; reset indices and active tag to new span.\r\n      if active_conll_tag:\r\n        spans.add((active_conll_tag, (span_start, span_end)))\r\n      active_conll_tag = conll_tag\r\n      span_start = index\r\n      span_end = index\r\n    elif bio_tag == ""I"" and conll_tag == active_conll_tag:\r\n      # We\'re inside a span.\r\n      span_end += 1\r\n    else:\r\n      # This is the case the bio label is an ""I"", but either:\r\n      # 1) the span hasn\'t started - i.e. an ill formed span.\r\n      # 2) We have IOB1 tagging scheme.\r\n      # We\'ll process the previous span if it exists, but also include this\r\n      # span. This is important, because otherwise, a model may get a perfect\r\n      # F1 score whilst still including false positive ill-formed spans.\r\n      if active_conll_tag:\r\n        spans.add((active_conll_tag, (span_start, span_end)))\r\n      active_conll_tag = conll_tag\r\n      span_start = index\r\n      span_end = index\r\n  # Last token might have been a part of a valid span.\r\n  if active_conll_tag:\r\n    spans.add((active_conll_tag, (span_start, span_end)))\r\n  # Return sorted list of spans\r\n  return sorted(list(spans), key=lambda x: x[1][0])\r\n\r\ndataset = load_dataset(\'wikiann\', \'en\', split=""train"")\r\nner_tags = {\r\n   0:""O"",\r\n   1:""B-PER"",\r\n   2:""I-PER"",\r\n   3:""B-ORG"",\r\n   4:""I-ORG"",\r\n   5:""B-LOC"",\r\n   6:""I-LOC""\r\n}\r\n\r\ndef get_spans(tokens, tags):\r\n  """"""Convert tags to textspans.""""""\r\n  spans = tags_to_spans(tags)\r\n  text_spans = [\r\n      x[0] + "": "" + "" "".join([tokens[i]\r\n                              for i in range(x[1][0], x[1][1] + 1)])\r\n      for x in spans\r\n  ]\r\n  if not text_spans:\r\n    text_spans = [""None""]\r\n  return text_spans\r\n\r\n\r\nfor i, d in enumerate(dataset):\r\n   tokens = d[\'tokens\']\r\n   tags = d[\'ner_tags\']\r\n   tags = [ner_tags[i] for i in tags]\r\n   spans = get_spans(tokens, tags)\r\n   print(""spans "", spans)\r\n   print(d)\r\n   if i > 10:\r\n     break; \r\n```\r\nI am not sure how to contribute to the repository and how things work, could you let me know how one can access the datasets to be able to contribute to the repository? Maybe I could do it then\r\nthanks \r\n'
 'Cool ! Let me give you some context:\r\n\r\n#### Contribution guide\r\n\r\nYou can find the contribution guide here:\r\n\r\nhttps://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md\r\n\r\nIt explains how to set up your dev environment in a few steps.\r\n\r\n#### Dataset loading\r\n\r\nEach Dataset is defined by a Table that have many rows (one row = one example) and columns (one column = one feature).\r\nTo change how a dataset is constructed, you have to modify its dataset script that you can find here:\r\n\r\nhttps://github.com/huggingface/datasets/blob/master/datasets/wikiann/wikiann.py\r\n\r\nIt includes everything needed to load the WikiANN dataset.\r\nYou can load locally a modified version of `wikiann.py` with `load_dataset(""path/to/wikiann.py"")`.\r\n\r\n#### Define a new column\r\n\r\nEach column has a name and a type. You can see how the features of WikiANN are defined here:\r\n\r\nhttps://github.com/huggingface/datasets/blob/c98e4b8f23e3770c401c6d9326e243e1ffd599ec/datasets/wikiann/wikiann.py#L245-L263\r\n\r\nIdeally we would have one additional feature ""spans"":\r\n```python\r\n        ""spans"": datasets.Sequence(datasets.Value(""string"")),\r\n```\r\n\r\n#### Compute the content of each row\r\n\r\nTo build the WikiANN rows, the _generate_examples method from [here](https://github.com/huggingface/nlp/blob/c98e4b8f23e3770c401c6d9326e243e1ffd599ec/datasets/wikiann/wikiann.py#L292-L316) is used. This function `yield` one python dictionary for each example:\r\n```python\r\nyield guid_index, {""tokens"": tokens, ""ner_tags"": ner_tags, ""langs"": langs}\r\n```\r\n\r\nThe objective would be to return instead something like\r\n```python\r\nspans = spans = get_spans(tokens, tags)\r\nyield guid_index, {""tokens"": tokens, ""ner_tags"": ner_tags, ""langs"": langs, ""spans"": spans}\r\n```\r\n\r\nLet me know if you have questions !'
 'The PR was merged. Issue should be closed.\r\n\r\nCC: @lhoestq ']","Hi
Wikiann dataset needs to have ""spans"" columns, which is necessary to be able to use this dataset, but this column is missing from huggingface datasets, could you please have a look? thank you @lhoestq "
https://github.com/huggingface/datasets/issues/2129,How to train BERT model with next sentence prediction?,"[""Hi !\r\nWe're not using `TextDatasetForNextSentencePrediction` in `datasets`.\r\nAlthough you can probably use the `TextDatasetForNextSentencePrediction.create_examples_from_document` on a dataset to prepare it for next sentence prediction.""
 'Thanks.\r\n\r\nDo you mean that `TextDatasetForNextSentencePrediction.create_exapmles_from_document` can be applied to dataset object other than `TextDatasetForNextSentencePrediction`  e.g. a `Dataset` object which is  loaded by  `datasets.load_dataset`?'
 'It would probably require a bit of tweaking, but you can apply it to a dataset, yes.\r\nThis should give you a new dataset with sentence pairs you can train a model on.\r\n\r\nYou can find the documentation about dataset processing here:\r\nhttps://huggingface.co/docs/datasets/processing.html#processing-data-with-map'
 ""Thank you for detail information.\r\n\r\nI'll try to apply `create_examples_from_document` to `Dataset` object.\r\n""]","Hello.

I'm trying to pretrain the BERT model with next sentence prediction. Is there any function that supports next sentence prediction 
like ` TextDatasetForNextSentencePrediction` of `huggingface/transformers` ?
"
https://github.com/huggingface/datasets/issues/2128,Dialogue action slot name and value are reversed in MultiWoZ 2.2,"['Hi\r\nGood catch ! Thanks for reporting\r\n\r\nIf you are interested in contributing, feel free to open a PR to fix this :) ']","Hi @yjernite, thank you for adding MultiWoZ 2.2 in the huggingface datasets platform. It is beneficial!

I spot an error that the order of Dialogue action slot names and values are reversed.

https://github.com/huggingface/datasets/blob/649b2c469779bc4221e1b6969aa2496d63eb5953/datasets/multi_woz_v22/multi_woz_v22.py#L251-L262"
https://github.com/huggingface/datasets/issues/2125,Is dataset timit_asr broken?,"['Hi,\r\n\r\nthanks for the report, but this is a duplicate of #2052. '
 ""@mariosasko \r\nThank you for your quick response! Following #2052, I've fixed the problem.""]","Using `timit_asr` dataset, I saw all records are the same.

``` python
from datasets import load_dataset, load_metric

timit = load_dataset(""timit_asr"")

from datasets import ClassLabel
import random
import pandas as pd
from IPython.display import display, HTML

def show_random_elements(dataset, num_examples=10):
    assert num_examples <= len(dataset), ""Can't pick more elements than there are in the dataset.""
    picks = []
    for _ in range(num_examples):
        pick = random.randint(0, len(dataset)-1)
        while pick in picks:
            pick = random.randint(0, len(dataset)-1)
        picks.append(pick)

    df = pd.DataFrame(dataset[picks])
    display(HTML(df.to_html()))


show_random_elements(timit['train'].remove_columns([""file"", ""phonetic_detail"", ""word_detail"", ""dialect_region"", ""id"", 
                                                    ""sentence_type"", ""speaker_id""]), num_examples=20)

```

`output`

<img width=""312"" alt=""Screen Shot 2021-03-28 at 17 29 04"" src=""https://user-images.githubusercontent.com/42398050/112746646-21acee80-8feb-11eb-84f3-dbb5d4269724.png"">


I double-checked it [here](https://huggingface.co/datasets/viewer/), and met the same problem.

<img width=""1374"" alt=""Screen Shot 2021-03-28 at 17 32 07"" src=""https://user-images.githubusercontent.com/42398050/112746698-9bdd7300-8feb-11eb-97ed-5babead385f4.png"">
"
https://github.com/huggingface/datasets/issues/2124,Adding ScaNN library to do MIPS?,"[""I haven't played with it (yet) but it sounds really cool !\r\n""]","@lhoestq Hi I am thinking of adding this new google library to do the MIPS similar to **add_faiss_idex**. As the paper suggests, it is really fast when it comes to retrieving the nearest neighbors. 

https://github.com/google-research/google-research/tree/master/scann

![image](https://user-images.githubusercontent.com/16892570/112738294-78ec9800-8fc6-11eb-9a5f-3d7ee5818e76.png)
"
https://github.com/huggingface/datasets/issues/2123,Problem downloading GEM wiki_auto_asset_turk dataset,"[""Hi,\r\n\r\nsadly I can't replicate the problem on my Windows machine. Try to update the library to the newest version with:\r\n```bash\r\npip install git+https://github.com/huggingface/datasets\r\n``` ""
 ""Thanks for the answer! I updated the library but unfortunately it didn't solve the problem.""
 'Is there an error message ?\r\nWhat stacktrace do you get if you interrupt the execution of the program while downloading ?'
 ""Sorry for the long time since my last comment, I tried again and don't seem to have the problem anymore, thanks for your support!""
 ""Great ! I'm closing the issue then. Feel free to re-open if you experience this issue again""]","@yjernite 

### Summary

I am currently working on the GEM datasets and do not manage to download the wiki_auto_asset_turk data, whereas all other datasets download well with the same code.

### Steps to reproduce
Code snippet:

from datasets import load_dataset
#dataset = load_dataset('gem', 'web_nlg_en')
dataset = load_dataset('gem', 'wiki_auto_asset_turk')

```

**Expected behavior:**

I expect the dataset to start downloading (download bar appears and progresses toward 100%)

**Actual behavior:**
Instead of seeing the download bar appearing, nothing happens; the following appears in the console as expected, but nothing more:

Downloading: 36.6kB [00:00, 37.2MB/s]
Downloading: 41.7kB [00:00, ?B/s]
Downloading and preparing dataset gem/wiki_auto_asset_turk (download: 121.37 MiB, generated: 145.69 MiB, post-processed: Unknown size, total: 267.07 MiB) to C:\Users\sfmil\.cache\huggingface\datasets\gem\wiki_auto_asset_turk\1.0.0\f252756d7f1b8f019aac71a1623b2950acfe10d25d956668ac4eae4e93c58b8d...

### Is this a regression?
No, it was the first time I was trying to download this dataset (same for the other ones).

### Debug info
- Python version: Python 3.8.2
- OS version: Windows 10 Family"
https://github.com/huggingface/datasets/issues/2120,dataset viewer does not work anymore ,"[""Thanks for reporting :) We're looking into it"" 'Back up. ']","Hi
I normally use this link to see all datasets and how I can load them 


https://huggingface.co/datasets/viewer/

Now I am getting 

502 Bad Gateway
nginx/1.18.0 (Ubuntu)

could you bring this webpage back ? this was very helpful @lhoestq 
thanks for your help "
https://github.com/huggingface/datasets/issues/2117,"load_metric from local ""glue.py"" meet error 'NoneType' object is not callable","['@Frankie123421 what was the resolution to this?'
 '> @Frankie123421 what was the resolution to this?\r\n\r\nuse glue_metric.py instead of glue.py in load_metric'
 'thank you!']","actual_task = ""mnli"" if task == ""mnli-mm"" else task
dataset = load_dataset(path='/home/glue.py', name=actual_task)
metric = load_metric(path='/home/glue.py', name=actual_task)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-8-7ab77a465d81> in <module>
      1 actual_task = ""mnli"" if task == ""mnli-mm"" else task
      2 dataset = load_dataset(path='/home/jcli/glue.py', name=actual_task)
----> 3 metric = load_metric(path='/home/jcli/glue.py', name=actual_task)

~/anaconda3/envs/pytorch/lib/python3.6/site-packages/datasets/load.py in load_metric(path, config_name, process_id, num_process, cache_dir, experiment_id, keep_in_memory, download_config, download_mode, script_version, **metric_init_kwargs)
    508         keep_in_memory=keep_in_memory,
    509         experiment_id=experiment_id,
--> 510         **metric_init_kwargs,
    511     )
    512 

TypeError: 'NoneType' object is not callable

Please help"
https://github.com/huggingface/datasets/issues/2116,Creating custom dataset results in error while calling the map() function,"['Hi,\r\n\r\nthe `_data` attribute is missing due to `MyDataset.__init__` not calling the parent `__init__`. However, I don\'t think it\'s a good idea to subclass the `datasets.Dataset` class (e.g. it\'s kind of dangerous to override `datasets.Dataset.__getitem__`). Instead, it\'s better to follow the ""association over inheritance"" approach with a simple wrapper class that delegates calls to a wrapped `Dataset` (map, etc.). Btw, the library offers the `datasets.Dataset.from_pandas` class method to directly create a `datasets.Dataset` from the dataframe.']","calling `map()` of `datasets` library results into an error while defining a Custom dataset.
Reproducible example:
```
import datasets
class MyDataset(datasets.Dataset):

    def __init__(self, sentences):
        ""Initialization""
        self.samples = sentences

    def __len__(self):
        ""Denotes the total number of samples""
        return len(self.samples)

    def __getitem__(self, index):
        ""Generates one sample of data""
        # Select sample
        # Load data and get label
        samples = self.samples[index]

        return samples

def preprocess_function_train(examples):
        inputs = examples
        labels = [example+tokenizer.eos_token for example in examples ]
        inputs = tokenizer(inputs, max_length=30, padding=True, truncation=True)
        labels = tokenizer(labels, max_length=30, padding=True, truncation=True)
        model_inputs = inputs
        model_inputs[""labels""] = labels[""input_ids""]
        print(""about to return"")
        return model_inputs


##train[""sentence""] is dataframe column
train_dataset = MyDataset(train['sentence'].values.tolist())
train_dataset = train_dataset.map(
            preprocess_function,
            batched = True,
            batch_size=32
        )
```

Stack trace of error:
```
Traceback (most recent call last):
  File ""dir/train_generate.py"", line 362, in <module>
    main()
  File ""dir/train_generate.py"", line 245, in main
    train_dataset = train_dataset.map(
  File ""anaconda_dir/anaconda3/envs/env1/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 1244, in map
    return self._map_single(
  File ""anaconda_dir/anaconda3/envs/env1/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 149, in wrapper
    unformatted_columns = set(self.column_names) - set(self._format_columns or [])
  File ""anaconda_dir/anaconda3/envs/env1/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 526, in column_names
    return self._data.column_names
AttributeError: 'MyDataset' object has no attribute '_data'
```"
https://github.com/huggingface/datasets/issues/2106,WMT19 Dataset for Kazakh-English is not formatted correctly,"['Hi ! Thanks for reporting\r\n\r\nBy looking at the raw `news-commentary-v14.en-kk.tsv` file, it looks like there are at least 17 lines with this issue.\r\nMoreover these issues are not always the same:\r\n- L97 is only `kk` text and must be appended at the end of the `kk` text of the **next** line\r\n- L2897 is only `kk` text and must be appended at the end of the `kk` text of the **previous** line\r\n- L1247 and L1248 are only `kk` texts and must be inserted at the **beginning** of the `kk` text of the next line\r\n- (and there are many others)\r\n\r\nIt would be nice to have a corrected version of this file ! The file is available in the `wmt/news-commentary` repository on the Datasets Hub here:\r\nhttps://huggingface.co/datasets/wmt/news-commentary/tree/main/v14/training\r\n\r\nThen maybe we can notify the WMT authors and host the corrected version somewhere']","In addition to the bug of languages being switched from Issue @415, there are incorrect translations in the dataset because the English-Kazakh translations have a one off formatting error.

The News Commentary v14 parallel data set for kk-en from http://www.statmt.org/wmt19/translation-task.html has a bug here:

> Line 94. The Swiss National Bank, for its part, has been battling with the deflationary effects of the franc’s dramatic appreciation over the past few years.	Швейцарияның Ұлттық банкі өз тарапынан, соңғы бірнеше жыл ішінде франк құнының қатты өсуінің дефляциялық әсерімен күресіп келеді.
> 
> Line 95. Дефляциялық күштер 2008 жылы терең және ұзаққа созылған жаһандық дағдарысқа байланысты орын алған ірі экономикалық және қаржылық орын алмасулардың арқасында босатылды.  Жеке қарыз қаражаты үлесінің қысқаруы орталық банктің рефляцияға жұмсалған күш-жігеріне тұрақты соққан қарсы желдей болды.
> 
> Line 96. The deflationary forces were unleashed by the major economic and financial dislocations associated with the deep and protracted global crisis that erupted in 2008. Private deleveraging became a steady headwind to central bank efforts to reflate.	2009 жылы, алдыңғы қатарлы экономикалардың шамамен үштен бірі бағаның төмендеуін көрсетті, бұл соғыстан кейінгі жоғары деңгей болды.

As you can see, line 95 has only the Kazakh translation which should be part of line 96. This causes all of the following English-Kazakh translation pairs to be one off rendering ALL of those translations incorrect. This issue was not fixed when the dataset was imported to Huggingface. By running this code 

```
import datasets
from datasets import load_dataset
dataset = load_dataset('wmt19', 'kk-en')
for key in dataset['train']['translation']:
    if 'The deflationary forces were unleashed by the major economic and financial dislocations associated with the deep and protracted global crisis that erupted in 2008.' in key['kk']:
        print(key['en'])
        print(key['kk'])
        break
```
we get: 
> 2009 жылы, алдыңғы қатарлы экономикалардың шамамен үштен бірі бағаның төмендеуін көрсетті, бұл соғыстан кейінгі жоғары деңгей болды.
> The deflationary forces were unleashed by the major economic and financial dislocations associated with the deep and protracted global crisis that erupted in 2008. Private deleveraging became a steady headwind to central bank efforts to reflate.

which shows that the issue still persists in the Huggingface dataset. The Kazakh sentence matches up to the next English sentence in the dataset instead of the current one.

Please let me know if there's you have any ideas to fix this one-off error from the dataset or if this can be fixed by Huggingface."
https://github.com/huggingface/datasets/issues/2105,Request to remove S2ORC dataset,"[""Hello @kyleclo! Currently, we are getting the data from your bucket, so if you remove it the HF script won't work anymore :) \r\n\r\nUntil you solve things on your end, @lhoestq suggested we just return a warning message when people try to load that dataset from HF. What would you like it to say?""
 'Hi @kyleclo, as of today, you have not removed your bucket data yet, and therefore HuggingFace can download it from there.\r\n\r\nIs it OK? Are you planning to eventually delete it? Thank you.'
 ""Hi!  Sorry I missed @yjernite 's previous message, thanks for responding!  \r\n\r\nIs there an option where we can keep our data in our bucket, but the HF script no longer pulls data from it?  ""]","Hi!  I was wondering if it's possible to remove [S2ORC](https://huggingface.co/datasets/s2orc) from hosting on Huggingface's platform?  Unfortunately, there are some legal considerations about how we make this data available.  Happy to add back to Huggingface's platform once we work out those hurdles!  Thanks!"
https://github.com/huggingface/datasets/issues/2104,Trouble loading wiki_movies,"['Hi ! `wiki_movies` was added in `datasets==1.2.0`. However it looks like you have `datasets==1.1.2`.\r\n\r\nTo use `wiki_movies`, please update `datasets` with\r\n```\r\npip install --upgrade datasets\r\n```'
 'Thanks a lot! That solved it and I was able to upload a model trained on it as well :)']","Hello,
I am trying to load_dataset(""wiki_movies"") and it gives me this error - 

`FileNotFoundError: Couldn't find file locally at wiki_movies/wiki_movies.py, or remotely at https://raw.githubusercontent.com/huggingface/datasets/1.1.2/datasets/wiki_movies/wiki_movies.py or https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/wiki_movies/wiki_movies.py`

Trying to do `python run_mlm.py \
    --model_name_or_path roberta-base \
    --dataset_name wiki_movies \` also gives the same error. 

Is this something on my end? From what I can tell, this dataset was re-added by @lhoestq a few months ago. 
Thank you!"
https://github.com/huggingface/datasets/issues/2103,"citation, homepage, and license fields of `dataset_info.json` are duplicated many times","[""Thanks for reporting :)\r\nMaybe we can concatenate fields only if they are different.\r\n\r\nCurrently this is done here:\r\n\r\nhttps://github.com/huggingface/nlp/blob/349ac4398a3bcae6356f14c5754483383a60e8a4/src/datasets/info.py#L180-L196\r\n\r\nThis can be a good first contribution to the library.\r\nPlease comment if you'd like to improve this and open a PR :)""]","This happens after a `map` operation when `num_proc` is set to `>1`. I tested this by cleaning up the json before running the `map` op on the dataset so it's unlikely it's coming from an earlier concatenation.

Example result:
```
""citation"": ""@ONLINE {wikidump,\n    author = {Wikimedia Foundation},\n    title  = {Wikimedia Downloads},\n    url    = {https://dumps.wikimedia.org}\n}\n\n@ONLINE {wikidump,\n    author = {Wikimedia Foundation},\n    title  = {Wikimedia Downloads},\n    url    = {https://dumps.wikimedia.org}\n}\n\n@ONLINE {wikidump,\n    author = {Wikimedia Foundation},\n    title  = {Wikimedia Downloads},\n    url    = {https://dumps.wikimedia.org}\n}\n\n@ONLINE {wikidump,\n    author = {Wikimedia Foundation},\n    title  = {Wikimedia Downloads},\n    url    = {https://dumps.wikimedia.org}\n}\n\n@ONLINE {wikidump,\n    author = {Wikimedia Foundation},\n    title  = {Wikimedia Downloads},\n    url    = {https://dumps.wikimedia.org}\n}\n\n@ONLINE {wikidump,\n    author = {Wikimedia Foundation},\n    title  = {Wikimedia Downloads},\n    url    = {https://dumps.wikimedia.org}\n}\n\n@ONLINE {wikidump,\n    author = {Wikimedia Foundation},\n    title  = {Wikimedia Downloads},\n    url    = {https://dumps.wikimedia.org}\n}\n\n@ONLINE {wikidump,\n    author = {Wikimedia Foundation},\n    title  = {Wikimedia Downloads},\n    url    = {https://dumps.wikimedia.org}\n}\n\n@ONLINE {wikidump,\n    author = {Wikimedia Foundation},\n    title  = {Wikimedia Downloads},\n    url    = {https://dumps.wikimedia.org}\n}\n\n@ONLINE {wikidump,\n    author = {Wikimedia Foundation},\n    title  = {Wikimedia Downloads},\n    url    = {https://dumps.wikimedia.org}\n}\n\n@ONLINE {wikidump,\n    author = {Wikimedia Foundation},\n    title  = {Wikimedia Downloads},\n    url    = {https://dumps.wikimedia.org}\n}\n\n@ONLINE {wikidump,\n    author = {Wikimedia Foundation},\n    title  = {Wikimedia Downloads},\n
```

@lhoestq and I believe this is happening due to the fields being concatenated `num_proc` times."
https://github.com/huggingface/datasets/issues/2099,load_from_disk takes a long time to load local dataset,"['Hi !\r\nCan you share more information about the features of your dataset ? You can get them by printing `my_dataset.features`\r\nCan you also share the code of your `map` function ?'
 ""It is actually just the tokenized `wikipedia` dataset with `input_ids`, `attention_mask`, etc, with one extra column which is a list of integers. The `text` column is removed during tokenization.\r\n\r\n```\r\ndef add_len_and_seq(example):\r\n    end_idx = example['input_ids'].index(SEP)\r\n    example['actual_len'] = end_idx-1\r\n    seq_len = len(example['input_ids'])\r\n    \r\n\r\n    example['seq'] = [PAD_ID] + [np.uint8(example['some_integer'])]*(end_idx-1) + [PAD_ID]*(seq_len-end_idx)\r\n    \r\n    return example\r\n```\r\n""
 ""Is `PAD_ID` a python integer ? You need all the integers in `example['seq']` to have the same type.\r\nDoes this work if you remove the `np.uint8` and use python integers instead ?""
 'yup I casted it to `np.uint8` outside the function where it was defined. It was originally using python integers.'
 'Strangely, even when I manually created `np.arrays` of specific `dtypes`, the types in the final `dataset_info.json` that gets written are still `int64`.\r\n\r\nUpdate: I tried creating lists of `int8`s and got the same result.'
 ""Yes this is a known issue: #625 \r\nWe're working on making the precision kept for numpy :)\r\nTo specify the precision of the integers, currently one needs to specify the output features with `.map(..., features=output_features)`""
 'Do you know what step is taking forever in the code ?\r\nWhat happens if you interrupt the execution of the dataset loading ?'
 'After a synchronous discussion, we found that the cache file sizes have an enormous effect on the loading speed: smaller cache files result in faster load times. `num_proc` controls the number of cache files that are being written and is inversely proportional to the individual file size. In other words, increase `num_proc` for smaller cache files :)\r\n\r\nMaybe this can be highlighted somewhere in the docs.']","I have an extremely large tokenized dataset (24M examples) that loads in a few minutes. However, after adding a column similar to `input_ids` (basically a list of integers) and saving the dataset to disk, the load time goes to >1 hour. I've even tried using `np.uint8` after seeing #1985 but it doesn't seem to be helping (the total size seems to be smaller though).

Does anyone know what could be the issue? Or does the casting of that column to `int8` need to happen in the function that writes the arrow table instead of in the `map` where I create the list of integers?

Tagging @lhoestq since you seem to be working on these issues and PRs :)"
https://github.com/huggingface/datasets/issues/2098,SQuAD version ,"['Hi ! This is 1.1 as specified by the download urls here:\r\n\r\nhttps://github.com/huggingface/nlp/blob/349ac4398a3bcae6356f14c5754483383a60e8a4/datasets/squad/squad.py#L50-L55'
 'Got it. Thank you~']","Hi~ 
I want train on squad dataset. What's the version of the squad? Is it 1.1 or 1.0? I'm new in QA, I don't find some descriptions about it. "
https://github.com/huggingface/datasets/issues/2092,How to disable making arrow tables in load_dataset ?,"['Hi ! We plan to add streaming features in the future.\r\n\r\nThis should allow to load a dataset instantaneously without generating the arrow table. The trade-off is that accessing examples from a streaming dataset must be done in an iterative way, and with an additional (but hopefully minor) overhead.\r\nWhat do you think about this ?\r\n\r\nIf you have ideas or suggestions of what you expect from such features as a user, feel free to share them, this is really valuable to us !'
 'People mainly want this feature either because it takes too much time too make arrow tables, or they occupy too much memory on the disk. I think both the problem can be solved if we provide arrow tables themselves on datasets hub. Can we do this currently @lhoestq ? \r\n'
 '@lhoestq I think the ```try_from_hf_gcs``` provide the same functionality. What all datasets are available on HF GCS? Are all the datasets on huggingFace datasets hub are made available on GCS, automatically?'
 'Only datasets like wikipedia, wiki40b, wiki_dpr and natural questions are available already processed on the HF google storage. This is used to download directly the arrow file instead of building it from the original data files.'
 '@lhoestq How can we make sure that the data we upload on HuggingFace hub is available in form of preprocessed arrow files ?'
 ""We're still working on this :) This will be available soon\r\nUsers will be able to put their processed arrow files on the Hub""]","Is there a way to disable the construction of arrow tables, or to make them on the fly as the dataset is being used ?"
https://github.com/huggingface/datasets/issues/2089,Add documentaton for dataset README.md files,"['Hi ! We are using the [datasets-tagging app](https://github.com/huggingface/datasets-tagging) to select the tags to add.\r\n\r\nWe are also adding the full list of tags in #2107 \r\nThis covers multilinguality, language_creators, licenses, size_categories and task_categories.\r\n\r\nIn general if you want to add a tag that doesn\'t exist (for example for a custom license) you must make it start with `other-` and then a custom tag name.\r\n\r\nedit (@theo-m) if you ever find yourself resorting to adding an `other-*` tag, please do ping us somewhere so we can think about adding it to the ""official"" list :)'
 '@lhoestq hmm - ok thanks for the answer.\r\nTo be honest I am not sure if this issue can be closed now.\r\nI just wanted to point out that this should either be documented or linked in the documentation.\r\nIf you feel like it is (will be) please just close this.'
 ""We're still working on the validation+documentation in this.\r\nFeel free to keep this issue open till we've added them""
 '@lhoestq what is the status on this? Did you add documentation?'
 ""Hi ! There's the tagging app at https://huggingface.co/datasets/tagging/ that you can use.\r\nIt shows the list of all the tags you can use.\r\n\r\nIt is based on all the tag sets defined in this folder:\r\nhttps://github.com/huggingface/datasets/tree/master/src/datasets/utils/resources""
 '@lhoestq is there something like this form Models?'
 ""I don't think so. Feel free to take a look at the tags of other models (example [here](https://huggingface.co/bert-base-uncased/blob/main/README.md)). But we should definitely have some docs or an app to write the tags. Feel free to open an issue in the `transformers` repo or in the `huggingface_hub` repo so we can discuss this""]","Hi,
the dataset README files have special headers.
Somehow a documenation of the allowed values and tags is missing.
Could you add that?

Just to give some concrete questions that should be answered imo:
- which values can be passted to multilinguality?
- what should be passed to language_creators?
- which values should licenses have? What do I say when it is a custom license? Should I add a link?
- how should I choose size_categories ? What are valid ranges?
- what are valid task_categories?

Thanks
Philip"
https://github.com/huggingface/datasets/issues/2084,CUAD - Contract Understanding Atticus Dataset,['+1 on this request'],"## Adding a Dataset
- **Name:** CUAD - Contract Understanding Atticus Dataset
- **Description:** As one of the only large, specialized NLP benchmarks annotated by experts, CUAD can serve as a challenging research benchmark for the broader NLP community.
- **Paper:** https://arxiv.org/abs/2103.06268
- **Data:** https://github.com/TheAtticusProject/cuad/
- **Motivation:** good domain specific datasets are valuable

Instructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).
"
https://github.com/huggingface/datasets/issues/2083,`concatenate_datasets` throws error when changing the order of datasets to concatenate,"[""Hi,\r\n\r\nthis bug is related to `Dataset.{remove_columns, rename_column, flatten}` not propagating the change to the schema metadata when the info features are updated, so this line is the culprit:\r\n```python\r\ncommon_voice_train = common_voice_train.remove_columns(['client_id', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'])\r\n\r\n``` \r\nThe order is important because the resulting dataset inherits the schema metadata of the first dataset passed to the `concatenate_datasets(...)` function (`pa.concat_tables` [docs](https://arrow.apache.org/docs/python/generated/pyarrow.concat_tables.html)). I'll try to fix this ASAP.""]","Hey, 

I played around with the `concatenate_datasets(...)` function: https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=concatenate_datasets#datasets.concatenate_datasets

and noticed that when the order in which the datasets are concatenated changes an error is thrown where it should not IMO.

Here is a google colab to reproduce the error: https://colab.research.google.com/drive/17VTFU4KQ735-waWZJjeOHS6yDTfV5ekK?usp=sharing"
https://github.com/huggingface/datasets/issues/2080,Multidimensional arrays in a Dataset,"['Hi !\r\n\r\nThis is actually supported ! but not yet in `from_pandas`.\r\nYou can use `from_dict` for now instead:\r\n```python\r\nfrom datasets import Dataset, Array2D, Features, Value\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\ndataset = {\r\n    \'bbox\': [\r\n        np.array([[1,2,3,4],[1,2,3,4],[1,2,3,4]]),\r\n        np.array([[1,2,3,4],[1,2,3,4],[1,2,3,4]]),\r\n        np.array([[1,2,3,4],[1,2,3,4],[1,2,3,4]]),\r\n        np.array([[1,2,3,4],[1,2,3,4],[1,2,3,4]])\r\n    ],\r\n    \'input_ids\': [1, 2, 3, 4]\r\n}\r\ndataset = Dataset.from_dict(dataset)\r\n```\r\n\r\nThis will work but to use it with the torch formatter you must specify the `Array2D` feature type in order to tell the shape:\r\n```python\r\nfrom datasets import Dataset, Array2D, Features, Value\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\ndataset = {\r\n    \'bbox\': [\r\n        np.array([[1,2,3,4],[1,2,3,4],[1,2,3,4]]),\r\n        np.array([[1,2,3,4],[1,2,3,4],[1,2,3,4]]),\r\n        np.array([[1,2,3,4],[1,2,3,4],[1,2,3,4]]),\r\n        np.array([[1,2,3,4],[1,2,3,4],[1,2,3,4]])\r\n    ],\r\n    \'input_ids\': [1, 2, 3, 4]\r\n}\r\ndataset = Dataset.from_dict(dataset, features=Features({\r\n    ""bbox"": Array2D(shape=(3, 4), dtype=""int64""),\r\n    ""input_ids"": Value(""int64"")\r\n}))\r\ndataset.set_format(""torch"")\r\nprint(dataset[0][\'bbox\'])\r\n# tensor([[1, 2, 3, 4],\r\n#         [1, 2, 3, 4],\r\n#         [1, 2, 3, 4]])\r\n```\r\nIf you don\'t specify the `Array2D` feature type, then the inferred type will be Sequence(Sequence(Value(""int64""))) and therefore the torch formatter will return list of tensors'
 'Thanks for the explanation. \r\nWith my original DataFrame, I did\r\n```\r\ndataset = dataset.to_dict(""list"")\r\n```\r\nand then the rest of the transformation from dictionary works just fine.']","Hi,

I'm trying to put together a `datasets.Dataset` to be used with LayoutLM which is available in `transformers`. This model requires as input the bounding boxes of each of the token of a sequence. This is when I realized that `Dataset` does not support multi-dimensional arrays as a value for a column in a row.

The following code results in conversion error in pyarrow (`pyarrow.lib.ArrowInvalid: ('Can only convert 1-dimensional array values', 'Conversion failed for column bbox with type object')`)

```
from datasets import Dataset
import pandas as pd
import numpy as np

dataset = pd.DataFrame({
    'bbox': [
        np.array([[1,2,3,4],[1,2,3,4],[1,2,3,4]]),
        np.array([[1,2,3,4],[1,2,3,4],[1,2,3,4]]),
        np.array([[1,2,3,4],[1,2,3,4],[1,2,3,4]]),
        np.array([[1,2,3,4],[1,2,3,4],[1,2,3,4]])
    ],
    'input_ids': [1, 2, 3, 4]
})
dataset = Dataset.from_pandas(dataset)
```

Since I wanted to use pytorch for the downstream training task, I also tried a few ways to directly put in a column of 2-D pytorch tensor in a formatted dataset, but I can only have a list of 1-D tensors, or a list of arrays, or a list of lists.

```
import torch
from datasets import Dataset
import pandas as pd

dataset = pd.DataFrame({
    'bbox': [
        [[1,2,3,4],[1,2,3,4],[1,2,3,4]],
        [[1,2,3,4],[1,2,3,4],[1,2,3,4]],
        [[1,2,3,4],[1,2,3,4],[1,2,3,4]],
        [[1,2,3,4],[1,2,3,4],[1,2,3,4]]
    ],
    'input_ids': [1, 2, 3, 4]
})
dataset = Dataset.from_pandas(dataset)

def test(examples):
    return {'bbbox': torch.Tensor(examples['bbox'])}
dataset = dataset.map(test)
print(dataset[0]['bbox'])
print(dataset[0]['bbbox'])

dataset.set_format(type='torch', columns=['input_ids', 'bbox'], output_all_columns=True)
print(dataset[0]['bbox'])
print(dataset[0]['bbbox'])

def test2(examples):
    return {'bbbox': torch.stack(examples['bbox'])}
dataset = dataset.map(test2)

print(dataset[0]['bbox'])
print(dataset[0]['bbbox'])
```

Is is possible to support n-D arrays/tensors in datasets? 
It seems that it can also be useful for this [feature request](https://github.com/huggingface/datasets/issues/263)."
https://github.com/huggingface/datasets/issues/2078,MemoryError when computing WER metric,"[""Hi ! Thanks for reporting.\r\nWe're indeed using `jiwer` to compute the WER.\r\n\r\nMaybe instead of calling `jiwer.wer` once for all the preditions/references we can compute the WER iteratively to avoid memory issues ? I'm not too familial with `jiwer` but this must be possible.\r\n\r\nCurrently the code to compute the WER is defined here:\r\n\r\nhttps://github.com/huggingface/nlp/blob/349ac4398a3bcae6356f14c5754483383a60e8a4/metrics/wer/wer.py#L93-L94""
 ""Hi,\r\n\r\nI've just pushed a pull request that is related to this issue https://github.com/huggingface/datasets/pull/2169. It's not iterative, but it should avoid memory errors. It's based on the editdistance python library. An iterative implementation should be as easy as storing scores and words stepwise and dividing at the end.  ""
 'I see, this was solved by other thread. Ok, let me know if you want to switch the implementation for any reason :)'
 ""Thanks for diving into this anyway ^^'\r\nAs you said this actually got solved a few days ago""
 ""Someone created an issue https://github.com/jitsi/jiwer/issues/40 at jiwer which shows that this is still a problem in the current version. Would be curious to figure out how this can be fixed by jiwer... :) I assume that it runs of out memory because it's trying to compute the WER over (too many) test samples?""
 ""Hi !\r\n\r\nIt's computed iteratively so not sure what could go wrong\r\n\r\nhttps://github.com/huggingface/datasets/blob/8afd0ba8c27800a55ea69d9fcd702dc97d9c16d8/metrics/wer/wer.py#L100-L106\r\n\r\n@NiklasHoltmeyer what version of `datasets` are you running ?\r\n""
 'One possible explanation might be that it is the user who is passing all the sentences in a single element to `wer.compute`?\r\n\r\nAs current implementation iterates over the elements of `predictions` and `references`, this can be problematic if `predictions` and `references` contain a single huge element each. \r\n\r\nThis could be the case, for example, with a single string with all sentences:\r\n```python\r\nresult[""predicted""] = ""One sentence. Other sentence.""\r\n```\r\nor with a __double__ nested list of sentence lists\r\n```python\r\nresult[""predicted""] = [[ [""One sentence.""], [""Other sentence""] ]]\r\n```\r\n\r\nThe user should check the dimensions of the data structure passed to `predictions` and `references`.'
 ""Hi all,\r\n\r\nin my case I was using and older version of datasets and, as @albertvillanova points out, passing the full list of sentences for the metric calculation. The problem was in the way jiwer implements WER, as it tries to compute WER for the full list at once instead of doing it element-wise. I think that with the latest implementation of datasets, or by using the alternative WER function that I've contributed on this [pull request](https://github.com/huggingface/datasets/pull/2169) there shouldn't be memory errors.""
 '@lhoestq i was using Datasets==1.5.0 with 1.6.1 it worked (atleast the first run) but 1.5.0 is not compatible with my preprocessing. i cant save my dataset to a parquet file while using the latest datasets version\r\n\r\n-> \r\n```\r\n  File ""../preprocess_dataset.py"", line 132, in <module>\r\n    pq.write_table(train_dataset.data, f\'{resampled_data_dir}/{data_args.dataset_config_name}.train.parquet\')\r\n  File ""/usr/local/lib/python3.8/dist-packages/pyarrow/parquet.py"", line 1674, in write_table\r\n    writer.write_table(table, row_group_size=row_group_size)\r\n  File ""/usr/local/lib/python3.8/dist-packages/pyarrow/parquet.py"", line 588, in write_table\r\n    self.writer.write_table(table, row_group_size=row_group_size)\r\nTypeError: Argument \'table\' has incorrect type (expected pyarrow.lib.Table, got ConcatenationTable)\r\n``` \r\n\r\nif i do \r\n```\r\nimport pyarrow.parquet as pq\r\n...\r\n...\r\npq.write_table(train_dataset.data, \'train.parquet\')\r\npq.write_table(eval_dataset.data, \'eval.parquet\')\r\n```\r\n\r\nwhile using 1.6.1. and its working with 1.5.0\r\n'
 'Hi ! You can pass dataset.data.table instead of dataset.data to pq.write_table'
 'This seems to be working so far! Thanks!']","Hi, I'm trying to follow the ASR example to try Wav2Vec. This is the code that I use for WER calculation:

```
wer = load_metric(""wer"")
print(wer.compute(predictions=result[""predicted""], references=result[""target""]))
```

However, I receive the following exception:

`Traceback (most recent call last):
  File ""/home/diego/IpGlobal/wav2vec/test_wav2vec.py"", line 51, in <module>
    print(wer.compute(predictions=result[""predicted""], references=result[""target""]))
  File ""/home/diego/miniconda3/envs/wav2vec3.6/lib/python3.6/site-packages/datasets/metric.py"", line 403, in compute
    output = self._compute(predictions=predictions, references=references, **kwargs)
  File ""/home/diego/.cache/huggingface/modules/datasets_modules/metrics/wer/73b2d32b723b7fb8f204d785c00980ae4d937f12a65466f8fdf78706e2951281/wer.py"", line 94, in _compute
    return wer(references, predictions)
  File ""/home/diego/miniconda3/envs/wav2vec3.6/lib/python3.6/site-packages/jiwer/measures.py"", line 81, in wer
    truth, hypothesis, truth_transform, hypothesis_transform, **kwargs
  File ""/home/diego/miniconda3/envs/wav2vec3.6/lib/python3.6/site-packages/jiwer/measures.py"", line 192, in compute_measures
    H, S, D, I = _get_operation_counts(truth, hypothesis)
  File ""/home/diego/miniconda3/envs/wav2vec3.6/lib/python3.6/site-packages/jiwer/measures.py"", line 273, in _get_operation_counts
    editops = Levenshtein.editops(source_string, destination_string)
MemoryError`

My system has more than 10GB of available RAM. Looking at the code, I think that it could be related to the way jiwer does the calculation, as it is pasting all the sentences in a single string before calling Levenshtein editops function.



"
https://github.com/huggingface/datasets/issues/2076,Issue: Dataset download error,"['Hi @XuhuiZhou, thanks for reporting this issue. \r\n\r\nIndeed, the old links are no longer valid (404 Not Found error), and the script must be updated with the new links to Google Drive.'
 'It would be nice to update the urls indeed !\r\n\r\nTo do this, you just need to replace the urls in `iwslt2017.py` and then update the dataset_infos.json file with\r\n```\r\ndatasets-cli test ./datasets/iwslt2017 --all_configs --save_infos --ignore_verifications\r\n```'
 'Is this a command to update my local files or fix the file Github repo in general? (I am not so familiar with the datasets-cli command here)\r\n\r\nI also took a brief look at the **Sharing your dataset** section, looks like I could fix that locally and push it to the repo? I guess we are ""canonical"" category?'
 'This command will update your local file. Then you can open a Pull Request to push your fix to the github repo :)\r\nAnd yes you are right, it is a ""canonical"" dataset, i.e. a dataset script defined in this github repo (as opposed to dataset repositories of users on the huggingface hub)'
 'Hi, thanks for the answer. \r\n\r\nI gave a try to the problem today. But I encountered an upload error: \r\n\r\n```\r\ngit push -u origin fix_link_iwslt\r\nEnter passphrase for key \'/home2/xuhuizh/.ssh/id_rsa\': \r\nERROR: Permission to huggingface/datasets.git denied to XuhuiZhou.\r\nfatal: Could not read from remote repository.\r\n\r\nPlease make sure you have the correct access rights\r\nand the repository exists.\r\n```\r\n\r\nAny insight here? \r\n\r\nBy the way, when I run the datasets-cli command, it shows the following error, but does not seem to be the error coming from `iwslt.py`\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File ""/home2/xuhuizh/anaconda3/envs/UMT/bin/datasets-cli"", line 33, in <module>\r\n    sys.exit(load_entry_point(\'datasets\', \'console_scripts\', \'datasets-cli\')())\r\n  File ""/home2/xuhuizh/projects/datasets/src/datasets/commands/datasets_cli.py"", line 35, in main\r\n    service.run()\r\n  File ""/home2/xuhuizh/projects/datasets/src/datasets/commands/test.py"", line 141, in run\r\n    try_from_hf_gcs=False,\r\n  File ""/home2/xuhuizh/projects/datasets/src/datasets/builder.py"", line 579, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File ""/home2/xuhuizh/projects/datasets/src/datasets/builder.py"", line 639, in _download_and_prepare\r\n    self.info.download_checksums, dl_manager.get_recorded_sizes_checksums(), ""dataset source files""\r\n  File ""/home2/xuhuizh/projects/datasets/src/datasets/utils/info_utils.py"", line 32, in verify_checksums\r\n    raise ExpectedMoreDownloadedFiles(str(set(expected_checksums) - set(recorded_checksums)))\r\ndatasets.utils.info_utils.ExpectedMoreDownloadedFiles: {\'https://wit3.fbk.eu/archive/2017-01-trnmted//texts/DeEnItNlRo/DeEnItNlRo/DeEnItNlRo-DeEnItNlRo.tgz\'}\r\n```'
 'Hi ! To create a PR on this repo your must fork it and create a branch on your fork. See how to fork the repo [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md#start-by-preparing-your-environment).\r\nAnd to make the command work without the `ExpectedMoreDownloadedFiles` error, you just need to use the `--ignore_verifications` flag.'
 'Hi @XuhuiZhou,\r\n\r\nAs @lhoestq has well explained, you need to fork HF\'s repository, create a feature branch in your fork, push your changes to it and then open a Pull Request to HF\'s upstream repository. This is so because at HuggingFace Datasets we follow a development model called ""Fork and Pull Model"". You can find more information here:\r\n- [Understanding the GitHub flow](https://guides.github.com/introduction/flow/)\r\n- [Forking Projects](https://guides.github.com/activities/forking/)\r\n\r\nAlternatively, if you find all these steps too complicated, you can use the GitHub official command line tool: [GitHub CLI](https://cli.github.com/). Once installed, in order to create a Pull Request, you only need to use this command:\r\n```shell\r\ngh pr create --web\r\n```\r\nThis utility will automatically create the fork, push your changes and open a Pull Request, under the hood.']","The download link in `iwslt2017.py` file does not seem to work anymore.

For example, `FileNotFoundError: Couldn't find file at https://wit3.fbk.eu/archive/2017-01-trnted/texts/zh/en/zh-en.tgz`

Would be nice if we could modify it script and use the new downloadable link?"
https://github.com/huggingface/datasets/issues/2075,ConnectionError: Couldn't reach common_voice.py,"['Hi @LifaSun, thanks for reporting this issue.\r\n\r\nSometimes, GitHub has some connectivity problems. Could you confirm that the problem persists?'
 '@albertvillanova Thanks! It works well now. ']","When I run: 
from datasets import load_dataset, load_metric

common_voice_train = load_dataset(""common_voice"", ""zh-CN"", split=""train+validation"")
common_voice_test = load_dataset(""common_voice"", ""zh-CN"", split=""test"")

Got:
ConnectionError: Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/master/datasets/common_voice/common_voice.py

Version:
1.4.1

Thanks!  @lhoestq @LysandreJik @thomwolf "
https://github.com/huggingface/datasets/issues/2071,Multiprocessing is slower than single process,['dupe of #1992'],"```python
# benchmark_filter.py
import logging
import sys
import time

from datasets import load_dataset, set_caching_enabled


if __name__ == ""__main__"":
    set_caching_enabled(False)
    logging.basicConfig(level=logging.DEBUG)

    bc = load_dataset(""bookcorpus"")

    now = time.time()
    try:
        bc[""train""].filter(lambda x: len(x[""text""]) < 64, num_proc=int(sys.argv[1]))
    except Exception as e:
        print(f""cancelled: {e}"")
    elapsed = time.time() - now

    print(elapsed)
```

Running `python benchmark_filter.py 1` (20min+) is faster than `python benchmark_filter.py 2` (2hrs+)"
https://github.com/huggingface/datasets/issues/2070,ArrowInvalid issue for squad v2 dataset,"[""Hi ! This error happens when you use `map` in batched mode and then your function doesn't return the same number of values per column.\r\n\r\nIndeed since you're using `map` in batched mode, `prepare_validation_features` must take a batch as input (i.e. a dictionary of multiple rows of the dataset), and return a batch.\r\n\r\nHowever it seems like `tokenized_examples` doesn't have the same number of elements in each field. One field seems to have `1180` elements while `candidate_attention_mask` only has `1178`.""]","Hello, I am using the huggingface official question answering example notebook (https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/question_answering.ipynb). 

In the prepare_validation_features function, I made some modifications to tokenize a new set of quesions with the original contexts and save them in three different list called candidate_input_dis, candidate_attetion_mask and candidate_token_type_ids. When I try to run the next cell for dataset.map, I got the following error:

`ArrowInvalid: Column 1 named candidate_attention_mask expected length 1180 but got length 1178`

My code is as follows:

```
def generate_candidate_questions(examples):
  val_questions = examples[""question""]
  candididate_questions = random.sample(datasets[""train""][""question""], len(val_questions))
  candididate_questions = [x[:max_length] for x in candididate_questions]
  return candididate_questions

def prepare_validation_features(examples, use_mixing=False):
  pad_on_right = tokenizer.padding_side == ""right""
  tokenized_examples = tokenizer(
      examples[""question"" if pad_on_right else ""context""],
      examples[""context"" if pad_on_right else ""question""],
      truncation=""only_second"" if pad_on_right else ""only_first"",
      max_length=max_length,
      stride=doc_stride,
      return_overflowing_tokens=True,
      return_offsets_mapping=True,
      padding=""max_length"",
  )
  if use_mixing:
    candidate_questions = generate_candidate_questions(examples)
    tokenized_candidates = tokenizer(
        candidate_questions if pad_on_right else examples[""context""],
        examples[""context""] if pad_on_right else candidate_questions,
        truncation=""only_second"" if pad_on_right else ""only_first"",
        max_length=max_length,
        stride=doc_stride,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding=""max_length"",
    )

  sample_mapping = tokenized_examples.pop(""overflow_to_sample_mapping"")

  tokenized_examples[""example_id""] = []

  if use_mixing:
    tokenized_examples[""candidate_input_ids""] = tokenized_candidates[""input_ids""]
    tokenized_examples[""candidate_attention_mask""] = tokenized_candidates[""attention_mask""]
    tokenized_examples[""candidate_token_type_ids""] = tokenized_candidates[""token_type_ids""]

  for i in range(len(tokenized_examples[""input_ids""])):
      sequence_ids = tokenized_examples.sequence_ids(i)
      context_index = 1 if pad_on_right else 0

      sample_index = sample_mapping[i]
      tokenized_examples[""example_id""].append(examples[""id""][sample_index])

      tokenized_examples[""offset_mapping""][i] = [
          (o if sequence_ids[k] == context_index else None)
          for k, o in enumerate(tokenized_examples[""offset_mapping""][i])
      ]

  return tokenized_examples



validation_features = datasets[""validation""].map(
    lambda xs: prepare_validation_features(xs, True),
    batched=True,
    remove_columns=datasets[""validation""].column_names
)
```

I guess this might happen because of the batched=True. I see similar issues in this repo related to arrow table length mismatch error, but in their cases, the numbers vary a lot. In my case, this error always happens when the expected length and unexpected length are very close. Thanks for the help!"
https://github.com/huggingface/datasets/issues/2068,PyTorch not available error on SageMaker GPU docker though it is installed ,"['cc @philschmid '
 'Hey @sivakhno,\r\n\r\nhow does your `requirements.txt` look like to install the `datasets` library and which version of it are you running? Can you try to install `datasets>=1.4.0`'
 'Hi @philschmid - thanks for suggestion. I am using `datasets==1.4.1`. \r\nI have also tried using `torch=1.6.0` (docker `763104351884.dkr.ecr.eu-central-1.amazonaws.com/pytorch-training:1.6.0-gpu-py3 `), but the error is the same. '
 'Could paste the code you use the start your training job and the fine-tuning script you run? '
 '@sivakhno this should be now fixed in `datasets>=1.5.0`. '
 '@philschmid Recently released tensorflow-macos seems to be missing. '
 ""I've created a PR to add this. ""]","I get en error when running data loading using SageMaker SDK

```
  File ""main.py"", line 34, in <module>
    run_training()
  File ""main.py"", line 25, in run_training
    dm.setup('fit')
  File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/core/datamodule.py"", line 92, in wrapped_fn
    return fn(*args, **kwargs)
  File ""/opt/ml/code/data_module.py"", line 103, in setup
    self.dataset[split].set_format(type=""torch"", columns=self.columns)
  File ""/opt/conda/lib/python3.6/site-packages/datasets/fingerprint.py"", line 337, in wrapper
    out = func(self, *args, **kwargs)
  File ""/opt/conda/lib/python3.6/site-packages/datasets/arrow_dataset.py"", line 995, in set_format
    _ = get_formatter(type, **format_kwargs)
File ""/opt/conda/lib/python3.6/site-packages/datasets/formatting/__init__.py"", line 114, in get_formatter
    raise _FORMAT_TYPES_ALIASES_UNAVAILABLE[format_type]
ValueError: PyTorch needs to be installed to be able to return PyTorch tensors.
```

when trying to execute dataset loading using this notebook  https://github.com/PyTorchLightning/pytorch-lightning/blob/master/notebooks/04-transformers-text-classification.ipynb, specifically lines 

```
self.columns = [c for c in self.dataset[split].column_names if c in self.loader_columns]
self.dataset[split].set_format(type=""torch"", columns=self.columns)
```

The SageMaker docker image used is 763104351884.dkr.ecr.eu-central-1.amazonaws.com/pytorch-training:1.4.0-gpu-py3 .

By running container interactively I have checked that torch loading completes successfully by executing `https://github.com/huggingface/datasets/blob/master/src/datasets/config.py#L39`. 

Also as a first line in the data loading module I have 

```
import os
os.environ[""USE_TF""] = ""0"" 
os.environ[""USE_TORCH""] = ""1"" 
````

But unfortunately the error stills persists. Any suggestions would be appreciated as I am stack.
Many Thanks! 

"
https://github.com/huggingface/datasets/issues/2067,Multiprocessing windows error,"['Hi ! Thanks for reporting.\r\nThis looks like a bug, could you try to provide a minimal code example that reproduces the issue ? This would be very helpful !\r\n\r\nOtherwise I can try to run the wav2vec2 code above on my side but probably not this week..'
 ""```\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset('glue', 'mrpc', split='train')\r\n\r\n\r\nupdated_dataset = dataset.map(lambda example: {'sentence1': 'My sentence: ' + example['sentence1']}, num_proc=4)\r\n\r\n```""
 '\r\n\r\n\r\n\r\n\r\nI was able to copy some of the shell \r\nThis is repeating every half second\r\nWin 10, Anaconda with python 3.8, datasets installed from main branche\r\n```\r\n\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\site-packages\\multiprocess\\spawn.py"", line 287, in _fixup_main_from_path\r\n    _check_not_importing_main()\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\site-packages\\multiprocess\\spawn.py"", line 116, in spawn_main\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\site-packages\\multiprocess\\spawn.py"", line 134, in _check_not_importing_main\r\n    main_content = runpy.run_path(main_path,\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\runpy.py"", line 265, in run_path\r\n    exitcode = _main(fd, parent_sentinel)\r\n    raise RuntimeError(\'\'\'\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\site-packages\\multiprocess\\spawn.py"", line 125, in _main\r\nRuntimeError:\r\n        An attempt has been made to start a new process before the\r\n        current process has finished its bootstrapping phase.\r\n\r\n        This probably means that you are not using fork to start your\r\n        child processes and you have forgotten to use the proper idiom\r\n        in the main module:\r\n\r\n            if __name__ == \'__main__\':\r\n                freeze_support()\r\n                ...\r\n\r\n        The ""freeze_support()"" line can be omitted if the program\r\n        is not going to be frozen to produce an executable.    return _run_module_code(code, init_globals, run_name,\r\n    prepare(preparation_data)\r\n\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\runpy.py"", line 97, in _run_module_code\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\site-packages\\multiprocess\\spawn.py"", line 236, in prepare\r\n    _run_code(code, mod_globals, init_globals,\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\runpy.py"", line 87, in _run_code\r\n    _fixup_main_from_path(data[\'init_main_from_path\'])\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\site-packages\\multiprocess\\spawn.py"", line 287, in _fixup_main_from_path\r\n    exec(code, run_globals)\r\n  File ""F:\\Codes\\Python Apps\\asr\\test.py"", line 6, in <module>\r\n    updated_dataset = dataset.map(lambda example: {\'sentence1\': \'My sentence: \' + example[\'sentence1\']}, num_proc=4)\r\n    main_content = runpy.run_path(main_path,\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\site-packages\\datasets\\arrow_dataset.py"", line 1370, in map\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\runpy.py"", line 265, in run_path\r\n    with Pool(num_proc, initargs=(RLock(),), initializer=tqdm.set_lock) as pool:\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\site-packages\\multiprocess\\context.py"", line 119, in Pool\r\n    return _run_module_code(code, init_globals, run_name,\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\runpy.py"", line 97, in _run_module_code\r\n    _run_code(code, mod_globals, init_globals,\r\n    return Pool(processes, initializer, initargs, maxtasksperchild,\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\runpy.py"", line 87, in _run_code\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\site-packages\\multiprocess\\pool.py"", line 212, in __init__\r\n    exec(code, run_globals)\r\n  File ""F:\\Codes\\Python Apps\\asr\\test.py"", line 6, in <module>\r\n    self._repopulate_pool()\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\site-packages\\multiprocess\\pool.py"", line 303, in _repopulate_pool\r\n    updated_dataset = dataset.map(lambda example: {\'sentence1\': \'My sentence: \' + example[\'sentence1\']}, num_proc=4)\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\site-packages\\datasets\\arrow_dataset.py"", line 1370, in map\r\n    return self._repopulate_pool_static(self._ctx, self.Process,\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\site-packages\\multiprocess\\pool.py"", line 326, in _repopulate_pool_static\r\n    with Pool(num_proc, initargs=(RLock(),), initializer=tqdm.set_lock) as pool:\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\site-packages\\multiprocess\\context.py"", line 119, in Pool\r\n    w.start()\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\site-packages\\multiprocess\\process.py"", line 121, in start\r\n    return Pool(processes, initializer, initargs, maxtasksperchild,\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\site-packages\\multiprocess\\pool.py"", line 212, in __init__\r\n    self._popen = self._Popen(self)\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\site-packages\\multiprocess\\context.py"", line 327, in _Popen\r\n    self._repopulate_pool()\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\site-packages\\multiprocess\\pool.py"", line 303, in _repopulate_pool\r\n    return Popen(process_obj)\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\site-packages\\multiprocess\\popen_spawn_win32.py"", line 45, in __init__\r\n    return self._repopulate_pool_static(self._ctx, self.Process,\r\n    prep_data = spawn.get_preparation_data(process_obj._name)\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\site-packages\\multiprocess\\pool.py"", line 326, in _repopulate_pool_static\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\site-packages\\multiprocess\\spawn.py"", line 154, in get_preparation_data\r\n    _check_not_importing_main()\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\site-packages\\multiprocess\\spawn.py"", line 134, in _check_not_importing_main\r\n    w.start()\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\site-packages\\multiprocess\\process.py"", line 121, in start\r\n    raise RuntimeError(\'\'\'\r\nRuntimeError:\r\n        An attempt has been made to start a new process before the\r\n        current process has finished its bootstrapping phase.\r\n\r\n        This probably means that you are not using fork to start your\r\n        child processes and you have forgotten to use the proper idiom\r\n        in the main module:\r\n\r\n            if __name__ == \'__main__\':\r\n                freeze_support()\r\n                ...\r\n```'
 ""Thanks this is really helpful !\r\nI'll try to reproduce on my side and come back to you""
 ""if __name__ == '__main__':\r\n\r\n\r\nThis line before calling the map function stops the error but the script still repeats endless""
 ""Indeed you needed `if __name__ == '__main__'` since accoding to [this stackoverflow post](https://stackoverflow.com/a/18205006):\r\n\r\n> On Windows the subprocesses will import (i.e. execute) the main module at start. You need to insert an if __name__ == '__main__': guard in the main module to avoid creating subprocesses recursively.\r\n\r\nRegarding the hanging issue, can you try to update `dill` and `multiprocess` ?""
 ""It's already on the newest version""
 '```\r\nTraceback (most recent call last):\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\shutil.py"", line 791, in move\r\n    os.rename(src, real_dst)\r\nFileExistsError: [WinError 183] Eine Datei kann nicht erstellt werden, wenn sie bereits vorhanden ist: \'D:\\\\huggingfacecache\\\\common_voice\\\\de\\\\6.1.0\\\\0041e06ab061b91d0a23234a2221e87970a19cf3a81b20901474cffffeb7869f\\\\tmpx9fl_jg8\' -> \'D:\\\\huggingfacecache\\\\common_voice\\\\de\\\\6.1.0\\\\0041e06ab061b91d0a23234a2221e87970a19cf3a81b20901474cffffeb7869f\\\\cache-9b4f203a63742dfc.arrow\'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File ""<string>"", line 1, in <module>\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\site-packages\\multiprocess\\spawn.py"", line 116, in spawn_main\r\n    exitcode = _main(fd, parent_sentinel)\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\site-packages\\multiprocess\\spawn.py"", line 125, in _main\r\n    prepare(preparation_data)\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\site-packages\\multiprocess\\spawn.py"", line 236, in prepare\r\n    _fixup_main_from_path(data[\'init_main_from_path\'])\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\site-packages\\multiprocess\\spawn.py"", line 287, in _fixup_main_from_path\r\n    main_content = runpy.run_path(main_path,\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\runpy.py"", line 265, in run_path\r\n    return _run_module_code(code, init_globals, run_name,\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\runpy.py"", line 97, in _run_module_code\r\n    _run_code(code, mod_globals, init_globals,\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\runpy.py"", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File ""F:\\Codes\\Python Apps\\asr\\cvtrain.py"", line 243, in <module>\r\n    common_voice_train = common_voice_train.map(remove_special_characters, remove_columns=[""sentence""])\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\site-packages\\datasets\\arrow_dataset.py"", line 1339, in map\r\n    return self._map_single(\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\site-packages\\datasets\\arrow_dataset.py"", line 203, in wrapper\r\n    out: Union[""Dataset"", ""DatasetDict""] = func(self, *args, **kwargs)\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\site-packages\\datasets\\fingerprint.py"", line 337, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\site-packages\\datasets\\arrow_dataset.py"", line 1646, in _map_single\r\n    shutil.move(tmp_file.name, cache_file_name)\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\shutil.py"", line 805, in move\r\n    copy_function(src, real_dst)\r\n  File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\shutil.py"", line 435, in copy2\r\n    copyfile(src, dst, follow_symlinks=follow_symlinks)\r\n  0%|                                                                                                                                                                                                 | 0/27771 [00:00<?, ?ex/s] \r\n File ""C:\\Users\\flozi\\anaconda3\\envs\\wav2vec\\lib\\shutil.py"", line 264, in copyfile\r\n    with open(src, \'rb\') as fsrc, open(dst, \'wb\') as fdst:\r\nOSError: [Errno 22] Invalid argument: \'D:\\\\huggingfacecache\\\\common_voice\\\\de\\\\6.1.0\\\\0041e06ab061b91d0a23234a2221e87970a19cf3a81b20901474cffffeb7869f\\\\cache-9b4f203a63742dfc.arrow\'\r\n```\r\n\r\nI was adding freeze support before calling the mapping function like this\r\nif __name__ == \'__main__\':\r\n    freeze_support()\r\n   dataset.map(....)'
 ""Usually OSError of an arrow file on windows means that the file is currently opened as a dataset object, so you can't overwrite it until the dataset object falls out of scope.\r\nCan you make sure that there's no dataset object that loaded the `cache-9b4f203a63742dfc.arrow` file ?""
 ""Now I understand\r\nThe error occures because the script got restarted in another thread, so the object is already loaded.\r\nStill don't have an idea why a new thread starts the whole script again""]","As described here https://huggingface.co/blog/fine-tune-xlsr-wav2vec2

When using the num_proc argument on windows the whole Python environment crashes and hanging in loop.
For example at the map_to_array part.
An error occures because the cache file already exists and windows throws and error. After this the log crashes into an loop "
https://github.com/huggingface/datasets/issues/2065,"Only user permission of saved cache files, not group","[""Hi ! Thanks for reporting.\r\n\r\nCurrently there's no way to specify this.\r\n\r\nWhen loading/processing a dataset, the arrow file is written using a temporary file. Then once writing is finished, it's moved to the cache directory (using `shutil.move` [here](https://github.com/huggingface/datasets/blob/f6b8251eb975f66a568356d2a40d86442c03beb9/src/datasets/arrow_dataset.py#L1646))\r\n\r\nThat means it keeps the permissions specified by the `tempfile.NamedTemporaryFile` object, i.e. `-rw-------` instead of `-rw-r--r--`. Improving this could be a nice first contribution to the library :)""
 ""Hi @lhoestq,\r\nI looked into this and yes you're right. The `NamedTemporaryFile` is always created with mode 0600, which prevents group from reading the file. Should we change the permissions of `tmp_file.name` [here](https://github.com/huggingface/datasets/blob/f6b8251eb975f66a568356d2a40d86442c03beb9/src/datasets/arrow_dataset.py#L1871) and [here](https://github.com/huggingface/datasets/blob/f6b8251eb975f66a568356d2a40d86442c03beb9/src/datasets/arrow_dataset.py#L1590), post creation to 0644 inorder for group and others to read it?""
 ""Good idea :) we could even update the permissions after the file has been moved by shutil.move [here](https://github.com/huggingface/datasets/blob/f6b8251eb975f66a568356d2a40d86442c03beb9/src/datasets/arrow_dataset.py#L1899) and [here](https://github.com/huggingface/datasets/blob/f6b8251eb975f66a568356d2a40d86442c03beb9/src/datasets/arrow_dataset.py#L1646) actually.\r\nApparently they set the default 0600 for temporary files for security reasons, so let's update the umask only after the file has been moved""
 ""Would it be possible to actually set the umask based on a user provided argument? For example, a popular usecase my team has is using a shared file-system for processing datasets. This may involve writing/deleting other files, or changing filenames, which a -rw-r--r-- wouldn't fix. ""
 'Note that you can get the cache files of a dataset with the `cache_files` attributes.\r\nThen you can `chmod` those files and all the other cache files in the same directory.\r\n\r\nMoreover we can probably keep the same permissions after each transform. This way you just need to set the permissions once after doing `load_dataset` for example, and then all the new transformed cached files will have the same permissions.\r\nWhat do you think ?'
 ""This means we'll check the permissions of other `cache_files` already created for a dataset before setting permissions for new  `cache_files`?""
 'You can just check the permission of `dataset.cache_files[0]` imo'
 '> This way you just need to set the permissions once after doing load_dataset for example, and then all the new transformed cached files will have the same permissions.\r\n\r\nI was referring to this. Ensuring that newly generated `cache_files` have the same permissions'
 'Yes exactly\r\n\r\nI imagine users can first do `load_dataset`, then chmod on the arrow files. After that all the new cache files could have the same permissions as the first arrow files. Opinions on this ?'
 ""Sounds nice but I feel this is a sub-part of the approach mentioned by @siddk. Instead of letting the user set new permissions by itself first and then making sure newly generated files have same permissions why don't we ask the user initially only what they want? What are your thoughts?""
 'Yes sounds good. Should this be a parameter in `load_dataset` ? Or an env variable ? Or use the value of `os.umask` ?'
 ""Ideally it should be a parameter in `load_dataset` but I'm not sure how important it is for the users (considering only important things should go into `load_dataset` parameters)""
 ""I think it's fairly important; for context, our team uses a shared file-system where many folks run experiments based on datasets that are cached by other users.\r\n\r\nFor example, I might start a training run, downloading a dataset. Then, a couple of days later, a collaborator using the same repository might want to use the same dataset on the same shared filesystem, but won't be able to under the default permissions.\r\n\r\nBeing able to specify directly in the top-level `load_dataset()` call seems important, but an equally valid option would be to just inherit from the running user's `umask` (this should probably be the default anyway).\r\n\r\nSo basically, argument that takes a custom set of permissions, and by default, use the running user's umask!""
 ""Maybe let's start by defaulting to the user's umask !\r\nDo you want to give it a try @bhavitvyamalik ?""
 ""Yeah sure! Instead of using default `0o644` should I first extract umask of current user and then use `os.umask` on it? We can do it inside `Dataset` class so that all folders/files created during the call use running user's umask\r\n\r\n""
 'You can get the umask using `os.umask` and then I guess you can just use `os.chmod` as in your previous PR, but with the right permissions depending on the umask.'
 'FWIW, we have this issue with other caches - e.g. `transformers` model files. So probably will need to backport this into `transformers` as well.\r\n\r\nthanks @thomwolf for the pointer.'
 'Hi @stas00,\r\nFor this should we use the same umask code in the respective model directory inside `TRANSFORMERS_CACHE`?'
 'That sounds very right to me, @bhavitvyamalik ']","Hello,

It seems when a cached file is saved from calling `dataset.map` for preprocessing, it gets the user permissions and none of the user's group permissions. As we share data files across members of our team, this is causing a bit of an issue as we have to continually reset the permission of the files. Do you know any ways around this or a way to correctly set the permissions?"
https://github.com/huggingface/datasets/issues/2061,Cannot load udpos subsets from xtreme dataset using load_dataset(),"['@lhoestq Adding ""_"" to the class labels in the dataset script will fix the issue.\r\n\r\nThe bigger issue IMO is that the data files are in conll format, but the examples are tokens, not sentences.'
 'Hi ! Thanks for reporting @adzcodez \r\n\r\n\r\n> @lhoestq Adding ""_"" to the class labels in the dataset script will fix the issue.\r\n> \r\n> The bigger issue IMO is that the data files are in conll format, but the examples are tokens, not sentences.\r\n\r\nYou\'re right: ""_"" should be added to the list of labels, and the examples must be sequences of tokens, not singles tokens.\r\n'
 '@lhoestq Can you please label this issue with the ""good first issue"" label? I\'m not sure I\'ll find time to fix this.\r\n\r\nTo resolve it, the user should:\r\n1. add `""_""` to the list of labels\r\n2. transform the udpos subset to the conll format (I think the preprocessing logic can be borrowed from [the original repo](https://github.com/google-research/xtreme/blob/58a76a0d02458c4b3b6a742d3fd4ffaca80ff0de/utils_preprocess.py#L187-L204))\r\n3. update the dummy data\r\n4. update the dataset info\r\n5. [optional] add info about the data fields structure of the udpos subset to the dataset readme'
 'I tried fixing this issue, but its working fine in the dev version : ""1.6.2.dev0""\r\n\r\nI think somebody already fixed it. '
 'Hi,\r\n\r\nafter #2326, the lines with pos tags equal to `""_""` are filtered out when generating the dataset, so this fixes the KeyError described above. However, the udpos subset should be in the conll format i.e. it should yield sequences of tokens and not single tokens, so it would be great to see this fixed (feel free to borrow the logic from [here](https://github.com/google-research/xtreme/blob/58a76a0d02458c4b3b6a742d3fd4ffaca80ff0de/utils_preprocess.py#L187-L204) if you decide to work on this). '
 'Closed by #2466.']","Hello, 

I am trying to load the udpos English subset from xtreme dataset, but this faces an error during loading. I am using datasets v1.4.1, pip install. I have tried with other udpos languages which also fail, though loading a different subset altogether (such as XNLI) has no issue. I have also tried on Colab and faced the same error. 

Reprex is: 

`from datasets import load_dataset `
`dataset = load_dataset('xtreme', 'udpos.English')`

The error is: 
`KeyError: '_'`

The full traceback is: 
KeyError                                  Traceback (most recent call last)
<ipython-input-5-7181359ea09d> in <module>
      1 from datasets import load_dataset
----> 2 dataset = load_dataset('xtreme', 'udpos.English')

~\Anaconda3\envs\mlenv\lib\site-packages\datasets\load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, script_version, use_auth_token, **config_kwargs)
    738 
    739     # Download and prepare data
--> 740     builder_instance.download_and_prepare(
    741         download_config=download_config,
    742         download_mode=download_mode,

~\Anaconda3\envs\mlenv\lib\site-packages\datasets\builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)
    576                             logger.warning(""HF google storage unreachable. Downloading and preparing it from source"")
    577                     if not downloaded_from_gcs:
--> 578                         self._download_and_prepare(
    579                             dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
    580                         )

~\Anaconda3\envs\mlenv\lib\site-packages\datasets\builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)
    654             try:
    655                 # Prepare split will record examples associated to the split
--> 656                 self._prepare_split(split_generator, **prepare_split_kwargs)
    657             except OSError as e:
    658                 raise OSError(

~\Anaconda3\envs\mlenv\lib\site-packages\datasets\builder.py in _prepare_split(self, split_generator)
    977                 generator, unit="" examples"", total=split_info.num_examples, leave=False, disable=not_verbose
    978             ):
--> 979                 example = self.info.features.encode_example(record)
    980                 writer.write(example)
    981         finally:

~\Anaconda3\envs\mlenv\lib\site-packages\datasets\features.py in encode_example(self, example)
    946     def encode_example(self, example):
    947         example = cast_to_python_objects(example)
--> 948         return encode_nested_example(self, example)
    949 
    950     def encode_batch(self, batch):

~\Anaconda3\envs\mlenv\lib\site-packages\datasets\features.py in encode_nested_example(schema, obj)
    840     # Nested structures: we allow dict, list/tuples, sequences
    841     if isinstance(schema, dict):
--> 842         return {
    843             k: encode_nested_example(sub_schema, sub_obj) for k, (sub_schema, sub_obj) in utils.zip_dict(schema, obj)
    844         }

~\Anaconda3\envs\mlenv\lib\site-packages\datasets\features.py in <dictcomp>(.0)
    841     if isinstance(schema, dict):
    842         return {
--> 843             k: encode_nested_example(sub_schema, sub_obj) for k, (sub_schema, sub_obj) in utils.zip_dict(schema, obj)
    844         }
    845     elif isinstance(schema, (list, tuple)):

~\Anaconda3\envs\mlenv\lib\site-packages\datasets\features.py in encode_nested_example(schema, obj)
    868     # ClassLabel will convert from string to int, TranslationVariableLanguages does some checks
    869     elif isinstance(schema, (ClassLabel, TranslationVariableLanguages, Value, _ArrayXD)):
--> 870         return schema.encode_example(obj)
    871     # Other object should be directly convertible to a native Arrow type (like Translation and Translation)
    872     return obj

~\Anaconda3\envs\mlenv\lib\site-packages\datasets\features.py in encode_example(self, example_data)
    647         # If a string is given, convert to associated integer
    648         if isinstance(example_data, str):
--> 649             example_data = self.str2int(example_data)
    650 
    651         # Allowing -1 to mean no label.

~\Anaconda3\envs\mlenv\lib\site-packages\datasets\features.py in str2int(self, values)
    605                 if value not in self._str2int:
    606                     value = value.strip()
--> 607                 output.append(self._str2int[str(value)])
    608             else:
    609                 # No names provided, try to integerize

KeyError: '_'

"
https://github.com/huggingface/datasets/issues/2059,Error while following docs to load the `ted_talks_iwslt` dataset,"['@skyprince999 as you authored the PR for this dataset, any comments?'
 ""This has been fixed in #2064  by @mariosasko (thanks again !)\r\n\r\nThe fix is available on the master branch and we'll do a new release very soon :)""]","I am currently trying to load the `ted_talks_iwslt` dataset into google colab.

The [docs](https://huggingface.co/datasets/ted_talks_iwslt) mention the following way of doing so.

```python
dataset = load_dataset(""ted_talks_iwslt"", language_pair=(""it"", ""pl""), year=""2014"")
```

Executing it results in the error attached below.

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-6-7dcc67154ef9> in <module>()
----> 1 dataset = load_dataset(""ted_talks_iwslt"", language_pair=(""it"", ""pl""), year=""2014"")

4 frames
/usr/local/lib/python3.7/dist-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, script_version, use_auth_token, **config_kwargs)
    730         hash=hash,
    731         features=features,
--> 732         **config_kwargs,
    733     )
    734 

/usr/local/lib/python3.7/dist-packages/datasets/builder.py in __init__(self, writer_batch_size, *args, **kwargs)
    927 
    928     def __init__(self, *args, writer_batch_size=None, **kwargs):
--> 929         super(GeneratorBasedBuilder, self).__init__(*args, **kwargs)
    930         # Batch size used by the ArrowWriter
    931         # It defines the number of samples that are kept in memory before writing them

/usr/local/lib/python3.7/dist-packages/datasets/builder.py in __init__(self, cache_dir, name, hash, features, **config_kwargs)
    241             name,
    242             custom_features=features,
--> 243             **config_kwargs,
    244         )
    245 

/usr/local/lib/python3.7/dist-packages/datasets/builder.py in _create_builder_config(self, name, custom_features, **config_kwargs)
    337             if ""version"" not in config_kwargs and hasattr(self, ""VERSION"") and self.VERSION:
    338                 config_kwargs[""version""] = self.VERSION
--> 339             builder_config = self.BUILDER_CONFIG_CLASS(**config_kwargs)
    340 
    341         # otherwise use the config_kwargs to overwrite the attributes

/root/.cache/huggingface/modules/datasets_modules/datasets/ted_talks_iwslt/024d06b1376b361e59245c5878ab8acf9a7576d765f2d0077f61751158e60914/ted_talks_iwslt.py in __init__(self, language_pair, year, **kwargs)
    219             description=description,
    220             version=datasets.Version(""1.1.0"", """"),
--> 221             **kwargs,
    222         )
    223 

TypeError: __init__() got multiple values for keyword argument 'version'
```

How to resolve this? 

PS: Thanks a lot @huggingface team for creating this great library!"
https://github.com/huggingface/datasets/issues/2056,issue with opus100/en-fr dataset ,"['@lhoestq  I also deleted the cache and redownload the file and still the same issue, I appreciate any help on this. thanks '
 'Here please find the minimal code to reproduce the issue @lhoestq  note this only happens with MT5TokenizerFast\r\n\r\n```\r\nfrom datasets import load_dataset\r\nfrom transformers import MT5TokenizerFast\r\n\r\ndef get_tokenized_dataset(dataset_name, dataset_config_name, tokenizer):\r\n    datasets = load_dataset(dataset_name, dataset_config_name, script_version=""master"")\r\n    column_names = datasets[""train""].column_names\r\n    text_column_name = ""translation""\r\n    def process_dataset(datasets):\r\n        def process_function(examples):\r\n            lang = ""fr""\r\n            return {""src_texts"": [example[lang] for example in examples[text_column_name]]}\r\n        datasets = datasets.map(\r\n            process_function,\r\n            batched=True,\r\n            num_proc=None,\r\n            remove_columns=column_names,\r\n            load_from_cache_file=True,\r\n        )\r\n        return datasets\r\n    datasets = process_dataset(datasets)\r\n    text_column_name = ""src_texts""\r\n    column_names = [""src_texts""]\r\n    def tokenize_function(examples):\r\n            return tokenizer(examples[text_column_name], return_special_tokens_mask=True)\r\n    tokenized_datasets = datasets.map(\r\n            tokenize_function,\r\n            batched=True,\r\n            num_proc=None,\r\n            remove_columns=column_names,\r\n            load_from_cache_file=True\r\n    )\r\n\r\nif __name__ == ""__main__"":\r\n     tokenizer_kwargs = {\r\n        ""cache_dir"": None,\r\n        ""use_fast"": True,\r\n        ""revision"": ""main"",\r\n        ""use_auth_token"": None\r\n     }\r\n     tokenizer = MT5TokenizerFast.from_pretrained(""google/mt5-small"", **tokenizer_kwargs)\r\n     get_tokenized_dataset(dataset_name=""opus100"", dataset_config_name=""en-fr"", tokenizer=tokenizer)\r\n~     \r\n```'
 'as per https://github.com/huggingface/tokenizers/issues/626 this looks like to be the tokenizer bug, I therefore, reported it there https://github.com/huggingface/tokenizers/issues/626 and  I am closing this one.']","Hi
I am running run_mlm.py code of huggingface repo with opus100/fr-en pair, I am getting this error, note that this error occurs for only this pairs and not the other pairs. Any idea why this is occurring? and how I can solve this? 

Thanks a lot  @lhoestq for your help in advance.

`
thread '<unnamed>' panicked at 'index out of bounds: the len is 617 but the index is 617', /__w/tokenizers/tokenizers/tokenizers/src/tokenizer/normalizer.rs:382:21
note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace
 63%|██████████████████████████████████████████████████████████▊                                   | 626/1000 [00:27<00:16, 22.69ba/s]

Traceback (most recent call last):
  File ""run_mlm.py"", line 550, in <module>
    main()
  File ""run_mlm.py"", line 412, in main
    in zip(data_args.dataset_name, data_args.dataset_config_name)]
  File ""run_mlm.py"", line 411, in <listcomp>
    logger) for dataset_name, dataset_config_name\
  File ""/user/dara/dev/codes/seq2seq/data/tokenize_datasets.py"", line 96, in get_tokenized_dataset
    load_from_cache_file=not data_args.overwrite_cache,
  File ""/user/dara/libs/anaconda3/envs/fast/lib/python3.7/site-packages/datasets/dataset_dict.py"", line 448, in map
    for k, dataset in self.items()
  File ""/user/dara/libs/anaconda3/envs/fast/lib/python3.7/site-packages/datasets/dataset_dict.py"", line 448, in <dictcomp>
    for k, dataset in self.items()
  File ""/user/dara/libs/anaconda3/envs/fast/lib/python3.7/site-packages/datasets/arrow_dataset.py"", line 1309, in map
    update_data=update_data,
  File ""/user/dara/libs/anaconda3/envs/fast/lib/python3.7/site-packages/datasets/arrow_dataset.py"", line 204, in wrapper
    out: Union[""Dataset"", ""DatasetDict""] = func(self, *args, **kwargs)
  File ""/user/dara/libs/anaconda3/envs/fast/lib/python3.7/site-packages/datasets/fingerprint.py"", line 337, in wrapper
    out = func(self, *args, **kwargs)
  File ""/user/dara/libs/anaconda3/envs/fast/lib/python3.7/site-packages/datasets/arrow_dataset.py"", line 1574, in _map_single
    batch, indices, check_same_num_examples=len(self.list_indexes()) > 0, offset=offset
  File ""/user/dara/libs/anaconda3/envs/fast/lib/python3.7/site-packages/datasets/arrow_dataset.py"", line 1490, in apply_function_on_filtered_inputs
    function(*fn_args, effective_indices, **fn_kwargs) if with_indices else function(*fn_args, **fn_kwargs)
  File ""/user/dara/dev/codes/seq2seq/data/tokenize_datasets.py"", line 89, in tokenize_function
    return tokenizer(examples[text_column_name], return_special_tokens_mask=True)
  File ""/user/dara/libs/anaconda3/envs/fast/lib/python3.7/site-packages/transformers/tokenization_utils_base.py"", line 2347, in __call__
    **kwargs,
  File ""/user/dara/libs/anaconda3/envs/fast/lib/python3.7/site-packages/transformers/tokenization_utils_base.py"", line 2532, in batch_encode_plus
    **kwargs,
  File ""/user/dara/libs/anaconda3/envs/fast/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py"", line 384, in _batch_encode_plus
    is_pretokenized=is_split_into_words,
pyo3_runtime.PanicException: index out of bounds: the len is 617 but the index is 617

`"
https://github.com/huggingface/datasets/issues/2055,is there a way to override a dataset object saved with save_to_disk?,"['Hi\r\nYou can rename the arrow file and update the name in `state.json`'
 'I tried this way, but when there is a mapping process to the dataset, it again uses a random cache name. atm, I am trying to use the following method by setting an exact cache file,\r\n\r\n```\r\n            dataset_with_embedding =csv_dataset.map(\r\n                partial(self.embed, ctx_encoder=ctx_encoder, ctx_tokenizer=self.context_tokenizer),\r\n                batched=True,\r\n                batch_size=1,\r\n                features=new_features,\r\n                cache_file_name=cache_arrow_path,\r\n                load_from_cache_file=False\r\n            )\r\n```\r\nSo here we set a cache_file_name , after this it uses the same  file name when saving again and again. '
 ""I'm not sure I understand your issue, can you elaborate ?\r\n\r\n`cache_file_name` is indeed an argument you can set to specify the cache file that will be used for the processed dataset. By default the file is named with something like `cache-<fingerprint>.arrow` where the fingerprint is a hash.""
 ""Let's say I am updating a set of embedding in a dataset that is around 40GB inside a training loop every 500 steps (Ex: calculating the embeddings in updated ctx_encoder in RAG and saving it to the passage path).  So when we use **dataset_object.save_to_disk('passage_path_directory')** it will save the new dataset object every time with a random file name, especially when we do some transformations to dataset objects such as map or shards. This way, we keep collecting unwanted files that will eventually eat up all the disk space. \r\n\r\nBut if we can save the dataset object every time by a single name like **data_shard_1.arrow**, it will automatically remove the previous file and save the new one in the same directory.  I found the above-mentioned code snippet useful to complete this task. \r\n\r\nIs this clear?""]","At the moment when I use save_to_disk, it uses the arbitrary name for the arrow file.  Is there a way to override such an object? "
https://github.com/huggingface/datasets/issues/2054,Could not find file for ZEST dataset,"['The zest dataset url was changed (allenai/zest#3) and #2057 should resolve this.'
 ""This has been fixed in #2057 by @matt-peters (thanks again !)\r\n\r\nThe fix is available on the master branch and we'll do a new release very soon :)""
 'Thanks @lhoestq and @matt-peters '
 'I am closing this issue since its fixed!']","I am trying to use zest dataset from Allen AI using below code in colab,
```
!pip install -q datasets
from datasets import load_dataset
dataset = load_dataset(""zest"")
```

I am getting the following error,
```
Using custom data configuration default

Downloading and preparing dataset zest/default (download: 5.53 MiB, generated: 19.96 MiB, post-processed: Unknown size, total: 25.48 MiB) to /root/.cache/huggingface/datasets/zest/default/0.0.0/1f7a230fbfc964d979bbca0f0130fbab3259fce547ee758ad8aa4f9c9bec6cca...
---------------------------------------------------------------------------
FileNotFoundError                         Traceback (most recent call last)
<ipython-input-6-18dbbc1a4b8a> in <module>()
      1 from datasets import load_dataset
      2 
----> 3 dataset = load_dataset(""zest"")

9 frames
/usr/local/lib/python3.7/dist-packages/datasets/utils/file_utils.py in get_from_cache(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, use_auth_token)
    612             )
    613         elif response is not None and response.status_code == 404:
--> 614             raise FileNotFoundError(""Couldn't find file at {}"".format(url))
    615         _raise_if_offline_mode_is_enabled(f""Tried to reach {url}"")
    616         raise ConnectionError(""Couldn't reach {}"".format(url))

FileNotFoundError: Couldn't find file at https://ai2-datasets.s3-us-west-2.amazonaws.com/zest/zest.zip
```"
https://github.com/huggingface/datasets/issues/2052,Timit_asr dataset repeats examples,"['Hi,\r\n\r\nthis was fixed by #1995, so you can wait for the next release or install the package directly from the master branch with the following command: \r\n```bash\r\npip install git+https://github.com/huggingface/datasets\r\n```'
 'Ty!']","Summary

When loading timit_asr dataset on datasets 1.4+, every row in the dataset is the same
Steps to reproduce

As an example, on this code there is the text from the training part:

Code snippet:
```
from datasets import load_dataset, load_metric

timit = load_dataset(""timit_asr"")
timit['train']['text']
#['Would such an act of refusal be useful?',
# 'Would such an act of refusal be useful?',
# 'Would such an act of refusal be useful?',
# 'Would such an act of refusal be useful?',
# 'Would such an act of refusal be useful?',
# 'Would such an act of refusal be useful?',
```
The same behavior happens for other columns

Expected behavior:

Different info on the actual timit_asr dataset

Actual behavior:

When loading timit_asr dataset on datasets 1.4+, every row in the dataset is the same. I've checked datasets 1.3 and the rows are different
Debug info

    Streamlit version: (get it with $ streamlit version)
    Python version: Python 3.6.12
    Using Conda? PipEnv? PyEnv? Pex? Using pip
    OS version: Centos-release-7-9.2009.1.el7.centos.x86_64

Additional information

You can check the same behavior on https://huggingface.co/datasets/viewer/?dataset=timit_asr"
https://github.com/huggingface/datasets/issues/2050,Build custom dataset to fine-tune Wav2Vec2,"['@lhoestq - We could simply use the ""general"" json dataset for this no? '
 'Sure you can use the json loader\r\n```python\r\ndata_files = {""train"": ""path/to/your/train_data.json"", ""test"": ""path/to/your/test_data.json""}\r\ntrain_dataset = load_dataset(""json"", data_files=data_files, split=""train"")\r\ntest_dataset = load_dataset(""json"", data_files=data_files, split=""test"")\r\n```\r\n\r\nYou just need to make sure that the data contain the paths to the audio files.\r\nIf not, feel free to use `.map()` to add them.'
 'Many thanks! that was what I was looking for. ']","Thank you for your recent tutorial on how to finetune Wav2Vec2 on a custom dataset. The example you gave here (https://huggingface.co/blog/fine-tune-xlsr-wav2vec2) was on the CommonVoice dataset. However, what if I want to load my own dataset?  I have a manifest (transcript and their audio files) in a JSON file. 
"
https://github.com/huggingface/datasets/issues/2046,add_faisis_index  gets very slow when doing it interatively  ,"['I think faiss automatically sets the number of threads to use to build the index.\r\nCan you check how many CPU cores are being used when you build the index in `use_own_knowleldge_dataset` as compared to this script ? Are there other programs running (maybe for rank>0) ?'
 'Hi,\r\n I am running the add_faiss_index during the training process of the RAG from the master process (rank 0). But at the exact moment, I do not run any other process since I do it in every 5000 training steps. \r\n \r\n I think what you say is correct. It depends on the number of CPU cores. I did an experiment to compare the time taken to finish the add_faiss_index process on use_own_knowleldge_dataset.py vs the training loop thing.  The training loop thing takes 40 mins more. It might be natural right? \r\n \r\n \r\n at the moment it uses around 40 cores of a 96 core machine (I am fine-tuning the entire process).  '
 'Can you try to set the number of threads manually ?\r\nIf you set the same number of threads for both the `use_own_knowledge_dataset.py` and RAG training, it should take the same amount of time.\r\nYou can see how to set the number of thread in the faiss wiki: https://github.com/facebookresearch/faiss/wiki/Threads-and-asynchronous-calls'
 'Ok, I will report the details too soon. I am the first one on the list and currently add_index being computed for the 3rd time in the loop. Actually seems like the time is taken to complete each interaction is the same, but around 1 hour more compared to running it without the training loop. A the moment this takes 5hrs and 30 mins.  If there is any way to faster the process, an end-to-end rag will be perfect. So I will also try out with different thread numbers too.  \r\n\r\n![image](https://user-images.githubusercontent.com/16892570/111453464-798c5f80-8778-11eb-86d0-19d212f58e38.png)\r\n'
 '@lhoestq  on a different note, I read about using Faiss-GPU, but the documentation says we should use it when the dataset has the ability to fit into the GPU memory. Although this might work, in the long-term this is not that practical for me.\r\n\r\nhttps://github.com/matsui528/faiss_tips'
 '@lhoestq \r\n\r\nHi,  I executed the **use_own_dataset.py** script independently and ask a few of my friends to run their programs in the HPC machine at the same time.  \r\n\r\n Once there are so many other processes are running the add_index function gets slows down naturally.  So basically the speed of the add_index depends entirely on the number of CPU processes.  Then I set the number of threads as you have mentioned and got actually the same time for RAG training and independat running.  So you are correct! :) \r\n\r\n \r\n Then I added this [issue in Faiss repostiary](https://github.com/facebookresearch/faiss/issues/1767). I got an answer saying our current **IndexHNSWFlat** can get slow for 30 million vectors and it would be better to use alternatives. What do you think?'
 'It\'s a matter of tradeoffs.\r\nHSNW is fast at query time but takes some time to build.\r\nA flat index is flat to build but is ""slow"" at query time.\r\nAn IVF index is probably a good choice for you: fast building and fast queries (but still slower queries than HSNW).\r\n\r\nNote that for an IVF index you would need to have an `nprobe` parameter (number of cells to visit for one query, there are `nlist` in total) that is not too small in order to have good retrieval accuracy, but not too big otherwise the queries will take too much time. From the faiss documentation:\r\n> The nprobe parameter is always a way of adjusting the tradeoff between speed and accuracy of the result. Setting nprobe = nlist gives the same result as the brute-force search (but slower).\r\n\r\nFrom my experience with indexes on DPR embeddings, setting nprobe around 1/4 of nlist gives really good retrieval accuracy and there\'s no need to have a value higher than that (or you would need to brute-force in order to see a difference).'
 '@lhoestq \r\n\r\nThanks a lot for sharing all this prior knowledge. \r\n\r\nJust asking what would be a good nlist of parameters for 30 million embeddings?'
 'When IVF is used alone, nlist should be between `4*sqrt(n)` and `16*sqrt(n)`.\r\nFor more details take a look at [this section of the Faiss wiki](https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index#how-big-is-the-dataset)'
 'Thanks a lot. I was lost with calling the index from class and using faiss_index_factory. '
 '@lhoestq  Thanks a lot for the help you have given to solve this issue. As per my experiments, IVF index suits well for my case and it is a lot faster. The use of this can make the entire RAG end-to-end trainable lot faster.  So I will close this issue. Will do the final PR soon. ']","As the below code suggests, I want to run add_faisis_index in every nth interaction from the training loop. I have 7.2 million documents. Usually, it takes 2.5 hours (if I run an as a separate process similar to the script given in rag/use_own_knowleldge_dataset.py). Now, this takes usually 5hrs. Is this normal?   Any way to make this process faster? 

@lhoestq 

```
   def training_step(self, batch, batch_idx) -> Dict:

    
        if (not batch_idx==0) and (batch_idx%5==0):

            print(""******************************************************"")
            ctx_encoder=self.trainer.model.module.module.model.rag.ctx_encoder
            model_copy =type(ctx_encoder)(self.config_dpr) # get a new instance  #this will be load in the CPU
            model_copy.load_state_dict(ctx_encoder.state_dict()) # copy weights and stuff


            list_of_gpus = ['cuda:2','cuda:3']
            c_dir='/custom/cache/dir'

            kb_dataset = load_dataset(""csv"", data_files=[self.custom_config.csv_path], split=""train"", delimiter=""\t"", column_names=[""title"", ""text""],cache_dir=c_dir) 

            print(kb_dataset)

      
            n=len(list_of_gpus) #nunber of dedicated GPUs
            kb_list=[kb_dataset.shard(n, i, contiguous=True) for i in range(n)]

            #kb_dataset.save_to_disk('/hpc/gsir059/MY-Test/RAY/transformers/examples/research_projects/rag/haha-dir')


            print(self.trainer.global_rank)
            dataset_shards = self.re_encode_kb(model_copy.to(device=list_of_gpus[self.trainer.global_rank]),kb_list[self.trainer.global_rank])
            output = [None for _ in list_of_gpus]

            #self.trainer.accelerator_connector.accelerator.barrier(""embedding_process"")
            dist.all_gather_object(output, dataset_shards)
            

            #This creation and re-initlaization of the new index
            if (self.trainer.global_rank==0):  #saving will be done in the main process 
           
                combined_dataset = concatenate_datasets(output)
    
                passages_path =self.config.passages_path

                logger.info(""saving the dataset with  "")
                #combined_dataset.save_to_disk('/hpc/gsir059/MY-Test/RAY/transformers/examples/research_projects/rag/MY-Passage')
                combined_dataset.save_to_disk(passages_path)
                logger.info(""Add faiss index to the dataset that consist of embeddings"") 

    
                embedding_dataset=combined_dataset
                index = faiss.IndexHNSWFlat(768, 128, faiss.METRIC_INNER_PRODUCT)
                embedding_dataset.add_faiss_index(""embeddings"", custom_index=index)

                embedding_dataset.get_index(""embeddings"").save(self.config.index_path)

"
https://github.com/huggingface/datasets/issues/2040,ValueError: datasets' indices [1] come from memory and datasets' indices [0] come from disk,"[""Hi ! To help me understand the situation, can you print the values of  `load_from_disk(PATH_DATA_CLS_A)['train']._indices_data_files` and `load_from_disk(PATH_DATA_CLS_B)['train']._indices_data_files` ?\r\nThey should both have a path to an arrow file\r\n\r\nAlso note that from #2025 concatenating datasets will no longer have such restrictions.""
 ""Sure, thanks for the fast reply!\r\n\r\nFor dataset A: `[{'filename': 'drive/MyDrive/data_target_task/dataset_a/train/cache-4797266bf4db1eb7.arrow'}]`\r\nFor dataset B: `[]`\r\n\r\nNo clue why for B it returns nothing. `PATH_DATA_CLS_B` is exactly the same in `save_to_disk` and `load_from_disk`... Also I can verify that the folder physically exists under 'drive/MyDrive/data_target_task/dataset_b/'""
 ""In the next release you'll be able to concatenate any kinds of dataset (either from memory or from disk).\r\n\r\nFor now I'd suggest you to flatten the indices of the A and B datasets. This will remove the indices mapping and you will be able to concatenate them. You can flatten the indices with\r\n```python\r\ndataset = dataset.flatten_indices()\r\n```""
 'Indeed this works. Not the most elegant solution, but it does the trick. Thanks a lot! ']","Hi there,

I am trying to concat two datasets that I've previously saved to disk via `save_to_disk()` like so (note that both are saved as `DataDict`, `PATH_DATA_CLS_*` are `Path`-objects):
```python
concatenate_datasets([load_from_disk(PATH_DATA_CLS_A)['train'], load_from_disk(PATH_DATA_CLS_B)['train']])
```
Yielding the following error:
```python
ValueError: Datasets' indices should ALL come from memory, or should ALL come from disk.
However datasets' indices [1] come from memory and datasets' indices [0] come from disk.
```
Been trying to solve this for quite some time now. Both `DataDict` have been created by reading in a `csv` via `load_dataset` and subsequently processed using the various `datasets` methods (i.e. filter, map, remove col, rename col). Can't figure out tho...

`load_from_disk(PATH_DATA_CLS_A)['train']` yields:
```python
Dataset({
    features: ['labels', 'text'],
    num_rows: 785
})
```
`load_from_disk(PATH_DATA_CLS_B)['train']` yields:
```python
Dataset({
    features: ['labels', 'text'],
    num_rows: 3341
})
```"
https://github.com/huggingface/datasets/issues/2038,outdated dataset_infos.json might fail verifications,"['Hi ! Thanks for reporting.\r\n\r\nTo update the dataset_infos.json you can run:\r\n```\r\ndatasets-cli test ./datasets/doc2dial --all_configs --save_infos --ignore_verifications\r\n```'
 'Fixed by #2041, thanks again @songfeng !']","The [doc2dial/dataset_infos.json](https://github.com/huggingface/datasets/blob/master/datasets/doc2dial/dataset_infos.json) is outdated. It would fail data_loader when verifying download checksum etc..

Could you please update this file or point me how to update this file?

Thank you."
https://github.com/huggingface/datasets/issues/2036,Cannot load wikitext,['Solved!'],"when I execute these codes
```
>>> from datasets import load_dataset
>>> test_dataset = load_dataset(""wikitext"")
```

I got an error,any help?

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/xxx/anaconda3/envs/transformer/lib/python3.7/site-packages/datasets/load.py"", line 589, in load_dataset
    path, script_version=script_version, download_config=download_config, download_mode=download_mode, dataset=True
  File ""/home/xxx/anaconda3/envs/transformer/lib/python3.7/site-packages/datasets/load.py"", line 267, in prepare_module
    local_path = cached_path(file_path, download_config=download_config)
  File ""/home/xxx/anaconda3/envs/transformer/lib/python3.7/site-packages/datasets/utils/file_utils.py"", line 308, in cached_path
    use_etag=download_config.use_etag,
  File ""/home/xxx/anaconda3/envs/transformer/lib/python3.7/site-packages/datasets/utils/file_utils.py"", line 487, in get_from_cache
    raise ConnectionError(""Couldn't reach {}"".format(url))
ConnectionError: Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/wikitext/wikitext.py
```"
https://github.com/huggingface/datasets/issues/2035,wiki40b/wikipedia for almost all languages cannot be downloaded,"['Dear @lhoestq for wikipedia dataset I also get the same error, I greatly appreciate if you could have a look into this dataset as well. Below please find the command to reproduce the error:\r\n\r\n```\r\ndataset = load_dataset(""wikipedia"", ""20200501.bg"")\r\nprint(dataset)\r\n```\r\n\r\nYour library is my only chance to be able training  the models at scale and I am grateful for your help.\r\n\r\n'
 'Hi @dorost1234,\r\nTry installing this library first, `pip install \'apache-beam[gcp]\' --use-feature=2020-resolver` followed by loading dataset like this using beam runner.\r\n\r\n`dataset = load_dataset(""wiki40b"", ""cs"", beam_runner=\'DirectRunner\')`\r\n\r\n I also read in error stack trace that:\r\n\r\n> Trying to generate a dataset using Apache Beam, yet no Beam Runner or PipelineOptions() has been provided in `load_dataset` or in the builder arguments. For big datasets it has to run on large-scale data processing tools like Dataflow, Spark, etc.\r\n\r\nWorked perfectly fine after this (Ignore these warnings)\r\n\r\n![image](https://user-images.githubusercontent.com/19718818/110908410-c7e2ce00-8334-11eb-8d10-7354359e9ec3.png)\r\n\r\n'
 ""For wikipedia dataset, looks like the files it's looking for are no longer available. For `bg`, I checked [here](https://dumps.wikimedia.org/bgwiki/). For this I think `dataset_infos.json` for this dataset has to made again? You'll have to load this dataset also using beam runner.\r\n\r\n""
 'Hello @dorost1234,\r\n\r\nIndeed, Wikipedia datasets need a lot of preprocessing and this is done using Apache Beam. That is the reason why it is required that you install Apache Beam in order to preform this preprocessing.\r\n\r\nFor some specific default parameters (English Wikipedia), Hugging Face has already preprocessed the dataset for you (and it is stored in the cloud). That is the reason why you do not get the error for English: the preprocessing is already done by HF and you just get the preprocessed dataset; Apache Beam is not required in that case.'
 'Hi\nI really appreciate if huggingface can kindly provide preprocessed\ndatasets, processing these datasets require sufficiently large resources\nand I do not have unfortunately access to, and perhaps many others too.\nthanks\n\nOn Fri, Mar 12, 2021 at 9:04 AM Albert Villanova del Moral <\n***@***.***> wrote:\n\n> Hello @dorost1234 <https://github.com/dorost1234>,\n>\n> Indeed, Wikipedia datasets need a lot of preprocessing and this is done\n> using Apache Beam. That is the reason why it is required that you install\n> Apache Beam in order to preform this preprocessing.\n>\n> For some specific default parameters (English Wikipedia), Hugging Face has\n> already preprocessed the dataset for you (and it is stored in the cloud).\n> That is the reason why you do not get the error for English: the\n> preprocessing is already done by HF and you just get the preprocessed\n> dataset; Apache Beam is not required in that case.\n>\n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/huggingface/datasets/issues/2035#issuecomment-797310899>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AS37NMXACFQZAGMK4VGXRETTDHDI3ANCNFSM4ZA5R2UA>\n> .\n>\n'
 'Hi everyone\r\nthanks for the helpful pointers, I did it as @bhavitvyamalik suggested, for me this freezes on this command for several hours, \r\n\r\n`Downloading and preparing dataset wiki40b/cs (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /users/dara/cache/datasets/wiki40b/cs/1.1.0/063778187363ffb294896eaa010fc254b42b73e31117c71573a953b0b0bf010f...\r\n`\r\n\r\nDo you know how long this takes? Any specific requirements the machine should have? like very large memory or so? @lhoestq \r\n\r\nthanks \r\n\r\n\r\n'
 ""HI @dorost1234, \r\nThe dataset size is 631.84 MiB so depending on your internet speed it'll take some time. You can monitor your internet speed meanwhile to see if it's downloading the dataset or not (use `nload` if you're using linux/mac to monitor the same). In my case it took around 3-4 mins. Since they haven't used `download_and_extract` here that's why there's no download progress bar.""
 'Hi\r\nthanks, my internet speed should be good, but this really freezes for me, this is how I try to get this dataset:\r\n\r\n`from datasets import load_dataset\r\ndataset = load_dataset(""wiki40b"", ""cs"", beam_runner=\'DirectRunner\')`\r\n\r\nthe output I see if different also from what you see after writing  this command:\r\n\r\n`Downloading and preparing dataset wiki40b/cs (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /users/dara/cache/datasets/wiki40b/cs/1.1.0/063778187363ffb294896eaa010fc254b42b73e31117c71573a953b0b0bf010f...`\r\n\r\ndo you have any idea why it might get freezed? anything I am missing @lhoestq @bhavitvyamalik. Do I need maybe to set anything special for apache-beam? \r\n\r\nthanks a lot \r\n\r\nOn Tue, Mar 16, 2021 at 9:03 AM Bhavitvya Malik ***@***.***>\r\nwrote:\r\n\r\n> HI @dorost1234 <https://github.com/dorost1234>,\r\n> The dataset size is 631.84 MiB so depending on your internet speed it\'ll\r\n> take some time. You can monitor your internet speed meanwhile to see if\r\n> it\'s downloading the dataset or not (use nload if you\'re using linux/mac\r\n> to monitor the same). In my case it took around 3-4 mins. Since they\r\n> haven\'t used download_and_extract here that\'s why there\'s no download\r\n> progress bar.\r\n>\r\n> —\r\n> You are receiving this because you were mentioned.\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/huggingface/datasets/issues/2035#issuecomment-800044303>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AS37NMQIHNNLM2LGG6QKZ73TD4GDJANCNFSM4ZA5R2UA>\r\n> .\r\n>\r\n'
 'I tried this on another machine (followed the same procedure I\'ve mentioned above). This is what it shows (during the freeze period) for me:\r\n```\r\n>>> dataset = load_dataset(""wiki40b"", ""cs"", beam_runner=\'DirectRunner\')\r\nDownloading: 5.26kB [00:00, 1.23MB/s]                                                                                                                                    \r\nDownloading: 1.40kB [00:00, 327kB/s]                                                                                                                                     \r\nDownloading and preparing dataset wiki40b/cs (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/bhavitvya/.cache/huggingface/datasets/wiki40b/cs/1.1.0/063778187363ffb294896eaa010fc254b42b73e31117c71573a953b0b0bf010f...\r\nWARNING:apache_beam.internal.gcp.auth:Unable to find default credentials to use: The Application Default Credentials are not available. They are available if running in Google Compute Engine. Otherwise, the environment variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing to a file defining the credentials. See https://developers.google.com/accounts/docs/application-default-credentials for more information.\r\nConnecting anonymously.\r\nWARNING:apache_beam.io.tfrecordio:Couldn\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\r\n```\r\nAfter around 10 minutes, here\'s the loading of dataset:\r\n```\r\n100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:16<00:00, 16.42s/sources]\r\n100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.12sources/s]\r\n100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.14sources/s]\r\nDataset wiki40b downloaded and prepared to /home/bhavitvya/.cache/huggingface/datasets/wiki40b/cs/1.1.0/063778187363ffb294896eaa010fc254b42b73e31117c71573a953b0b0bf010f. Subsequent calls will reuse this data.\r\n```'
 'Hi\r\nI honestly also now tried on another machine and nothing shows up after\r\nhours of waiting. Are you sure you have not set any specific setting? maybe\r\ngoogle cloud which seems it is used here, needs some credential setting?\r\nthanks for any suggestions on this\r\n\r\nOn Tue, Mar 16, 2021 at 10:02 AM Bhavitvya Malik ***@***.***>\r\nwrote:\r\n\r\n> I tried this on another machine (followed the same procedure I\'ve\r\n> mentioned above). This is what it shows (during the freeze period) for me:\r\n>\r\n> >>> dataset = load_dataset(""wiki40b"", ""cs"", beam_runner=\'DirectRunner\')\r\n> Downloading: 5.26kB [00:00, 1.23MB/s]\r\n> Downloading: 1.40kB [00:00, 327kB/s]\r\n> Downloading and preparing dataset wiki40b/cs (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/bhavitvya/.cache/huggingface/datasets/wiki40b/cs/1.1.0/063778187363ffb294896eaa010fc254b42b73e31117c71573a953b0b0bf010f...\r\n> WARNING:apache_beam.internal.gcp.auth:Unable to find default credentials to use: The Application Default Credentials are not available. They are available if running in Google Compute Engine. Otherwise, the environment variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing to a file defining the credentials. See https://developers.google.com/accounts/docs/application-default-credentials for more information.\r\n> Connecting anonymously.\r\n> WARNING:apache_beam.io.tfrecordio:Couldn\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\r\n>\r\n> After around 10 minutes, here\'s the loading of dataset:\r\n>\r\n> 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:16<00:00, 16.42s/sources]\r\n> 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.12sources/s]\r\n> 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.14sources/s]\r\n> Dataset wiki40b downloaded and prepared to /home/bhavitvya/.cache/huggingface/datasets/wiki40b/cs/1.1.0/063778187363ffb294896eaa010fc254b42b73e31117c71573a953b0b0bf010f. Subsequent calls will reuse this data.\r\n>\r\n> —\r\n> You are receiving this because you were mentioned.\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/huggingface/datasets/issues/2035#issuecomment-800081772>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AS37NMX6A2ZTRZUIIZVFRCDTD4NC3ANCNFSM4ZA5R2UA>\r\n> .\r\n>\r\n']","Hi
I am trying to download the data as below:

```
from datasets import load_dataset
dataset = load_dataset(""wiki40b"", ""cs"")
print(dataset)
```

I am getting this error. @lhoestq I will be grateful if you could assist me with this error. For almost all languages except english I am getting this error.

I really need majority of languages in this dataset to be able to train my models for a deadline and your great scalable super well-written library is my only hope to train the models at scale while being low on resources. 

thank you very much.

```
(fast) dara@vgne046:/user/dara/dev/codes/seq2seq$ python test_data.py
Downloading and preparing dataset wiki40b/cs (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to temp/dara/cache_home_2/datasets/wiki40b/cs/1.1.0/063778187363ffb294896eaa010fc254b42b73e31117c71573a953b0b0bf010f...
Traceback (most recent call last):
  File ""test_data.py"", line 3, in <module>
    dataset = load_dataset(""wiki40b"", ""cs"")
  File ""/user/dara/libs/anaconda3/envs/fast/lib/python3.7/site-packages/datasets/load.py"", line 746, in load_dataset
    use_auth_token=use_auth_token,
  File ""/user/dara/libs/anaconda3/envs/fast/lib/python3.7/site-packages/datasets/builder.py"", line 579, in download_and_prepare
    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
  File ""/user/dara/libs/anaconda3/envs/fast/lib/python3.7/site-packages/datasets/builder.py"", line 1105, in _download_and_prepare
    import apache_beam as beam
  File ""/user/dara/libs/anaconda3/envs/fast/lib/python3.7/site-packages/apache_beam-2.28.0-py3.7-linux-x86_64.egg/apache_beam/__init__.py"", line 96, in <module>
    from apache_beam import io
  File ""/user/dara/libs/anaconda3/envs/fast/lib/python3.7/site-packages/apache_beam-2.28.0-py3.7-linux-x86_64.egg/apache_beam/io/__init__.py"", line 23, in <module>
    from apache_beam.io.avroio import *
  File ""/user/dara/libs/anaconda3/envs/fast/lib/python3.7/site-packages/apache_beam-2.28.0-py3.7-linux-x86_64.egg/apache_beam/io/avroio.py"", line 55, in <module>
    import avro
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 668, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 638, in _load_backward_compatible
  File ""/user/dara/libs/anaconda3/envs/fast/lib/python3.7/site-packages/avro_python3-1.9.2.1-py3.7.egg/avro/__init__.py"", line 34, in <module>
  File ""/user/dara/libs/anaconda3/envs/fast/lib/python3.7/site-packages/avro_python3-1.9.2.1-py3.7.egg/avro/__init__.py"", line 30, in LoadResource
NotADirectoryError: [Errno 20] Not a directory: '/user/dara/libs/anaconda3/envs/fast/lib/python3.7/site-packages/avro_python3-1.9.2.1-py3.7.egg/avro/VERSION.txt'
```"
https://github.com/huggingface/datasets/issues/2031,wikipedia.py generator that extracts XML doesn't release memory,"['Hi @miyamonz \r\nThanks for investigating this issue, good job !\r\nIt would be awesome to integrate your fix in the library, could you open a pull request ?'
 ""OK! I'll send it later.""]","I tried downloading Japanese wikipedia, but it always failed because of out of memory maybe.

I found that the generator function that extracts XML data in wikipedia.py doesn't release memory in the loop.

https://github.com/huggingface/datasets/blob/13a5b7db992ad5cf77895e4c0f76595314390418/datasets/wikipedia/wikipedia.py#L464-L502

`root.clear()` intend to clear memory, but it doesn't.
https://github.com/huggingface/datasets/blob/13a5b7db992ad5cf77895e4c0f76595314390418/datasets/wikipedia/wikipedia.py#L490
https://github.com/huggingface/datasets/blob/13a5b7db992ad5cf77895e4c0f76595314390418/datasets/wikipedia/wikipedia.py#L494
I replaced them with `elem.clear()`, then it seems to work correctly.

here is the notebook to reproduce it.
https://gist.github.com/miyamonz/dc06117302b6e85fa51cbf46dde6bb51#file-xtract_content-ipynb"
https://github.com/huggingface/datasets/issues/2029,Loading a faiss index KeyError,"['In your code `dataset2` doesn\'t contain the ""embeddings"" column, since it is created from the pandas DataFrame with columns ""text"" and ""label"".\r\n\r\nTherefore when you call `dataset2[embeddings_name]`, you get a `KeyError`.\r\n\r\nIf you want the ""embeddings"" column back, you can create `dataset2` with\r\n```python\r\ndataset2 = load_from_disk(dataset_filename)\r\n```\r\nwhere `dataset_filename` is the place where you saved you dataset with the embeddings in the first place.'
 ""Ok in that case HF should fix their misleading example at https://huggingface.co/docs/datasets/faiss_and_ea.html#adding-a-faiss-index \r\n\r\nI copy-pasted it here.\r\n\r\n> When you are done with your queries you can save your index on disk:\r\n> \r\n> ```python\r\n> ds_with_embeddings.save_faiss_index('embeddings', 'my_index.faiss')\r\n> ```\r\n> Then reload it later:\r\n> \r\n> ```python\r\n> ds = load_dataset('crime_and_punish', split='train[:100]')\r\n> ds.load_faiss_index('embeddings', 'my_index.faiss')\r\n> ```""
 'Hi !\r\n\r\nThe code of the example is valid.\r\nAn index is a search engine, it\'s not considered a column of a dataset.\r\nWhen you do `ds.load_faiss_index(""embeddings"", \'my_index.faiss\')`, it attaches an index named ""embeddings"" to the dataset but it doesn\'t re-add the ""embeddings"" column. You can list the indexes of a dataset by using `ds.list_indexes()`.\r\n\r\nIf I understand correctly by reading this example you thought that it was re-adding the ""embeddings"" column.\r\nThis looks misleading indeed, and we should add a note to make it more explicit that it doesn\'t store the column that was used to build the index.\r\n\r\nFeel free to open a PR to suggest an improvement on the documentation if you want to contribute :)'
 '> If I understand correctly by reading this example you thought that it was re-adding the ""embeddings"" column.\r\nYes. I was trying to use the dataset in RAG and it complained that the dataset didn\'t have the right columns. No problems when loading the dataset with `load_from_disk` and then doing `load_faiss_index`\r\n\r\nWhat I learned was\r\n1. column and index are different\r\n2. loading the index does not create a column\r\n3. the column is not needed to be able to use the index\r\n4. RAG needs both the embeddings column and the index\r\n\r\nIf I can come up with a way to articulate this in the right spot in the docs, I\'ll open a PR']","I've recently been testing out RAG and DPR embeddings, and I've run into an issue that is not apparent in the documentation.

The basic steps are:

1. Create a dataset (dataset1)
2. Create an embeddings column using DPR
3. Add a faiss index to the dataset
4. Save faiss index to a file
5. Create a new dataset (dataset2) with the same text and label information as dataset1
6. Try to load the faiss index from file to dataset2
7. Get `KeyError: ""Column embeddings not in the dataset""`

I've made a colab notebook that should show exactly what I did. Please switch to GPU runtime; I didn't check on CPU.

https://colab.research.google.com/drive/1X0S9ZuZ8k0ybcoei4w7so6dS_WrABmIx?usp=sharing

Ubuntu Version
VERSION=""18.04.5 LTS (Bionic Beaver)""

datasets==1.4.1
faiss==1.5.3
faiss-gpu==1.7.0
torch==1.8.0+cu101
transformers==4.3.3

NVIDIA-SMI 460.56
Driver Version: 460.32.03
CUDA Version: 11.2    
Tesla K80           

I was basically following the steps here: https://huggingface.co/docs/datasets/faiss_and_ea.html#adding-a-faiss-index

I included the exact code from the documentation at the end of the notebook to show that they don't work either.
"
https://github.com/huggingface/datasets/issues/2026,KeyError on using map after renaming a column,"[""Hi,\r\n\r\nActually, the error occurs due to these two lines:\r\n```python\r\nraw_dataset.set_format('torch',columns=['img','label'])\r\nraw_dataset = raw_dataset.rename_column('img','image')\r\n```\r\n`Dataset.rename_column` doesn't update the `_format_columns` attribute, previously defined by `Dataset.set_format`, with a new column name which is why this new column is missing in the output.""
 'Hi @mariosasko,\n\nThanks for opening a PR on this :)\nWhy does the old name also disappear?'
 ""I just merged a @mariosasko 's PR that fixes this issue.\r\nIf it happens again, feel free to re-open :)""]","Hi,

I'm trying to use `cifar10` dataset. I want to rename the `img` feature to `image` in order to make it consistent with `mnist`, which I'm also planning to use. By doing this, I was trying to avoid modifying `prepare_train_features` function.

Here is what I try:

```python
transform = Compose([ToPILImage(),ToTensor(),Normalize([0.0,0.0,0.0],[1.0,1.0,1.0])])
def prepare_features(examples):
    images = []
    labels = []
    print(examples)
    for example_idx, example in enumerate(examples[""image""]):
        if transform is not None:
            images.append(transform(examples[""image""][example_idx].permute(2,0,1)))
        else:
            images.append(examples[""image""][example_idx].permute(2,0,1))
        labels.append(examples[""label""][example_idx])
    output = {""label"":labels, ""image"":images}
    return output

raw_dataset = load_dataset('cifar10')
raw_dataset.set_format('torch',columns=['img','label'])
raw_dataset = raw_dataset.rename_column('img','image')

features = datasets.Features({
            ""image"": datasets.Array3D(shape=(3,32,32),dtype=""float32""),
            ""label"": datasets.features.ClassLabel(names=[
                            ""airplane"",
                            ""automobile"",
                            ""bird"",
                            ""cat"",
                            ""deer"",
                            ""dog"",
                            ""frog"",
                            ""horse"",
                            ""ship"",
                            ""truck"",
                        ]),
        })
train_dataset = raw_dataset.map(prepare_features, features = features,batched=True, batch_size=10000)
```
The error:
```python
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-54-bf29672c53ee> in <module>()
     14                         ]),
     15         })
---> 16 train_dataset = raw_dataset.map(prepare_features, features = features,batched=True, batch_size=10000)

2 frames
/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py in map(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint)
   1287         test_inputs = self[:2] if batched else self[0]
   1288         test_indices = [0, 1] if batched else 0
-> 1289         update_data = does_function_return_dict(test_inputs, test_indices)
   1290         logger.info(""Testing finished, running the mapping function on the dataset"")
   1291 

/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py in does_function_return_dict(inputs, indices)
   1258             fn_args = [inputs] if input_columns is None else [inputs[col] for col in input_columns]
   1259             processed_inputs = (
-> 1260                 function(*fn_args, indices, **fn_kwargs) if with_indices else function(*fn_args, **fn_kwargs)
   1261             )
   1262             does_return_dict = isinstance(processed_inputs, Mapping)

<ipython-input-52-b4dccbafb70d> in prepare_features(examples)
      3     labels = []
      4     print(examples)
----> 5     for example_idx, example in enumerate(examples[""image""]):
      6         if transform is not None:
      7             images.append(transform(examples[""image""][example_idx].permute(2,0,1)))

KeyError: 'image'
```

The print statement inside returns this:
```python
{'label': tensor([6, 9])}
```
Apparently, both `img` and `image` do not exist after renaming. 

Note that this code works fine with `img` everywhere.

Notebook: https://colab.research.google.com/drive/1SzESAlz3BnVYrgQeJ838vbMp1OsukiA2?usp=sharing

"
https://github.com/huggingface/datasets/issues/2022,ValueError when rename_column on splitted dataset,"[""Hi,\r\n\r\nThis is a bug so thanks for reporting it. `Dataset.__setstate__`  is the problem, which is called when `Dataset.rename_column` tries to copy the dataset with `copy.deepcopy(self)`. This only happens if the `split` arg in `load_dataset` was defined as `ReadInstruction`.\r\n\r\nTo overcome this issue, use the named splits API (for now):\r\n```python\r\ntrain_ds, test_ds = load_dataset(\r\n    path='csv',               \r\n    delimiter='\\t',          \r\n    data_files=text_files,    \r\n    split=['train[:90%]', 'train[-10%:]'],\r\n)\r\n\r\ntrain_ds = train_ds.rename_column('sentence', 'text')\r\n```""
 ""This has been fixed in #2043 , thanks @mariosasko \r\nThe fix is available on master and we'll do a new release soon :)\r\n\r\nfeel free to re-open if you still have issues""]","Hi there,
I am loading `.tsv` file via `load_dataset` and subsequently split the rows into training and test set via the `ReadInstruction` API like so:

```python
split = {
    'train': ReadInstruction('train', to=90, unit='%'),
    'test': ReadInstruction('train', from_=-10, unit='%')
}

dataset = load_dataset(
    path='csv',               # use 'text' loading script to load from local txt-files
    delimiter='\t',           # xxx
    data_files=text_files,    # list of paths to local text files
    split=split,              # xxx
)

dataset
```

Part of output:
```python
DatasetDict({
    train: Dataset({
        features: ['sentence', 'sentiment'],
        num_rows: 900
    })
    test: Dataset({
        features: ['sentence', 'sentiment'],
        num_rows: 100
    })
})
```
Afterwards I'd like to rename the 'sentence' column to 'text' in order to be compatible with my modelin pipeline. If I run the following code I experience a `ValueError` however:
```python
dataset['train'].rename_column('sentence', 'text')
```
```python
/usr/local/lib/python3.7/dist-packages/datasets/splits.py in __init__(self, name)
    353         for split_name in split_names_from_instruction:
    354             if not re.match(_split_re, split_name):
--> 355                 raise ValueError(f""Split name should match '{_split_re}'' but got '{split_name}'."")
    356 
    357     def __str__(self):

ValueError: Split name should match '^\w+(\.\w+)*$'' but got 'ReadInstruction('.
```
In particular, these behavior does not arise if I use the deprecated `rename_column_` method. Any idea what causes the error? Would assume something in the way I defined the split.

Thanks in advance! :)"
https://github.com/huggingface/datasets/issues/2021,Interactively doing  save_to_disk and load_from_disk corrupts the datasets object?,"['Hi,\r\n\r\nCan you give us a minimal reproducible example? This [part](https://huggingface.co/docs/datasets/master/processing.html#controling-the-cache-behavior) of the docs explains how to control caching.']"," dataset_info.json file saved after using  save_to_disk gets corrupted as follows. 
 
 
![image](https://user-images.githubusercontent.com/16892570/110568474-ed969880-81b7-11eb-832f-2e5129656016.png)

Is there a way to disable the cache that will save to /tmp/huggiface/datastes ? 
I have a feeling there is a serious issue with cashing."
https://github.com/huggingface/datasets/issues/2012,No upstream branch,"[""What's the issue exactly ?\r\n\r\nGiven an `upstream` remote repository with url `https://github.com/huggingface/datasets.git`, you can totally rebase from `upstream/master`.\r\n\r\nIt's mentioned at the beginning how to add the `upstream` remote repository\r\n\r\nhttps://github.com/huggingface/datasets/blob/987df6b4e9e20fc0c92bc9df48137d170756fd7b/ADD_NEW_DATASET.md#L10-L14""
 ""~~What difference is there with the default `origin` remote that is set when you clone the repo?~~ I've just understood that this applies to **forks** of the repo 🤡 ""]","Feels like the documentation on adding a new dataset is outdated?

https://github.com/huggingface/datasets/blob/987df6b4e9e20fc0c92bc9df48137d170756fd7b/ADD_NEW_DATASET.md#L49-L54

There is no upstream branch on remote. "
https://github.com/huggingface/datasets/issues/2010,Local testing fails,"[""I'm not able to reproduce on my side.\r\nCan you provide the full stacktrace please ?\r\nWhat version of `python` and `dill` do you have ? Which OS are you using ?""
 '```\r\nco_filename = \'<ipython-input-2-e0383a102aae>\', returned_obj = [0]\r\n                                                                                                                                                                       \r\n    def create_ipython_func(co_filename, returned_obj):\r\n        def func():\r\n            return returned_obj\r\n     \r\n        code = func.__code__\r\n>       code = CodeType(*[getattr(code, k) if k != ""co_filename"" else co_filename for k in code_args])\r\nE       TypeError: an integer is required (got type bytes)\r\n\r\ntests/test_caching.py:152: TypeError\r\n```\r\n\r\nPython 3.8.8 \r\ndill==0.3.1.1\r\n'
 'I managed to reproduce. This comes from the CodeType init signature that is different in python 3.8.8\r\nI opened a PR to fix this test\r\nThanks !']","I'm following the CI setup as described in 

https://github.com/huggingface/datasets/blob/8eee4fa9e133fe873a7993ba746d32ca2b687551/.circleci/config.yml#L16-L19

in a new conda environment, at commit https://github.com/huggingface/datasets/commit/4de6dbf84e93dad97e1000120d6628c88954e5d4

and getting

```
FAILED tests/test_caching.py::RecurseDumpTest::test_dump_ipython_function - TypeError: an integer is required (got type bytes)
1 failed, 2321 passed, 5109 skipped, 10 warnings in 124.32s (0:02:04)
```

Seems like a discrepancy with CI, perhaps a lib version that's not controlled? 
Tried with `pyarrow=={1.0.0,0.17.1,2.0.0}`"
https://github.com/huggingface/datasets/issues/2009,Ambiguous documentation,"['Hi @theo-m !\r\n\r\nA few lines above this line, you\'ll find that the `_split_generators` method returns a list of `SplitGenerator`s objects:\r\n\r\n```python\r\ndatasets.SplitGenerator(\r\n    name=datasets.Split.VALIDATION,\r\n    # These kwargs will be passed to _generate_examples\r\n    gen_kwargs={\r\n        ""filepath"": os.path.join(data_dir, ""dev.jsonl""),\r\n        ""split"": ""dev"",\r\n    },\r\n),\r\n```\r\n\r\nNotice the `gen_kwargs` argument passed to the constructor of `SplitGenerator`: this dict will be unpacked as keyword arguments to pass to the `_generat_examples` method (in this case the `filepath` and `split` arguments).\r\n\r\nLet me know if that helps!'
 ""Oh ok I hadn't made the connection between those two, will offer a tweak to the comment and the template then - thanks!""]","https://github.com/huggingface/datasets/blob/2ac9a0d24a091989f869af55f9f6411b37ff5188/templates/new_dataset_script.py#L156-L158

Looking at the template, I find this documentation line to be confusing, the method parameters don't include the `gen_kwargs` so I'm unclear where they're coming from.

Happy to push a PR with a clearer statement when I understand the meaning."
https://github.com/huggingface/datasets/issues/2007,How to not load huggingface datasets into memory ,"['So maybe a summary here: \r\nIf I could fit a large model with batch_size = X into memory, is there a way I could train this model for huge datasets with keeping setting the same? thanks '
 ""The `datastets` library doesn't load datasets into memory. Therefore you can load a dataset that is terabytes big without filling up your RAM.\r\n\r\nThe only thing that's loaded into memory during training is the batch used in the training step.\r\nSo as long as your model works with batch_size = X, then you can load an even bigger dataset and it will work as well with the same batch_size.\r\n\r\nNote that you still have to take into account that some batches take more memory than others, depending on the texts lengths. If it works for a batch with batch_size = X and with texts of maximum length, then it will work for all batches.\r\n\r\nIn your case I guess that there are a few long sentences in the dataset. For those long sentences you get a memory error on your GPU because they're too long. By passing `max_train_samples` you may have taken a subset of the dataset that only contain short sentences. That's probably why in your case it worked only when you set `max_train_samples`.\r\nI'd suggest you to reduce the batch size so that the batches with long sentences can be loaded on the GPU.\r\n\r\nLet me know if that helps or if you have other questions""]","Hi
I am running this example from transformers library version 4.3.3:
(Here is the full documentation https://github.com/huggingface/transformers/issues/8771 but the running command should work out of the box)

 USE_TF=0  deepspeed  run_seq2seq.py --model_name_or_path google/mt5-base --dataset_name wmt16 --dataset_config_name ro-en --source_prefix ""translate English to Romanian: "" --task translation_en_to_ro   --output_dir /test/test_large  --do_train --do_eval --predict_with_generate  --max_train_samples 500   --max_val_samples 500  --max_source_length 128 --max_target_length 128 --sortish_sampler --per_device_train_batch_size 8   --val_max_target_length 128 --deepspeed ds_config.json --num_train_epochs 1 --eval_steps 25000 --warmup_steps 500 --overwrite_output_dir

(Here please find the script: https://github.com/huggingface/transformers/blob/master/examples/seq2seq/run_seq2seq.py)

If you do not pass max_train_samples in above command to load the full dataset, then I get memory issue on a gpu with 24 GigBytes of memory.
 
I need to train large-scale mt5 model on large-scale datasets of wikipedia (multiple of them concatenated or other datasets in multiple languages like OPUS), could you help me how I can avoid loading the full data into memory? to make the scripts not related to data size? 

In above example, I was hoping the script could work without relying on dataset size, so I can still train the model without subsampling training set.

thank you so much @lhoestq for your great help in advance


"
https://github.com/huggingface/datasets/issues/2005,Setting to torch format not working with torchvision and MNIST,"[""Adding to the previous information, I think `torch.utils.data.DataLoader` is doing some conversion. \r\nWhat I tried:\r\n```python\r\ntrain_dataset = load_dataset('mnist')\r\n```\r\nI don't use any `map` or `set_format` or any `transform`. I use this directly, and try to load batches using the `DataLoader` with batch size 2, I get an output like this for the `image`:\r\n\r\n```\r\n[[tensor([0, 0]), tensor([0, 0]), tensor([0, 0]), tensor([0, 0]), tensor([0, 0]), tensor([0, 0]), tensor([0, 0]), tensor([0, 0]), tensor([0, 0]), tensor...\r\n```\r\nFor `label`, it works fine:\r\n```\r\ntensor([7, 6])\r\n```\r\nNote that I didn't specify conversion to torch tensors anywhere.\r\n\r\nBasically, there are two problems here:\r\n1. `dataset.map` doesn't return tensor type objects, even though it uses the transforms, the grayscale conversion in transform was done, but the output was lists only.\r\n2.  The `DataLoader` performs its own conversion, which may be not desired.\r\n\r\nI understand that we can't change `DataLoader` because it is a torch functionality, however, is there a way we can handle image data to allow using it with torch `DataLoader` and `torchvision` properly?\r\n\r\nI think if the `image` was a torch tensor (N,H,W,C), or  a list of torch tensors (H,W,C), before it is passed to `DataLoader`, then we might not face this issue. ""
 'What\'s the feature types of your new dataset after `.map` ?\r\n\r\nCan you try with adding `features=` in the `.map` call in order to set the ""image"" feature type to `Array2D` ?\r\nThe default feature type is lists of lists, we\'ve not implemented shape verification to use ArrayXD instead of nested lists yet'
 ""Hi @lhoestq\r\n\r\nRaw feature types are like this:\r\n```\r\nImage:\r\n<class 'list'> 60000  #(type, len)\r\n<class 'list'> 28\r\n<class 'list'> 28\r\n<class 'int'>\r\nLabel:\r\n<class 'list'> 60000\r\n<class 'int'>\r\n```\r\nInside the `prepare_feature` method with batch size 100000 , after processing, they are like this:\r\n\r\nInside Prepare Train Features\r\n```\r\nImage:\r\n<class 'list'> 10000\r\n<class 'torch.Tensor'> 1\r\n<class 'torch.Tensor'> 28\r\n<class 'torch.Tensor'> 28\r\n<class 'torch.Tensor'>\r\nLabel:\r\n<class 'list'> 10000\r\n<class 'torch.Tensor'>\r\n```\r\n\r\nAfter map, the feature type are like this:\r\n```\r\nImage:\r\n<class 'list'> 60000\r\n<class 'list'> 1\r\n<class 'list'> 28\r\n<class 'list'> 28\r\n<class 'float'>\r\nLabel:\r\n<class 'list'> 60000\r\n<class 'int'>\r\n```\r\n\r\nAfter dataloader with batch size 2, the batch features are like this:\r\n```\r\nImage:\r\n<class 'list'> 1\r\n<class 'list'> 28\r\n<class 'list'> 28\r\n<class 'torch.Tensor'> 2\r\n<class 'torch.Tensor'>\r\nLabel:\r\n<class 'torch.Tensor'> 2\r\n<class 'torch.Tensor'>\r\n```\r\n<hr>\r\n\r\nWhen I was setting the format of `train_dataset` to 'torch' after mapping  - \r\n```\r\nImage:\r\n<class 'list'> 60000\r\n<class 'list'> 1\r\n<class 'list'> 28\r\n<class 'torch.Tensor'> 28\r\n<class 'torch.Tensor'>\r\nLabel:\r\n<class 'torch.Tensor'> 60000\r\n<class 'torch.Tensor'>\r\n```\r\n\r\nCorresponding DataLoader batch:\r\n```\r\nFrom DataLoader batch features\r\nImage:\r\n<class 'list'> 1\r\n<class 'list'> 28\r\n<class 'torch.Tensor'> 2\r\n<class 'torch.Tensor'> 28\r\n<class 'torch.Tensor'>\r\nLabel:\r\n<class 'torch.Tensor'> 2\r\n<class 'torch.Tensor'>\r\n```\r\n\r\nI will check with features and get back.\r\n\r\n\r\n\r\n""
 'Hi @lhoestq\r\n\r\n# Using Array3D\r\nI tried this:\r\n```python\r\nfeatures = datasets.Features({\r\n            ""image"": datasets.Array3D(shape=(1,28,28),dtype=""float32""),\r\n            ""label"": datasets.features.ClassLabel(names=[""0"", ""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9""]),\r\n        })\r\ntrain_dataset = raw_dataset.map(prepare_features, features = features,batched=True, batch_size=10000)\r\n```\r\nand it didn\'t fix the issue.\r\n\r\nDuring the `prepare_train_features:\r\n```\r\nImage:\r\n<class \'list\'> 10000\r\n<class \'torch.Tensor\'> 1\r\n<class \'torch.Tensor\'> 28\r\n<class \'torch.Tensor\'> 28\r\n<class \'torch.Tensor\'>\r\nLabel:\r\n<class \'list\'> 10000\r\n<class \'torch.Tensor\'>\r\n```\r\n\r\nAfter the `map`:\r\n\r\n```\r\nImage:\r\n<class \'list\'> 60000\r\n<class \'list\'> 1\r\n<class \'list\'> 28\r\n<class \'list\'> 28\r\n<class \'float\'>\r\nLabel:\r\n<class \'list\'> 60000\r\n<class \'int\'>\r\n```\r\nFrom the DataLoader batch:\r\n```\r\nImage:\r\n<class \'list\'> 1\r\n<class \'list\'> 28\r\n<class \'list\'> 28\r\n<class \'torch.Tensor\'> 2\r\n<class \'torch.Tensor\'>\r\nLabel:\r\n<class \'torch.Tensor\'> 2\r\n<class \'torch.Tensor\'>\r\n```\r\nIt is the same as before.\r\n\r\n---\r\n\r\nUsing `datasets.Sequence(datasets.Array2D(shape=(28,28),dtype=""float32""))` gave an error during `map`:\r\n\r\n```python\r\nArrowNotImplementedError                  Traceback (most recent call last)\r\n<ipython-input-95-d28e69289084> in <module>()\r\n      3             ""label"": datasets.features.ClassLabel(names=[""0"", ""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9""]),\r\n      4         })\r\n----> 5 train_dataset = raw_dataset.map(prepare_features, features = features,batched=True, batch_size=10000)\r\n\r\n15 frames\r\n/usr/local/lib/python3.7/dist-packages/datasets/dataset_dict.py in map(self, function, with_indices, input_columns, batched, batch_size, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc)\r\n    446                     num_proc=num_proc,\r\n    447                 )\r\n--> 448                 for k, dataset in self.items()\r\n    449             }\r\n    450         )\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/dataset_dict.py in <dictcomp>(.0)\r\n    446                     num_proc=num_proc,\r\n    447                 )\r\n--> 448                 for k, dataset in self.items()\r\n    449             }\r\n    450         )\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py in map(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint)\r\n   1307                 fn_kwargs=fn_kwargs,\r\n   1308                 new_fingerprint=new_fingerprint,\r\n-> 1309                 update_data=update_data,\r\n   1310             )\r\n   1311         else:\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py in wrapper(*args, **kwargs)\r\n    202         }\r\n    203         # apply actual function\r\n--> 204         out: Union[""Dataset"", ""DatasetDict""] = func(self, *args, **kwargs)\r\n    205         datasets: List[""Dataset""] = list(out.values()) if isinstance(out, dict) else [out]\r\n    206         # re-apply format to the output\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/fingerprint.py in wrapper(*args, **kwargs)\r\n    335             # Call actual function\r\n    336 \r\n--> 337             out = func(self, *args, **kwargs)\r\n    338 \r\n    339             # Update fingerprint of in-place transforms + update in-place history of transforms\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py in _map_single(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, update_data)\r\n   1580                     if update_data:\r\n   1581                         batch = cast_to_python_objects(batch)\r\n-> 1582                         writer.write_batch(batch)\r\n   1583             if update_data:\r\n   1584                 writer.finalize()  # close_stream=bool(buf_writer is None))  # We only close if we are writing in a file\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/arrow_writer.py in write_batch(self, batch_examples, writer_batch_size)\r\n    274             typed_sequence = TypedSequence(batch_examples[col], type=col_type, try_type=col_try_type)\r\n    275             typed_sequence_examples[col] = typed_sequence\r\n--> 276         pa_table = pa.Table.from_pydict(typed_sequence_examples)\r\n    277         self.write_table(pa_table, writer_batch_size)\r\n    278 \r\n\r\n/usr/local/lib/python3.7/dist-packages/pyarrow/table.pxi in pyarrow.lib.Table.from_pydict()\r\n\r\n/usr/local/lib/python3.7/dist-packages/pyarrow/array.pxi in pyarrow.lib.asarray()\r\n\r\n/usr/local/lib/python3.7/dist-packages/pyarrow/array.pxi in pyarrow.lib.array()\r\n\r\n/usr/local/lib/python3.7/dist-packages/pyarrow/array.pxi in pyarrow.lib._handle_arrow_array_protocol()\r\n\r\n/usr/local/lib/python3.7/dist-packages/datasets/arrow_writer.py in __arrow_array__(self, type)\r\n     95                 out = pa.ExtensionArray.from_storage(type, pa.array(self.data, type.storage_dtype))\r\n     96             else:\r\n---> 97                 out = pa.array(self.data, type=type)\r\n     98             if trying_type and out[0].as_py() != self.data[0]:\r\n     99                 raise TypeError(\r\n\r\n/usr/local/lib/python3.7/dist-packages/pyarrow/array.pxi in pyarrow.lib.array()\r\n\r\n/usr/local/lib/python3.7/dist-packages/pyarrow/array.pxi in pyarrow.lib._sequence_to_array()\r\n\r\n/usr/local/lib/python3.7/dist-packages/pyarrow/error.pxi in pyarrow.lib.pyarrow_internal_check_status()\r\n\r\n/usr/local/lib/python3.7/dist-packages/pyarrow/error.pxi in pyarrow.lib.check_status()\r\n\r\nArrowNotImplementedError: extension\r\n```'
 '# Convert raw tensors to torch format\r\nStrangely, converting to torch tensors works perfectly on `raw_dataset`:\r\n```python\r\nraw_dataset.set_format(\'torch\',columns=[\'image\',\'label\'])\r\n```\r\nTypes:\r\n```\r\nImage:\r\n<class \'torch.Tensor\'> 60000\r\n<class \'torch.Tensor\'> 28\r\n<class \'torch.Tensor\'> 28\r\n<class \'torch.Tensor\'>\r\nLabel:\r\n<class \'torch.Tensor\'> 60000\r\n<class \'torch.Tensor\'>\r\n```\r\n\r\nUsing this for transforms:\r\n```python\r\ndef prepare_features(examples):\r\n    images = []\r\n    labels = []\r\n    for example_idx, example in enumerate(examples[""image""]):\r\n        if transform is not None:\r\n            images.append(transform(\r\n                examples[""image""][example_idx].numpy()\r\n            ))\r\n        else:\r\n            images.append(examples[""image""][example_idx].numpy())\r\n        labels.append(examples[""label""][example_idx])\r\n    output = {""label"":labels, ""image"":images}\r\n    return output\r\n```\r\n\r\nInside `prepare_train_features`:\r\n```\r\nImage:\r\n<class \'list\'> 10000\r\n<class \'torch.Tensor\'> 1\r\n<class \'torch.Tensor\'> 28\r\n<class \'torch.Tensor\'> 28\r\n<class \'torch.Tensor\'>\r\nLabel:\r\n<class \'list\'> 10000\r\n<class \'torch.Tensor\'>\r\n```\r\n\r\nAfter `map`:\r\n```\r\nImage:\r\n<class \'list\'> 60000\r\n<class \'list\'> 1\r\n<class \'list\'> 28\r\n<class \'torch.Tensor\'> 28\r\n<class \'torch.Tensor\'>\r\nLabel:\r\n<class \'torch.Tensor\'> 60000\r\n<class \'torch.Tensor\'>\r\n```\r\nDataLoader batch:\r\n\r\n```\r\nImage:\r\n<class \'list\'> 1\r\n<class \'list\'> 28\r\n<class \'torch.Tensor\'> 2\r\n<class \'torch.Tensor\'> 28\r\n<class \'torch.Tensor\'>\r\nLabel:\r\n<class \'torch.Tensor\'> 2\r\n<class \'torch.Tensor\'>\r\n```\r\n\r\n---\r\n\r\n## Using `torch` format:\r\n```\r\nImage:\r\n<class \'list\'> 60000\r\n<class \'list\'> 1\r\n<class \'list\'> 28\r\n<class \'torch.Tensor\'> 28\r\n<class \'torch.Tensor\'>\r\nLabel:\r\n<class \'torch.Tensor\'> 60000\r\n<class \'torch.Tensor\'>\r\n```\r\nDataLoader batches:\r\n\r\n```\r\nImage:\r\n<class \'list\'> 1\r\n<class \'list\'> 28\r\n<class \'torch.Tensor\'> 2\r\n<class \'torch.Tensor\'> 28\r\n<class \'torch.Tensor\'>\r\nLabel:\r\n<class \'torch.Tensor\'> 2\r\n<class \'torch.Tensor\'>\r\n```\r\n\r\n---\r\n## Using the features - `Array3D`:\r\n\r\n```\r\nImage:\r\n<class \'list\'> 10000\r\n<class \'torch.Tensor\'> 1\r\n<class \'torch.Tensor\'> 28\r\n<class \'torch.Tensor\'> 28\r\n<class \'torch.Tensor\'>\r\nLabel:\r\n<class \'list\'> 10000\r\n<class \'torch.Tensor\'>\r\n```\r\n\r\nAfter `map`:\r\n```\r\nImage:\r\n<class \'torch.Tensor\'> 60000\r\n<class \'torch.Tensor\'> 1\r\n<class \'torch.Tensor\'> 28\r\n<class \'torch.Tensor\'> 28\r\n<class \'torch.Tensor\'>\r\nLabel:\r\n<class \'torch.Tensor\'> 60000\r\n<class \'torch.Tensor\'>\r\n```\r\n\r\nAfter DataLoader `batch`:\r\n```\r\nImage:\r\n<class \'torch.Tensor\'> 2\r\n<class \'torch.Tensor\'> 1\r\n<class \'torch.Tensor\'> 28\r\n<class \'torch.Tensor\'> 28\r\n<class \'torch.Tensor\'>\r\nLabel:\r\n<class \'torch.Tensor\'> 2\r\n<class \'torch.Tensor\'>\r\n```\r\n\r\nThe last one works perfectly.\r\n\r\n![image](https://user-images.githubusercontent.com/29076344/110491452-4cf09c00-8117-11eb-8a47-73bf3fc0c3dc.png)\r\n\r\nI wonder why this worked, and others didn\'t.\r\n\r\n\r\n\r\n\r\n\r\n\r\n'
 'Concluding, the way it works right now is:\r\n\r\n1. Converting raw dataset to `torch` format.\r\n2. Use the transform and apply using `map`, ensure the returned values are tensors. \r\n3. When mapping, use `features` with `image` being `Array3D` type.'
 'What the dataset returns depends on the feature type.\r\nFor a feature type that is Sequence(Sequence(Sequence(Value(""uint8"")))), a dataset formatted as ""torch"" return lists of lists of tensors. This is because the lists lengths may vary.\r\nFor a feature type that is Array3D on the other hand it returns one tensor. This is because the size of the tensor is fixed and defined bu the Array3D type.'
 ""Okay, that makes sense.\r\nRaw images are list of Array2D, hence we get a single tensor when `set_format` is used. But, why should I need to convert the raw images to `torch` format when `map` does this internally?\r\n\r\nUsing `Array3D` did not work with `map` when raw images weren't `set_format`ted to torch type.""
 'I understand that `map` needs to know what kind of output tensors are expected, and thus converting the raw dataset to `torch` format is necessary. Closing the issue since it is resolved.']","Hi

I am trying to use `torchvision.transforms` to handle the transformation of the image data in the `mnist` dataset. Assume I have a `transform` variable which contains the `torchvision.transforms` object.

A snippet of what I am trying to do:
```python
def prepare_features(examples):
    images = []
    labels = []
    for example_idx, example in enumerate(examples[""image""]):
        if transform is not None:
            images.append(transform(
                np.array(examples[""image""][example_idx], dtype=np.uint8)
            ))
        else:
            images.append(torch.tensor(np.array(examples[""image""][example_idx], dtype=np.uint8)))
        labels.append(torch.tensor(examples[""label""][example_idx]))
    output = {""label"":labels, ""image"":images}
    return output

raw_dataset = load_dataset('mnist')
train_dataset = raw_dataset.map(prepare_features, batched=True, batch_size=10000)
train_dataset.set_format(""torch"",columns=[""image"",""label""])
```

After this, I check the type of the following:
```python
print(type(train_dataset[""train""][""label""]))
print(type(train_dataset[""train""][""image""][0]))
```
This leads to the following output:

```python
<class 'torch.Tensor'>
<class 'list'>
```
I use `torch.utils.DataLoader` for batches, the type of `batch[""train""][""image""]` is also `<class 'list'>`.

I don't understand why only the `label` is converted to a torch tensor, why does the image not get converted? How can I fix this issue?

Thanks,
Gunjan

EDIT:
I just checked the shapes, and the types, `batch[image]` is a actually a list of list of tensors. Shape is (1,28,2,28), where `batch_size` is 2. I don't understand why this is happening. Ideally it should be a tensor of shape (2,1,28,28).

EDIT 2:
Inside `prepare_train_features`, the shape of `images[0]` is `torch.Size([1,28,28])`, the conversion is working. However, the output of the `map` is a list of list of list of list."
https://github.com/huggingface/datasets/issues/2003,Messages are being printed to the `stdout`,"['This is expected to show this message to the user via stdout.\r\nThis way the users see it directly and can cancel the downloading if they want to.\r\nCould you elaborate why it would be better to have it in stderr instead of stdout ?'
 '@lhoestq, sorry for the late reply\r\n\r\nI completely understand why you decided to output a message that is always shown. The only problem is that the message is printed to the `stdout`. For example, if the user runs `python run_glue.py > log_file`, it will redirect `stdout` to the file named  `log_file`, and the message will not be shown to the user.\r\n\r\nInstead, we should print this message to `stderr`.  Even in the case of `python run_glue.py > log_file` only `stdout` is being redirected and so the message is always shown.']","In this code segment, we can see some messages are being printed to the `stdout`.
https://github.com/huggingface/datasets/blob/7e60bb509b595e8edc60a87f32b2bacfc065d607/src/datasets/builder.py#L545-L554
According to the comment, it is done intentionally, but I don't really understand why don't we log it with a higher level or print it directly to the `stderr`.
In my opinion, this kind of messages should never printed to the stdout. At least some configuration/flag should make it possible to provide in order to explicitly prevent the package to contaminate the stdout.
"
https://github.com/huggingface/datasets/issues/2000,Windows Permission Error (most recent version of datasets),"[""Hi @itsLuisa !\r\n\r\nCould you give us more information about the error you're getting, please?\r\nA copy-paste of the Traceback would be nice to get a better understanding of what is wrong :) ""
 'Hello @SBrandeis , this is it:\r\n```\r\nTraceback (most recent call last):\r\n  File ""C:\\Users\\Luisa\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\datasets\\builder.py"", line 537, in incomplete_dir\r\n    yield tmp_dir\r\n  File ""C:\\Users\\Luisa\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\datasets\\builder.py"", line 578, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File ""C:\\Users\\Luisa\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\datasets\\builder.py"", line 656, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File ""C:\\Users\\Luisa\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\datasets\\builder.py"", line 982, in _prepare_split\r\n    num_examples, num_bytes = writer.finalize()\r\n  File ""C:\\Users\\Luisa\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\datasets\\arrow_writer.py"", line 297, in finalize\r\n    self.write_on_file()\r\n  File ""C:\\Users\\Luisa\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\datasets\\arrow_writer.py"", line 230, in write_on_file\r\n    pa_array = pa.array(typed_sequence)\r\n  File ""pyarrow\\array.pxi"", line 222, in pyarrow.lib.array\r\n  File ""pyarrow\\array.pxi"", line 110, in pyarrow.lib._handle_arrow_array_protocol\r\n  File ""C:\\Users\\Luisa\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\datasets\\arrow_writer.py"", line 97, in __arrow_array__\r\n    out = pa.array(self.data, type=type)\r\n  File ""pyarrow\\array.pxi"", line 305, in pyarrow.lib.array\r\n  File ""pyarrow\\array.pxi"", line 39, in pyarrow.lib._sequence_to_array\r\n  File ""pyarrow\\error.pxi"", line 122, in pyarrow.lib.pyarrow_internal_check_status\r\n  File ""pyarrow\\error.pxi"", line 107, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowTypeError: Expected bytes, got a \'list\' object\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File ""C:/Users/Luisa/Documents/Uni/WS 2020,21/Neural Networks/Final_Project/NN_Project/data_loading.py"", line 122, in <module>\r\n    main()\r\n  File ""C:/Users/Luisa/Documents/Uni/WS 2020,21/Neural Networks/Final_Project/NN_Project/data_loading.py"", line 111, in main\r\n    dataset = datasets.load_dataset(\r\n  File ""C:\\Users\\Luisa\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\datasets\\load.py"", line 740, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File ""C:\\Users\\Luisa\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\datasets\\builder.py"", line 586, in download_and_prepare\r\n    self._save_info()\r\n  File ""C:\\Users\\Luisa\\AppData\\Local\\Programs\\Python\\Python38\\lib\\contextlib.py"", line 131, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File ""C:\\Users\\Luisa\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\datasets\\builder.py"", line 543, in incomplete_dir\r\n    shutil.rmtree(tmp_dir)\r\n  File ""C:\\Users\\Luisa\\AppData\\Local\\Programs\\Python\\Python38\\lib\\shutil.py"", line 740, in rmtree\r\n    return _rmtree_unsafe(path, onerror)\r\n  File ""C:\\Users\\Luisa\\AppData\\Local\\Programs\\Python\\Python38\\lib\\shutil.py"", line 618, in _rmtree_unsafe\r\n    onerror(os.unlink, fullname, sys.exc_info())\r\n  File ""C:\\Users\\Luisa\\AppData\\Local\\Programs\\Python\\Python38\\lib\\shutil.py"", line 616, in _rmtree_unsafe\r\n    os.unlink(fullname)\r\nPermissionError: [WinError 32] Der Prozess kann nicht auf die Datei zugreifen, da sie von einem anderen Prozess verwendet wird: \'C:\\\\Users\\\\Luisa\\\\.cache\\\\huggingface\\\\datasets\\\\sample\\\\default-20ee7d51a6a9454f\\\\0.0.0\\\\5fc4c3a355ea77ab446bd31fca5082437600b8364d29b2b95264048bd1f398b1.incomplete\\\\sample-train.arrow\'\r\n\r\nProcess finished with exit code 1\r\n```'
 'Hi @itsLuisa, thanks for sharing the Traceback.\r\n\r\nYou are defining the ""id"" field as a `string` feature:\r\n```python\r\nclass Sample(datasets.GeneratorBasedBuilder):\r\n    ...\r\n\r\n    def _info(self):\r\n        return datasets.DatasetInfo(\r\n            features=datasets.Features(\r\n                {\r\n                    ""id"": datasets.Value(""string""),\r\n                    # ^^ here\r\n                    ""tokens"": datasets.Sequence(datasets.Value(""string"")),\r\n                    ""pos_tags"": datasets.Sequence(datasets.features.ClassLabel(names=[...])),\r\n[...]\r\n```\r\n\r\nBut in the `_generate_examples`, the ""id"" field is a list:\r\n```python\r\nids = list()\r\n```\r\n\r\nChanging:\r\n```python\r\n""id"": datasets.Value(""string""),\r\n```\r\nInto:\r\n```python\r\n""id"": datasets.Sequence(datasets.Value(""string"")),\r\n```\r\n\r\nShould fix your issue.\r\n\r\nLet me know if this helps!'
 'It seems to be working now, thanks a lot for the help, @SBrandeis !'
 ""Glad to hear it!\r\nI'm closing the issue""]","Hi everyone,
Can anyone help me with why the dataset loading script below raises a Windows Permission Error? I stuck quite closely to https://github.com/huggingface/datasets/blob/master/datasets/conll2003/conll2003.py , only I want to load the data from three local three-column tsv-files (id\ttokens\tpos_tags\n). I am using the most recent version of datasets. Thank you in advance!
Luisa

My script:
```
import datasets
import csv

logger = datasets.logging.get_logger(__name__)


class SampleConfig(datasets.BuilderConfig):

    def __init__(self, **kwargs):
        super(SampleConfig, self).__init__(**kwargs)


class Sample(datasets.GeneratorBasedBuilder):
    BUILDER_CONFIGS = [
        SampleConfig(name=""conll2003"", version=datasets.Version(""1.0.0""), description=""Conll2003 dataset""),
    ]

    def _info(self):
        return datasets.DatasetInfo(
            description=""Dataset with words and their POS-Tags"",
            features=datasets.Features(
                {
                    ""id"": datasets.Value(""string""),
                    ""tokens"": datasets.Sequence(datasets.Value(""string"")),
                    ""pos_tags"": datasets.Sequence(
                        datasets.features.ClassLabel(
                            names=[
                                ""''"",
                                "","",
                                ""-LRB-"",
                                ""-RRB-"",
                                ""."",
                                "":"",
                                ""CC"",
                                ""CD"",
                                ""DT"",
                                ""EX"",
                                ""FW"",
                                ""HYPH"",
                                ""IN"",
                                ""JJ"",
                                ""JJR"",
                                ""JJS"",
                                ""MD"",
                                ""NN"",
                                ""NNP"",
                                ""NNPS"",
                                ""NNS"",
                                ""PDT"",
                                ""POS"",
                                ""PRP"",
                                ""PRP$"",
                                ""RB"",
                                ""RBR"",
                                ""RBS"",
                                ""RP"",
                                ""TO"",
                                ""UH"",
                                ""VB"",
                                ""VBD"",
                                ""VBG"",
                                ""VBN"",
                                ""VBP"",
                                ""VBZ"",
                                ""WDT"",
                                ""WP"",
                                ""WRB"",
                                ""``""
                            ]
                        )
                    ),
                }
            ),
            supervised_keys=None,
            homepage=""https://catalog.ldc.upenn.edu/LDC2011T03"",
            citation=""Weischedel, Ralph, et al. OntoNotes Release 4.0 LDC2011T03. Web Download. Philadelphia: Linguistic Data Consortium, 2011."",
        )

    def _split_generators(self, dl_manager):
        loaded_files = dl_manager.download_and_extract(self.config.data_files)
        return [
            datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={""filepath"": loaded_files[""train""]}),
            datasets.SplitGenerator(name=datasets.Split.TEST, gen_kwargs={""filepath"": loaded_files[""test""]}),
            datasets.SplitGenerator(name=datasets.Split.VALIDATION, gen_kwargs={""filepath"": loaded_files[""val""]})
        ]

    def _generate_examples(self, filepath):
        logger.info(""generating examples from = %s"", filepath)
        with open(filepath, encoding=""cp1252"") as f:
            data = csv.reader(f, delimiter=""\t"")
            ids = list()
            tokens = list()
            pos_tags = list()
            for id_, line in enumerate(data):
                #print(line)
                if len(line) == 1:
                    if tokens:
                        yield id_, {""id"": ids, ""tokens"": tokens, ""pos_tags"": pos_tags}
                        ids = list()
                        tokens = list()
                        pos_tags = list()
                else:
                    ids.append(line[0])
                    tokens.append(line[1])
                    pos_tags.append(line[2])
            # last example
            yield id_, {""id"": ids, ""tokens"": tokens, ""pos_tags"": pos_tags}


def main():
    dataset = datasets.load_dataset(
        ""data_loading.py"", data_files={
            ""train"": ""train.tsv"",
            ""test"": ""test.tsv"",
            ""val"": ""val.tsv""
        }
    )

    #print(dataset)

if __name__==""__main__"":
    main()
```
"
https://github.com/huggingface/datasets/issues/1996,Error when exploring `arabic_speech_corpus`,"[""Thanks for reporting! We'll fix that as soon as possible""
 ""Actually soundfile is not a dependency of this dataset.\r\nThe error comes from a bug that was fixed in this commit: https://github.com/huggingface/datasets/pull/1767/commits/c304e63629f4453367de2fd42883a78768055532\r\nBasically the library used to consider the `import soundfile` in the docstring as a dependency, while it's just here as a code example.\r\n\r\nUpdating the viewer to the latest version of `datasets` should fix this issue\r\n""]","Navigate to https://huggingface.co/datasets/viewer/?dataset=arabic_speech_corpus

Error:
```
ImportError: To be able to use this dataset, you need to install the following dependencies['soundfile'] using 'pip install soundfile' for instance'
Traceback:
File ""/home/sasha/.local/share/virtualenvs/lib-ogGKnCK_/lib/python3.7/site-packages/streamlit/script_runner.py"", line 332, in _run_script
    exec(code, module.__dict__)
File ""/home/sasha/nlp-viewer/run.py"", line 233, in <module>
    configs = get_confs(option)
File ""/home/sasha/.local/share/virtualenvs/lib-ogGKnCK_/lib/python3.7/site-packages/streamlit/caching.py"", line 604, in wrapped_func
    return get_or_create_cached_value()
File ""/home/sasha/.local/share/virtualenvs/lib-ogGKnCK_/lib/python3.7/site-packages/streamlit/caching.py"", line 588, in get_or_create_cached_value
    return_value = func(*args, **kwargs)
File ""/home/sasha/nlp-viewer/run.py"", line 145, in get_confs
    module_path = nlp.load.prepare_module(path, dataset=True
File ""/home/sasha/.local/share/virtualenvs/lib-ogGKnCK_/lib/python3.7/site-packages/datasets/load.py"", line 342, in prepare_module
    f""To be able to use this {module_type}, you need to install the following dependencies""
```"
https://github.com/huggingface/datasets/issues/1994,not being able to get wikipedia es language,"['@lhoestq  I really appreciate if you could help me providiing processed datasets, I do not really have access to enough resources to run the apache-beam and need to run the codes on these datasets. Only en/de/fr currently works, but I need all the languages more or less. thanks '
 ""Hi @dorost1234, I think I can help you a little. I’ve processed some Wikipedia datasets (Spanish inclusive) using the HF/datasets library during recent research.\r\n\r\n@lhoestq Could you help me to upload these preprocessed datasets to Huggingface's repositories? To be more precise, I've built datasets from the following languages using the 20201201 dumps: Spanish, Portuguese, Russian, French, Japanese, Chinese, and Turkish. Process these datasets have high costs that most of the community can't afford. I think these preprocessed datasets I have could be helpful for someone without access to high-resource machines to process Wikipedia's dumps like @dorost1234\r\n\r\n""
 ""Thank you so much @jonatasgrosman , I greatly appreciate your help with them. \r\nYes, I unfortunately does not have access to a good resource and need it for my\r\nresearch. I greatly appreciate @lhoestq  your help with uploading the processed datasets in huggingface datasets. This would be really helpful for some users like me with not access to high-memory GPU resources.\r\n\r\nthank you both so much again.\r\n\r\nOn Sat, Mar 6, 2021 at 12:55 AM Jonatas Grosman <notifications@github.com>\r\nwrote:\r\n\r\n> Hi @dorost1234 <https://github.com/dorost1234>, I think I can help you a\r\n> little. I’ve processed some Wikipedia datasets (Spanish inclusive) using\r\n> the HF/datasets library during recent research.\r\n>\r\n> @lhoestq <https://github.com/lhoestq> Could you help me to upload these\r\n> preprocessed datasets to Huggingface's repositories? To be more precise,\r\n> I've built datasets from the following languages using the 20201201 dumps:\r\n> Spanish, Portuguese, Russian, French, Japanese, Chinese, and Turkish.\r\n> Process these datasets have high costs that most of the community can't\r\n> afford. I think these preprocessed datasets I have could be helpful for\r\n> someone without access to high-resource machines to process Wikipedia's\r\n> dumps like @dorost1234 <https://github.com/dorost1234>\r\n>\r\n> —\r\n> You are receiving this because you were mentioned.\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/huggingface/datasets/issues/1994#issuecomment-791798195>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AS37NMWMK5GFJFU3ACCJFUDTCFVNZANCNFSM4YUZIF4A>\r\n> .\r\n>\r\n""
 ""Hi @dorost1234, so sorry, but looking at my files here, I figure out that I've preprocessed files using the HF/datasets for all the languages previously listed by me (Portuguese, Russian, French, Japanese, Chinese, and Turkish) except the Spanish (on my tests I've used the [wikicorpus](https://www.cs.upc.edu/~nlp/wikicorpus/) instead).\r\n\r\nOnly with the Spanish Wikipedia's dump, I had the same `KeyError: '000nbsp'` problem already reported here https://github.com/huggingface/datasets/issues/577\r\n\r\nSo nowadays, even with access to a high resource machine, you couldn't be able to get Wikipedia's Spanish data using the HF/datasets :(\r\n\r\n\r\n\r\n\r\n""
 ""Thanks a lot for the information and help. This would be great to have\nthese datasets.\n@lhoestq <https://github.com/lhoestq>  Do you know a way I could get\nsmaller amount of these data like 1 GBtype of each language to deal with\ncomputatioanl requirements? thanks\n\nOn Sat, Mar 6, 2021 at 5:36 PM Jonatas Grosman <notifications@github.com>\nwrote:\n\n> Hi @dorost1234 <https://github.com/dorost1234>, so sorry, but looking at\n> my files here, I figure out that I've preprocessed files using the\n> HF/datasets for all the languages previously listed by me (Portuguese,\n> Russian, French, Japanese, Chinese, and Turkish) except the Spanish (on my\n> tests I've used the wikicorpus <https://www.cs.upc.edu/~nlp/wikicorpus/>\n> instead).\n>\n> Only with the Spanish Wikipedia's dump, I had the same KeyError: '000nbsp'\n> problem already reported here #577\n> <https://github.com/huggingface/datasets/issues/577>\n>\n> So nowadays, even with access to a high resource machine, you couldn't be\n> able to get Wikipedia's Spanish data using the HF/datasets :(\n>\n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/huggingface/datasets/issues/1994#issuecomment-791985546>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AS37NMWMO7WOHWLOROPD6Q3TCJKXPANCNFSM4YUZIF4A>\n> .\n>\n""
 'Hi ! As mentioned above the Spanish configuration have parsing issues from `mwparserfromhell`. I haven\'t tested with the latest `mwparserfromhell` >=0.6 though. Which version of `mwparserfromhell` are you using ?\r\n\r\n> @lhoestq Could you help me to upload these preprocessed datasets to Huggingface\'s repositories? To be more precise, I\'ve built datasets from the following languages using the 20201201 dumps: Spanish, Portuguese, Russian, French, Japanese, Chinese, and Turkish. Process these datasets have high costs that most of the community can\'t afford. I think these preprocessed datasets I have could be helpful for someone without access to high-resource machines to process Wikipedia\'s dumps like @dorost1234\r\n\r\nThat would be awesome ! Feel free to ping me on slack so we can put the processed wikipedia files on google storage with the other ones we\'ve already preprocessed.\r\n\r\n> Do you know a way I could get smaller amount of these data like 1 GBtype of each language to deal with computatioanl requirements? thanks\r\n\r\nI\'d suggest to copy the [wikipedia.py](https://github.com/huggingface/datasets/blob/master/datasets/wikipedia/wikipedia.py) to a new script `custom_wikipedia.py` and modify it to only download and process only a subset of the raw data files.\r\nYou can for example replace [this line](https://github.com/huggingface/datasets/blob/64e59fc45ca2134218b3e42e83fddddbe840ff74/datasets/wikipedia/wikipedia.py#L446) by:\r\n```python\r\n            if total_bytes >= (1 << 30):  # stop if the total amount of data is >= 1GB\r\n                break\r\n            else:\r\n                xml_urls.append(_base_url(lang) + fname)\r\n```\r\n\r\nThen you can load your custom wikipedia dataset with\r\n```python\r\nload_dataset(""path/to/my/custom_wikipedia.py"", f""{date}.{language}"")\r\n```'
 ""Hi @lhoestq!\r\n\r\n> Hi ! As mentioned above the Spanish configuration have parsing issues from mwparserfromhell. I haven't tested with the latest mwparserfromhell >=0.6 though. Which version of mwparserfromhell are you using ?\r\n\r\nI'm using the latest mwparserfromhell version (0.6)\r\n\r\n> That would be awesome ! Feel free to ping me on slack so we can put the processed wikipedia files on google storage with the other ones we've already preprocessed.\r\n\r\nI'll ping you there 👍 ""
 'Thank you so much @jonatasgrosman  and @lhoestq  this would be a great help. I am really thankful to you both and to wonderful Huggingface dataset library allowing us to train models at scale.']","Hi
I am trying to run a code with wikipedia of config 20200501.es, getting:

Traceback (most recent call last):
  File ""run_mlm_t5.py"", line 608, in <module>
    main()
  File ""run_mlm_t5.py"", line 359, in main
    datasets = load_dataset(data_args.dataset_name, data_args.dataset_config_name)
  File ""/dara/libs/anaconda3/envs/success432/lib/python3.7/site-packages/datasets-1.2.1-py3.7.egg/datasets/load.py"", line 612, in load_dataset
    ignore_verifications=ignore_verifications,
  File ""/dara/libs/anaconda3/envs/success432/lib/python3.7/site-packages/datasets-1.2.1-py3.7.egg/datasets/builder.py"", line 527, in download_and_prepare
    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
  File ""/dara/libs/anaconda3/envs/success432/lib/python3.7/site-packages/datasets-1.2.1-py3.7.egg/datasets/builder.py"", line 1050, in _download_and_prepare
    ""\n\t`{}`"".format(usage_example)
datasets.builder.MissingBeamOptions: Trying to generate a dataset using Apache Beam, yet no Beam Runner or PipelineOptions() has been provided in `load_dataset` or in the builder arguments. For big datasets it has to run on large-scale data processing tools like Dataflow, Spark, etc. More information about Apache Beam runners at https://beam.apache.org/documentation/runners/capability-matrix/
If you really want to run it locally because you feel like the Dataset is small enough, you can use the local beam runner called `DirectRunner` (you may run out of memory). 
Example of usage: 
	`load_dataset('wikipedia', '20200501.es', beam_runner='DirectRunner')`

thanks @lhoestq  for any suggestion/help "
https://github.com/huggingface/datasets/issues/1993,How to load a dataset with load_from disk and save it again after doing transformations without changing the original? ,"[""Hi ! That looks like a bug, can you provide some code so that we can reproduce ?\r\nIt's not supposed to update the original dataset""
 'Hi, I experimented with RAG. \r\n\r\nActually, you can run the [use_own_knowldge_dataset.py](https://github.com/shamanez/transformers/blob/rag-end-to-end-retrieval/examples/research_projects/rag/use_own_knowledge_dataset.py#L80). In the 80 you can save the dataset object to the disk with save_to_disk. Then in order to compute the embeddings in this use **load_from_disk**. \r\n\r\nThen finally save it. You can see the original dataset object (CSV after splitting also will be changed)\r\n\r\nOne more thing- when I save the dataset object with **save_to_disk** it name the arrow file with cache.... rather than using dataset. arrow.  Can you add a variable that we can feed a name to save_to_disk function?'
 ""@lhoestq   I also found that cache in tmp directory gets updated after transformations. This is really problematic when using datasets interactively. Let's say we use the shards function to a dataset loaded with csv, atm when we do transformations to shards and combine them it updates the original csv cache. ""
 ""I plan to update the save_to_disk method in #2025 so I can make sure the new save_to_disk doesn't corrupt your cache files.\r\nBut from your last message it looks like save_to_disk isn't the root cause right ?""
 'ok, one more thing. When we use save_to_disk there are two files other than .arrow. dataset_info.json and state.json. Sometimes most of the fields in the dataset_infor.json are null, especially when saving dataset objects. Anyways I think load_from_disk uses the arrow files mentioned in state.json right? '
 '> Anyways I think load_from_disk uses the arrow files mentioned in state.json right?\r\n\r\nYes exactly'
 'Perfect.  For now, I am loading the dataset from CSV in my interactive process and will wait until you make the PR!']","I am using the latest datasets library.  In my work, I first use **load_from_disk** to load a data set that contains 3.8Gb information. Then during my training process, I update that dataset object and add new elements and save it in a different place.  

When I save the dataset with **save_to_disk**, the original dataset which is already in the disk also gets updated. I do not want to update it.  How to prevent from this?
"
https://github.com/huggingface/datasets/issues/1992,`datasets.map` multi processing much slower than single processing ,"['Hi @hwijeen, you might want to look at issues #1796 and #1949. I think it could be something related to the I/O operations being performed.'
 'I see that many people are experiencing the same issue. Is this problem considered an ""official"" bug that is worth a closer look? @lhoestq'
 ""Yes this is an official bug. On my side I haven't managed to reproduce it but @theo-m has. We'll investigate this !""
 'Thank you for the reply! I would be happy to follow the discussions related to the issue.\r\nIf you do not mind, could you also give a little more explanation on my p.s.2? I am having a hard time figuring out why the single processing `map` uses all of my cores.\r\n@lhoestq @theo-m '
 'Regarding your ps2: It depends what function you pass to `map`.\r\nFor example, fast tokenizers from `transformers` in Rust tokenize texts and parallelize the tokenization over all the cores.'
 'I am still experiencing this issue with datasets 1.9.0..\r\nHas there been a further investigation? \r\n<img width=""442"" alt=""image"" src=""https://user-images.githubusercontent.com/29157715/126143387-8b5ddca2-a896-4e18-abf7-4fbf62a48b41.png"">\r\n']","Hi, thank you for the great library.

I've been using datasets to pretrain language models, and it often involves datasets as large as ~70G.
My data preparation step is roughly two steps: `load_dataset` which splits corpora into a table of sentences, and `map` converts a sentence into a list of integers, using a tokenizer.

I noticed that `map` function with `num_proc=mp.cpu_count() //2` takes more than 20 hours to finish the job where as `num_proc=1` gets the job done in about 5 hours. The machine I used has 40 cores, with 126G of RAM. There were no other jobs when `map` function was running.

What could be the reason? I would be happy to provide information necessary to spot the reason.

p.s. I was experiencing the imbalance issue mentioned in [here](https://github.com/huggingface/datasets/issues/610#issuecomment-705177036) when I was using multi processing.
p.s.2 When I run `map` with `num_proc=1`, I see one tqdm bar but all the cores are working. When `num_proc=20`, only 20 cores work. 
![Screen Shot 2021-03-05 at 11 04 59](https://user-images.githubusercontent.com/29157715/110056895-ef6cf000-7da2-11eb-8307-6698e9fb1ad4.png)
"
https://github.com/huggingface/datasets/issues/1990,OSError: Memory mapping file failed: Cannot allocate memory,"['Do you think this is trying to bring the dataset into memory and if I can avoid it to save on memory so it only brings a batch into memory? @lhoestq  thank you'
 ""It's not trying to bring the dataset into memory.\r\n\r\nActually, it's trying to memory map the dataset file, which is different. It allows to load large dataset files without filling up memory.\r\n\r\nWhat dataset did you use to get this error ?\r\nOn what OS are you running ? What's your python and pyarrow version ?""
 'Dear @lhoestq \r\nthank you so much for coming back to me. Please find info below:\r\n1) Dataset name: I used  wikipedia with config 20200501.en\r\n2) I got these pyarrow in my environment:\r\npyarrow                   2.0.0                     <pip>\r\npyarrow                   3.0.0                     <pip>\r\n\r\n3) python version 3.7.10\r\n4) OS version \r\n\r\nlsb_release -a\r\nNo LSB modules are available.\r\nDistributor ID:\tDebian\r\nDescription:\tDebian GNU/Linux 10 (buster)\r\nRelease:\t10\r\nCodename:\tbuster\r\n\r\n\r\nIs there a way I could solve the memory issue and if I could run this model, I am using  GeForce GTX 108, \r\nthanks \r\n'
 'I noticed that the error happens when loading the validation dataset.\r\nWhat value of `data_args.validation_split_percentage` did you use ?'
 'Dear @lhoestq \r\n\r\nthank you very much for the very sharp observation, indeed, this happens there, I use the default value of 5, I basically plan to subsample a part of the large  dataset and choose it as validation set. Do you think this is bringing the data into memory during subsampling? Is there a way I could avoid this?\r\n\r\nThank you very much for the great help.\r\n\r\n\r\nOn Mon, Mar 8, 2021 at 11:28 AM Quentin Lhoest ***@***.***>\r\nwrote:\r\n\r\n> I noticed that the error happens when loading the validation dataset.\r\n> What value of data_args.validation_split_percentage did you use ?\r\n>\r\n> —\r\n> You are receiving this because you authored the thread.\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/huggingface/datasets/issues/1990#issuecomment-792655644>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AS37NMS337ZUJ7HGGVVCCR3TCSREFANCNFSM4YTYAQ2A>\r\n> .\r\n>\r\n'
 'Methods like `dataset.shard`, `dataset.train_test_split`, `dataset.select` etc. don\'t bring the dataset in memory. \r\nThe only time when samples are brought to memory is when you access elements via `dataset[0]`, `dataset[:10]`, `dataset[""my_column_names""]`.\r\n\r\nBut it\'s possible that trying to use those methods to build your validation set doesn\'t fix the issue since, if I understand correctly, the error happens when when the dataset arrow file is opened (just before the 5% percentage is applied).\r\n\r\nDid you try to reproduce this issue in a google colab ? This would be super helpful to investigate why this happened.\r\n\r\nAlso maybe you can try clearing your cache at `~/.cache/huggingface/datasets` and try again. If the arrow file was corrupted somehow, removing it and rebuilding may fix the issue.']","Hi,
I am trying to run a code with a wikipedia dataset, here is the command to reproduce the error. You can find the codes for run_mlm.py in huggingface repo here: https://github.com/huggingface/transformers/blob/v4.3.2/examples/language-modeling/run_mlm.py 
```
python run_mlm.py --model_name_or_path bert-base-multilingual-cased --dataset_name wikipedia --dataset_config_name 20200501.en --do_train --do_eval --output_dir /dara/test  --max_seq_length 128
```

I am using transformer version: 4.3.2 

But I got memory erorr using this dataset, is there a way I could save on memory with dataset library with wikipedia dataset?
Specially I need to train a model with multiple of wikipedia datasets concatenated. thank you very much @lhoestq  for your help and suggestions:

```
  File ""run_mlm.py"", line 441, in <module>
    main()
  File ""run_mlm.py"", line 233, in main
    split=f""train[{data_args.validation_split_percentage}%:]"",
  File ""/dara/libs/anaconda3/envs/code/lib/python3.7/site-packages/datasets-1.3.0-py3.7.egg/datasets/load.py"", line 750, in load_dataset
    ds = builder_instance.as_dataset(split=split, ignore_verifications=ignore_verifications, in_memory=keep_in_memory)
  File ""/dara/libs/anaconda3/envs/code/lib/python3.7/site-packages/datasets-1.3.0-py3.7.egg/datasets/builder.py"", line 740, in as_dataset
    map_tuple=True,
  File ""/dara/libs/anaconda3/envs/code/lib/python3.7/site-packages/datasets-1.3.0-py3.7.egg/datasets/utils/py_utils.py"", line 225, in map_nested
    return function(data_struct)
  File ""/dara/libs/anaconda3/envs/code/lib/python3.7/site-packages/datasets-1.3.0-py3.7.egg/datasets/builder.py"", line 757, in _build_single_dataset
    in_memory=in_memory,
  File ""/dara/libs/anaconda3/envs/code/lib/python3.7/site-packages/datasets-1.3.0-py3.7.egg/datasets/builder.py"", line 829, in _as_dataset
    in_memory=in_memory,
  File ""/dara/libs/anaconda3/envs/code/lib/python3.7/site-packages/datasets-1.3.0-py3.7.egg/datasets/arrow_reader.py"", line 215, in read
    return self.read_files(files=files, original_instructions=instructions, in_memory=in_memory)
  File ""/dara/libs/anaconda3/envs/code/lib/python3.7/site-packages/datasets-1.3.0-py3.7.egg/datasets/arrow_reader.py"", line 236, in read_files
    pa_table = self._read_files(files, in_memory=in_memory)
  File ""/dara/libs/anaconda3/envs/code/lib/python3.7/site-packages/datasets-1.3.0-py3.7.egg/datasets/arrow_reader.py"", line 171, in _read_files
    pa_table: pa.Table = self._get_dataset_from_filename(f_dict, in_memory=in_memory)
  File ""/dara/libs/anaconda3/envs/code/lib/python3.7/site-packages/datasets-1.3.0-py3.7.egg/datasets/arrow_reader.py"", line 302, in _get_dataset_from_filename
    pa_table = ArrowReader.read_table(filename, in_memory=in_memory)
  File ""/dara/libs/anaconda3/envs/code/lib/python3.7/site-packages/datasets-1.3.0-py3.7.egg/datasets/arrow_reader.py"", line 322, in read_table
    stream = stream_from(filename)
  File ""pyarrow/io.pxi"", line 782, in pyarrow.lib.memory_map
  File ""pyarrow/io.pxi"", line 743, in pyarrow.lib.MemoryMappedFile._open
  File ""pyarrow/error.pxi"", line 122, in pyarrow.lib.pyarrow_internal_check_status
  File ""pyarrow/error.pxi"", line 99, in pyarrow.lib.check_status
OSError: Memory mapping file failed: Cannot allocate memory
```


"
https://github.com/huggingface/datasets/issues/1989,Question/problem with dataset labels,"['It seems that I get parsing errors for various fields in my data. For example now I get this:\r\n```\r\n  File ""../../../models/tr-4.3.2/run_puppets.py"", line 523, in <module>\r\n    main()\r\n  File ""../../../models/tr-4.3.2/run_puppets.py"", line 249, in main\r\n    datasets = load_dataset(""csv"", data_files=data_files)\r\n  File ""/dccstor/redrug_ier/envs/last-tr/lib/python3.8/site-packages/datasets/load.py"", line 740, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File ""/dccstor/redrug_ier/envs/last-tr/lib/python3.8/site-packages/datasets/builder.py"", line 572, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File ""/dccstor/redrug_ier/envs/last-tr/lib/python3.8/site-packages/datasets/builder.py"", line 650, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File ""/dccstor/redrug_ier/envs/last-tr/lib/python3.8/site-packages/datasets/builder.py"", line 1028, in _prepare_split\r\n    writer.write_table(table)\r\n  File ""/dccstor/redrug_ier/envs/last-tr/lib/python3.8/site-packages/datasets/arrow_writer.py"", line 292, in write_table\r\n    pa_table = pa_table.cast(self._schema)\r\n  File ""pyarrow/table.pxi"", line 1311, in pyarrow.lib.Table.cast\r\n  File ""pyarrow/table.pxi"", line 265, in pyarrow.lib.ChunkedArray.cast\r\n  File ""/dccstor/redrug_ier/envs/last-tr/lib/python3.8/site-packages/pyarrow/compute.py"", line 87, in cast\r\n    return call_function(""cast"", [arr], options)\r\n  File ""pyarrow/_compute.pyx"", line 298, in pyarrow._compute.call_function\r\n  File ""pyarrow/_compute.pyx"", line 192, in pyarrow._compute.Function.call\r\n  File ""pyarrow/error.pxi"", line 122, in pyarrow.lib.pyarrow_internal_check_status\r\n  File ""pyarrow/error.pxi"", line 84, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Failed to parse string: https://www.netgalley.com/catalog/book/121872\r\n```'
 'Not sure if this helps, this is how I load my files (as in the sample scripts on transformers):\r\n\r\n```\r\n    if data_args.train_file.endswith("".csv""):\r\n        # Loading a dataset from local csv files\r\n        datasets = load_dataset(""csv"", data_files=data_files)\r\n```'
 ""Since this worked out of the box in a few examples before, I wonder if it's some quoting issue or something else. ""
 'Hi @ioana-blue,\r\nCan you share a sample from your .csv? A dummy where you get this error will also help.\r\n\r\nI tried this csv:\r\n```csv\r\nfeature,label\r\n1.2,not nurse\r\n1.3,nurse\r\n1.5,surgeon\r\n```\r\nand the following snippet:\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nd = load_dataset(""csv"",data_files=[\'test.csv\'])\r\n\r\nprint(d)\r\nprint(d[\'train\'][\'label\'])\r\n```\r\nand this works perfectly fine for me:\r\n```sh\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: [\'feature\', \'label\'],\r\n        num_rows: 3\r\n    })\r\n})\r\n[\'not nurse\', \'nurse\', \'surgeon\']\r\n```\r\nI\'m sure your csv is more complicated than this one. But it is hard to tell where the issue might be without looking at a sample.'
 ""I've had versions where it worked fain. For this dataset, I had all kind of parsing issues that I couldn't understand. What I ended up doing is strip all the columns that I didn't need and also make the label 0/1. \r\n\r\nI think one line that may have caused a problem was the csv version of this:\r\n\r\n```crawl-data/CC-MAIN-2017-47/segments/1510934806225.78/wet/CC-MAIN-20171120203833-20171120223833-00571.warc.wet.gz        Rose Blakey is an aspiring journalist. She is desperate to escape the from the small Australian town in which she lives. Rejection after rejection mean she is stuck in what she sees as a dead-end waitressing job. ^M ('Rose', '', 'Blakey')  journalist      F       38      journalist      https://www.netgalley.com/catalog/book/121872   _ is desperate to escape the from the small Australian town in which _ lives. Rejection after rejection mean _ is stuck in what _ sees as a dead-end waitressing job.    She is desperate to escape the from the small Australian town in which she lives. Rejection after rejection mean she is stuck in what she sees as a dead-end waitressing job.```\r\n\r\nThe error I got in this case is this one: https://github.com/huggingface/datasets/issues/1989#issuecomment-790842771\r\n\r\nNote, this line was part of a much larger file and until this line I guess it was working fine. ""
 ""Hi @ioana-blue,\r\n\r\nWhat is the separator you're using for the csv? I see there are only two commas in the given line, but they don't seem like appropriate points. Also, is this a string part of one line, or an entire line? There should also be a label, right?""
 'Sorry for the confusion, the sample above was from a tsv that was used to derive the csv. Let me construct the csv again (I had remove it). \r\n\r\nThis is the line in the csv - this is the whole line:\r\n```crawl-data/CC-MAIN-2017-47/segments/1510934806225.78/wet/CC-MAIN-20171120203833-20171120223833-00571.warc.wet.gz,Rose Blakey is an aspiring journalist. She is desperate to escape the from the small Australian town in which she lives. Rejection after rejection mean she is stuck in what she sees as a dead,""(\'Rose\', \'\', \'Blakey\')"",journalist,F,38,journalist,https://www.netgalley.com/catalog/book/121872,_ is desperate to escape the from the small Australian town in which _ lives. Rejection after rejection mean _ is stuck in what _ sees as a dead-end waitressing job., She is desperate to escape the from the small Australian town in which she lives. Rejection after rejection mean she is stuck in what she sees as a dead-end waitressing job.```'
 'Hi,\r\nJust in case you want to use tsv directly, you can use the separator argument while loading the dataset.\r\n```python\r\nd = load_dataset(""csv"",data_files=[\'test.csv\'],sep=""\\t"")\r\n```\r\n\r\nAdditionally, I don\'t face the issues with the following csv (same as the one you provided):\r\n\r\n```sh\r\nlink1,text1,info1,info2,info3,info4,info5,link2,text2,text3\r\ncrawl-data/CC-MAIN-2017-47/segments/1510934806225.78/wet/CC-MAIN-20171120203833-20171120223833-00571.warc.wet.gz,Rose Blakey is an aspiring journalist. She is desperate to escape the from the small Australian town in which she lives. Rejection after rejection mean she is stuck in what she sees as a dead,""(\'Rose\', \'\', \'Blakey\')"",journalist,F,38,journalist,https://www.netgalley.com/catalog/book/121872,_ is desperate to escape the from the small Australian town in which _ lives. Rejection after rejection mean _ is stuck in what _ sees as a dead-end waitressing job., She is desperate to escape the from the small Australian town in which she lives. Rejection after rejection mean she is stuck in what she sees as a dead-end waitressing job.\r\n```\r\nOutput after loading:\r\n```sh\r\n{\'link1\': \'crawl-data/CC-MAIN-2017-47/segments/1510934806225.78/wet/CC-MAIN-20171120203833-20171120223833-00571.warc.wet.gz\', \'text1\': \'Rose Blakey is an aspiring journalist. She is desperate to escape the from the small Australian town in which she lives. Rejection after rejection mean she is stuck in what she sees as a dead\', \'info1\': ""(\'Rose\', \'\', \'Blakey\')"", \'info2\': \'journalist\', \'info3\': \'F\', \'info4\': 38, \'info5\': \'journalist\', \'link2\': \'https://www.netgalley.com/catalog/book/121872\', \'text2\': \'_ is desperate to escape the from the small Australian town in which _ lives. Rejection after rejection mean _ is stuck in what _ sees as a dead-end waitressing job.\', \'text3\': \' She is desperate to escape the from the small Australian town in which she lives. Rejection after rejection mean she is stuck in what she sees as a dead-end waitressing job.\'}\r\n```\r\nCan you check once if the tsv works for you directly using the separator argument? The conversion from tsv to csv could create issues, I\'m only guessing though.'
 ""thanks for the tip. very strange :/ I'll check my datasets version as well. \r\n\r\nI will have more similar experiments soon so I'll let you know if I manage to get rid of this. ""
 ""No problem at all. I thought I'd be able to solve this but I'm unable to replicate the issue :/""]","Hi, I'm using a dataset with two labels ""nurse"" and ""not nurse"". For whatever reason (that I don't understand), I get an error that I think comes from the datasets package (using csv). Everything works fine if the labels are ""nurse"" and ""surgeon"". 

This is the trace I get:

```
File ""../../../models/tr-4.3.2/run_puppets.py"", line 523, in <module>
    main()
  File ""../../../models/tr-4.3.2/run_puppets.py"", line 249, in main
    datasets = load_dataset(""csv"", data_files=data_files)
  File ""/dccstor/redrug_ier/envs/last-tr/lib/python3.8/site-packages/datasets/load.py"", line 740, in load_dataset
    builder_instance.download_and_prepare(
  File ""/dccstor/redrug_ier/envs/last-tr/lib/python3.8/site-packages/datasets/builder.py"", line 572, in download_and_prepare
    self._download_and_prepare(
  File ""/dccstor/redrug_ier/envs/last-tr/lib/python3.8/site-packages/datasets/builder.py"", line 650, in _download_and_prepare
    self._prepare_split(split_generator, **prepare_split_kwargs)
  File ""/dccstor/redrug_ier/envs/last-tr/lib/python3.8/site-packages/datasets/builder.py"", line 1028, in _prepare_split
    writer.write_table(table)
  File ""/dccstor/redrug_ier/envs/last-tr/lib/python3.8/site-packages/datasets/arrow_writer.py"", line 292, in write_table
    pa_table = pa_table.cast(self._schema)
  File ""pyarrow/table.pxi"", line 1311, in pyarrow.lib.Table.cast
  File ""pyarrow/table.pxi"", line 265, in pyarrow.lib.ChunkedArray.cast
  File ""/dccstor/redrug_ier/envs/last-tr/lib/python3.8/site-packages/pyarrow/compute.py"", line 87, in cast
    return call_function(""cast"", [arr], options)
  File ""pyarrow/_compute.pyx"", line 298, in pyarrow._compute.call_function
  File ""pyarrow/_compute.pyx"", line 192, in pyarrow._compute.Function.call
  File ""pyarrow/error.pxi"", line 122, in pyarrow.lib.pyarrow_internal_check_status
  File ""pyarrow/error.pxi"", line 84, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: Failed to parse string: not nurse
```

Any ideas how to fix this? For now, I'll probably make them numeric. "
https://github.com/huggingface/datasets/issues/1988,Readme.md is misleading about kinds of datasets?,"[""Hi ! Yes it's possible to use image data. There are already a few of them available (MNIST, CIFAR..)""]","Hi!

At the README.MD, you say: ""efficient data pre-processing: simple, fast and reproducible data pre-processing for the above public datasets as well as your own local datasets in CSV/JSON/text. ""

But here:
https://github.com/huggingface/datasets/blob/master/templates/new_dataset_script.py#L82-L117

You mention other kinds of datasets, with images and so on. I'm confused. 

Is it possible to use it to store, say, imagenet locally? "
https://github.com/huggingface/datasets/issues/1986,wmt datasets fail to load,"['caching issue, seems to work again..']","~\.cache\huggingface\modules\datasets_modules\datasets\wmt14\43e717d978d2261502b0194999583acb874ba73b0f4aed0ada2889d1bb00f36e\wmt_utils.py in _split_generators(self, dl_manager)
    758         # Extract manually downloaded files.
    759         manual_files = dl_manager.extract(manual_paths_dict)
--> 760         extraction_map = dict(downloaded_files, **manual_files)
    761 
    762         for language in self.config.language_pair:

TypeError: type object argument after ** must be a mapping, not list"
https://github.com/huggingface/datasets/issues/1983,The size of CoNLL-2003 is not consistant with the official release.,"['Hi,\r\n\r\nif you inspect the raw data, you can find there are 946 occurrences of `-DOCSTART- -X- -X- O` in the train split and `14041 + 946 = 14987`, which is exactly the number of sentences the authors report. `-DOCSTART-` is a special line that acts as a boundary between two different documents and is filtered out in our implementation.\r\n\r\n@lhoestq What do you think about including these lines? ([Link](https://github.com/flairNLP/flair/issues/1097) to a similar issue in the flairNLP repo)'
 'We should mention in the Conll2003 dataset card that these lines have been removed indeed.\r\n\r\nIf some users are interested in using these lines (maybe to recombine documents ?) then we can add a parameter to the conll2003 dataset to include them.\r\n\r\nBut IMO the default config should stay the current one (without the `-DOCSTART-` stuff), so that you can directly train NER models without additional preprocessing. Let me know what you think'
 ""@lhoestq Yes, I agree adding a small note should be sufficient.\r\n\r\nCurrently, NLTK's `ConllCorpusReader` ignores the `-DOCSTART-` lines so I think it's ok if we do the same. If there is an interest in the future to use these lines, then we can include them.""
 ""I added a mention of this in conll2003's dataset card:\r\nhttps://github.com/huggingface/datasets/blob/fc9796920da88486c3b97690969aabf03d6b4088/datasets/conll2003/README.md#conll2003\r\n\r\nEdit: just saw your PR @mariosasko (noticed it too late ^^)\r\nLet me take a look at it :)""]","Thanks for the dataset sharing! But when I use conll-2003, I meet some questions.
The statistics of conll-2003 in this repo is  : 
\#train 14041  \#dev 3250 \#test 3453
While the official statistics is:
\#train 14987 \#dev 3466 \#test 3684
Wish for your reply~"
https://github.com/huggingface/datasets/issues/1981,wmt datasets fail to load,"['@stas00 Mea culpa... May I fix this tomorrow morning?'
 'yes, of course, I reverted to the version before that and it works ;)\r\n\r\nbut since a new release was just made you will probably need to make a hotfix.\r\n\r\nand add the wmt to the tests?'
 'Sure, I will implement a regression test!'
 '@stas00 it is fixed. @lhoestq are you releasing the hot fix or would you prefer me to do it?'
 ""I'll do a patch release for this issue early tomorrow.\r\n\r\nAnd yes we absolutly need tests for the wmt datasets: The missing tests for wmt are an artifact from the early development of the lib but now we have tools to generate automatically the dummy data used for tests :)""
 'still facing the same issue or similar:\r\nfrom datasets import load_dataset\r\nwtm14_test = load_dataset(\'wmt14\',""de-en"",cache_dir=\'./datasets\')\r\n\r\n~.cache\\huggingface\\modules\\datasets_modules\\datasets\\wmt14\\43e717d978d2261502b0194999583acb874ba73b0f4aed0ada2889d1bb00f36e\\wmt_utils.py in _split_generators(self, dl_manager)\r\n758 # Extract manually downloaded files.\r\n759 manual_files = dl_manager.extract(manual_paths_dict)\r\n--> 760 extraction_map = dict(downloaded_files, **manual_files)\r\n761\r\n762 for language in self.config.language_pair:\r\n\r\nTypeError: type object argument after ** must be a mapping, not list']","on master:
```
python -c 'from datasets import load_dataset; load_dataset(""wmt14"", ""de-en"")'
Downloading and preparing dataset wmt14/de-en (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/stas/.cache/huggingface/datasets/wmt14/de-en/1.0.0/43e717d978d2261502b0194999583acb874ba73b0f4aed0ada2889d1bb00f36e...
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/mnt/nvme1/code/huggingface/datasets-master/src/datasets/load.py"", line 740, in load_dataset
    builder_instance.download_and_prepare(
  File ""/mnt/nvme1/code/huggingface/datasets-master/src/datasets/builder.py"", line 578, in download_and_prepare
    self._download_and_prepare(
  File ""/mnt/nvme1/code/huggingface/datasets-master/src/datasets/builder.py"", line 634, in _download_and_prepare
    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)
  File ""/home/stas/.cache/huggingface/modules/datasets_modules/datasets/wmt14/43e717d978d2261502b0194999583acb874ba73b0f4aed0ada2889d1bb00f36e/wmt_utils.py"", line 760, in _split_generators
    extraction_map = dict(downloaded_files, **manual_files)
```

it worked fine recently. same problem if I try wmt16.

git bisect points to this commit from Feb 25 as the culprit https://github.com/huggingface/datasets/commit/792f1d9bb1c5361908f73e2ef7f0181b2be409fa

@albertvillanova "
https://github.com/huggingface/datasets/issues/1977,ModuleNotFoundError: No module named 'apache_beam' for wikipedia datasets ,"['I sometimes also get this error with other languages of the same dataset:\r\n\r\n  File ""/dara/libs/anaconda3/envs/code/lib/python3.7/site-packages/datasets-1.3.0-py3.7.egg/datasets/arrow_reader.py"", line 322, in read_table\r\n    stream = stream_from(filename)\r\n  File ""pyarrow/io.pxi"", line 782, in pyarrow.lib.memory_map\r\n  File ""pyarrow/io.pxi"", line 743, in pyarrow.lib.MemoryMappedFile._open\r\n  File ""pyarrow/error.pxi"", line 122, in pyarrow.lib.pyarrow_internal_check_status\r\n  File ""pyarrow/error.pxi"", line 99, in pyarrow.lib.check_status\r\nOSError: Memory mapping file failed: Cannot allocate memory\r\n\r\n@lhoestq \r\n'
 ""Hi ! Thanks for reporting\r\nSome wikipedia configurations do require the user to have `apache_beam` in order to parse the wikimedia data.\r\n\r\nOn the other hand regarding your second issue\r\n```\r\nOSError: Memory mapping file failed: Cannot allocate memory\r\n```\r\nI've never experienced this, can you open a new issue for this specific error and provide more details please ?\r\nFor example what script did you use to get this, what language did you use, what's your environment details (os, python version, pyarrow version)..""]","Hi
I am trying to run run_mlm.py code [1] of huggingface with following ""wikipedia""/ ""20200501.aa""  dataset:

`python run_mlm.py     --model_name_or_path bert-base-multilingual-cased --dataset_name wikipedia     --dataset_config_name 20200501.aa     --do_train     --do_eval     --output_dir /tmp/test-mlm --max_seq_length 256
`

I am getting this error, but as per documentation, huggingface dataset provide processed version of this dataset and users can load it without requiring setup extra settings for apache-beam. could you help me please to load this dataset? 
Do you think I can run run_ml.py with this dataset? or anyway I could subsample and train the model? I greatly appreciate providing the processed version of all languages for this dataset, which allow the user to use them without setting up apache-beam,. thanks 

I really appreciate your help.
@lhoestq 

thanks.

[1] https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py

error I get: 

```
>>> import datasets 
>>> datasets.load_dataset(""wikipedia"", ""20200501.aa"")
Downloading and preparing dataset wikipedia/20200501.aa (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /dara/temp/cache_home_2/datasets/wikipedia/20200501.aa/1.0.0/4021357e28509391eab2f8300d9b689e7e8f3a877ebb3d354b01577d497ebc63...
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/dara/temp/libs/anaconda3/envs/codes/lib/python3.7/site-packages/datasets-1.3.0-py3.7.egg/datasets/load.py"", line 746, in load_dataset
    use_auth_token=use_auth_token,
  File ""/dara/temp/libs/anaconda3/envs/codes/lib/python3.7/site-packages/datasets-1.3.0-py3.7.egg/datasets/builder.py"", line 573, in download_and_prepare
    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
  File ""/dara/temp/libs/anaconda3/envs/codes/lib/python3.7/site-packages/datasets-1.3.0-py3.7.egg/datasets/builder.py"", line 1099, in _download_and_prepare
    import apache_beam as beam
ModuleNotFoundError: No module named 'apache_beam'

```"
https://github.com/huggingface/datasets/issues/1973,Question: what gets stored in the datasets cache and why is it so huge?,"[""Echo'ing this observation: I have a few datasets in the neighborhood of 2GB CSVs uncompressed, and when I use something like `Dataset.save_to_disk()` it's ~18GB on disk.\r\n\r\nIf this is unexpected behavior, would be happy to help run debugging as needed.""
 'Thanks @ioana-blue for pointing out this problem (and thanks also @justin-yan). You are right that current implementation of the datasets caching files take too much memory. We are definitely changing this and optimizing the defaults, so that the file sizes are considerably reduced. I will come back to you as soon as this is fixed.'
 ""Thank you! Also I noticed that the files don't seem to be cleaned after the jobs finish. Last night I had only 3 jobs running, but the cache was still at 180GB. ""
 ""And to clarify, it's not memory, it's disk space. Thank you!""
 ""Hi ! As Albert said they can sometimes take more space that expected but we'll fix that soon.\r\n\r\nAlso, to give more details about caching: computations on a dataset are cached by default so that you don't have to recompute them the next time you run them.\r\n\r\nSo by default the cache files stay on your disk when you job is finished (so that if you re-execute it, it will be reloaded from the cache).\r\nFeel free to clear your cache after your job has finished, or disable caching using\r\n```python\r\nimport datasets\r\n\r\ndatasets.set_caching_enabled(False)\r\n```""
 'Thanks for the tip, this is useful. '
 ""Hi @ioana-blue, we have optimized Datasets' disk usage in the latest release v1.5.\r\n\r\nFeel free to update your Datasets version\r\n```shell\r\npip install -U datasets\r\n```\r\nand see if it better suits your needs.""
 'Thank you!']",I'm running several training jobs (around 10) with a relatively large dataset (3M samples). The datasets cache reached 178G and it seems really large. What is it stored in there and why is it so large? I don't think I noticed this problem before and seems to be related to the new version of the datasets library. Any insight? Thank you!
https://github.com/huggingface/datasets/issues/1972,'Dataset' object has no attribute 'rename_column',['Hi ! `rename_column` has been added recently and will be available in the next release'],'Dataset' object has no attribute 'rename_column'
https://github.com/huggingface/datasets/issues/1965,Can we parallelized the add_faiss_index process over dataset shards ?,"['Hi !\r\nAs far as I know not all faiss indexes can be computed in parallel and then merged. \r\nFor example [here](https://github.com/facebookresearch/faiss/wiki/Special-operations-on-indexes#splitting-and-merging-indexes) is is mentioned that only IndexIVF indexes can be merged.\r\nMoreover faiss already works using multithreading to parallelize the workload over your different CPU cores. You can find more info [here](https://github.com/facebookresearch/faiss/wiki/Threads-and-asynchronous-calls#internal-threading)\r\nSo I feel like the gains we would get by implementing a parallel `add_faiss_index` would not be that important, but let me know what you think.\r\n'
 'Actually, you are right. I also had the same idea. I am trying this in the context of end-ton-end retrieval training in RAG.  So far I have parallelized the embedding re-computation within the training loop by using datasets shards. \r\n\r\nThen I was thinking of can I calculate the indexes for each shard and combined them with **concatenate**  before I save.'
 '@lhoestq  As you mentioned faiss is already using multiprocessing. I tried to do the add_index with faiss for a dataset object inside a RAY actor and the process became very slow... if fact it takes so much time. It is because a ray actor comes with a single CPU core unless we assign it more. I also tried assigning more cores but still running add_index in the main process is very fast. ']","I am thinking of making the  **add_faiss_index** process faster. What if we run the add_faiss_index process on separate dataset shards and then combine them before (dataset.concatenate) saving the faiss.index file ?

I feel theoretically this will reduce the accuracy of retrieval since it affects the indexing process.

@lhoestq
"
https://github.com/huggingface/datasets/issues/1964,Datasets.py function load_dataset does not match squad dataset,"['Hi !\r\n\r\nTo fix 1, an you try to run this code ?\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nload_dataset(""squad"", download_mode=""force_redownload"")\r\n```\r\nMaybe the file your downloaded was corrupted, in this case redownloading this way should fix your issue 1.\r\n\r\nRegarding your 2nd point, you\'re right that loading the raw json this way doesn\'t give you a dataset with the column ""context"", ""question"" and ""answers"". Indeed the squad format is a very nested format so you have to preprocess the data. You can do it this way:\r\n```python\r\ndef process_squad(examples):\r\n    """"""\r\n    Process a dataset in the squad format with columns ""title"" and ""paragraphs""\r\n    to return the dataset with columns ""context"", ""question"" and ""answers"".\r\n    """"""\r\n    out = {""context"": [], ""question"": [], ""answers"":[]} \r\n    for paragraphs in examples[""paragraphs""]: \r\n        for paragraph in paragraphs: \r\n            for qa in paragraph[""qas""]: \r\n                answers = [{""answer_start"": answer[""answer_start""], ""text"": answer[""text""].strip()} for answer in qa[""answers""]] \r\n                out[""context""].append(paragraph[""context""].strip()) \r\n                out[""question""].append(qa[""question""].strip()) \r\n                out[""answers""].append(answers) \r\n    return out\r\n\r\ndatasets = load_dataset(extension, data_files=data_files, field=""data"")\r\ncolumn_names = datasets[""train""].column_names\r\n\r\nif set(column_names) == {""title"", ""paragraphs""}:\r\n    datasets = datasets.map(process_squad, batched=True, remove_columns=column_names)\r\n```\r\n\r\nHope that helps :)'
 'Thks for quickly answering！\r\n### 1 I try the first way,but seems not work \r\n```\r\nTraceback (most recent call last):\r\n  File ""examples/question-answering/run_qa.py"", line 503, in <module>\r\n    main()\r\n  File ""examples/question-answering/run_qa.py"", line 218, in main\r\n    datasets = load_dataset(data_args.dataset_name, download_mode=""force_redownload"")\r\n  File ""/home2/zhenggo1/anaconda3/envs/lpot/lib/python3.7/site-packages/datasets/load.py"", line 746, in load_dataset\r\n    use_auth_token=use_auth_token,\r\n  File ""/home2/zhenggo1/anaconda3/envs/lpot/lib/python3.7/site-packages/datasets/builder.py"", line 573, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File ""/home2/zhenggo1/anaconda3/envs/lpot/lib/python3.7/site-packages/datasets/builder.py"", line 633, in _download_and_prepare\r\n    self.info.download_checksums, dl_manager.get_recorded_sizes_checksums(), ""dataset source files""\r\n  File ""/home2/zhenggo1/anaconda3/envs/lpot/lib/python3.7/site-packages/datasets/utils/info_utils.py"", line 39, in verify_checksums\r\n    raise NonMatchingChecksumError(error_msg + str(bad_urls))\r\ndatasets.utils.info_utils.NonMatchingChecksumError: Checksums didn\'t match for dataset source files:\r\n[\'https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\']\r\n```\r\n### 2 I try the second way,and run the examples/question-answering/run_qa.py,it lead to another bug orz..\r\n```\r\nTraceback (most recent call last):\r\n  File ""examples/question-answering/run_qa.py"", line 523, in <module>\r\n    main()\r\n  File ""examples/question-answering/run_qa.py"", line 379, in main\r\n    load_from_cache_file=not data_args.overwrite_cache,\r\n  File ""/home2/zhenggo1/anaconda3/envs/lpot/lib/python3.7/site-packages/datasets/arrow_dataset.py"", line 1120, in map\r\n    update_data = does_function_return_dict(test_inputs, test_indices)\r\n  File ""/home2/zhenggo1/anaconda3/envs/lpot/lib/python3.7/site-packages/datasets/arrow_dataset.py"", line 1091, in does_function_return_dict\r\n    function(*fn_args, indices, **fn_kwargs) if with_indices else function(*fn_args, **fn_kwargs)\r\n  File ""examples/question-answering/run_qa.py"", line 339, in prepare_train_features\r\n    if len(answers[""answer_start""]) == 0:\r\nTypeError: list indices must be integers or slices, not str\r\n```\r\n## may be the function prepare_train_features in run_qa.py need to fix,I think is that the prep\r\n```python\r\nfor i, offsets in enumerate(offset_mapping):\r\n        # We will label impossible answers with the index of the CLS token.\r\n        input_ids = tokenized_examples[""input_ids""][i]\r\n        cls_index = input_ids.index(tokenizer.cls_token_id)\r\n\r\n        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\r\n        sequence_ids = tokenized_examples.sequence_ids(i)\r\n\r\n        # One example can give several spans, this is the index of the example containing this span of text.\r\n        sample_index = sample_mapping[i]\r\n        answers = examples[answer_column_name][sample_index]\r\n        print(examples,answers)\r\n        # If no answers are given, set the cls_index as answer.\r\n        if len(answers[""answer_start""]) == 0:\r\n            tokenized_examples[""start_positions""].append(cls_index)\r\n            tokenized_examples[""end_positions""].append(cls_index)\r\n        else:\r\n            # Start/end character index of the answer in the text.\r\n            start_char = answers[""answer_start""][0]\r\n            end_char = start_char + len(answers[""text""][0])\r\n\r\n            # Start token index of the current span in the text.\r\n            token_start_index = 0\r\n            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\r\n                token_start_index += 1\r\n\r\n            # End token index of the current span in the text.\r\n            token_end_index = len(input_ids) - 1\r\n            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\r\n                token_end_index -= 1\r\n\r\n            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\r\n            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\r\n                tokenized_examples[""start_positions""].append(cls_index)\r\n                tokenized_examples[""end_positions""].append(cls_index)\r\n            else:\r\n                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\r\n                # Note: we could go after the last offset if the answer is the last word (edge case).\r\n                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\r\n                    token_start_index += 1\r\n                tokenized_examples[""start_positions""].append(token_start_index - 1)\r\n                while offsets[token_end_index][1] >= end_char:\r\n                    token_end_index -= 1\r\n                tokenized_examples[""end_positions""].append(token_end_index + 1)\r\n\r\n    return tokenized_examples\r\n``` '
 '## I have fixed it, @lhoestq \r\n### the first section change as you said and add [""id""]\r\n```python\r\ndef process_squad(examples):\r\n    """"""\r\n    Process a dataset in the squad format with columns ""title"" and ""paragraphs""\r\n    to return the dataset with columns ""context"", ""question"" and ""answers"".\r\n    """"""\r\n    # print(examples)\r\n    out = {""context"": [], ""question"": [], ""answers"":[],""id"":[]} \r\n    for paragraphs in examples[""paragraphs""]: \r\n        for paragraph in paragraphs: \r\n            for qa in paragraph[""qas""]: \r\n                answers = [{""answer_start"": answer[""answer_start""], ""text"": answer[""text""].strip()} for answer in qa[""answers""]] \r\n                out[""context""].append(paragraph[""context""].strip()) \r\n                out[""question""].append(qa[""question""].strip()) \r\n                out[""answers""].append(answers) \r\n                out[""id""].append(qa[""id""]) \r\n    return out\r\ncolumn_names = datasets[""train""].column_names if training_args.do_train else datasets[""validation""].column_names\r\n# print(datasets[""train""].column_names)\r\nif set(column_names) == {""title"", ""paragraphs""}:\r\n    datasets = datasets.map(process_squad, batched=True, remove_columns=column_names)\r\n# Preprocessing the datasets.\r\n# Preprocessing is slighlty different for training and evaluation.\r\nif training_args.do_train:\r\n    column_names = datasets[""train""].column_names\r\nelse:\r\n    column_names = datasets[""validation""].column_names\r\n# print(column_names)\r\nquestion_column_name = ""question"" if ""question"" in column_names else column_names[0]\r\ncontext_column_name = ""context"" if ""context"" in column_names else column_names[1]\r\nanswer_column_name = ""answers"" if ""answers"" in column_names else column_names[2]\r\n```\r\n### the second section\r\n```python\r\ndef prepare_train_features(examples):\r\n    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\r\n    # in one example possible giving several features when a context is long, each of those features having a\r\n    # context that overlaps a bit the context of the previous feature.\r\n    tokenized_examples = tokenizer(\r\n        examples[question_column_name if pad_on_right else context_column_name],\r\n        examples[context_column_name if pad_on_right else question_column_name],\r\n        truncation=""only_second"" if pad_on_right else ""only_first"",\r\n        max_length=data_args.max_seq_length,\r\n        stride=data_args.doc_stride,\r\n        return_overflowing_tokens=True,\r\n        return_offsets_mapping=True,\r\n        padding=""max_length"" if data_args.pad_to_max_length else False,\r\n    )\r\n\r\n    # Since one example might give us several features if it has a long context, we need a map from a feature to\r\n    # its corresponding example. This key gives us just that.\r\n    sample_mapping = tokenized_examples.pop(""overflow_to_sample_mapping"")\r\n    # The offset mappings will give us a map from token to character position in the original context. This will\r\n    # help us compute the start_positions and end_positions.\r\n    offset_mapping = tokenized_examples.pop(""offset_mapping"")\r\n\r\n    # Let\'s label those examples!\r\n    tokenized_examples[""start_positions""] = []\r\n    tokenized_examples[""end_positions""] = []\r\n\r\n    for i, offsets in enumerate(offset_mapping):\r\n        # We will label impossible answers with the index of the CLS token.\r\n        input_ids = tokenized_examples[""input_ids""][i]\r\n        cls_index = input_ids.index(tokenizer.cls_token_id)\r\n\r\n        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\r\n        sequence_ids = tokenized_examples.sequence_ids(i)\r\n\r\n        # One example can give several spans, this is the index of the example containing this span of text.\r\n        sample_index = sample_mapping[i]\r\n        answers = examples[answer_column_name][sample_index]\r\n        # print(examples,answers,offset_mapping,tokenized_examples)\r\n        # If no answers are given, set the cls_index as answer.\r\n        if len(answers) == 0:#len(answers[""answer_start""]) == 0:\r\n            tokenized_examples[""start_positions""].append(cls_index)\r\n            tokenized_examples[""end_positions""].append(cls_index)\r\n        else:\r\n            # Start/end character index of the answer in the text.\r\n            start_char = answers[0][""answer_start""]\r\n            end_char = start_char + len(answers[0][""text""])\r\n\r\n            # Start token index of the current span in the text.\r\n            token_start_index = 0\r\n            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\r\n                token_start_index += 1\r\n\r\n            # End token index of the current span in the text.\r\n            token_end_index = len(input_ids) - 1\r\n            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\r\n                token_end_index -= 1\r\n\r\n            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\r\n            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\r\n                tokenized_examples[""start_positions""].append(cls_index)\r\n                tokenized_examples[""end_positions""].append(cls_index)\r\n            else:\r\n                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\r\n                # Note: we could go after the last offset if the answer is the last word (edge case).\r\n                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\r\n                    token_start_index += 1\r\n                tokenized_examples[""start_positions""].append(token_start_index - 1)\r\n                while offsets[token_end_index][1] >= end_char:\r\n                    token_end_index -= 1\r\n                tokenized_examples[""end_positions""].append(token_end_index + 1)\r\n    return tokenized_examples\r\n```'
 'I\'m glad you managed to fix run_qa.py for your case :)\r\n\r\nRegarding the checksum error, I\'m not able to reproduce on my side.\r\nThis errors says that the downloaded file doesn\'t match the expected file.\r\n\r\nCould you try running this and let me know if you get the same output as me ?\r\n```python\r\nfrom datasets.utils.info_utils import get_size_checksum_dict\r\nfrom datasets import cached_path\r\n\r\nget_size_checksum_dict(cached_path(""https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json""))\r\n# {\'num_bytes\': 30288272, \'checksum\': \'3527663986b8295af4f7fcdff1ba1ff3f72d07d61a20f487cb238a6ef92fd955\'}\r\n```'
 'I run the code,and it show below:\r\n```\r\n>>> from datasets.utils.info_utils import get_size_checksum_dict\r\n>>> from datasets import cached_path\r\n>>> get_size_checksum_dict(cached_path(""https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json""))\r\nDownloading: 30.3MB [04:13, 120kB/s]\r\n{\'num_bytes\': 30288272, \'checksum\': \'3527663986b8295af4f7fcdff1ba1ff3f72d07d61a20f487cb238a6ef92fd955\'}\r\n```'
 'Alright ! So in this case redownloading the file with `download_mode=""force_redownload""` should fix it. Can you try using `download_mode=""force_redownload""` again ?\r\n\r\nNot sure why it didn\'t work for you the first time though :/']","### 1 When I try to train lxmert,and follow the code in README that --dataset name:
```shell 
python examples/question-answering/run_qa.py --model_name_or_path unc-nlp/lxmert-base-uncased --dataset_name squad --do_train --do_eval --per_device_train_batch_size 12 --learning_rate 3e-5 --num_train_epochs 2 --max_seq_length 384 --doc_stride 128 --output_dir /home2/zhenggo1/checkpoint/lxmert_squad
```
the bug is that:
```
Downloading and preparing dataset squad/plain_text (download: 33.51 MiB, generated: 85.75 MiB, post-processed: Unknown size, total: 119.27 MiB) to /home2/zhenggo1/.cache/huggingface/datasets/squad/plain_text/1.0.0/4c81550d83a2ac7c7ce23783bd8ff36642800e6633c1f18417fb58c3ff50cdd7...
Traceback (most recent call last):
  File ""examples/question-answering/run_qa.py"", line 501, in <module>
    main()
  File ""examples/question-answering/run_qa.py"", line 217, in main
    datasets = load_dataset(data_args.dataset_name, data_args.dataset_config_name)
  File ""/home2/zhenggo1/anaconda3/envs/lpot/lib/python3.7/site-packages/datasets/load.py"", line 746, in load_dataset
    use_auth_token=use_auth_token,
  File ""/home2/zhenggo1/anaconda3/envs/lpot/lib/python3.7/site-packages/datasets/builder.py"", line 573, in download_and_prepare
    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
  File ""/home2/zhenggo1/anaconda3/envs/lpot/lib/python3.7/site-packages/datasets/builder.py"", line 633, in _download_and_prepare
    self.info.download_checksums, dl_manager.get_recorded_sizes_checksums(), ""dataset source files""
  File ""/home2/zhenggo1/anaconda3/envs/lpot/lib/python3.7/site-packages/datasets/utils/info_utils.py"", line 39, in verify_checksums
    raise NonMatchingChecksumError(error_msg + str(bad_urls))
datasets.utils.info_utils.NonMatchingChecksumError: Checksums didn't match for dataset source files:
['https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json']
```
And I try to find the [checksum link](https://github.com/huggingface/datasets/blob/master/datasets/squad/dataset_infos.json)
,is the problem plain_text do not have a checksum?

### 2 When I try to train lxmert,and use local dataset:
```
python examples/question-answering/run_qa.py --model_name_or_path unc-nlp/lxmert-base-uncased --train_file $SQUAD_DIR/train-v1.1.json --validation_file $SQUAD_DIR/dev-v1.1.json --do_train --do_eval --per_device_train_batch_size 12 --learning_rate 3e-5 --num_train_epochs 2 --max_seq_length 384 --doc_stride 128 --output_dir /home2/zhenggo1/checkpoint/lxmert_squad
```
The bug is that 
```
['title', 'paragraphs']
Traceback (most recent call last):
  File ""examples/question-answering/run_qa.py"", line 501, in <module>
    main()
  File ""examples/question-answering/run_qa.py"", line 273, in main
    answer_column_name = ""answers"" if ""answers"" in column_names else column_names[2]
IndexError: list index out of range
```
I print the answer_column_name and find that local squad dataset need the package datasets to preprocessing so that the code below can work:
```
if training_args.do_train:
        column_names = datasets[""train""].column_names
    else:
        column_names = datasets[""validation""].column_names
    print(datasets[""train""].column_names)
    question_column_name = ""question"" if ""question"" in column_names else column_names[0]
    context_column_name = ""context"" if ""context"" in column_names else column_names[1]
    answer_column_name = ""answers"" if ""answers"" in column_names else column_names[2]
``` 
## Please tell me how to fix the bug,thks a lot!"
https://github.com/huggingface/datasets/issues/1963,bug in SNLI dataset ,"['Hi ! The labels -1 correspond to the examples without gold labels in the original snli dataset.\r\nFeel free to remove these examples if you don\'t need them by using\r\n```python\r\ndata = data.filter(lambda x: x[""label""] != -1)\r\n```']","Hi
There is label of -1 in train set of SNLI dataset, please find the code below:

```
import numpy as np 
import datasets 
data = datasets.load_dataset(""snli"")[""train""]
labels = []
for d in data:
   labels.append(d[""label""])
print(np.unique(labels))
```

and results:

`[-1  0  1  2]`

version of datasets used:
`datasets                  1.2.1                     <pip>
`

thanks for your help. @lhoestq "
https://github.com/huggingface/datasets/issues/1959,Bug in skip_rows argument of load_dataset function ?,"[""Hi,\r\n\r\ntry `skiprows` instead. This part is not properly documented in the docs it seems.\r\n\r\n@lhoestq I'll fix this as part of a bigger PR that fixes typos in the docs.""]","Hello everyone,

I'm quite new to Git so sorry in advance if I'm breaking some ground rules of issues posting... :/
I tried to use the load_dataset function, from Huggingface datasets library, on a csv file using the skip_rows argument described on Huggingface page to skip the first row containing column names

`test_dataset = load_dataset('csv', data_files=['test_wLabel.tsv'], delimiter='\t', column_names=[""id"", ""sentence"", ""label""], skip_rows=1)`

But I got the following error message

`__init__() got an unexpected keyword argument 'skip_rows'`

Have I used the wrong argument ? Am I missing something or is this a bug ?

Thank you very much for your time,
Best regards,
Arthur"
https://github.com/huggingface/datasets/issues/1958,XSum dataset download link broken,"['Never mind, I ran it again and it worked this time. Strange.']","I did 
```
from datasets import load_dataset

dataset = load_dataset(""xsum"")
```

This returns
`ConnectionError: Couldn't reach http://bollin.inf.ed.ac.uk/public/direct/XSUM-EMNLP18-Summary-Data-Original.tar.gz`"
https://github.com/huggingface/datasets/issues/1956,[distributed env] potentially unsafe parallel execution,"['You can pass the same `experiment_id` for all the metrics of the same group, and use another `experiment_id` for the other groups.\r\nMaybe we can add an environment variable that sets the default value for `experiment_id` ? What do you think ?'
 ""Ah, you're absolutely correct, @lhoestq - it's exactly the equivalent of the shared secret. Thank you!""]","```
metric = load_metric('glue', 'mrpc', num_process=num_process, process_id=rank)
```

presumes that there is only one set of parallel processes running - and will intermittently fail if you have multiple sets running as they will surely overwrite each other. Similar to https://github.com/huggingface/datasets/issues/1942 (but for a different reason).
That's why dist environments use some unique to a group identifier so that each group is dealt with separately. 

e.g. the env-way of pytorch dist syncing is done with a unique per set `MASTER_ADDRESS+MASTER_PORT`

So ideally this interface should ask for a shared secret to do the right thing.

I'm not reporting an immediate need, but am only flagging that this will hit someone down the road.

This problem can be remedied by adding a new optional `shared_secret` option, which can then be used to differentiate different groups of processes. and this secret should be part of the file lock name and the experiment.

Thank you"
https://github.com/huggingface/datasets/issues/1954,add a new column ,"['Hi\r\nnot sure how change the lable after creation, but this is an issue not dataset request. thanks '
 ""Hi ! Currently you have to use `map` . You can see an example of how to do it in this comment: https://github.com/huggingface/datasets/issues/853#issuecomment-727872188\r\n\r\nIn the future we'll add support for a more native way of adding a new column ;)""]","Hi
I'd need to add a new column to the dataset, I was wondering how this can be done? thanks 
@lhoestq "
https://github.com/huggingface/datasets/issues/1949,Enable Fast Filtering using Arrow Dataset,"['Hi @gchhablani :)\r\nThanks for proposing your help !\r\n\r\nI\'ll be doing a refactor of some parts related to filtering in the scope of https://github.com/huggingface/datasets/issues/1877\r\nSo I would first wait for this refactor to be done before working on the filtering. In particular because I plan to make things simpler to manipulate.\r\n\r\nYour feedback on this refactor would also be appreciated since it also aims at making the core code more accessible (basically my goal is that no one\'s ever ""having troubles getting started"" ^^)\r\n\r\nThis will be available in a few days, I will be able to give you more details at that time if you don\'t mind waiting a bit !'
 ""Sure! I don't mind waiting. I'll check the refactor and try to understand what you're trying to do :)""]","Hi @lhoestq,

As mentioned in Issue #1796, I would love to work on enabling fast filtering/mapping. Can you please share the expectations? It would be great if you could point me to the relevant methods/files involved. Or the docs or maybe an overview of `arrow_dataset.py`. I only ask this because I am having trouble getting started ;-;

Any help would be appreciated.

Thanks,
Gunjan"
https://github.com/huggingface/datasets/issues/1948,dataset loading logger level,"[""These warnings are showed when there's a call to `.map` to say to the user that a dataset is reloaded from the cache instead of being recomputed.\r\nThey are warnings since we want to make sure the users know that it's not recomputed.""
 ""Thank you for explaining the intention, @lhoestq \r\n\r\n1. Could it be then made more human-friendly? Currently the hex gibberish tells me nothing of what's really going on. e.g. the following is instructive, IMHO:\r\n\r\n```\r\nWARNING: wmt16/ro-en/train dataset was loaded from cache instead of being recomputed\r\nWARNING: wmt16/ro-en/validation dataset was loaded from cache instead of being recomputed\r\nWARNING: wmt16/ro-en/test dataset was loaded from cache instead of being recomputed\r\n```\r\nnote that it removes the not so useful hex info and tells the user instead which split it's referring to - but probably no harm in keeping the path if it helps the debug. But the key is that now the warning is telling me what it is it's warning me about.\r\n```\r\nWarning:Loading cache path\r\n```\r\non the other hand isn't telling what it is warning about.\r\n\r\nAnd I still suggest this is INFO level, otherwise you need to turn all 'using cache' statements to WARNING to be consistent. The user is most likely well aware the cache is used for models, etc. So this feels very similar.\r\n\r\n2. Should there be a way for a user to void warranty by having a flag - `I know I'm expecting the cached version to load if it's available - please do not warn me about it=True`\r\n\r\nTo explain the need: Warnings are a problem, they constantly take attention away because they could be the harbinger of a problem. Therefore I prefer not to have any warnings in the log, and if I get any I usually try to deal with those so that my log is clean. \r\n\r\nIt's less of an issue for somebody doing long runs. It's a huge issue for someone who does a new run every few minutes and on the lookout for any potential problems which is what I have been doing a lot of integrating DeepSpeed and other things. And since there are already problems to deal with during the integration it's nice to have a clean log to start with. \r\n\r\nI hope my need is not unreasonable and I was able to explain it adequately. \r\n\r\nThank you.""]","on master I get this with `--dataset_name wmt16 --dataset_config ro-en`:

```
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/stas/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/9dc00622c30446e99c4c63d12a484ea4fb653f2f37c867d6edcec839d7eae50f/cache-2e01bead8cf42e26.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/stas/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/9dc00622c30446e99c4c63d12a484ea4fb653f2f37c867d6edcec839d7eae50f/cache-ac3bebaf4f91f776.arrow
WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/stas/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/9dc00622c30446e99c4c63d12a484ea4fb653f2f37c867d6edcec839d7eae50f/cache-810c3e61259d73a9.arrow
```

why are those WARNINGs? Should be INFO, no?

warnings should only be used when a user needs to pay attention to something, this is just informative - I'd even say it should be DEBUG, but definitely not WARNING.

Thank you.
"
https://github.com/huggingface/datasets/issues/1945,AttributeError: 'DatasetDict' object has no attribute 'concatenate_datasets',"['sorry my mistake, datasets were overwritten closing now, thanks a lot']","Hi
I am trying to concatenate a list of huggingface datastes as:

` train_dataset = datasets.concatenate_datasets(train_datasets)
`
Here is the `train_datasets` when I print:

```
[Dataset({
    features: ['attention_mask', 'idx', 'input_ids', 'label', 'question1', 'question2', 'token_type_ids'],
    num_rows: 120361
}), Dataset({
    features: ['attention_mask', 'idx', 'input_ids', 'label', 'question1', 'question2', 'token_type_ids'],
    num_rows: 2670
}), Dataset({
    features: ['attention_mask', 'idx', 'input_ids', 'label', 'question1', 'question2', 'token_type_ids'],
    num_rows: 6944
}), Dataset({
    features: ['attention_mask', 'idx', 'input_ids', 'label', 'question1', 'question2', 'token_type_ids'],
    num_rows: 38140
}), Dataset({
    features: ['attention_mask', 'idx', 'input_ids', 'label', 'question1', 'question2', 'token_type_ids'],
    num_rows: 173711
}), Dataset({
    features: ['attention_mask', 'idx', 'input_ids', 'label', 'question1', 'question2', 'token_type_ids'],
    num_rows: 1655
}), Dataset({
    features: ['attention_mask', 'idx', 'input_ids', 'label', 'question1', 'question2', 'token_type_ids'],
    num_rows: 4274
}), Dataset({
    features: ['attention_mask', 'idx', 'input_ids', 'label', 'question1', 'question2', 'token_type_ids'],
    num_rows: 2019
}), Dataset({
    features: ['attention_mask', 'idx', 'input_ids', 'label', 'question1', 'question2', 'token_type_ids'],
    num_rows: 2109
}), Dataset({
    features: ['attention_mask', 'idx', 'input_ids', 'label', 'question1', 'question2', 'token_type_ids'],
    num_rows: 11963
})]
```

I am getting the following error:

`AttributeError: 'DatasetDict' object has no attribute 'concatenate_datasets'
`

I was wondering if you could help me with this issue, thanks a lot "
https://github.com/huggingface/datasets/issues/1942,[experiment] missing default_experiment-1-0.arrow,"[""Hi !\r\n\r\nThe cache at `~/.cache/huggingface/metrics` stores the users data for metrics computations (hence the arrow files).\r\n\r\nHowever python modules (i.e. dataset scripts, metric scripts) are stored in `~/.cache/huggingface/modules/datasets_modules`.\r\n\r\nIn particular the metrics are cached in `~/.cache/huggingface/modules/datasets_modules/metrics/`\r\n\r\nFeel free to take a look at your cache and let me know if you find any issue that would help explaining why you had an issue with `rouge` with no connection. I'm doing some tests on my side to try to reproduce the issue you have\r\n""
 ""Thank you for clarifying that the metrics files are to be found elsewhere, @lhoestq \r\n\r\n> The cache at ~/.cache/huggingface/metrics stores the users data for metrics computations (hence the arrow files).\r\n\r\ncould it be renamed to reflect that? otherwise it misleadingly suggests that it's the metrics. Perhaps `~/.cache/huggingface/metrics-user-data`?\r\n\r\nAnd there are so many `.lock` files w/o corresponding files under `~/.cache/huggingface/metrics/`. Why are they there? \r\n\r\nfor example after I wipe out the dir completely and do one training I end up with:\r\n```\r\n~/.cache/huggingface/metrics/sacrebleu/default/default_experiment-1-0.arrow.lock\r\n```\r\nwhat is that lock file locking when nothing is running?""
 ""The lock files come from an issue with filelock (see comment in the code [here](https://github.com/benediktschmitt/py-filelock/blob/master/filelock.py#L394-L398)). Basically on unix there're always .lock files left behind. I haven't dove into this issue""
 'are you sure you need an external lock file? if it\'s a single purpose locking in the same scope you can lock the caller `__file__` instead, e.g. here is how one can `flock` the script file itself to ensure atomic printing:\r\n\r\n```\r\nimport fcntl\r\ndef printflock(*msgs):\r\n    """""" print in multiprocess env so that the outputs from different processes don\'t get interleaved """"""\r\n    with open(__file__, ""r"") as fh:\r\n        fcntl.flock(fh, fcntl.LOCK_EX)\r\n        try:\r\n            print(*msgs)\r\n        finally:\r\n            fcntl.flock(fh, fcntl.LOCK_UN)\r\n```\r\n'
 'OK, this issue is not about caching but some internal conflict/race condition it seems, I have just run into it on my normal env:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File ""/mnt/nvme1/code/huggingface/datasets-master/src/datasets/metric.py"", line 356, in _finalize\r\n    self.data = Dataset(**reader.read_files([{""filename"": f} for f in file_paths]))\r\n  File ""/mnt/nvme1/code/huggingface/datasets-master/src/datasets/arrow_reader.py"", line 236, in read_files\r\n    pa_table = self._read_files(files, in_memory=in_memory)\r\n  File ""/mnt/nvme1/code/huggingface/datasets-master/src/datasets/arrow_reader.py"", line 171, in _read_files\r\n    pa_table: pa.Table = self._get_dataset_from_filename(f_dict, in_memory=in_memory)\r\n  File ""/mnt/nvme1/code/huggingface/datasets-master/src/datasets/arrow_reader.py"", line 302, in _get_dataset_from_filename\r\n    pa_table = ArrowReader.read_table(filename, in_memory=in_memory)\r\n  File ""/mnt/nvme1/code/huggingface/datasets-master/src/datasets/arrow_reader.py"", line 322, in read_table\r\n    stream = stream_from(filename)\r\n  File ""pyarrow/io.pxi"", line 782, in pyarrow.lib.memory_map\r\n  File ""pyarrow/io.pxi"", line 743, in pyarrow.lib.MemoryMappedFile._open\r\n  File ""pyarrow/error.pxi"", line 122, in pyarrow.lib.pyarrow_internal_check_status\r\n  File ""pyarrow/error.pxi"", line 97, in pyarrow.lib.check_status\r\nFileNotFoundError: [Errno 2] Failed to open local file \'/home/stas/.cache/huggingface/metrics/sacrebleu/default/default_experiment-1-0.arrow\'. Detail: [errno 2] No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File ""examples/seq2seq/run_seq2seq.py"", line 655, in <module>\r\n    main()\r\n  File ""examples/seq2seq/run_seq2seq.py"", line 619, in main\r\n    test_results = trainer.predict(\r\n  File ""/mnt/nvme1/code/huggingface/transformers-master/src/transformers/trainer_seq2seq.py"", line 121, in predict\r\n    return super().predict(test_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)\r\n  File ""/mnt/nvme1/code/huggingface/transformers-master/src/transformers/trainer.py"", line 1706, in predict\r\n    output = self.prediction_loop(\r\n  File ""/mnt/nvme1/code/huggingface/transformers-master/src/transformers/trainer.py"", line 1813, in prediction_loop\r\n    metrics = self.compute_metrics(EvalPrediction(predictions=preds, label_ids=label_ids))\r\n  File ""examples/seq2seq/run_seq2seq.py"", line 556, in compute_metrics\r\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\r\n  File ""/mnt/nvme1/code/huggingface/datasets-master/src/datasets/metric.py"", line 388, in compute\r\n    self._finalize()\r\n  File ""/mnt/nvme1/code/huggingface/datasets-master/src/datasets/metric.py"", line 358, in _finalize\r\n    raise ValueError(\r\nValueError: Error in finalize: another metric instance is already using the local cache file. Please specify an experiment_id to avoid colision between distributed metric instances.\r\n```\r\n\r\nI\'m just running `run_seq2seq.py` under DeepSpeed:\r\n\r\n```\r\nexport BS=16; rm -r output_dir; PYTHONPATH=src USE_TF=0 CUDA_VISIBLE_DEVICES=0,1 deepspeed --num_gpus=2 examples/seq2seq/run_seq2seq.py --model_name_or_path t5-small --output_dir output_dir --adam_eps 1e-06 --do_eval --do_train --do_predict --evaluation_strategy=steps  --label_smoothing 0.1 --learning_rate 3e-5 --logging_first_step --logging_steps 1000 --max_source_length 128 --max_target_length 128 --num_train_epochs 1 --overwrite_output_dir --per_device_eval_batch_size $BS --per_device_train_batch_size $BS --predict_with_generate --eval_steps 25000  --sortish_sampler --task translation_en_to_ro  --val_max_target_length 128 --warmup_steps 500 --max_train_samples 100 --max_val_samples 100 --max_test_samples 100 --dataset_name wmt16 --dataset_config ro-en  --source_prefix ""translate English to Romanian: "" --deepspeed examples/tests/deepspeed/ds_config.json\r\n```\r\n\r\nIt finished the evaluation OK and crashed on the prediction part of the Trainer. But the eval / predict parts no longer run under Deepspeed, it\'s just plain ddp.\r\n\r\nIs this some kind of race condition? It happens intermittently - there is nothing else running at the same time.\r\n\r\nBut if 2 independent instances  of the same script were to run at the same time it\'s clear to see that this problem would happen. Perhaps it\'d help to create a unique hash which is shared between all processes in the group and use that as the default experiment id?\r\n'
 ""When you're using metrics in a distributed setup, there are two cases:\r\n1. you're doing two completely different experiments (two evaluations) and the 2 metrics jobs have nothing to do with each other\r\n2. you're doing one experiment (one evaluation) but use multiple processes to feed the data to the metric.\r\n\r\nIn case 1. you just need to provide two different `experiment_id` so that the metrics don't collide.\r\nIn case 2. they must have the same experiment_id (or use the default one), but in this case you also need to provide the `num_processes` and `process_id`\r\n\r\nIf understand correctly you're in situation 2.\r\n\r\nIf so, you make sure that you instantiate the metrics with both the right `num_processes` and `process_id` parameters ?\r\n\r\nIf they're not set, then the cache files of the two metrics collide it can cause issues. For example if one metric finishes before the other, then the cache file is deleted and the other metric gets a FileNotFoundError\r\nThere's more information in the [documentation](https://huggingface.co/docs/datasets/loading_metrics.html#distributed-setups) if you want\r\n\r\nHope that helps !""
 ""Thank you for explaining that in a great way, @lhoestq \r\n\r\nSo the bottom line is that the `transformers` examples are broken since they don't do any of that. At least `run_seq2seq.py` just does `metric = load_metric(metric_name)`\r\n\r\nWhat test would you recommend to reliably reproduce this bug in `examples/seq2seq/run_seq2seq.py`?""
 ""To give more context, we are just using the metrics for the `comput_metric` function and nothing else. Is there something else we can use that just applies the function to the full arrays of predictions and labels? Because that's all we need, all the gathering has already been done because the datasets Metric multiprocessing relies on file storage and thus does not work in a multi-node distributed setup (whereas the Trainer does).\r\n\r\nOtherwise, we'll have to switch to something else to compute the metrics :-(""
 'OK, it definitely leads to a race condition in how it\'s used right now. Here is how you can reproduce it - by injecting a random  sleep time different for each process before the locks are acquired. \r\n```\r\n--- a/src/datasets/metric.py\r\n+++ b/src/datasets/metric.py\r\n@@ -348,6 +348,16 @@ class Metric(MetricInfoMixin):\r\n\r\n         elif self.process_id == 0:\r\n             # Let\'s acquire a lock on each node files to be sure they are finished writing\r\n+\r\n+            import time\r\n+            import random\r\n+            import os\r\n+            pid = os.getpid()\r\n+            random.seed(pid)\r\n+            secs = random.randint(1, 15)\r\n+            time.sleep(secs)\r\n+            print(f""sleeping {secs}"")\r\n+\r\n             file_paths, filelocks = self._get_all_cache_files()\r\n\r\n             # Read the predictions and references\r\n@@ -385,7 +395,10 @@ class Metric(MetricInfoMixin):\r\n\r\n         if predictions is not None:\r\n             self.add_batch(predictions=predictions, references=references)\r\n+        print(""FINALIZE START"")\r\n+\r\n         self._finalize()\r\n+        print(""FINALIZE END"")\r\n\r\n         self.cache_file_name = None\r\n         self.filelock = None\r\n```\r\n\r\nthen run with 2 procs: `python -m torch.distributed.launch --nproc_per_node=2`\r\n```\r\nexport BS=16; rm -r output_dir; PYTHONPATH=src USE_TF=0 CUDA_VISIBLE_DEVICES=0,1 python -m torch.distributed.launch --nproc_per_node=2 examples/seq2seq/run_seq2seq.py --model_name_or_path t5-small --output_dir output_dir --adam_eps 1e-06 --do_eval --do_train --do_predict --evaluation_strategy=steps  --label_smoothing 0.1 --learning_rate 3e-5 --logging_first_step --logging_steps 1000 --max_source_length 128 --max_target_length 128 --num_train_epochs 1 --overwrite_output_dir --per_device_eval_batch_size $BS --per_device_train_batch_size $BS --predict_with_generate --eval_steps 25000  --sortish_sampler --task translation_en_to_ro  --val_max_target_length 128 --warmup_steps 500 --max_train_samples 10 --max_val_samples 10 --max_test_samples 10  --dataset_name wmt16 --dataset_config ro-en --source_prefix ""translate English to Romanian: ""\r\n```\r\n\r\n```\r\n***** Running Evaluation *****\r\n  Num examples = 10\r\n  Batch size = 16\r\n  0%|                                                                                                                                      | 0/1 [00:00<?, ?it/s]FINALIZE START\r\nFINALIZE START\r\nsleeping 11\r\nFINALIZE END\r\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:11<00:00, 11.06s/it]\r\nsleeping 11\r\nTraceback (most recent call last):\r\n  File ""/mnt/nvme1/code/huggingface/datasets-master/src/datasets/metric.py"", line 368, in _finalize\r\n    self.data = Dataset(**reader.read_files([{""filename"": f} for f in file_paths]))\r\n  File ""/mnt/nvme1/code/huggingface/datasets-master/src/datasets/arrow_reader.py"", line 236, in read_files\r\n    pa_table = self._read_files(files, in_memory=in_memory)\r\n  File ""/mnt/nvme1/code/huggingface/datasets-master/src/datasets/arrow_reader.py"", line 171, in _read_files\r\n    pa_table: pa.Table = self._get_dataset_from_filename(f_dict, in_memory=in_memory)\r\n  File ""/mnt/nvme1/code/huggingface/datasets-master/src/datasets/arrow_reader.py"", line 302, in _get_dataset_from_filename\r\n    pa_table = ArrowReader.read_table(filename, in_memory=in_memory)\r\n  File ""/mnt/nvme1/code/huggingface/datasets-master/src/datasets/arrow_reader.py"", line 322, in read_table\r\n    stream = stream_from(filename)\r\n  File ""pyarrow/io.pxi"", line 782, in pyarrow.lib.memory_map\r\n  File ""pyarrow/io.pxi"", line 743, in pyarrow.lib.MemoryMappedFile._open\r\n  File ""pyarrow/error.pxi"", line 122, in pyarrow.lib.pyarrow_internal_check_status\r\n  File ""pyarrow/error.pxi"", line 97, in pyarrow.lib.check_status\r\nFileNotFoundError: [Errno 2] Failed to open local file \'/home/stas/.cache/huggingface/metrics/sacrebleu/default/default_experiment-1-0.arrow\'. Detail: [errno 2] No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File ""examples/seq2seq/run_seq2seq.py"", line 645, in <module>\r\n    main()\r\n  File ""examples/seq2seq/run_seq2seq.py"", line 601, in main\r\n    metrics = trainer.evaluate(\r\n  File ""/mnt/nvme1/code/huggingface/transformers-mp-pp/src/transformers/trainer_seq2seq.py"", line 74, in evaluate\r\n    return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)\r\n  File ""/mnt/nvme1/code/huggingface/transformers-mp-pp/src/transformers/trainer.py"", line 1703, in evaluate\r\n    output = self.prediction_loop(\r\n  File ""/mnt/nvme1/code/huggingface/transformers-mp-pp/src/transformers/trainer.py"", line 1876, in prediction_loop\r\n    metrics = self.compute_metrics(EvalPrediction(predictions=preds, label_ids=label_ids))\r\n  File ""examples/seq2seq/run_seq2seq.py"", line 556, in compute_metrics\r\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\r\n  File ""/mnt/nvme1/code/huggingface/datasets-master/src/datasets/metric.py"", line 402, in compute\r\n    self._finalize()\r\n  File ""/mnt/nvme1/code/huggingface/datasets-master/src/datasets/metric.py"", line 370, in _finalize\r\n    raise ValueError(\r\nValueError: Error in finalize: another metric instance is already using the local cache file. Please specify an experiment_id to avoid colision between distributed metric instances.\r\n```'
 'I tried to adjust `run_seq2seq.py` and trainer to use the suggested dist env:\r\n```\r\n    import torch.distributed as dist\r\n    metric = load_metric(metric_name, num_process=dist.get_world_size(), process_id=dist.get_rank())\r\n```\r\nand in `trainer.py` added to call just for rank 0:\r\n```\r\n        if self.is_world_process_zero() and self.compute_metrics is not None and preds is not None and label_ids is not None:\r\n            metrics = self.compute_metrics(EvalPrediction(predictions=preds, label_ids=label_ids))\r\n```\r\nand then the process hangs in a deadlock. \r\n\r\nHere is the tb:\r\n```\r\n  File ""/mnt/nvme1/code/huggingface/datasets-master/src/datasets/utils/filelock.py"", line 275 in acquire\r\n  File ""/mnt/nvme1/code/huggingface/datasets-master/src/datasets/metric.py"", line 306 in _check_all_processes_locks\r\n  File ""/mnt/nvme1/code/huggingface/datasets-master/src/datasets/metric.py"", line 501 in _init_writer\r\n  File ""/mnt/nvme1/code/huggingface/datasets-master/src/datasets/metric.py"", line 440 in add_batch\r\n  File ""/mnt/nvme1/code/huggingface/datasets-master/src/datasets/metric.py"", line 397 in compute\r\n  File ""examples/seq2seq/run_seq2seq.py"", line 558 in compute_metrics\r\n  File ""/mnt/nvme1/code/huggingface/transformers-mp-pp/src/transformers/trainer.py"", line 1876 in prediction_loop\r\n  File ""/mnt/nvme1/code/huggingface/transformers-mp-pp/src/transformers/trainer.py"", line 1703 in evaluate\r\n  File ""/mnt/nvme1/code/huggingface/transformers-mp-pp/src/transformers/trainer_seq2seq.py"", line 74 in evaluate\r\n  File ""examples/seq2seq/run_seq2seq.py"", line 603 in main\r\n  File ""examples/seq2seq/run_seq2seq.py"", line 651 in <module>\r\n```\r\n\r\nBut this sounds right, since in the above diff I set up a distributed metric and only called one process - so it\'s blocking on waiting for other processes to do the same.\r\n\r\nSo one working solution is to leave:\r\n\r\n```\r\n    metric = load_metric(metric_name)\r\n```\r\nalone, and only call `compute_metrics` from rank 0\r\n```\r\n        if self.is_world_process_zero() and self.compute_metrics is not None and preds is not None and label_ids is not None:\r\n            metrics = self.compute_metrics(EvalPrediction(predictions=preds, label_ids=label_ids))\r\n```\r\n\r\nso we now no longer use the distributed env as far as `datasets` is concerned, it\'s just a single process.\r\n\r\nAre there any repercussions/side-effects to this proposed change in Trainer?  If it always gathers all inputs on rank 0 then this is how it should have been done in first place - i.e. only run for rank 0. It appears that currently it was re-calculating the metrics on all processes on the same data just to throw the results away other than for rank 0. Unless I missed something.\r\n'
 ""But no, since \r\n`\r\n metric = load_metric(metric_name)\r\n`\r\nis called for each process, the race condition is still there. So still getting:\r\n\r\n```\r\nValueError: Error in finalize: another metric instance is already using the local cache file. Please specify an experiment_id to avoid colision between distributed metric instances.\r\n```\r\n\r\ni.e. the only way to fix this is to `load_metric` only for rank 0, but this requires huge changes in the code and all end users' code.\r\n""
 'OK, here is a workaround that works. The onus here is absolutely on the user:\r\n\r\n```\r\ndiff --git a/examples/seq2seq/run_seq2seq.py b/examples/seq2seq/run_seq2seq.py\r\nindex 2a060dac5..c82fd83ea 100755\r\n--- a/examples/seq2seq/run_seq2seq.py\r\n+++ b/examples/seq2seq/run_seq2seq.py\r\n@@ -520,7 +520,11 @@ def main():\r\n\r\n     # Metric\r\n     metric_name = ""rouge"" if data_args.task.startswith(""summarization"") else ""sacrebleu""\r\n-    metric = load_metric(metric_name)\r\n+    import torch.distributed as dist\r\n+    if dist.is_initialized():\r\n+        metric = load_metric(metric_name, num_process=dist.get_world_size(), process_id=dist.get_rank())\r\n+    else:\r\n+        metric = load_metric(metric_name)\r\n\r\n     def postprocess_text(preds, labels):\r\n         preds = [pred.strip() for pred in preds]\r\n@@ -548,12 +552,17 @@ def main():\r\n         # Some simple post-processing\r\n         decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\r\n\r\n+        kwargs = dict(predictions=decoded_preds, references=decoded_labels)\r\n+        if metric_name == ""rouge"":\r\n+            kwargs.update(use_stemmer=True)\r\n+        result = metric.compute(**kwargs) # must call for all processes\r\n+        if result is None: # only process with rank-0 will return metrics, others None\r\n+            return {}\r\n+\r\n         if metric_name == ""rouge"":\r\n-            result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\r\n             # Extract a few results from ROUGE\r\n             result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\r\n         else:\r\n-            result = metric.compute(predictions=decoded_preds, references=decoded_labels)\r\n             result = {""bleu"": result[""score""]}\r\n\r\n         prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\r\n```\r\n\r\nThis is not user-friendly to say the least. And it\'s still wasteful as we don\'t need other processes to do anything.\r\n\r\nBut it solves the current race condition.\r\n\r\nClearly this calls for a design discussion as it\'s the responsibility of the Trainer to handle this and not user\'s. Perhaps in the `transformers` land?'
 ""I don't see how this could be the responsibility of `Trainer`, who hasn't the faintest idea of what a `datasets.Metric` is. The trainer takes a function `compute_metrics` that goes from predictions + labels to metric results, there is nothing there. That computation is done on all processes  \r\n\r\nThe fact a `datasets.Metric` object cannot be used as a simple compute function in a multi-process environment is, in my opinion, a bug in `datasets`. Especially since, as I mentioned before, the multiprocessing part of `datasets.Metric` has a deep flaw since it can't work in a multinode environment. So you actually need to do the job of gather predictions and labels yourself.\r\n\r\nThe changes you are proposing Stas are making the code less readable and also concatenate all the predictions and labels `number_of_processes` times I believe, which is not going to make the metric computation any faster.\r\n\r\n""
 ""Right, to clarify, I meant it'd be good to have it sorted on the library side and not requiring the user to figure it out. This is too complex and error-prone and if not coded correctly the bug will be intermittent which is even worse.\r\n\r\nOh I guess I wasn't clear in my message - in no way I'm proposing that we use this workaround code - I was just showing what I had to do to make it work.\r\n\r\nWe are on the same page.\r\n\r\n> The changes you are proposing Stas are making the code less readable and also concatenate all the predictions and labels number_of_processes times I believe, which is not going to make the metric computation any faster.\r\n\r\nAnd yes, this is another problem that my workaround introduces. Thank you for pointing it out, @sgugger \r\n""
 ""> The fact a datasets.Metric object cannot be used as a simple compute function in a multi-process environment is, in my opinion, a bug in datasets\r\n\r\nYes totally, this use case is supposed to be supported by `datasets`. And in this case there shouldn't be any collision between the metrics. I'm looking into it :)\r\nMy guess is that at one point the metric isn't using the right file name. It's supposed to use one with a unique uuid in order to avoid the collisions.""
 'I just opened #1966 to fix this :)\r\n@stas00 if have a chance feel free to try it !'
 'Thank you, @lhoestq - I will experiment and report back. \r\n\r\nedit: It works! Thank you']","the original report was pretty bad and incomplete - my apologies!

Please see the complete version here: https://github.com/huggingface/datasets/issues/1942#issuecomment-786336481

------------

As mentioned here https://github.com/huggingface/datasets/issues/1939 metrics don't get cached, looking at my local `~/.cache/huggingface/metrics` - there are many `*.arrow.lock` files but zero metrics files.

w/o the network I get:
```
FileNotFoundError: [Errno 2] No such file or directory: '~/.cache/huggingface/metrics/sacrebleu/default/default_experiment-1-0.arrow
```
there is just `~/.cache/huggingface/metrics/sacrebleu/default/default_experiment-1-0.arrow.lock`

I did run the same `run_seq2seq.py` script on the instance with network and it worked just fine, but only the lock file was left behind.

this is with master.

Thank you."
https://github.com/huggingface/datasets/issues/1941,Loading of FAISS index fails for index_name = 'exact',"[""Thanks for reporting ! I'm taking a look""
 'Index training was missing, I fixed it here: https://github.com/huggingface/datasets/commit/f5986c46323583989f6ed1dabaf267854424a521\r\n\r\nCan you try again please ?'
 'Works great 👍 I just put a minor comment on the commit, I think you meant to pass the `train_size` from the one obtained from the config.\r\n\r\nThanks for a quick response!']","Hi,

It looks like loading of FAISS index now fails when using index_name = 'exact'.

For example, from the RAG [model card](https://huggingface.co/facebook/rag-token-nq?fbclid=IwAR3bTfhls5U_t9DqsX2Vzb7NhtRHxJxfQ-uwFT7VuCPMZUM2AdAlKF_qkI8#usage).

Running `transformers==4.3.2` and datasets installed from source on latest `master` branch.

```bash
(venv) sergey_mkrtchyan datasets (master) $ python
Python 3.8.6 (v3.8.6:db455296be, Sep 23 2020, 13:31:39)
[Clang 6.0 (clang-600.0.57)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration
>>> tokenizer = RagTokenizer.from_pretrained(""facebook/rag-token-nq"")
>>> retriever = RagRetriever.from_pretrained(""facebook/rag-token-nq"", index_name=""exact"", use_dummy_dataset=True)
Using custom data configuration dummy.psgs_w100.nq.no_index-dummy=True,with_index=False
Reusing dataset wiki_dpr (/Users/sergey_mkrtchyan/.cache/huggingface/datasets/wiki_dpr/dummy.psgs_w100.nq.no_index-dummy=True,with_index=False/0.0.0/8a97e0f4fa5bc46e179474db6a61b09d5d2419d2911835bd3f91d110c936d8bb)
Using custom data configuration dummy.psgs_w100.nq.exact-50b6cda57ff32ab4
Reusing dataset wiki_dpr (/Users/sergey_mkrtchyan/.cache/huggingface/datasets/wiki_dpr/dummy.psgs_w100.nq.exact-50b6cda57ff32ab4/0.0.0/8a97e0f4fa5bc46e179474db6a61b09d5d2419d2911835bd3f91d110c936d8bb)
  0%|                                                                                                                                                                                                                   | 0/10 [00:00<?, ?it/s]
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/sergey_mkrtchyan/workspace/cformers/venv/lib/python3.8/site-packages/transformers/models/rag/retrieval_rag.py"", line 425, in from_pretrained
    return cls(
  File ""/Users/sergey_mkrtchyan/workspace/cformers/venv/lib/python3.8/site-packages/transformers/models/rag/retrieval_rag.py"", line 387, in __init__
    self.init_retrieval()
  File ""/Users/sergey_mkrtchyan/workspace/cformers/venv/lib/python3.8/site-packages/transformers/models/rag/retrieval_rag.py"", line 458, in init_retrieval
    self.index.init_index()
  File ""/Users/sergey_mkrtchyan/workspace/cformers/venv/lib/python3.8/site-packages/transformers/models/rag/retrieval_rag.py"", line 284, in init_index
    self.dataset = load_dataset(
  File ""/Users/sergey_mkrtchyan/workspace/huggingface/datasets/src/datasets/load.py"", line 750, in load_dataset
    ds = builder_instance.as_dataset(split=split, ignore_verifications=ignore_verifications, in_memory=keep_in_memory)
  File ""/Users/sergey_mkrtchyan/workspace/huggingface/datasets/src/datasets/builder.py"", line 734, in as_dataset
    datasets = utils.map_nested(
  File ""/Users/sergey_mkrtchyan/workspace/huggingface/datasets/src/datasets/utils/py_utils.py"", line 195, in map_nested
    return function(data_struct)
  File ""/Users/sergey_mkrtchyan/workspace/huggingface/datasets/src/datasets/builder.py"", line 769, in _build_single_dataset
    post_processed = self._post_process(ds, resources_paths)
  File ""/Users/sergey_mkrtchyan/.cache/huggingface/modules/datasets_modules/datasets/wiki_dpr/8a97e0f4fa5bc46e179474db6a61b09d5d2419d2911835bd3f91d110c936d8bb/wiki_dpr.py"", line 205, in _post_process
    dataset.add_faiss_index(""embeddings"", custom_index=index)
  File ""/Users/sergey_mkrtchyan/workspace/huggingface/datasets/src/datasets/arrow_dataset.py"", line 2516, in add_faiss_index
    super().add_faiss_index(
  File ""/Users/sergey_mkrtchyan/workspace/huggingface/datasets/src/datasets/search.py"", line 416, in add_faiss_index
    faiss_index.add_vectors(self, column=column, train_size=train_size, faiss_verbose=faiss_verbose)
  File ""/Users/sergey_mkrtchyan/workspace/huggingface/datasets/src/datasets/search.py"", line 281, in add_vectors
    self.faiss_index.add(vecs)
  File ""/Users/sergey_mkrtchyan/workspace/cformers/venv/lib/python3.8/site-packages/faiss/__init__.py"", line 104, in replacement_add
    self.add_c(n, swig_ptr(x))
  File ""/Users/sergey_mkrtchyan/workspace/cformers/venv/lib/python3.8/site-packages/faiss/swigfaiss.py"", line 3263, in add
    return _swigfaiss.IndexHNSW_add(self, n, x)
RuntimeError: Error in virtual void faiss::IndexHNSW::add(faiss::Index::idx_t, const float *) at /Users/runner/work/faiss-wheels/faiss-wheels/faiss/faiss/IndexHNSW.cpp:356: Error: 'is_trained' failed
>>>
```

The issue seems to be related to the scalar quantization in faiss added in this commit: 8c5220307c33f00e01c3bf7b8. Reverting it fixes the issue.


"
https://github.com/huggingface/datasets/issues/1940,Side effect when filtering data due to `does_function_return_dict` call in `Dataset.map()`,"[""Thanks for the report !\r\n\r\nCurrently we don't have a way to let the user easily disable this behavior.\r\nHowever I agree that we should support stateful processing functions, ideally by removing `does_function_return_dict`.\r\n\r\nWe needed this function in order to know whether the `map` functions needs to write data or not. if `does_function_return_dict` returns False then we don't write anything.\r\n\r\nInstead of checking the output of the processing function outside of the for loop that iterates through the dataset to process it, we can check the output of the first processed example and at that point decide if we need to write data or not.\r\n\r\nTherefore it's definitely possible to fix this unwanted behavior, any contribution going into this direction is welcome :)""
 'Thanks @mariosasko for the PR!']","Hi there!

In my codebase I have a function to filter rows in a dataset, selecting only a certain number of examples per class. The function passes a extra argument to maintain a counter of the number of dataset rows/examples already selected per each class, which are the ones I want to keep in the end:

```python
      def fill_train_examples_per_class(example, per_class_limit: int, counter: collections.Counter):
          label = int(example['label'])
          current_counter = counter.get(label, 0)
          if current_counter < per_class_limit:
              counter[label] = current_counter + 1
              return True
          return False
```

At some point I invoke it through the `Dataset.filter()` method in the `arrow_dataset.py` module like this:

```python
...
kwargs = {""per_class_limit"": train_examples_per_class_limit, ""counter"": Counter()}
datasets['train'] = datasets['train'].filter(fill_train_examples_per_class,  num_proc=1, fn_kwargs=kwargs)
...
```

The problem is that, passing a stateful container (the counter,) provokes a side effect in the new filtered dataset obtained. This is due to the fact that at some point in `filter()`, the `map()`'s function `does_function_return_dict` is invoked in  line [1290](https://github.com/huggingface/datasets/blob/96578adface7e4bc1f3e8bafbac920d72ca1ca60/src/datasets/arrow_dataset.py#L1290). 

When this occurs, the state of the counter is initially modified by the effects of the function call on the 1 or 2 rows selected in lines 1288 and 1289 of the same file (which are marked as `test_inputs` & `test_indices` respectively in lines 1288 and 1289. This happens out of the control of the user (which for example can't reset the state of the counter before continuing the execution,) provoking in the end an undesired side effect in the results obtained. 

In my case, the resulting dataset -despite of the counter results are ok- lacks an instance of the classes 0 and 1 (which happen to be the classes of the first two examples of my dataset.) The rest of the classes I have in my dataset, contain the right number of examples as they were not affected by the effects of `does_function_return_dict` call.

I've debugged my code extensively and made a workaround myself hardcoding the necessary stuff (basically putting `update_data=True` in line 1290,) and then I obtain the results I expected without the side effect.

Is there a way to avoid that call to `does_function_return_dict` in map()'s line 1290 ? (e.g. extracting the required information that `does_function_return_dict` returns without making the testing calls to the user function on dataset rows 0 & 1) 

Thanks in advance,

Francisco Perez-Sorrosal

"
https://github.com/huggingface/datasets/issues/1939,[firewalled env] OFFLINE mode,"[""Thanks for reporting and for all the details and suggestions.\r\n\r\nI'm totally in favor of having a HF_DATASETS_OFFLINE env variable to disable manually all the connection checks, remove retries etc.\r\n\r\nMoreover you may know that the use case that you are mentioning is already supported from `datasets` 1.3.0, i.e. you already can:\r\n- first load datasets and metrics from an instance with internet connection\r\n- then be able to reload datasets and metrics from another instance without connection (as long as the filesystem is shared)\r\n\r\nThis is already implemented, but currently it only works if the requests return a `ConnectionError` (or any error actually). Not sure why it would hang instead of returning an error.\r\n\r\nMaybe this is just a issue with the timeout value being not set or too high ?\r\nIs there a way I can have access to one of the instances on which there's this issue (we can discuss this offline) ?\r\n""
 'I\'m on master, so using all the available bells and whistles already.\r\n\r\nIf you look at the common issues - it for example tries to look up files if they appear in `_PACKAGED_DATASETS_MODULES` which it shouldn\'t do.\r\n\r\n--------------\r\n\r\nYes, there is a nuance to it. As I mentioned it\'s firewalled - that is it has a network but making any calls outside - it just hangs in:\r\n\r\n```\r\nsin_addr=inet_addr(""xx.xx.xx.xx"")}, [28->16]) = 0\r\nclose(5)                                = 0\r\nsocket(AF_INET, SOCK_STREAM|SOCK_CLOEXEC, IPPROTO_TCP) = 5\r\nconnect(5, {sa_family=AF_INET, sin_port=htons(3128), sin_addr=inet_addr(""yy.yy.yy.yy"")}, 16^C) = ? ERESTARTSYS (To be restarted if SA_RESTART is set)\r\n```\r\nuntil it times out.\r\n\r\nThat\'s why we need to be able to tell the software that there is no network to rely on even if there is one (good for testing too).\r\n\r\nSo what I\'m thinking is that this is a simple matter of pre-ambling any network call wrappers with:\r\n\r\n```\r\nif HF_DATASETS_OFFLINE:\r\n    assert ""Attempting to make a network call under Offline mode""\r\n```\r\n\r\nand then fixing up if there is anything else to fix to make it work.\r\n\r\n--------------\r\n\r\nOtherwise I think the only other problem I encountered is that we need to find a way to pre-cache metrics, for some reason it\'s not caching it and wanting to fetch it from online.\r\n\r\nWhich is extra strange since it already has those files in the `datasets` repo itself that is on the filesystem.\r\n\r\nThe workaround I had to do is to copy `rouge/rouge.py` (with the parent folder) from the datasets repo to the current dir - and then it proceeded.'
 ""Ok understand better the hanging issue.\r\nI guess catching connection errors is not enough, we should also avoid all the hangings.\r\nCurrently the offline mode tests are only done by simulating an instant connection fail that returns an error, let's have another connection mock that hangs instead.\r\n\r\nI'll also take a look at why you had to do this for `rouge`.\r\n""
 ""FWIW, I think instant failure on the behalf of a network call is the simplest solution to correctly represent the environment and having the caller to sort it out is the next thing to do, since here it is the case of having no functional network, it's just that the software doesn't know this is the case, because there is some network. So we just need to help it to bail out instantly rather than hang waiting for it to time out. And afterwards everything else you said.""
 ""Update on this: \r\n\r\nI managed to create a mock environment for tests that makes the connections hang until timeout.\r\nI managed to reproduce the issue you're having in this environment.\r\n\r\nI'll update the offline test cases to also test the robustness to connection hangings, and make sure we set proper timeouts where it's needed in the code. This should cover the _automatic_ section you mentioned.""
 ""Fabulous! I'm glad you were able to reproduce the issues, @lhoestq!""
 ""I lost access to the firewalled setup, but I emulated it with:\r\n\r\n```\r\nsudo ufw enable\r\nsudo ufw default deny outgoing\r\n```\r\n(thanks @mfuntowicz)\r\n\r\nI was able to test `HF_DATASETS_OFFLINE=1` and it worked great - i.e. didn't try to reach out with it and used the cached files instead.\r\n\r\nThank you!""]","This issue comes from a need to be able to run `datasets` in a firewalled env, which currently makes the software hang until it times out, as it's unable to complete the network calls.

I propose the following approach to solving this problem, using the example of `run_seq2seq.py` as a sample program. There are 2 possible ways to going about it.

## 1. Manual

manually prepare data and metrics files, that is transfer to the firewalled instance the dataset and the metrics and run:

```
DATASETS_OFFLINE=1 run_seq2seq.py  --train_file xyz.csv --validation_file xyz.csv ...
```

`datasets` must not make any network calls and if there is a logic to do that and something is missing it should assert that this or that action requires network and therefore it can't proceed.

## 2. Automatic

In some clouds one can prepare a datastorage ahead of time with a normal networked environment but which doesn't have gpus and then one switches to the gpu instance which is firewalled, but it can access all the cached data. This is the ideal situation, since in this scenario we don't have to do anything manually, but simply run the same application twice:

1. on the non-firewalled instance:
```
run_seq2seq.py  --dataset_name wmt16 --dataset_config ro-en ...
```

which should download and cached everything.

2. and then immediately after on the firewalled instance, which shares the same filesystem
```
DATASETS_OFFLINE=1 run_seq2seq.py  --dataset_name wmt16 --dataset_config ro-en ...
```

and the metrics and datasets should be cached by the invocation number 1 and any network calls be skipped and if the logic is missing data it should assert and not try to fetch any data from online.

## Common Issues

1. for example currently `datasets` tries to look up online datasets if the files contain json or csv, despite the paths already provided

```
     if dataset and path in _PACKAGED_DATASETS_MODULES:
```

2. it has an issue with metrics. e.g. I had to manually copy `rouge/rouge.py` from the `datasets` repo to the current dir - or it was hanging.

I had to comment out `head_hf_s3(...)` calls to make things work. So all those `try: head_hf_s3(...)` shouldn't be tried with `DATASETS_OFFLINE=1`

Here is the corresponding issue for `transformers`: https://github.com/huggingface/transformers/issues/10379

Thanks."
https://github.com/huggingface/datasets/issues/1937,CommonGen dataset page shows an error OSError: [Errno 28] No space left on device,"['Facing the same issue for [Squad](https://huggingface.co/datasets/viewer/?dataset=squad) and [TriviaQA](https://huggingface.co/datasets/viewer/?dataset=trivia_qa) datasets as well.'
 'We just fixed the issue, thanks for reporting !']","The page of the CommonGen data https://huggingface.co/datasets/viewer/?dataset=common_gen  shows 
![image](https://user-images.githubusercontent.com/10104354/108959311-1865e600-7629-11eb-868c-cf4cb27034ea.png)
"
https://github.com/huggingface/datasets/issues/1934,Add Stanford Sentiment Treebank (SST),"['Dataset added in release [1.5.0](https://github.com/huggingface/datasets/releases/tag/1.5.0), I think I can close this.']","I am going to add SST:

- **Name:** The Stanford Sentiment Treebank
- **Description:** The first corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language
- **Paper:** [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)
- **Data:** https://nlp.stanford.edu/sentiment/index.html
- **Motivation:** Already requested in #353, SST is a popular dataset for Sentiment Classification

What's the difference with the [_SST-2_](https://huggingface.co/datasets/viewer/?dataset=glue&config=sst2) dataset included in GLUE? Essentially, SST-2 is a version of SST where:
- the labels were mapped from real numbers in [0.0, 1.0] to a binary label: {0, 1}
- the labels of the *sub-sentences* were included only in the training set
- the labels in the test set are obfuscated

So there is a lot more information in the original SST. The tricky bit is, the data is scattered into many text files and, for one in particular, I couldn't find the original encoding ([*but I'm not the only one*](https://groups.google.com/g/word2vec-toolkit/c/QIUjLw6RqFk/m/_iEeyt428wkJ) 🎵). The only solution I found was to manually replace all the è, ë, ç and so on into an `utf-8` copy of the text file. I uploaded the result in my Dropbox and I am using that as the main repo for the dataset.

Also, the _sub-sentences_ are built at run-time from the information encoded in several text files, so generating the examples is a bit more cumbersome than usual. Luckily, the dataset is not enormous.

I plan to divide the dataset in 2 configs: one with just whole sentences with their labels, the other with sentences _and their sub-sentences_ with their labels. Each config will be split in train, validation and test. Hopefully this makes sense, we may discuss it in the PR I'm going to submit.



"
https://github.com/huggingface/datasets/issues/1924,Anonymous Dataset Addition (i.e Anonymous PR?),"[""Hi !\r\nI guess you can add a dataset without the fields that must be kept anonymous, and then update those when the anonymity period is over.\r\nYou can also make the PR from an anonymous org.\r\nPinging @yjernite just to make sure it's ok""
 'Hello,\r\nI would prefer to do the reverse: adding a link to an anonymous paper without the people names/institution in the PR. Would it be conceivable ?\r\nCheers\r\n'
 ""Sure, I think it's ok on our side"" 'Yup, sounds good!']","Hello,
Thanks a lot for your librairy.
We plan to submit a paper on OpenReview using the Anonymous setting. Is it possible to add a new dataset without breaking the anonimity, with a link to the paper ? 
Cheers 
@eusip"
https://github.com/huggingface/datasets/issues/1922,"How to update the ""wino_bias"" dataset","['Hi @JieyuZhao !\r\n\r\nYou can edit the dataset card of wino_bias to update the URL via a Pull Request. This would be really appreciated :)\r\n\r\nThe dataset card is the README.md file you can find at https://github.com/huggingface/datasets/tree/master/datasets/wino_bias\r\nAlso the homepage url is also mentioned in the wino_bias.py so feel free to update it there as well.\r\n\r\nYou can create a Pull Request directly from the github interface by editing the files you want and submit a PR, or from a local clone of the repository.\r\n\r\nThanks for noticing !']","Hi all,

Thanks for the efforts to collect all the datasets! But I think there is a problem with the wino_bias dataset. The current link is not correct. How can I update that?

Thanks!"
https://github.com/huggingface/datasets/issues/1919,Failure to save with save_to_disk,"['Hi thanks for reporting and for proposing a fix :)\r\n\r\nI just merged a fix, feel free to try it from the master branch !'
 'Closing since this has been fixed by #1923']","When I try to save a dataset locally using the `save_to_disk` method I get the error:

```bash
FileNotFoundError: [Errno 2] No such file or directory: '/content/squad/train/squad-train.arrow'
```

To replicate:

1. Install `datasets` from master
2. Run this code:

    ```python
    from datasets import load_dataset
    squad = load_dataset(""squad"")   # or any other dataset
    squad.save_to_disk(""squad"")     # error here
    ```

The problem is that the method is not creating a directory with the name `dataset_path` for saving the dataset in (i.e. it's not creating the *train* and *validation* directories in this case). After creating the directory the problem resolves.
I'll open a PR soon doing that and linking this issue.
"
https://github.com/huggingface/datasets/issues/1917,UnicodeDecodeError: windows 10 machine,['upgraded to php 3.9.2 and it works!'],"Windows 10
Php 3.6.8

when running

```
import datasets

oscar_am = datasets.load_dataset(""oscar"", ""unshuffled_deduplicated_am"")
print(oscar_am[""train""][0])
```
I get the following error

```
file ""C:\PYTHON\3.6.8\lib\encodings\cp1252.py"", line 23, in decode
    return codecs.charmap_decode(input,self.errors,decoding_table)[0]
UnicodeDecodeError: 'charmap' codec can't decode byte 0x9d in position 58: character maps to <undefined>
```"
https://github.com/huggingface/datasets/issues/1915,Unable to download `wiki_dpr`,"[""Thanks for reporting ! This is a bug. For now feel free to set `ignore_verifications=False` in `load_dataset`.\r\nI'm working on a fix""
 ""I just merged a fix :)\r\n\r\nWe'll do a patch release soon. In the meantime feel free to try it from the master branch\r\nThanks again for reporting !""
 'Closing since this has been fixed by #1925']","I am trying to download the `wiki_dpr` dataset. Specifically, I want to download `psgs_w100.multiset.no_index` with no embeddings/no index. In order to do so, I ran:

`curr_dataset = load_dataset(""wiki_dpr"", embeddings_name=""multiset"", index_name=""no_index"")` 

However, I got the following error:
`datasets.utils.info_utils.UnexpectedDownloadedFile: {'embeddings_index'}`

I tried adding in flags `with_embeddings=False` and `with_index=False`:

`curr_dataset = load_dataset(""wiki_dpr"", with_embeddings=False, with_index=False, embeddings_name=""multiset"", index_name=""no_index"")`

But I got the following error:
`raise ExpectedMoreDownloadedFiles(str(set(expected_checksums) - set(recorded_checksums)))
datasets.utils.info_utils.ExpectedMoreDownloadedFiles: {‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_5’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_15’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_30’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_36’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_18’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_41’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_13’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_48’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_10’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_23’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_14’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_34’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_43’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_40’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_47’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_3’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_24’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_7’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_33’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_46’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_42’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_27’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_29’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_26’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_22’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_4’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_20’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_39’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_6’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_16’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_8’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_35’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_49’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_17’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_25’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_0’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_38’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_12’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_44’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_1’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_32’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_19’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_31’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_37’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_9’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_11’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_21’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_28’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_45’, ‘https://dl.fbaipublicfiles.com/rag/rag_multiset_embeddings/wiki_passages_2’}`

Is there anything else I need to set to download the dataset?

**UPDATE**: just running `curr_dataset = load_dataset(""wiki_dpr"", with_embeddings=False, with_index=False)` gives me the same error.
"
https://github.com/huggingface/datasets/issues/1911,Saving processed dataset running infinitely,"['@thomwolf @lhoestq can you guys please take a look and recommend some solution.'
 'am suspicious of this thing? what\'s the purpose of this? pickling and unplickling\r\n`self = pickle.loads(pickle.dumps(self))`\r\n\r\n```\r\n    def save_to_disk(self, dataset_path: str, fs=None):\r\n        """"""\r\n        Saves a dataset to a dataset directory, or in a filesystem using either :class:`datasets.filesystem.S3FileSystem` or any implementation of ``fsspec.spec.AbstractFileSystem``.\r\n\r\n        Args:\r\n            dataset_path (``str``): path (e.g. ``dataset/train``) or remote uri (e.g. ``s3://my-bucket/dataset/train``) of the dataset directory where the dataset will be saved to\r\n            fs (Optional[:class:`datasets.filesystem.S3FileSystem`,``fsspec.spec.AbstractFileSystem``],  `optional`, defaults ``None``): instance of :class:`datasets.filesystem.S3FileSystem` or ``fsspec.spec.AbstractFileSystem`` used to download the files from remote filesystem.\r\n        """"""\r\n        assert (\r\n            not self.list_indexes()\r\n        ), ""please remove all the indexes using `dataset.drop_index` before saving a dataset""\r\n        self = pickle.loads(pickle.dumps(self))\r\n  ```'
 ""It's been 24 hours and sadly it's still running. With not a single byte written""
 'Tried finding the root cause but was unsuccessful.\r\nI am using lazy tokenization with `dataset.set_transform()`, it works like a charm with almost same performance as pre-compute.'
 'Hi ! This very probably comes from the hack you used.\r\n\r\nThe pickling line was added an a sanity check because save_to_disk uses the same assumptions as pickling for a dataset object. The main assumption is that memory mapped pyarrow tables must be reloadable from the disk. In your case it\'s not possible since you altered the pyarrow table.\r\nI would suggest you to rebuild a valid Dataset object from your new pyarrow table. To do so you must first save your new table to a file, and then make a new Dataset object from that arrow file.\r\n\r\nYou can save the raw arrow table (without all the `datasets.Datasets` metadata) by calling `map` with `cache_file_name=""path/to/outut.arrow""` and `function=None`. Having `function=None` makes the `map` write your dataset on disk with no data transformation.\r\n\r\nOnce you have your new arrow file, load it with `datasets.Dataset.from_file` to have a brand new Dataset object :)\r\n\r\nIn the future we\'ll have a better support for the fast filtering method from pyarrow so you don\'t have to do this very unpractical workaround. Since it breaks somes assumptions regarding the core behavior of Dataset objects, this is very discouraged.'
 'Thanks, @lhoestq for your response. Will try your solution and let you know.']","I have a text dataset of size 220M.

For pre-processing, I need to tokenize this and filter rows with the large sequence.

My tokenization took roughly 3hrs. I used map() with batch size 1024 and multi-process with 96 processes.

filter() function was way to slow, so I used a hack to use pyarrow filter table function, which is damm fast. Mentioned [here](https://github.com/huggingface/datasets/issues/1796)

```dataset._data = dataset._data.filter(...)```
It took 1 hr for the filter.

Then i use `save_to_disk()` on processed dataset and it is running forever.

I have been waiting since 8 hrs, it has not written a single byte. 

Infact it has actually read from disk more than 100GB, screenshot below shows the stats using `iotop`. 
Second process is the one.
<img width=""1672"" alt=""Screenshot 2021-02-19 at 6 36 53 PM"" src=""https://user-images.githubusercontent.com/20911334/108508197-7325d780-72e1-11eb-8369-7c057d137d81.png"">


I am not able to figure out, whether this is some issue with dataset library or that it is due to my hack for filter() function."
https://github.com/huggingface/datasets/issues/1907,DBPedia14 Dataset Checksum bug?,"['Hi ! :)\r\n\r\nThis looks like the same issue as https://github.com/huggingface/datasets/issues/1856 \r\nBasically google drive has quota issues that makes it inconvenient for downloading files.\r\n\r\nIf the quota of a file is exceeded, you have to wait 24h for the quota to reset (which is painful).\r\n\r\nThe error says that the checksum of the downloaded file doesn\'t match because google drive returns a text file with the ""Quota Exceeded"" error instead of the actual data file.'
 'Thanks @lhoestq! Yes, it seems back to normal after a couple of days.']","Hi there!!!

I've been using successfully the DBPedia dataset (https://huggingface.co/datasets/dbpedia_14) with my codebase in the last couple of weeks, but in the last couple of days now I get this error:

```
Traceback (most recent call last):
  File ""./conditional_classification/basic_pipeline.py"", line 178, in <module>
    main()
  File ""./conditional_classification/basic_pipeline.py"", line 128, in main
    corpus.load_data(limit_train_examples_per_class=args.data_args.train_examples_per_class,
  File ""/home/fp/dev/conditional_classification/conditional_classification/datasets_base.py"", line 83, in load_data
    datasets = load_dataset(self.name, split=dataset_split)
  File ""/home/fp/anaconda3/envs/conditional/lib/python3.8/site-packages/datasets/load.py"", line 609, in load_dataset
    builder_instance.download_and_prepare(
  File ""/home/fp/anaconda3/envs/conditional/lib/python3.8/site-packages/datasets/builder.py"", line 526, in download_and_prepare
    self._download_and_prepare(
  File ""/home/fp/anaconda3/envs/conditional/lib/python3.8/site-packages/datasets/builder.py"", line 586, in _download_and_prepare
    verify_checksums(
  File ""/home/fp/anaconda3/envs/conditional/lib/python3.8/site-packages/datasets/utils/info_utils.py"", line 39, in verify_checksums
    raise NonMatchingChecksumError(error_msg + str(bad_urls))
datasets.utils.info_utils.NonMatchingChecksumError: Checksums didn't match for dataset source files:
['https://drive.google.com/uc?export=download&id=0Bz8a_Dbh9QhbQ2Vic1kxMmZZQ1k']
```

I've seen this has happened before in other datasets as reported in #537.

I've tried clearing my cache and call again `load_dataset` but still is not working. My same codebase is successfully downloading and using other datasets (e.g. AGNews) without any problem, so I guess something has happened specifically to the DBPedia dataset in the last few days. 

Can you please check if there's a problem with the checksums? 

Or this is related to any other stuff? I've seen that the path in the cache for the dataset is `/home/fp/.cache/huggingface/datasets/d_bpedia14/dbpedia_14/2.0.0/a70413e39e7a716afd0e90c9e53cb053691f56f9ef5fe317bd07f2c368e8e897...` and includes `d_bpedia14` instead maybe of `dbpedia_14`. Was this maybe a bug introduced recently?

Thanks!"
https://github.com/huggingface/datasets/issues/1906,Feature Request: Support for Pandas `Categorical`,"[""We already have a ClassLabel type that does this kind of mapping between the label ids (integers) and actual label values (strings).\r\n\r\nI wonder if actually we should use the DictionaryType from Arrow and the Categorical type from pandas for the `datasets` ClassLabel feature type.\r\nCurrently ClassLabel corresponds to `pa.int64()` in pyarrow and `dtype('int64')` in pandas (so the label names are lost during conversions).\r\n\r\nWhat do you think ?""
 ""Now that I've heard you explain ClassLabel, that makes a lot of sense!  While DictionaryType for Arrow (I think) can have arbitrarily typed keys, so it won't cover all potential cases, pandas' Category is *probably* the most common use for that pyarrow type, and ClassLabel should match that perfectly?\r\n\r\nOther thoughts:\r\n\r\n- changing the resulting patype on ClassLabel might be backward-incompatible?  I'm not totally sure if users of the `datasets` library tend to directly access the `patype` attribute (I don't think we really do, but we haven't been using it for very long yet).\r\n- would ClassLabel's dtype change to `dict[int64, string]`?  It seems like in practice a ClassLabel (when not explicitly specified) would be constructed from the DictionaryType branch of `generate_from_arrow_type`, so it's not totally clear to me that anyone ever actually accesses/uses that dtype?\r\n- I don't quite know how `.int2str` and `.str2int` are used in practice - would those be kept?  Perhaps the implementation might actually be substantially smaller if we can just delegate to pyarrow's dict methods?\r\n\r\nAnother idea that just occurred to me: add a branch in here to generate a ClassLabel if the dict key is int64 and the values are string: https://github.com/huggingface/datasets/blob/master/src/datasets/features.py#L932 , and then don't touch anything else.\r\n\r\nIn practice, I don't think this would be backward-incompatible in a way anyone would care about since the current behavior just throws an exception, and this way, we could support *reading* a pandas Categorical into a `Dataset` as a ClassLabel.  I *think* from there, while it would require some custom glue it wouldn't be too hard to convert the ClassLabel into a pandas Category if we want to go back - I think this would improve on the current behavior without risking changing the behavior of ClassLabel in a backward-incompat way.\r\n\r\nThoughts?  I'm not sure if this is overly cautious.  Whichever approach you think is better, I'd be happy to take it on!\r\n""
 ""I think we can first keep the int64 precision but with an arrow Dictionary for ClassLabel, and focus on the connection with arrow and pandas.\r\n\r\nIn this scope, I really like the idea of checking for the dictionary type:\r\n\r\n> Another idea that just occurred to me: add a branch in here to generate a ClassLabel if the dict key is int64 and the values are string: https://github.com/huggingface/datasets/blob/master/src/datasets/features.py#L932 , and then don't touch anything else.\r\n\r\nThis looks like a great start.\r\n\r\nThen as you said we'd have to add the conversion from classlabel to the correct arrow dictionary type. Arrow is already able to convert from arrow Dictionary to pandas Categorical so it should be enough.\r\n\r\nI can see two things that we must take case of to make this change backward compatible:\r\n- first we must still be able to load an arrow file with arrow int64 dtype and `datasets` ClassLabel type without crashing. This can be fixed by casting the arrow int64 array to an arrow Dictionary array on-the-fly when loading the table in the ArrowReader.\r\n- then we still have to return integers when accessing examples from a ClassLabel column. Currently it would return the strings values since it's based on the pandas behavior for converting from pandas to python/numpy. To do so we just have to adapt the python/numpy extractors in formatting.py (it takes care of converting an arrow table to a dictionary of python objects by doing arrow table -> pandas dataframe -> python dictionary)\r\n\r\nAny help on this matter is very much welcome :)""]","```
from datasets import Dataset
import pandas as pd
import pyarrow

df = pd.DataFrame(pd.Series([""a"", ""b"", ""c"", ""a""], dtype=""category""))
pyarrow.Table.from_pandas(df)
Dataset.from_pandas(df)
# Throws NotImplementedError
# TODO(thom) this will need access to the dictionary as well (for labels). I.e. to the py_table
```

I'm curious if https://github.com/huggingface/datasets/blob/master/src/datasets/features.py#L796 could be built out in a way similar to `Sequence`?

e.g. a `Map` class (or whatever name the maintainers might prefer) that can accept:

```
index_type = generate_from_arrow_type(pa_type.index_type)
value_type = generate_from_arrow_type(pa_type.value_type)
```

and then additional code points to modify:

- FeatureType: https://github.com/huggingface/datasets/blob/master/src/datasets/features.py#L694
- A branch to handle Map in get_nested_type: https://github.com/huggingface/datasets/blob/master/src/datasets/features.py#L719
- I don't quite understand what `encode_nested_example` does but perhaps a branch there? https://github.com/huggingface/datasets/blob/master/src/datasets/features.py#L755
- Similarly, I don't quite understand why `Sequence` is used this way in `generate_from_dict`, but perhaps a branch here? https://github.com/huggingface/datasets/blob/master/src/datasets/features.py#L775

I couldn't find other usages of `Sequence` outside of defining specific datasets, so I'm not sure if that's a comprehensive set of touchpoints."
https://github.com/huggingface/datasets/issues/1898,ALT dataset has repeating instances in all splits,"[""Thanks for reporting. This looks like a very bad issue. I'm looking into it""
 ""I just merged a fix, we'll do a patch release soon. Thanks again for reporting, and sorry for the inconvenience.\r\nIn the meantime you can load `ALT` using `datasets` from the master branch""
 'Thanks!!! works perfectly in the bleading edge master version'
 'Closed by #1899']","The [ALT](https://huggingface.co/datasets/alt) dataset has all the same instances within each split :/
Seemed like a great dataset for some experiments I wanted to carry out, especially since its medium-sized, and has all splits.

Would be great if this could be fixed :)

Added a snapshot of the contents from `explore-datset` feature, for quick reference.

![image](https://user-images.githubusercontent.com/33179372/108206321-442a2d00-714c-11eb-882f-b4b6e708ef9c.png)
"
https://github.com/huggingface/datasets/issues/1895,Bug Report: timestamp[ns] not recognized,"['Thanks for reporting !\r\n\r\nYou\'re right, `string_to_arrow` should be able to take `""timestamp[ns]""` as input and return the right pyarrow timestamp type.\r\nFeel free to suggest a fix for `string_to_arrow` and open a PR if you want to contribute ! This would be very appreciated :)\r\n\r\nTo give you more context:\r\n\r\nAs you may know we define the features types of a dataset using the `Features` object in combination with feature types like `Value`. For example\r\n```python\r\nfeatures = Features({\r\n    ""age"": Value(""int32"")\r\n})\r\n```\r\nHowever under the hood we are actually using pyarrow to store the data, and so we have a mapping between the feature types of `datasets` and the types of pyarrow.\r\n\r\nFor example, the `Value` feature types are created from a pyarrow type with `Value(str(pa_type))`.\r\nHowever it looks like the conversion back to a pyarrow type doesn\'t work with `""timestamp[ns]""`.\r\nThis is the `string_to_arrow` function you highlighted that does this conversion, so we should fix that.\r\n\r\n'
 ""Thanks for the clarification @lhoestq !\r\n\r\nThis may be a little bit of a stupid question, but I wanted to clarify one more thing before I took a stab at this:\r\n\r\nWhen the features get inferred, I believe they already have a pyarrow schema (https://github.com/huggingface/datasets/blob/master/src/datasets/arrow_dataset.py#L234).\r\n\r\nWe then convert it to a string (https://github.com/huggingface/datasets/blob/master/src/datasets/features.py#L778) only to convert it back into the arrow type (https://github.com/huggingface/datasets/blob/master/src/datasets/features.py#L143, and https://github.com/huggingface/datasets/blob/master/src/datasets/features.py#L35).  Is there a reason for this round-trip?\r\n\r\nI'll open a PR later to add `timestamp` support to `string_to_arrow`, but I'd be curious to understand since it feels like there may be some opportunities to simplify!""
 'The objective in terms of design is to make it easy to create Features in a pythonic way. So for example we use a string to define a Value type.\r\nThat\'s why when inferring the Features from an arrow schema we have to find the right string definitions for Value types. I guess we could also have a constructor `Value.from_arrow_type` to avoid recreating the arrow type, but this could create silent errors if the pyarrow type doesn\'t have a valid mapping with the string definition. The ""round-trip"" is used to enforce that the ground truth is the string definition, not the pyarrow type, and also as a sanity check.\r\n\r\nLet me know if that makes sense '
 'OK I think I understand now:\r\n\r\nFeatures are datasets\' internal representation of a schema type, distinct from pyarrow\'s schema.\r\nValue() corresponds to pyarrow\'s ""primitive"" types (e.g. `int` or `string`, but not things like `list` or `dict`).\r\n`get_nested_type()` (https://github.com/huggingface/datasets/blob/master/src/datasets/features.py#L698) and `generate_from_arrow_type()` (https://github.com/huggingface/datasets/blob/master/src/datasets/features.py#L778) *should* be inverses of each other, and similarly, for the primitive values, `string_to_arrow()` and `Value.__call__` (https://github.com/huggingface/datasets/blob/master/src/datasets/features.py#L146) should be inverses of each other?\r\n\r\nThanks for taking the time to answer - I just wanted to make sure I understood before opening a PR so I\'m not disrupting anything about how the codebase is expected to work!'
 ""Yes you're totally right :)""]","Repro:

```
from datasets import Dataset
import pandas as pd
import pyarrow

df = pd.DataFrame(pd.date_range(""2018-01-01"", periods=3, freq=""H""))
pyarrow.Table.from_pandas(df)
Dataset.from_pandas(df)
# Throws ValueError: Neither timestamp[ns] nor timestamp[ns]_ seems to be a pyarrow data type.
```

The factory function seems to be just ""timestamp"": https://arrow.apache.org/docs/python/generated/pyarrow.timestamp.html#pyarrow.timestamp

It seems like https://github.com/huggingface/datasets/blob/master/src/datasets/features.py#L36-L43 could have a little bit of additional structure for handling these cases?  I'd be happy to take a shot at opening a PR if I could receive some guidance on whether parsing something like `timestamp[ns]` and resolving it to timestamp('ns') is the goal of this method.

Alternatively, if I'm using this incorrectly (e.g. is the expectation that we always provide a schema when timestamps are involved?), that would be very helpful to know as well!

```
$ pip list  # only the relevant libraries/versions
datasets                      1.2.1
pandas                        1.0.3
pyarrow                       3.0.0
```"
https://github.com/huggingface/datasets/issues/1894,benchmarking against MMapIndexedDataset,"[""Hi sam !\r\nIndeed we can expect the performances to be very close since both MMapIndexedDataset and the `datasets` implem use memory mapping. With memory mapping what determines the I/O performance is the speed of your hard drive/SSD.\r\n\r\nIn terms of performance we're pretty close to the optimal speed for reading text, even though I found recently that we could still slightly improve speed for big datasets (see [here](https://github.com/huggingface/datasets/issues/1803)).\r\n\r\nIn terms of number of examples and example sizes, the only limit is the available disk space you have.\r\n\r\nI haven't used `psrecord` yet but it seems to be a very interesting tool for benchmarking. Currently for benchmarks we only have github actions to avoid regressions in terms of speed. But it would be cool to have benchmarks with comparisons with other dataset tools ! This would be useful to many people""
 'Also I would be interested to know what data types `MMapIndexedDataset` supports. Is there some documentation somewhere ?'
 ""no docs haha, it's written to support integer numpy arrays.\r\n\r\nYou can build one in fairseq with, roughly:\r\n```bash\r\n\r\nwget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\r\nunzip wikitext-103-raw-v1.zip\r\nexport dd=$HOME/fairseq-py/wikitext-103-raw\r\n\r\nexport mm_dir=$HOME/mmap_wikitext2\r\nmkdir -p gpt2_bpe\r\nwget -O gpt2_bpe/encoder.json https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json\r\nwget -O gpt2_bpe/vocab.bpe https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe\r\nwget -O gpt2_bpe/dict.txt https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/dict.txt\r\nfor SPLIT in train valid; do \\\r\n    python -m examples.roberta.multiprocessing_bpe_encoder \\\r\n        --encoder-json gpt2_bpe/encoder.json \\\r\n        --vocab-bpe gpt2_bpe/vocab.bpe \\\r\n        --inputs /scratch/stories_small/${SPLIT}.txt \\\r\n        --outputs /scratch/stories_small/${SPLIT}.bpe \\\r\n        --keep-empty \\\r\n        --workers 60; \\\r\ndone\r\n\r\nmkdir -p $mm_dir\r\nfairseq-preprocess \\\r\n    --only-source \\\r\n    --srcdict gpt2_bpe/dict.txt \\\r\n    --trainpref $dd/wiki.train.bpe \\\r\n    --validpref $dd/wiki.valid.bpe \\\r\n    --destdir $mm_dir \\\r\n    --workers 60 \\\r\n    --dataset-impl mmap\r\n```\r\n\r\nI'm noticing in my benchmarking that it's much smaller on disk than arrow (200mb vs 900mb), and that both incur significant cost by increasing the number of data loader workers. \r\nThis somewhat old [post](https://ray-project.github.io/2017/10/15/fast-python-serialization-with-ray-and-arrow.html) suggests there are some gains to be had from using `pyarrow.serialize(array).tobuffer()`. I haven't yet figured out how much of this stuff `pa.Table` does under the hood.\r\n\r\nThe `MMapIndexedDataset` bottlenecks we are working on improving (by using arrow) are:\r\n1) `MMapIndexedDataset`'s index, which stores offsets, basically gets read in its entirety by each dataloading process.\r\n2) we have separate, identical, `MMapIndexedDatasets` on each dataloading worker, so there's redundancy there; we wonder if there is a way that arrow can somehow dedupe these in shared memory.\r\n\r\nIt will take me a few hours to get `MMapIndexedDataset` benchmarks out of `fairseq`/onto a branch in this repo, but I'm happy to invest the time if you're interested in collaborating on some performance hacking.""]","I am trying to benchmark my datasets based implementation against fairseq's [`MMapIndexedDataset`](https://github.com/pytorch/fairseq/blob/master/fairseq/data/indexed_dataset.py#L365) and finding that, according to psrecord, my `datasets` implem uses about 3% more CPU memory and runs 1% slower for `wikitext103` (~1GB of tokens).

Questions:
1) Is this (basically identical) performance expected? 
2) Is there a scenario where this library will outperform `MMapIndexedDataset`? (maybe more examples/larger examples?)
3) Should I be using different benchmarking tools than `psrecord`/how do you guys do benchmarks?

Thanks in advance! Sam"
https://github.com/huggingface/datasets/issues/1893,wmt19 is broken,"['This was also mentioned in https://github.com/huggingface/datasets/issues/488 \r\n\r\nThe bucket where is data was stored seems to be unavailable now. Maybe we can change the URL to the ones in https://conferences.unite.un.org/uncorpus/en/downloadoverview ?'
 'Closing since this has been fixed by #1912']","1. Check which lang pairs we have: `--dataset_name wmt19`:

Please pick one among the available configs: ['cs-en', 'de-en', 'fi-en', 'gu-en', 'kk-en', 'lt-en', 'ru-en', 'zh-en', 'fr-de']

 
2. OK, let's pick `ru-en`:

`--dataset_name wmt19 --dataset_config ""ru-en""`

no cookies:

```
Traceback (most recent call last):
  File ""./run_seq2seq.py"", line 661, in <module>
    main()
  File ""./run_seq2seq.py"", line 317, in main
    datasets = load_dataset(data_args.dataset_name, data_args.dataset_config_name)
  File ""/mnt/nvme1/code/huggingface/datasets-master/src/datasets/load.py"", line 740, in load_dataset
    builder_instance.download_and_prepare(
  File ""/mnt/nvme1/code/huggingface/datasets-master/src/datasets/builder.py"", line 572, in download_and_prepare
    self._download_and_prepare(
  File ""/mnt/nvme1/code/huggingface/datasets-master/src/datasets/builder.py"", line 628, in _download_and_prepare
    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)
  File ""/home/stas/.cache/huggingface/modules/datasets_modules/datasets/wmt19/436092de5f3faaf0fc28bc84875475b384e90a5470fa6afaee11039ceddc5052/wmt_utils.py"", line 755, in _split_generators
    downloaded_files = dl_manager.download_and_extract(urls_to_download)
  File ""/mnt/nvme1/code/huggingface/datasets-master/src/datasets/utils/download_manager.py"", line 276, in download_and_extract
    return self.extract(self.download(url_or_urls))
  File ""/mnt/nvme1/code/huggingface/datasets-master/src/datasets/utils/download_manager.py"", line 191, in download
    downloaded_path_or_paths = map_nested(
  File ""/mnt/nvme1/code/huggingface/datasets-master/src/datasets/utils/py_utils.py"", line 233, in map_nested
    mapped = [
  File ""/mnt/nvme1/code/huggingface/datasets-master/src/datasets/utils/py_utils.py"", line 234, in <listcomp>
    _single_map_nested((function, obj, types, None, True)) for obj in tqdm(iterable, disable=disable_tqdm)
  File ""/mnt/nvme1/code/huggingface/datasets-master/src/datasets/utils/py_utils.py"", line 190, in _single_map_nested
    mapped = [_single_map_nested((function, v, types, None, True)) for v in pbar]
  File ""/mnt/nvme1/code/huggingface/datasets-master/src/datasets/utils/py_utils.py"", line 190, in <listcomp>
    mapped = [_single_map_nested((function, v, types, None, True)) for v in pbar]
  File ""/mnt/nvme1/code/huggingface/datasets-master/src/datasets/utils/py_utils.py"", line 172, in _single_map_nested
    return function(data_struct)
  File ""/mnt/nvme1/code/huggingface/datasets-master/src/datasets/utils/download_manager.py"", line 211, in _download
    return cached_path(url_or_filename, download_config=download_config)
  File ""/mnt/nvme1/code/huggingface/datasets-master/src/datasets/utils/file_utils.py"", line 274, in cached_path
    output_path = get_from_cache(
  File ""/mnt/nvme1/code/huggingface/datasets-master/src/datasets/utils/file_utils.py"", line 584, in get_from_cache
    raise FileNotFoundError(""Couldn't find file at {}"".format(url))
FileNotFoundError: Couldn't find file at https://storage.googleapis.com/tfdataset-data/downloadataset/uncorpus/UNv1.0.en-ru.tar.gz
```"
https://github.com/huggingface/datasets/issues/1892,"request to mirror wmt datasets, as they are really slow to download","['Yes that would be awesome. Not only the download speeds are awful, but also some files are missing.\r\nWe list all the URLs in the datasets/wmt19/wmt_utils.py so we can make a script to download them all and host on S3.\r\nAlso I think most of the materials are under the CC BY-NC-SA 3.0 license (must double check) so it should be possible to redistribute the data with no issues.\r\n\r\ncc @patrickvonplaten who knows more about the wmt scripts'
 'Yeah, the scripts are pretty ugly! A big refactor would make sense here...and I also remember that the datasets were veeery slow to download'
 ""I'm downloading them.\r\nI'm starting with the ones hosted on http://data.statmt.org which are the slowest ones""
 '@lhoestq better to use our new git-based system than just raw S3, no? (that way we have built-in CDN etc.)'
 'Closing since the urls were changed to mirror urls in #1912 ']","Would it be possible to mirror the wmt data files under hf? Some of them take hours to download and not because of the local speed. They are all quite small datasets, just extremely slow to download.

Thank you!"
https://github.com/huggingface/datasets/issues/1877,Allow concatenation of both in-memory and on-disk datasets,"[""I started working on this. My idea is to first add the pyarrow Table wrappers InMemoryTable and MemoryMappedTable that both implement what's necessary regarding copy/pickle. Then have another wrapper that takes the concatenation of InMemoryTable/MemoryMappedTable objects.\r\n\r\nWhat's important here is that concatenating two tables into one doesn't double the memory used (`total_allocated_bytes()` stays the same).""
 ""Hi @lhoestq @albertvillanova,\r\n\r\nI checked the linked issues and PR, this seems like a great idea. Would you mind elaborating on the in-memory and memory-mapped datasets? \r\nBased on my understanding, it is something like this, please correct me if I am wrong:\r\n1. For in-memory datasets, we don't have any dataset files so the entire dataset is pickled to the cache during loading, and then whenever required it is unpickled .\r\n2. For on-disk/memory-mapped datasets, we have the data files provided, so they can be re-loaded from the paths, and only the file-paths are stored while pickling.\r\n\r\nIf this is correct, will the feature also handle pickling/unpickling of a concatenated dataset? Will this be cached?\r\n\r\nThis also leads me to ask whether datasets are chunked during pickling? \r\n\r\nThanks,\r\nGunjan""
 ""Hi ! Yes you're totally right about your two points :)\r\n\r\nAnd in the case of a concatenated dataset, then we should reload each sub-table depending on whether it's in-memory or memory mapped. That means the dataset will be made of several blocks in order to keep track of what's from memory and what's memory mapped. This allows to pickle/unpickle concatenated datasets""
 'Hi @lhoestq\r\n\r\nThanks, that sounds nice. Can you explain where the issue of the double memory may arise? Also, why is the existing `concatenate_datasets` not sufficient for this purpose?'
 'Hi @lhoestq,\r\n\r\nWill the `add_item` feature also help with lazy writing (or no caching) during `map`/`filter`?'
 ""> Can you explain where the issue of the double memory may arise?\r\n\r\nWe have to keep each block (in-memory vs memory mapped) separated in order to be able to reload them with pickle.\r\nOn the other hand we also need to have the full table from mixed in-memory and memory mapped data in order to iterate or extract data conveniently. That means that each block is accessible twice: once in the full table, and once in the separated blocks. But since pyarrow tables concatenation doesn't double the memory, then building the full table doesn't cost memory which is what we want :)\r\n\r\n> Also, why is the existing concatenate_datasets not sufficient for this purpose?\r\n\r\nThe existing `concatenate_datasets` doesn't support having both in-memory and memory mapped data together (there's no fancy block separation logic). It works for datasets fully in-memory or fully memory mapped but not a mix of the two.\r\n\r\n> Will the add_item feature also help with lazy writing (or no caching) during map/filter?\r\n\r\nIt will enable the implementation of the fast, masked filter from this discussion: https://github.com/huggingface/datasets/issues/1949\r\nHowever I don't think this will affect map.""]","This is a prerequisite for the addition of the `add_item` feature (see #1870).
Currently there is one assumption that we would need to change: a dataset is either fully in memory (dataset._data_files is empty), or the dataset can be reloaded from disk (using the dataset._data_files).
This assumption is used for pickling for example:
- in-memory dataset can just be pickled/unpickled in-memory
- on-disk dataset can be unloaded to only keep the filepaths when pickling, and then reloaded from the disk when unpickling

Maybe let's have a design that allows a Dataset to have a Table that can be rebuilt from heterogenous sources like in-memory tables or on-disk tables ? This could also be further extended in the future

One idea would be to define a list of sources and each source implements a way to reload its corresponding pyarrow Table.
Then the dataset would be the concatenation of all these tables.

Depending on the source type, the serialization using pickle would be different. In-memory data would be copied while on-disk data would simply be replaced by the path to these data.

If you have some ideas you would like to share about the design/API feel free to do so :)

cc @albertvillanova "
https://github.com/huggingface/datasets/issues/1876," load_dataset(""multi_woz_v22"") NonMatchingChecksumError","[""Thanks for reporting !\r\nThis is due to the changes made in the data files in the multiwoz repo: https://github.com/budzianowski/multiwoz/pull/59\r\nI'm opening a PR to update the checksums of the data files.""
 ""I just merged the fix. It will be available in the new release of `datasets` later today.\r\nYou'll be able to get the new version with\r\n```\r\npip install --upgrade datasets\r\n```""
 ""Hi, I still meet the error when loading the datasets after upgradeing datasets.\r\n\r\nraise NonMatchingChecksumError(error_msg + str(bad_urls))\r\ndatasets.utils.info_utils.NonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/dialog_acts.json', 'https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/test/dialogues_001.json']""
 'This must be related to https://github.com/budzianowski/multiwoz/pull/72\r\nThose files have changed, let me update the checksums for this dataset.\r\n\r\nFor now you can use `ignore_verifications=True` in `load_dataset` to skip the checksum verification.']","Hi, it seems that loading the multi_woz_v22 dataset gives a NonMatchingChecksumError.

To reproduce:

`dataset = load_dataset('multi_woz_v22','v2.2_active_only',split='train')`


This will give the following error:

```
    raise NonMatchingChecksumError(error_msg + str(bad_urls))
datasets.utils.info_utils.NonMatchingChecksumError: Checksums didn't match for dataset source files:
['https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/dialog_acts.json', 'https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/train/dialogues_001.json', 'https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/train/dialogues_003.json', 'https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/train/dialogues_004.json', 'https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/train/dialogues_005.json', 'https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/train/dialogues_006.json', 'https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/train/dialogues_007.json', 'https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/train/dialogues_008.json', 'https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/train/dialogues_009.json', 'https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/train/dialogues_010.json', 'https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/train/dialogues_012.json', 'https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/train/dialogues_013.json', 'https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/train/dialogues_014.json', 'https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/train/dialogues_015.json', 'https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/train/dialogues_016.json', 'https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/train/dialogues_017.json', 'https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/dev/dialogues_001.json', 'https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/dev/dialogues_002.json', 'https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/test/dialogues_001.json', 'https://github.com/budzianowski/multiwoz/raw/master/data/MultiWOZ_2.2/test/dialogues_002.json']
```
"
https://github.com/huggingface/datasets/issues/1872,Adding a new column to the dataset after set_format was called,"['Hi ! Indeed if you add a column to a formatted dataset, then the new dataset gets a new formatting in which:\r\n```\r\nnew formatted columns = (all columns - previously unformatted columns)\r\n```\r\nTherefore the new column is going to be formatted using the `torch` formatting.\r\n\r\nIf you want your new column to be unformatted you can re-run this line:\r\n```python\r\ndata.set_format(""torch"", columns=[""some_integer_column1"", ""some_integer_column2""], output_all_columns=True)\r\n```'
 'Hi, thanks that solved my problem. Maybe mention that in the documentation. '
 'Ok cool :) \r\nAlso I just did a PR to mention this behavior in the documentation'
 'Closed by #1888']","Hi, 

thanks for the nice library. I'm in the process of creating a custom dataset, which has a mix of tensors and lists of strings. I stumbled upon an error and want to know if its a problem on my side. 

I load some lists of strings and integers, then call `data.set_format(""torch"", columns=[""some_integer_column1"", ""some_integer_column2""], output_all_columns=True)`. This converts the integer columns into tensors, but keeps the lists of strings as they are. I then call `map` to add a new column to my dataset, which is a **list of strings**. Once I iterate through my dataset, I get an error that the new column can't be converted into a tensor (which is probably caused by `set_format`). 

Below some pseudo code:
```python
    def augment_func(sample: Dict) -> Dict:
        # do something
        return {
         ""some_integer_column1"" : augmented_data[""some_integer_column1""],  # <-- tensor
         ""some_integer_column2"" : augmented_data[""some_integer_column2""],  # <-- tensor
         ""NEW_COLUMN"": targets,  # <-- list of strings
        }


    data = datasets.load_dataset(__file__, data_dir=""..."", split=""train"")
    data.set_format(""torch"", columns=[""some_integer_column1"", ""some_integer_column2""], output_all_columns=True)

    augmented_dataset = data.map(augment_func, batched=False)
    
    for sample in augmented_dataset:
        print(sample)  # fails

```

and the exception:
```python
Traceback (most recent call last):
  File ""dataset.py"", line 487, in <module>
    main()
  File ""dataset.py"", line 471, in main
    for sample in augmented_dataset:
  File ""lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 697, in __iter__
    yield self._getitem(
  File ""lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 1069, in _getitem
    outputs = self._convert_outputs(
  File ""lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 890, in _convert_outputs
    v = map_nested(command, v, **map_nested_kwargs)
  File ""lib/python3.8/site-packages/datasets/utils/py_utils.py"", line 225, in map_nested
    return function(data_struct)
  File ""lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 850, in command
    return [map_nested(command, i, **map_nested_kwargs) for i in x]
  File ""lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 850, in <listcomp>
    return [map_nested(command, i, **map_nested_kwargs) for i in x]
  File ""lib/python3.8/site-packages/datasets/utils/py_utils.py"", line 225, in map_nested
    return function(data_struct)
  File ""lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 850, in command
    return [map_nested(command, i, **map_nested_kwargs) for i in x]
  File ""lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 850, in <listcomp>
    return [map_nested(command, i, **map_nested_kwargs) for i in x]
  File ""lib/python3.8/site-packages/datasets/utils/py_utils.py"", line 225, in map_nested
    return function(data_struct)
  File ""lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 851, in command
    return torch.tensor(x, **format_kwargs)
TypeError: new(): invalid data type 'str'
```

Thanks!
"
https://github.com/huggingface/datasets/issues/1867,ERROR WHEN USING SET_TRANSFORM() ,"['Hi @alejandrocros it looks like an incompatibility with the current Trainer @sgugger \r\nIndeed currently the Trainer of `transformers` doesn\'t support a dataset with a transform\r\n\r\nIt looks like it comes from this line: https://github.com/huggingface/transformers/blob/f51188cbe74195c14c5b3e2e8f10c2f435f9751a/src/transformers/trainer.py#L442\r\n\r\nThis line sets the format to not return certain unused columns. But this has two issues:\r\n1. it forgets to also set the format_kwargs (this causes the error you got):\r\n```python\r\ndataset.set_format(type=dataset.format[""type""], columns=columns, format_kwargs=dataset.format[""format_kwargs""])\r\n```\r\n2. the Trainer wants to keep only the fields that are used as input for a model. However for a dataset with a transform, the output fields are often different from the columns fields. For example from a column ""text"" in the dataset, the strings can be transformed on-the-fly into ""input_ids"". If you want your dataset to only output certain fields and not other you must change your transform function.\r\n'
 'FYI that option can be removed with `remove_unused_columns = False` in your `TrainingArguments`, so there is a workaround @alexvaca0 while the fix in `Trainer` is underway.\r\n\r\n@lhoestq I think I will just use the line you suggested and if someone is using the columns that are removed in their transform they will need to change `remove_unused_columns` to `False`. We might switch the default of that argument in the next version if that proves too bug-proof.'
 'I\'ve tried your solutions @sgugger @lhoestq and the good news is that it throws no error. However, TPU training is taking forever, in 1 hour it has only trained 1 batch of 8192 elements, which doesn\'t make much sense... Is it possible that ""on the fly"" tokenization of batches is slowing down TPU training to that extent?'
 ""I'm pretty sure this is because of padding but @sgugger might know better""
 ""I don't know what the value of `padding` is in your lines of code pasted above so I can't say for sure. The first batch will be very slow on TPU since it compiles everything, so that's normal (1 hour is long but 8192 elements is also large). Then if your batches are not of the same lengths, it will recompile everything at each step instead of using the same graph, which will be very slow, so you should double check you are using padding to make everything the exact same shape. ""
 'I have tried now on a GPU and it goes smooth! Amazing feature .set_transform() instead of .map()! Now I can pre-train my model without the hard disk limitation. Thanks for your work all HuggingFace team!! :clap: '
 ""In the end, to make it work I turned to A-100 gpus instead of TPUS, among other changes. Set_transform doesn't work as expected and slows down training very much even in GPUs, and applying map destroys the disk, as it multiplies by 100 the size of the data passed to it (due to inefficient implementation converting strings to int64 floats I guess). For that reason, I chose to use datasets to load the data as text, and then edit the Collator from Transformers to tokenize every batch it receives before processing it. That way, I'm being able to train fast, without memory breaks, without the disk being unnecessarily filled, while making use of GPUs almost all the time I'm paying for them (the map function over the whole dataset took ~15hrs, in which you're not training at all). I hope this info helps others that are looking for training a language model from scratch cheaply, I'm going to close the issue as the optimal solution I found after many experiments to the problem posted in it is explained above. ""
 'Great comment @alexvaca0 . I think that we could re-open the issue as a reformulation of why it takes so much space to save the arrow. Saving a 1% of oscar corpus takes more thank 600 GB (it breaks when it pass 600GB because it is the free memory that I have at this moment) when the full dataset is 1,3 TB. I have a 1TB M.2 NVMe disk that I can not train on because the saved .arrow files goes crazily big. If you can share your Collator I will be grateful. ']","Hi, I'm trying to use dataset.set_transform(encode) as @lhoestq told me in this issue: https://github.com/huggingface/datasets/issues/1825#issuecomment-774202797

However, when I try to use Trainer from transformers with such dataset, it throws an error:

```
TypeError: __init__() missing 1 required positional argument: 'transform'
[INFO|trainer.py:357] 2021-02-12 10:18:09,893 >> The following columns in the training set don't have a corresponding argument in `AlbertForMaskedLM.forward` and have been ignored: text.
Exception in device=TPU:0: __init__() missing 1 required positional argument: 'transform'
Traceback (most recent call last):
  File ""/anaconda3/envs/torch-xla-1.7/lib/python3.6/site-packages/torch_xla/distributed/xla_multiprocessing.py"", line 330, in _mp_start_fn
    _start_fn(index, pf_cfg, fn, args)
  File ""/anaconda3/envs/torch-xla-1.7/lib/python3.6/site-packages/torch_xla/distributed/xla_multiprocessing.py"", line 324, in _start_fn
    fn(gindex, *args)
  File ""/home/alejandro_vaca/transformers/examples/language-modeling/run_mlm_wwm.py"", line 368, in _mp_fn
    main()
  File ""/home/alejandro_vaca/transformers/examples/language-modeling/run_mlm_wwm.py"", line 332, in main
    data_collator=data_collator,
  File ""/anaconda3/envs/torch-xla-1.7/lib/python3.6/site-packages/transformers/trainer.py"", line 286, in __init__
    self._remove_unused_columns(self.train_dataset, description=""training"")
  File ""/anaconda3/envs/torch-xla-1.7/lib/python3.6/site-packages/transformers/trainer.py"", line 359, in _remove_unused_columns
    dataset.set_format(type=dataset.format[""type""], columns=columns)
  File ""/home/alejandro_vaca/datasets/src/datasets/fingerprint.py"", line 312, in wrapper
    out = func(self, *args, **kwargs)
  File ""/home/alejandro_vaca/datasets/src/datasets/arrow_dataset.py"", line 818, in set_format
    _ = get_formatter(type, **format_kwargs)
  File ""/home/alejandro_vaca/datasets/src/datasets/formatting/__init__.py"", line 112, in get_formatter
    return _FORMAT_TYPES[format_type](**format_kwargs)
TypeError: __init__() missing 1 required positional argument: 'transform'
```

The code I'm using:

```{python}

    def tokenize_function(examples):
        # Remove empty lines
        examples[""text""] = [line for line in examples[""text""] if len(line) > 0 and not line.isspace()]
        return tokenizer(examples[""text""], padding=padding, truncation=True, max_length=data_args.max_seq_length)

    datasets.set_transform(tokenize_function)

    data_collator = DataCollatorForWholeWordMask(tokenizer=tokenizer, mlm_probability=data_args.mlm_probability)

    # Initialize our Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=datasets[""train""] if training_args.do_train else None,
        eval_dataset=datasets[""val""] if training_args.do_eval else None,
        tokenizer=tokenizer,
        data_collator=data_collator,
    )
```

I've installed from source, master branch.
"
https://github.com/huggingface/datasets/issues/1864,Add Winogender Schemas,"[""Nevermind, this one is already available on the hub under the name `'wino_bias'`: https://huggingface.co/datasets/wino_bias""]","## Adding a Dataset
- **Name:** Winogender Schemas
- **Description:** Winogender Schemas (inspired by Winograd Schemas) are minimal pairs of sentences that differ only by the gender of one pronoun in the sentence, designed to test for the presence of gender bias in automated coreference resolution systems.
- **Paper:** https://arxiv.org/abs/1804.09301
- **Data:** https://github.com/rudinger/winogender-schemas (see data directory)
- **Motivation:** Testing gender bias in automated coreference resolution systems, improve coreference resolution in general.

Instructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).
"
https://github.com/huggingface/datasets/issues/1863,Add WikiCREM,"['Hi @NielsRogge I would like to work on this dataset.\r\n\r\nThanks!'
 'Hi @udapy, are you working on this?']","## Adding a Dataset
- **Name:** WikiCREM
- **Description:** A large unsupervised corpus for coreference resolution.
- **Paper:** https://arxiv.org/abs/1905.06290
- **Github repo:**: https://github.com/vid-koci/bert-commonsense
- **Data:** https://ora.ox.ac.uk/objects/uuid:c83e94bb-7584-41a1-aef9-85b0e764d9e3
- **Motivation:** Coreference resolution, common sense reasoning

Instructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).
"
https://github.com/huggingface/datasets/issues/1859,"Error ""in void don't know how to serialize this type of index"" when saving index to disk when device=0 (GPU)","[""Hi @corticalstack ! Thanks for reporting. Indeed in the recent versions of Faiss we must use `getDevice` to check if the index in on GPU.\r\n\r\nI'm opening a PR""
 'I fixed this issue. It should work fine now.\r\nFeel free to try it out by installing `datasets` from source.\r\nOtherwise you can wait for the next release of `datasets` (in a few days)'
 'Thanks for such a quick fix and merge to master, pip installed git master, tested all OK']","Error serializing faiss index.  Error as follows:

`Error in void faiss::write_index(const faiss::Index*, faiss::IOWriter*) at /home/conda/feedstock_root/build_artifacts/faiss-split_1612472484670/work/faiss/impl/index_write.cpp:453: don't know how to serialize this type of index`


Note:

`torch.cuda.is_available()` reports:

```
Cuda is available
cuda:0

```

Adding index, device=0 for GPU.

`dataset.add_faiss_index(column='embeddings', index_name='idx_embeddings', device=0)`

However, during a quick debug, self.faiss_index has no attr ""device"" when checked in` search.py, method save`, so fails to transform gpu index to cpu index.  If I add index without device, index is saved OK.


```
def save(self, file: str):
        """"""Serialize the FaissIndex on disk""""""
        import faiss  # noqa: F811

        if (
            hasattr(self.faiss_index, ""device"")
            and self.faiss_index.device is not None
            and self.faiss_index.device > -1
        ):
            index = faiss.index_gpu_to_cpu(self.faiss_index)
        else:
            index = self.faiss_index
        faiss.write_index(index, file)
```
"
https://github.com/huggingface/datasets/issues/1857,"Unable to upload ""community provided"" dataset - 400 Client Error","[""Hi ! We're in the process of switching the community datasets to git repos, exactly like what we're doing for models.\r\nYou can find an example here:\r\nhttps://huggingface.co/datasets/lhoestq/custom_squad/tree/main\r\n\r\nWe'll update the CLI in the coming days and do a new release :)\r\n\r\nAlso cc @julien-c maybe we can make improve the error message ?""]","Hi,
i'm trying to a upload a dataset as described [here](https://huggingface.co/docs/datasets/v1.2.0/share_dataset.html#sharing-a-community-provided-dataset). This is what happens:

``` 
$ datasets-cli login
$ datasets-cli upload_dataset my_dataset
About to upload file /path/to/my_dataset/dataset_infos.json to S3 under filename my_dataset/dataset_infos.json and namespace username
About to upload file /path/to/my_dataset/my_dataset.py to S3 under filename my_dataset/my_dataset.py and namespace username
Proceed? [Y/n] Y
Uploading... This might take a while if files are large
400 Client Error: Bad Request for url: https://huggingface.co/api/datasets/presign
huggingface.co migrated to a new model hosting system.
You need to upgrade to transformers v3.5+ to upload new models.
More info at https://discuss.hugginface.co or https://twitter.com/julien_c. Thank you! 
```
I'm using the latest releases of datasets and transformers."
https://github.com/huggingface/datasets/issues/1856,"load_dataset(""amazon_polarity"") NonMatchingChecksumError","['Hi ! This issue may be related to #996 \r\nThis comes probably from the Quota Exceeded error from Google Drive.\r\nCan you try again tomorrow and see if you still have the error ?\r\n\r\nOn my side I didn\'t get any error today with `load_dataset(""amazon_polarity"")`'
 '+1 encountering this issue as well'
 '@lhoestq Hi! I encounter the same error when loading `yelp_review_full`.\r\n\r\n```\r\nfrom datasets import load_dataset\r\ndataset_yp = load_dataset(""yelp_review_full"")\r\n```\r\n\r\nWhen you say the ""Quota Exceeded from Google drive"". Is this a quota from the dataset owner? or the quota from our (the runner) Google Drive?'
 '+1 Also encountering this issue'
 '> When you say the ""Quota Exceeded from Google drive"". Is this a quota from the dataset owner? or the quota from our (the runner) Google Drive?\r\n\r\nEach file on Google Drive can be downloaded only a certain amount of times per day because of a quota. The quota is reset every day. So if too many people download the dataset the same day, then the quota is likely to exceed.\r\nThat\'s a really bad limitations of Google Drive and we should definitely find another host for these dataset than Google Drive.\r\nFor now I would suggest to wait and try again later..\r\n\r\nSo far the issue happened with CNN DailyMail, Amazon Polarity and Yelp Reviews. \r\nAre you experiencing the issue with other datasets ? @calebchiam @dtch1997 '
 ""@lhoestq Gotcha, that is quite problematic...for what it's worth, I've had no issues with the other datasets I tried, such as `yelp_reviews_full` and `amazon_reviews_multi`.""
 'Same issue today with ""big_patent"", though the symptoms are slightly different.\r\n\r\nWhen running\r\n\r\n```py\r\nfrom datasets import load_dataset\r\nload_dataset(""big_patent"", split=""validation"")\r\n```\r\n\r\nI get the following\r\n`FileNotFoundError: Local file \\huggingface\\datasets\\downloads\\6159313604f4f2c01e7d1cac52139343b6c07f73f6de348d09be6213478455c5\\bigPatentData\\train.tar.gz doesn\'t exist`\r\n\r\nI had to look into `6159313604f4f2c01e7d1cac52139343b6c07f73f6de348d09be6213478455c5`  (which is a file instead of a folder) and got the following:\r\n\r\n`<!DOCTYPE html><html><head><title>Google Drive - Quota exceeded</title><meta http-equiv=""content-type"" content=""text/html; charset=utf-8""/><link href=&#47;static&#47;doclist&#47;client&#47;css&#47;4033072956&#45;untrustedcontent.css rel=""stylesheet"" nonce=""JV0t61Smks2TEKdFCGAUFA""><link rel=""icon"" href=""//ssl.gstatic.com/images/branding/product/1x/drive_2020q4_32dp.png""/><style nonce=""JV0t61Smks2TEKdFCGAUFA"">#gbar,#guser{font-size:13px;padding-top:0px !important;}#gbar{height:22px}#guser{padding-bottom:7px !important;text-align:right}.gbh,.gbd{border-top:1px solid #c9d7f1;font-size:1px}.gbh{height:0;position:absolute;top:24px;width:100%}@media all{.gb1{height:22px;margin-right:.5em;vertical-align:top}#gbar{float:left}}a.gb1,a.gb4{text-decoration:underline !important}a.gb1,a.gb4{color:#00c !important}.gbi .gb4{color:#dd8e27 !important}.gbf .gb4{color:#900 !important}\r\n</style><script nonce=""iNUHigT+ENVQ3UZrLkFtRw""></script></head><body><div id=gbar><nobr><a target=_blank class=gb1 href=""https://www.google.fr/webhp?tab=ow"">Search</a> <a target=_blank class=gb1 href=""http://www.google.fr/imghp?hl=en&tab=oi"">Images</a> <a target=_blank class=gb1 href=""https://maps.google.fr/maps?hl=en&tab=ol"">Maps</a> <a target=_blank class=gb1 href=""https://play.google.com/?hl=en&tab=o8"">Play</a> <a target=_blank class=gb1 href=""https://www.youtube.com/?gl=FR&tab=o1"">YouTube</a> <a target=_blank class=gb1 href=""https://news.google.com/?tab=on"">News</a> <a target=_blank class=gb1 href=""https://mail.google.com/mail/?tab=om"">Gmail</a> <b class=gb1>Drive</b> <a target=_blank class=gb1 style=""text-decoration:none"" href=""https://www.google.fr/intl/en/about/products?tab=oh""><u>More</u> &raquo;</a></nobr></div><div id=guser width=100%><nobr><span id=gbn class=gbi></span><span id=gbf class=gbf></span><span id=gbe></span><a target=""_self"" href=""/settings?hl=en_US"" class=gb4>Settings</a> | <a target=_blank  href=""//support.google.com/drive/?p=web_home&hl=en_US"" class=gb4>Help</a> | <a target=_top id=gb_70 href=""https://accounts.google.com/ServiceLogin?hl=en&passive=true&continue=https://drive.google.com/uc%3Fexport%3Ddownload%26id%3D1J3mucMFTWrgAYa3LuBZoLRR3CzzYD3fa&service=writely&ec=GAZAMQ"" class=gb4>Sign in</a></nobr></div><div class=gbh style=left:0></div><div class=gbh style=right:0></div><div class=""uc-main""><div id=""uc-text""><p class=""uc-error-caption"">Sorry, you can&#39;t view or download this file at this time.</p><p class=""uc-error-subcaption"">Too many users have viewed or downloaded this file recently. Please try accessing the file again later. If the file you are trying to access is particularly large or is shared with many people, it may take up to 24 hours to be able to view or download the file. If you still can\'t access a file after 24 hours, contact your domain administrator.</p></div></div><div class=""uc-footer""><hr class=""uc-footer-divider"">&copy; 2021 Google - <a class=""goog-link"" href=""//support.google.com/drive/?p=web_home"">Help</a> - <a class=""goog-link"" href=""//support.google.com/drive/bin/answer.py?hl=en_US&amp;answer=2450387"">Privacy & Terms</a></div></body></html>`']","Hi, it seems that loading the amazon_polarity dataset gives a NonMatchingChecksumError.

To reproduce:
```
load_dataset(""amazon_polarity"")
```
This will give the following error:
```
---------------------------------------------------------------------------
NonMatchingChecksumError                  Traceback (most recent call last)
<ipython-input-3-8559a03fe0f8> in <module>()
----> 1 dataset = load_dataset(""amazon_polarity"")

3 frames
/usr/local/lib/python3.6/dist-packages/datasets/utils/info_utils.py in verify_checksums(expected_checksums, recorded_checksums, verification_name)
     37     if len(bad_urls) > 0:
     38         error_msg = ""Checksums didn't match"" + for_verification_name + "":\n""
---> 39         raise NonMatchingChecksumError(error_msg + str(bad_urls))
     40     logger.info(""All the checksums matched successfully"" + for_verification_name)
     41 

NonMatchingChecksumError: Checksums didn't match for dataset source files:
['https://drive.google.com/u/0/uc?id=0Bz8a_Dbh9QhbaW12WVVZS2drcnM&export=download']
```"
https://github.com/huggingface/datasets/issues/1854,Feature Request: Dataset.add_item,"['Hi @sshleifer.\r\n\r\nI am not sure of understanding the need of the `add_item` approach...\r\n\r\nBy just reading your ""Desired API"" section, I would say you could (nearly) get it with a 1-column Dataset:\r\n```python\r\ndata = {""input_ids"": [np.array([4,4,2]), np.array([8,6,5,5,2]), np.array([3,3,31,5])]}\r\nds = Dataset.from_dict(data)\r\nassert (ds[""input_ids""][0] == np.array([4,4,2])).all()\r\n```'
 'Hi @sshleifer :) \r\n\r\nWe don\'t have methods like `Dataset.add_batch` or `Dataset.add_entry/add_item` yet.\r\nBut that\'s something we\'ll add pretty soon. Would an API that looks roughly like this help ? Do you have suggestions ?\r\n```python\r\nimport numpy as np\r\nfrom datasets import Dataset\r\n\r\ntokenized = [np.array([4,4,2]), np.array([8,6,5,5,2]), np.array([3,3,31,5])\r\n\r\n# API suggestion (not available yet)\r\nd = Dataset()\r\nfor input_ids in tokenized:\r\n    d.add_item({""input_ids"": input_ids})\r\n\r\nprint(d[0][""input_ids""])\r\n# [4, 4, 2]\r\n```\r\n\r\nCurrently you can define a dataset with what @albertvillanova suggest, or via a generator using dataset builders. It\'s also possible to [concatenate datasets](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=concatenate#datasets.concatenate_datasets).'
 'Your API looks perfect @lhoestq, thanks!']","I'm trying to integrate `huggingface/datasets` functionality into `fairseq`, which requires (afaict) being able to build a dataset through an `add_item` method, such as https://github.com/pytorch/fairseq/blob/master/fairseq/data/indexed_dataset.py#L318, as opposed to loading all the text into arrow, and then `dataset.map(binarizer)`.
Is this possible at the moment? Is there an example? I'm happy to use raw `pa.Table` but not sure whether it will support uneven length entries.

### Desired API

```python
import numpy as np
tokenized: List[np.NDArray[np.int64]] = [np.array([4,4,2]), np.array([8,6,5,5,2]), np.array([3,3,31,5])

def build_dataset_from_tokenized(tokenized: List[np.NDArray[int]]) -> Dataset:
   """"""FIXME""""""
   dataset = EmptyDataset()
   for t in tokenized: dataset.append(t)
   return dataset
ds = build_dataset_from_tokenized(tokenized)
assert (ds[0] == np.array([4,4,2])).all()
```

### What I tried
grep, google for ""add one entry at a time"", ""datasets.append""

### Current Code
This code achieves the same result but doesn't fit into the `add_item` abstraction.

```python
    dataset = load_dataset('text', data_files={'train': 'train.txt'})
    tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', max_length=4096)
    def tokenize_function(examples):
        ids = tokenizer(examples['text'], return_attention_mask=False)['input_ids']
        return {'input_ids': [x[1:] for x in ids]}
    ds = dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=['text'], load_from_cache_file=not overwrite_cache)
	print(ds['train'][0]) => np array
```

Thanks in advance!"
https://github.com/huggingface/datasets/issues/1849,Add TIMIT,"['@patrickvonplaten Could you please help me with how the output text has to be represented in the data? TIMIT has Words, Phonemes and texts. Also has lot on info on the speaker and the dialect. Could you please help me? An example of how to arrange it would be super helpful!\r\n\r\n'
 'Hey @vrindaprabhu - sure I\'ll help you :-) Could you open a first PR for TIMIT where you copy-paste more or less the `librispeech_asr` script: https://github.com/huggingface/datasets/blob/28be129db862ec89a87ac9349c64df6b6118aff4/datasets/librispeech_asr/librispeech_asr.py#L93 (obviously replacing all the naming and links correctly...) and then you can list all possible outputs in the features dict: https://github.com/huggingface/datasets/blob/28be129db862ec89a87ac9349c64df6b6118aff4/datasets/librispeech_asr/librispeech_asr.py#L104 (words, phonemes should probably be of kind `datasets.Sequence(datasets.Value(""string""))` and texts I think should be of type `""text"": datasets.Value(""string"")`.\r\n\r\nWhen you\'ve opened a first PR, I think it\'ll be much easier for us to take a look together :-) '
 'I am sorry! I created the PR [#1903](https://github.com/huggingface/datasets/pull/1903#). Requesting your comments! CircleCI tests are failing, will address them along with your comments!']","## Adding a Dataset
- **Name:** *TIMIT*
- **Description:** *The TIMIT corpus of read speech has been designed to provide speech data for the acquisition of acoustic-phonetic knowledge and for the development and evaluation of automatic speech recognition systems*

- **Paper:** *Homepage*: http://groups.inf.ed.ac.uk/ami/corpus/ / *Wikipedia*: https://en.wikipedia.org/wiki/TIMIT
- **Data:** *https://deepai.org/dataset/timit*
- **Motivation:** Important speech dataset


If interested in tackling this issue, feel free to tag @patrickvonplaten


Instructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).
"
https://github.com/huggingface/datasets/issues/1844,Update Open Subtitles corpus with original sentence IDs,"[""Hi ! You're right this can can useful.\r\nThis should be easy to add, so feel free to give it a try if you want to contribute :)\r\nI think we just need to add it to the _generate_examples method of the OpenSubtitles dataset builder [here](https://github.com/huggingface/datasets/blob/master/datasets/open_subtitles/open_subtitles.py#L103)""
 ""Hey @lhoestq , absolutely yes! Just one question before I start implementing. The ids found in the zip file have this format: \r\n(the following is line `22497315` of the `ids` file of the `de-en` dump)\r\n\r\n\r\n`de/2017/7006210/7063319.xml.gz  en/2017/7006210/7050201.xml.gz  335     339 340` (every space is actually a tab, aside from the space between `339` and `340`)\r\n\r\n\r\nWhere filenames encode the information like this: `lang/year/imdb_id/opensubtitles_id.xml.gz` whereas the numbers correspond to the sentence ids which are linked together (i.e. sentence `335` of the German subtitle corresponds to lines `339` and `340` of the English file)\r\n\r\nThat being said, do you think I should stick to the raw sentence id (and replace the current sequential id) or should I include more detailed metadata (or both things maybe)?\r\n\r\nGoing with raw ID is surely simpler, but including `year`, `imdbId` and `subtitleId` should save space as they're just integers; besides, any operation (like filtering or grouping) will be much easier if users don't have to manually parse the ids every time.\r\nAs for the language-specific sentenceIds, what could be the best option? A list of integers or a comma-separated string?\r\n\r\n**Note:** I did not find any official information about this encoding, but it appears to check out:\r\nhttps://www.imdb.com/title/tt7006210/, https://www.opensubtitles.org/en/subtitles/7063319 and https://www.opensubtitles.org/en/subtitles/7050201 all link to the same episode, so I guess (I hope!) it's correct.\r\n\r\n""
 'I like the idea of having `year`, `imdbId` and `subtitleId` as columns for filtering for example.\r\nAnd for the `sentenceIds` a list of integers is fine.'
 'Thanks for improving it @Valahaar :) '
 'Something like this? (adapted from [here](https://github.com/huggingface/datasets/blob/master/datasets/open_subtitles/open_subtitles.py#L114))\r\n\r\n```python\r\nresult = (\r\n  sentence_counter,\r\n    {\r\n      ""id"": str(sentence_counter),\r\n      ""meta"": {\r\n        ""year"": year,\r\n        ""imdbId"": imdb_id,\r\n        ""subtitleId"": {l1: l1_sub_id, l2: l2_sub_id},\r\n        ""sentenceIds"": {l1: [... source_sids ...], l2: [... target_sids ...]},\r\n        # or maybe src/tgt? I\'d go with the first one for consistency with \'translation\'\r\n        ""subtitleId"": {""src"": l1_sub_id, ""tgt"": l2_sub_id},\r\n        ""sentenceIds"": {""src"": [... source_sids ...], ""tgt"": [... target_sids ...]},\r\n      },\r\n      ""translation"": {l1: x, l2: y},\r\n    },\r\n  )\r\n```\r\nOr at top level, avoiding nesting into \'meta\'?'
 'Merged in #1865, closing. Thanks :)']","Hi! It would be great if you could add the original sentence ids to [Open Subtitles](https://huggingface.co/datasets/open_subtitles).

I can think of two reasons: first, it's possible to gather sentences for an entire document (the original ids contain media id, subtitle file id and sentence id), therefore somewhat allowing for document-level machine translation (and other document-level stuff which could be cool to have); second, it's possible to have parallel sentences in multiple languages, as they share the same ids across bitexts.

I think I should tag @abhishekkrthakur as he's the one who added it in the first place.

Thanks!"
https://github.com/huggingface/datasets/issues/1843,MustC Speech Translation,"['Hi @patrickvonplaten  I would like to work on this dataset. \r\n\r\nThanks! '
 ""That's awesome! Actually, I just noticed that this dataset might become a bit too big!\r\n\r\nMuST-C is the main dataset used for IWSLT19 and should probably be added as a standalone dataset. Would you be interested also in adding `datasets/MuST-C` instead?\r\n\r\nDescription: \r\n_MuST-C is a multilingual speech translation corpus whose size and quality facilitates the training of end-to-end systems for speech translation from English into several languages. For each target language, MuST-C comprises several hundred hours of audio recordings from English TED Talks, which are automatically aligned at the sentence level with their manual transcriptions and translations._\r\n\r\nPaper: https://www.aclweb.org/anthology/N19-1202.pdf\r\n\r\nDataset: https://ict.fbk.eu/must-c/ (One needs to fill out a short from to download the data, but it's very easy).\r\n\r\nIt would be awesome if you're interested in adding this datates. I'm very happy to guide you through the PR! I think the easiest way to start would probably be to read [this README on how to add a dataset](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md) and open a PR. Think you can copy & paste some code from:\r\n\r\n- Librispeech_asr: https://github.com/huggingface/datasets/blob/master/datasets/librispeech_asr/librispeech_asr.py\r\n- Flores Translation: https://github.com/huggingface/datasets/blob/master/datasets/flores/flores.py\r\n\r\nThink all the rest can be handled on the PR :-) ""
 'Hi @patrickvonplaten \r\nI have tried downloading this dataset, but the connection seems to reset all the time. I have tried it via the browser, wget, and using gdown . But it gives me an error message. _""The server is busy or down, pls try again""_ (rephrasing the message here)\r\n\r\nI have completed adding 4 datasets in the previous data sprint (including the IWSLT dataset #1676 ) ...so just checking if you are able to download it at your end. Otherwise will write to the dataset authors to update the links. \r\n\r\n\r\n\r\n\r\n'
 'Let me check tomorrow! Thanks for leaving this message!'
 'cc @patil-suraj for notification '
 ""@skyprince999, I think I'm getting the same error you're getting :-/\r\n\r\n```\r\nSorry, you can't view or download this file at this time.\r\n\r\nToo many users have viewed or downloaded this file recently. Please try accessing the file again later. If the file you are trying to access is particularly large or is shared with many people, it may take up to 24 hours to be able to view or download the file. If you still can't access a file after 24 hours, contact your domain administrator.\r\n```\r\n\r\nIt would be great if you could write the authors to see whether they can fix it.\r\nAlso cc @lhoestq - do you think we could mirror the dataset? ""
 ""Also there are huge those datasets. Think downloading MuST-C v1.2 amounts to ~ 1000GB... because there are 14 possible configs each around 60-70GB. I think users mostly will only use one of the 14 configs so that they would only need, in theory, will have to download ~60GB which is ok. But I think this functionality doesn't exist yet in `datasets` no? cc @lhoestq ""
 ""> Also cc @lhoestq - do you think we could mirror the dataset?\r\n\r\nYes we can mirror it if the authors are fine with it. You can create a dataset repo on huggingface.co (possibly under the relevant org) and add the mirrored data files.\r\n\r\n> I think users mostly will only use one of the 14 configs so that they would only need, in theory, will have to download ~60GB which is ok. But I think this functionality doesn't exist yet in datasets no? cc @lhoestq\r\n\r\nIf there are different download links for each configuration we can make the dataset builder download only the files related to the requested configuration.""
 ""I have written to the dataset authors, highlighting this issue. Waiting for their response. \r\n\r\nUpdate on 25th Feb: \r\nThe authors have replied back, they are updating the download link and will revert back shortly! \r\n\r\n```\r\nfirst of all thanks a lot for being interested in MuST-C and for building the data-loader.\r\n\r\nBefore answering your request, I'd like to clarify that the creation, maintenance, and expansion of MuST-c are not supported by any funded project, so this means that we need to find economic support for all these activities. This also includes permanently moving all the data to AWS or GCP.  We are working at this with the goal of facilitating the use of MuST-C, but this is not something that can happen today. We hope to have some news ASAP and you will be among the first to be informed.\r\n\r\nI hope you understand our situation.\r\n```\r\n\r\n""
 ""Awesome, actually @lhoestq let's just ask the authors if we should host the dataset no? They could just use our links then as well for their website - what do you think? Is it fine to use our AWS dataset storage also as external links? ""
 'Yes definitely. Shall we suggest them to create a dataset repository under their org on huggingface.co ? @julien-c \r\nThe dataset is around 1TB'
 'Sounds good! \r\n\r\nOrder of magnitude is storage costs ~$20 per TB per month (not including bandwidth). \r\n\r\nHappy to provide this to the community as I feel this is an important dataset. Let us know what the authors want to do!\r\n\r\n'
 'Great! @skyprince999, do you think you could ping the authors here or link to this thread? I think it could be a cool idea to host the dataset on our side then'
 'Done. They replied back, and they want to have a call over a meet/ skype. Is that possible ? \r\nBtw @patrickvonplaten you are looped in that email (_pls check you gmail account_)  '
 'Hello! Any news on this?'
 ""@gegallego  there were some concerns regarding dataset usage & attribution by a for-profit company, so couldn't take it forward. Also the download links were unstable. \r\nBut I guess if you want to test the fairseq benchmarks, you can connect with them directly for downloading the dataset.  ""
 'Yes, that dataset is not easy to download... I had to copy it to my Google Drive and use `rsync` to be able to download it.\r\nHowever, we could add the dataset with a manual download, right?'
 ""yes that is possible. I couldn't unfortunately complete this PR, If you would like to add it, please feel free to do it. ""]","## Adding a Dataset
- **Name:** *IWSLT19*
- **Description:** *The Speech Translation Task addresses the translation of English audio into German and Portuguese text.*
- **Hompage:** *https://sites.google.com/view/iwslt-evaluation-2019/speech-translation*
- **Data:** *https://sites.google.com/view/iwslt-evaluation-2019/speech-translation* - all data under ""Allowed Training Data"" and ""Development and Evalutaion Data for TED/How2""
- **Motivation:** Important speech dataset

If interested in tackling this issue, feel free to tag @patrickvonplaten

Instructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).
"
https://github.com/huggingface/datasets/issues/1840,Add common voice,"['I have started working on adding this dataset.'
 'Hey @BirgerMoell - awesome that you started working on Common Voice. Common Voice is a bit special since, there is no direct download link to download the data. In these cases we usually consider two options:\r\n\r\n1) Find a hacky solution to extract the download link somehow from the XLM tree of the website \r\n2) If this doesn\'t work we force the user to download the data himself and add a `""data_dir""` as an input parameter. E.g. you can take a look at how it is done for [this](https://github.com/huggingface/datasets/blob/66f2a7eece98d2778bd22bb5034cb7c2376032d4/datasets/arxiv_dataset/arxiv_dataset.py#L66) \r\n\r\nAlso the documentation here: https://huggingface.co/docs/datasets/add_dataset.html?highlight=data_dir#downloading-data-files-and-organizing-splits (especially the ""note"") might be helpful.'
 'Let me know if you have any other questions'
 ""I added a Work in Progress pull request (hope that is ok). I've made a card for the dataset and filled out the common_voice.py file with information about the datset (not completely).\r\n\r\nI didn't manage to get the tagging tool working locally on my machine but will look into that later.\r\n\r\nLeft to do.\r\n\r\n- Tag the dataset\r\n- Add missing information and update common_voice.py\r\n\r\nhttps://github.com/huggingface/datasets/pull/1886""
 'Awesome! I left a longer comment on the PR :-)']","## Adding a Dataset
- **Name:** *common voice*
- **Description:** *Mozilla Common Voice Dataset*
- **Paper:** Homepage: https://voice.mozilla.org/en/datasets
- **Data:** https://voice.mozilla.org/en/datasets
- **Motivation:** Important speech dataset
- **TFDatasets Implementation**: https://www.tensorflow.org/datasets/catalog/common_voice
If interested in tackling this issue, feel free to tag @patrickvonplaten

Instructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).
"
https://github.com/huggingface/datasets/issues/1838,Add tedlium,['Hi @patrickvonplaten \r\nI can have a look to this dataset later since I am trying to add the OpenSLR dataset https://github.com/huggingface/datasets/pull/2173\r\nHopefully I have enough space since the compressed file is 21GB. The release 3 is even bigger: 54GB :-0'],"## Adding a Dataset
- **Name:** *tedlium*
- **Description:** *The TED-LIUM 1-3 corpus is English-language TED talks, with transcriptions, sampled at 16kHz. It contains about 118 hours of speech.*
- **Paper:** Homepage: http://www.openslr.org/7/, https://lium.univ-lemans.fr/en/ted-lium2/ &, https://www.openslr.org/51/
- **Data:** http://www.openslr.org/7/
- **Motivation:** Important speech dataset
- **TFDatasets Implementation**: https://www.tensorflow.org/datasets/catalog/tedlium
If interested in tackling this issue, feel free to tag @patrickvonplaten

Instructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).
"
https://github.com/huggingface/datasets/issues/1836,test.json has been removed from the limit dataset repo (breaks dataset),"[""Thanks for the heads up ! I'm opening a PR to fix that""]","https://github.com/huggingface/datasets/blob/16042b233dbff2a7585110134e969204c69322c3/datasets/limit/limit.py#L51

The URL is not valid anymore since test.json has been removed in master for some reason. Directly referencing the last commit works:

`https://raw.githubusercontent.com/ilmgut/limit_dataset/0707d3989cd8848f0f11527c77dcf168fefd2b23/data`"
https://github.com/huggingface/datasets/issues/1831,Some question about raw dataset download info in the project .,"['Hi ! The `dl_manager` is a `DownloadManager` object and is responsible for downloading the raw data files.\r\nIt is used by dataset builders in their `_split_generators` method to download the raw data files that are necessary to build the datasets splits.\r\n\r\nThe `Conll2003` class is a dataset builder, and so you can download all the raw data files by calling `_split_generators` with a download manager:\r\n```python\r\nfrom datasets import DownloadManager\r\nfrom datasets.load import import_main_class\r\n\r\nconll2003_builder = import_main_class(...)\r\n\r\ndl_manager = DownloadManager()\r\nsplis_generators = conll2003_builder._split_generators(dl_manager)\r\n```\r\n\r\nThen you can see what files have been downloaded with\r\n```python\r\ndl_manager.get_recorded_sizes_checksums()\r\n```\r\nIt returns a dictionary with the format {url: {num_bytes: int, checksum: str}}\r\n\r\nThen you can get the actual location of the downloaded files with\r\n```python\r\nfrom datasets import cached_path\r\n\r\nlocal_path_to_downloaded_file = cached_path(url)\r\n```\r\n\r\n------------------\r\n\r\nNote that you can also get the urls from the Dataset object:\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nconll2003 = load_dataset(""conll2003"")\r\nprint(conll2003[""train""].download_checksums)\r\n```\r\nIt returns the same dictionary with the format {url: {num_bytes: int, checksum: str}}'
 'I am afraid that there is not a very straightforward way to get that location.\r\n\r\nAnother option, from _split_generators would be to use:\r\n- `dl_manager._download_config.cache_dir` to get the directory where all the raw downloaded files are:\r\n  ```python\r\n  download_dir = dl_manager._download_config.cache_dir\r\n  ```\r\n- the function `datasets.utils.file_utils.hash_url_to_filename` to get the filenames of the raw downloaded files:\r\n  ```python\r\n  filenames = [hash_url_to_filename(url) for url in urls_to_download.values()]\r\n  ```\r\nTherefore the complete path to the raw downloaded files would be the join of both:\r\n```python\r\ndownloaded_paths = [os.path.join(download_dir, filename) for filename in filenames]\r\n```\r\n\r\nMaybe it would be interesting to make these paths accessible more easily. I could work on this. What do you think, @lhoestq ?'
 'Sure it would be nice to have an easier access to these paths !\r\nThe dataset builder could have a method to return those, what do you think ?\r\nFeel free to work on this @albertvillanova , it would be a nice addition :) \r\n\r\nYour suggestion does work as well @albertvillanova if you complete it by specifying `etag=` to `hash_url_to_filename`.\r\n\r\nThe ETag is obtained by a HEAD request and is used to know if the file on the remote host has changed. Therefore if a file is updated on the remote host, then the hash returned by `hash_url_to_filename` is different.'
 'Once #1846 will be merged, the paths to the raw downloaded files will be accessible as:\r\n```python\r\nbuilder_instance.dl_manager.downloaded_paths\r\n``` ']","Hi , i review the code in 
https://github.com/huggingface/datasets/blob/master/datasets/conll2003/conll2003.py
in the _split_generators function is the truly logic of download raw datasets with dl_manager
and use Conll2003 cls by use import_main_class in load_dataset function
My question is that , with this logic it seems that i can not have the raw dataset download location
in variable in downloaded_files in _split_generators.
If someone also want use huggingface datasets as raw dataset downloader,
how can he retrieve the raw dataset download path from attributes in 
datasets.dataset_dict.DatasetDict ?"
https://github.com/huggingface/datasets/issues/1830,using map on loaded Tokenizer 10x - 100x slower than default Tokenizer?,"[""Hi @wumpusman \r\n`datasets` has a caching mechanism that allows to cache the results of `.map` so that when you want to re-run it later it doesn't recompute it again.\r\nSo when you do `.map`, what actually happens is:\r\n1. compute the hash used to identify your `map` for the cache\r\n2. apply your function on every batch\r\n\r\nThis can explain the time difference between your different experiments.\r\n\r\nThe hash computation time depends of how complex your function is. For a tokenizer, the hash computation scans the lists of the words in the tokenizer to identify this tokenizer. Usually it takes 2-3 seconds.\r\n\r\nAlso note that you can disable caching though using\r\n```python\r\nimport datasets\r\n\r\ndatasets.set_caching_enabled(False)\r\n```""
 ""Hi @lhoestq ,\r\n\r\nThanks for the reply. It's entirely possible that is the issue. Since it's a side project I won't be looking at it till later this week, but, I'll verify it by disabling caching and hopefully I'll see the same runtime. \r\n\r\nAppreciate the reference,\r\n\r\nMichael""
 ""I believe this is an actual issue, tokenizing a ~4GB txt file went from an hour and a half to ~10 minutes when I switched from my pre-trained tokenizer(on the same dataset) to the default gpt2 tokenizer.\r\nBoth were loaded using:\r\n```\r\nAutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\r\n```\r\nI trained the tokenizer using ByteLevelBPETokenizer from the Tokenizers library and save it to a tokenizer.json file.\r\n\r\nI have tested the caching ideas above, changing the number of process, the TOKENIZERS_PARALLELISM env variable, keep_in_memory=True and batching with different sizes.\r\n\r\nApologies I can't really upload much code, but wanted to back up the finding and hopefully a fix/the problem can be found.\r\nI will comment back if I find a fix as well.""
 'Hi @johncookds do you think this can come from one tokenizer being faster than the other one ? Can you try to compare their speed without using `datasets` just to make sure ?'
 ""Hi yes, I'm closing the loop here with some timings below. The issue seems to be at least somewhat/mainly with the tokenizer's themselves. Moreover legacy saves of the trainer tokenizer perform faster but differently than the new tokenizer.json saves(note nothing about the training process/adding of special tokens changed between the top two trained tokenizer tests, only the way it was saved). This is only a 3x slowdown vs like a 10x but I think the slowdown is most likely due to this.\r\n\r\n```\r\ntrained tokenizer - tokenizer.json save (same results for AutoTokenizer legacy_format=False):\r\nTokenizer time(seconds): 0.32767510414123535\r\nTokenized avg. length: 323.01\r\n\r\ntrained tokenizer - AutoTokenizer legacy_format=True:\r\nTokenizer time(seconds): 0.09258866310119629\r\nTokenized avg. length: 301.01\r\n\r\nGPT2 Tokenizer from huggingface\r\nTokenizer time(seconds): 0.1010282039642334\r\nTokenized avg. length: 461.21\r\n```""
 '@lhoestq ,\r\n\r\nHi, which version of datasets has datasets.set_caching_enabled(False)? I get \r\nmodule \'datasets\' has no attribute \'set_caching_enabled\'. To hopefully get around this, I reran my code on a new set of data, and did so only once.\r\n\r\n@johncookds , thanks for chiming in, it looks this might be an issue of Tokenizer.\r\n\r\n**Tokenizer**: The runtime of GPT2TokenizerFast.from_pretrained(""gpt2"") on 1000 chars is: **143 ms**\r\n**SlowTokenizer**: The runtime of a locally saved and loaded Tokenizer using the same vocab  on 1000 chars is:  **4.43 s**\r\n\r\nThat being said, I compared performance on the map function:\r\n\r\nRunning Tokenizer versus using it in the map function for 1000 chars goes from **141 ms** to **356 ms** \r\nRunning SlowTokenizer versus using it in the map function for 1000 chars with a single element goes from **4.43 s** to **9.76 s**\r\n\r\nI\'m trying to figure out why the overhead of map would increase the time by double (figured it would be a fixed increase in time)? Though maybe this is expected behavior.\r\n\r\n@lhoestq, do you by chance know how I can redirect this issue to Tokenizer?\r\n\r\nRegards,\r\n\r\nMichael'
 ""Thanks for the experiments @johncookds and @wumpusman ! \r\n\r\n> Hi, which version of datasets has datasets.set_caching_enabled(False)?\r\n\r\nCurrently you have to install `datasets` from source to have this feature, but this will be available in the next release in a few days.\r\n\r\n> I'm trying to figure out why the overhead of map would increase the time by double (figured it would be a fixed increase in time)? Though maybe this is expected behavior.\r\n\r\nCould you also try with double the number of characters ? This should let us have an idea of the fixed cost (hashing) and the dynamic cost (actual tokenization, grows with the size of the input)\r\n\r\n> @lhoestq, do you by chance know how I can redirect this issue to Tokenizer?\r\n\r\nFeel free to post an issue on the `transformers` repo. Also I'm sure there should be related issues so you can also look for someone with the same concerns on the `transformers` repo.""
 ""@lhoestq,\r\n\r\nI just checked that previous run time was actually  3000 chars. I increased it to 6k chars, again, roughly double.\r\n\r\nSlowTokenizer **7.4 s** to **15.7 s**\r\nTokenizer: **276 ms** to **616 ms**\r\n\r\nI'll post this issue on Tokenizer, seems it hasn't quite been raised (albeit I noticed a similar issue that might relate).\r\n\r\nRegards,\r\n\r\nMichael""
 ""Hi, \r\nI'm following up here as I found my exact issue. It was with saving and re-loading the tokenizer. When I trained then processed the data without saving and reloading it, it was 10x-100x faster than when I saved and re-loaded it.\r\nBoth resulted in the exact same tokenized datasets as well. \r\nThere is additionally a bug where the older legacy tokenizer save does not preserve a learned tokenizing behavior if trained from scratch.\r\nUnderstand its not exactly Datasets related but hope it can help someone if they have the same issue.\r\nThanks!""]","This could total relate to me misunderstanding particular call functions, but I added words to a GPT2Tokenizer, and saved it to disk (note I'm only showing snippets but I can share more) and the map function ran much slower: 

````
def save_tokenizer(original_tokenizer,text,path=""simpledata/tokenizer""):
    words_unique = set(text.split("" ""))
    for i in words_unique:
        original_tokenizer.add_tokens(i)
    original_tokenizer.save_pretrained(path)

tokenizer2 = GPT2Tokenizer.from_pretrained(os.path.join(experiment_path,experiment_name,""tokenizer_squad""))

train_set_baby=Dataset.from_dict({""text"":[train_set[""text""][0][0:50]]})
````

I then applied the dataset map function on a fairly small set of text:

```
%%time
train_set_baby = train_set_baby.map(lambda d:tokenizer2(d[""text""]),batched=True)

```


The run time for train_set_baby.map was 6 seconds, and the batch itself was 2.6 seconds

**100% 1/1 [00:02<00:00, 2.60s/ba] CPU times: user 5.96 s, sys: 36 ms, total: 5.99 s Wall time: 5.99 s**

In comparison using (even after adding additional tokens): 
`
tokenizer = GPT2TokenizerFast.from_pretrained(""gpt2"")`

```
%%time
train_set_baby = train_set_baby.map(lambda d:tokenizer2(d[""text""]),batched=True)

```
The time is 
**100% 1/1 [00:00<00:00, 34.09ba/s] CPU times: user 68.1 ms, sys: 16 µs, total: 68.1 ms Wall time: 62.9 ms**

It seems this might relate to the tokenizer save or load function, however, the issue appears to come up when I apply the loaded tokenizer  to the map function. 

I should also add that playing around with the amount of words I add to the tokenizer before I save it to disk and load it into memory  appears to impact the time it takes to run the map function. 


"
https://github.com/huggingface/datasets/issues/1827,Regarding On-the-fly Data Loading,"['Possible duplicate\r\n\r\n#1776 https://github.com/huggingface/datasets/issues/\r\n\r\nreally looking PR for this feature'
 'Hi @acul3 \r\n\r\nIssue #1776 talks about  doing on-the-fly data pre-processing, which I think is solved in the next release as mentioned in the issue #1825. I also look forward to using this feature, though :)\r\n\r\nI wanted to ask about on-the-fly data loading from the cache (before pre-processing).'
 ""Hi ! Currently when you load a dataset via `load_dataset` for example, then the dataset is memory-mapped from an Arrow file on disk. Therefore there's almost no RAM usage even if your dataset contains TB of data.\r\nUsually at training time only one batch of data at a time is loaded in memory.\r\n\r\nDoes that answer your question or were you thinking about something else ?""
 'Hi @lhoestq,\r\n\r\nI apologize for the late response. This answers my question. Thanks a lot.']","Hi,

I was wondering if it is possible to load images/texts as a batch during the training process, without loading the entire dataset on the RAM at any given point.

Thanks,
Gunjan"
https://github.com/huggingface/datasets/issues/1825,Datasets library not suitable for huge text datasets.,"['Hi ! Looks related to #861 \r\n\r\nYou are right: tokenizing a dataset using map takes a lot of space since it can store `input_ids` but also `token_type_ids`, `attention_mask` and `special_tokens_mask`. Moreover if your tokenization function returns python integers then by default they\'ll be stored as int64 which can take a lot of space. Padding can also increase the size of the tokenized dataset.\r\n\r\nTo make things more convenient, we recently added a ""lazy map"" feature that allows to tokenize each batch at training time as you mentioned. For example you\'ll be able to do\r\n```python\r\nfrom transformers import BertTokenizer\r\n\r\ntokenizer = BertTokenizer.from_pretrained(""bert-base-uncased"")\r\n\r\ndef encode(batch):\r\n    return tokenizer(batch[""text""], padding=""longest"", truncation=True, max_length=512, return_tensors=""pt"")\r\n\r\ndataset.set_transform(encode)\r\nprint(dataset.format)\r\n# {\'type\': \'custom\', \'format_kwargs\': {\'transform\': <function __main__.encode(batch)>}, \'columns\': [\'idx\', \'label\', \'sentence1\', \'sentence2\'], \'output_all_columns\': False}\r\nprint(dataset[:2])\r\n# {\'input_ids\': tensor([[  101,  2572,  3217, ... 102]]), \'token_type_ids\': tensor([[0, 0, 0, ... 0]]), \'attention_mask\': tensor([[1, 1, 1, ... 1]])}\r\n\r\n```\r\nIn this example the `encode` transform is applied on-the-fly on the ""text"" column.\r\n\r\nThis feature will be available in the next release 2.0 which will happen in a few days.\r\nYou can already play with it by installing `datasets` from source if you want :)\r\n\r\nHope that helps !'
 ""How recently was `set_transform` added? I am actually trying to implement it and getting an error:\r\n\r\n`AttributeError: 'Dataset' object has no attribute 'set_transform'\r\n`\r\n\r\nI'm on v.1.2.1.\r\n\r\nEDIT: Oh, wait I see now it's in the v.2.0. Whoops! This should be really useful.""
 ""Yes indeed it was added a few days ago. The code is available on master\r\nWe'll do a release next week :)\r\n\r\nFeel free to install `datasets` from source to try it out though, I would love to have some feedbacks""
 ""For information: it's now available in `datasets` 1.3.0.\r\nThe 2.0 is reserved for even cooler features ;)""
 ""Hi @alexvaca0 , we have optimized Datasets' disk usage in the latest release v1.5.\r\n\r\nFeel free to update your Datasets version\r\n```shell\r\npip install -U datasets\r\n```\r\nand see if it better suits your needs.""]","Hi,

I'm trying to use datasets library to load a 187GB dataset of pure text, with the intention of building a Language Model. The problem is that from the 187GB it goes to some TB when processed by Datasets. First of all, I think the pre-tokenizing step (with tokenizer.map()) is not really thought for datasets this big, but for fine-tuning datasets, as this process alone takes so much time, usually in expensive machines (due to the need of tpus - gpus) which is not being used for training. It would possibly be more efficient in such cases to tokenize each batch at training time (receive batch - tokenize batch - train with batch), so that the whole time the machine is up it's being used for training. 
Moreover, the pyarrow objects created from a 187 GB datasets are huge, I mean, we always receive OOM, or No Space left on device errors when only 10-12% of the dataset has been processed, and only that part occupies 2.1TB in disk, which is so many times the disk  usage of the pure text (and this doesn't make sense, as tokenized texts should be lighter than pure texts).

Any suggestions??"
https://github.com/huggingface/datasets/issues/1821,Provide better exception message when one of many files results in an exception,"['Hi!\r\n\r\nThank you for reporting this issue. I agree that the information about the exception should be more clear and explicit.\r\n\r\nI could take on this issue.\r\n\r\nOn the meantime, as you can see from the exception stack trace, HF Datasets uses pandas to read the CSV files. You can pass arguments to `pandas.read_csv` by passing additional keyword arguments to `load_dataset`. For example, you may find useful this argument:\r\n- `error_bad_lines` : bool, default True\r\n  Lines with too many fields (e.g. a csv line with too many commas) will by default cause an exception to be raised, and no DataFrame will be returned. If False, then these “bad lines” will be dropped from the DataFrame that is returned.\r\n\r\nYou could try:\r\n```python\r\ndatasets = load_dataset(""csv"", data_files=dict(train=train_files, validation=validation_files), error_bad_lines=False)\r\n```\r\n']","I find when I process many files, i.e.

```
train_files = glob.glob('rain*.csv')
validation_files = glob.glob(validation*.csv')
datasets = load_dataset(""csv"", data_files=dict(train=train_files, validation=validation_files))
```

I sometimes encounter an error due to one of the files being misformed (i.e. no data, or a comma in a field that isn't quoted, etc).

For example, this is the tail of an exception which I suspect is due to a stray comma.

>   File ""pandas/_libs/parsers.pyx"", line 756, in pandas._libs.parsers.TextReader.read
>   File ""pandas/_libs/parsers.pyx"", line 783, in pandas._libs.parsers.TextReader._read_low_memory
>   File ""pandas/_libs/parsers.pyx"", line 827, in pandas._libs.parsers.TextReader._read_rows
>   File ""pandas/_libs/parsers.pyx"", line 814, in pandas._libs.parsers.TextReader._tokenize_rows
>   File ""pandas/_libs/parsers.pyx"", line 1951, in pandas._libs.parsers.raise_parser_error
> pandas.errors.ParserError: Error tokenizing data. C error: Expected 2 fields in line 559, saw 3

It would be nice if the exception trace contained the name of the file being processed (I have 250 separate files!)"
https://github.com/huggingface/datasets/issues/1818,Loading local dataset raise requests.exceptions.ConnectTimeout,"['Hi ! Thanks for reporting. This was indeed a bug introduced when we moved the `json` dataset loader inside the `datasets` package (before that, the `json` loader was fetched online, as all the other dataset scripts).\r\n\r\nThis should be fixed on master now. Feel free to install `datasets` from source to try it out.\r\nThe fix will be available in the next release of `datasets` in a few days']","Load local dataset:
```
dataset = load_dataset('json', data_files=[""../../data/json.json""])
train = dataset[""train""]
print(train.features)
train1 = train.map(lambda x: {""labels"": 1})
print(train1[:2])
```

but it raised requests.exceptions.ConnectTimeout:

```
/Users/littlely/myvirtual/tf2/bin/python3.7 /Users/littlely/projects/python_projects/pytorch_learning/nlp/dataset/transformers_datasets.py
Traceback (most recent call last):
  File ""/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/urllib3/connection.py"", line 160, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw
  File ""/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/urllib3/util/connection.py"", line 84, in create_connection
    raise err
  File ""/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/urllib3/util/connection.py"", line 74, in create_connection
    sock.connect(sa)
socket.timeout: timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 677, in urlopen
    chunked=chunked,
  File ""/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 381, in _make_request
    self._validate_conn(conn)
  File ""/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 978, in _validate_conn
    conn.connect()
  File ""/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/urllib3/connection.py"", line 309, in connect
    conn = self._new_conn()
  File ""/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/urllib3/connection.py"", line 167, in _new_conn
    % (self.host, self.timeout),
urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPSConnection object at 0x1181e9940>, 'Connection to s3.amazonaws.com timed out. (connect timeout=10)')

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send
    timeout=timeout
  File ""/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 727, in urlopen
    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]
  File ""/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/urllib3/util/retry.py"", line 439, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3.amazonaws.com', port=443): Max retries exceeded with url: /datasets.huggingface.co/datasets/datasets/json/json.py (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x1181e9940>, 'Connection to s3.amazonaws.com timed out. (connect timeout=10)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/littlely/projects/python_projects/pytorch_learning/nlp/dataset/transformers_datasets.py"", line 12, in <module>
    dataset = load_dataset('json', data_files=[""../../data/json.json""])
  File ""/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/datasets/load.py"", line 591, in load_dataset
    path, script_version=script_version, download_config=download_config, download_mode=download_mode, dataset=True
  File ""/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/datasets/load.py"", line 263, in prepare_module
    head_hf_s3(path, filename=name, dataset=dataset, max_retries=download_config.max_retries)
  File ""/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/datasets/utils/file_utils.py"", line 232, in head_hf_s3
    max_retries=max_retries,
  File ""/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/datasets/utils/file_utils.py"", line 523, in http_head
    max_retries=max_retries,
  File ""/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/datasets/utils/file_utils.py"", line 458, in _request_with_retry
    raise err
  File ""/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/datasets/utils/file_utils.py"", line 454, in _request_with_retry
    response = requests.request(verb.upper(), url, **params)
  File ""/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/requests/api.py"", line 61, in request
    return session.request(method=method, url=url, **kwargs)
  File ""/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/requests/sessions.py"", line 530, in request
    resp = self.send(prep, **send_kwargs)
  File ""/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/requests/sessions.py"", line 643, in send
    r = adapter.send(request, **kwargs)
  File ""/Users/littlely/myvirtual/tf2/lib/python3.7/site-packages/requests/adapters.py"", line 504, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='s3.amazonaws.com', port=443): Max retries exceeded with url: /datasets.huggingface.co/datasets/datasets/json/json.py (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x1181e9940>, 'Connection to s3.amazonaws.com timed out. (connect timeout=10)'))

Process finished with exit code 1

```

Why it want to connect a remote url when I load local datasets, and how can I fix it?"
https://github.com/huggingface/datasets/issues/1817,pyarrow.lib.ArrowInvalid: Column 1 named input_ids expected length 599 but got length 1500,"['Hi !\r\nThe error you have is due to the `input_ids` column not having the same number of examples as the other columns.\r\nIndeed you\'re concatenating the `input_ids` at this line:\r\n\r\nhttps://github.com/LuCeHe/GenericTools/blob/431835d8e13ec24dceb5ee4dc4ae58f0e873b091/KerasTools/lm_preprocessing.py#L134\r\n\r\nHowever the other columns are kept unchanged, and therefore you end up with an `input_ids` column with 599 elements while the others columns like `attention_mask` have 1500.\r\n\r\nTo fix that you can instead concatenate them all using\r\n```python\r\nconcatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\r\n```\r\n\r\nAlso you may need to drop the ""text"" column before applying `group_texts` since strings can\'t be concatenated with lists. You can drop it at the tokenization step:\r\n```python\r\ndset = dset.map(\r\n    tokenize_function,\r\n    batched=True,\r\n    remove_columns=[""text""]\r\n)\r\n```'
 'You saved my life.']","I am trying to preprocess any dataset in this package with GPT-2 tokenizer, so I need to structure the datasets as long sequences of text without padding. I've been following a couple of your tutorials and here you can find the script that is failing right at the end

https://github.com/LuCeHe/GenericTools/blob/master/KerasTools/lm_preprocessing.py

In the last iteration of the last dset.map, it gives the error that I copied in the title. Another issue that I have, if I leave the batch_size set as 1000 in the last .map, I'm afraid it's going to lose most text, so I'm considering setting both writer_batch_size and batch_size to 300 K, but I'm not sure it's the best way to go.

Can you help me?
Thanks!"
https://github.com/huggingface/datasets/issues/1811,Unable to add Multi-label Datasets,"[""Thanks for adding this dataset! As far as I know `supervised_keys` is mostly a holdover from TFDS, but isn't really used, so feel free to drop it (@lhoestq  or @thomwolf correct me if I'm wrong). It definitely shouldn't be blocking :) ""
 'I can confirm that it comes from TFDS and is not used at the moment.'
 'Thanks @yjernite @lhoestq \r\n\r\nThe template for new dataset makes it slightly confusing. I suppose the comment suggesting its update can be removed.'
 'Closing this issue since it was answered.']","I am trying to add [CIFAR-100](https://www.cs.toronto.edu/~kriz/cifar.html) dataset. The dataset contains two labels per image - `fine label` and `coarse label`. Using just one label in supervised keys as 
`supervised_keys=(""img"", ""fine_label"")` raises no issue. But trying `supervised_keys=(""img"", ""fine_label"",""coarse_label"")` leads to this error : 

```python
Traceback (most recent call last):
  File ""test_script.py"", line 2, in <module>
    d = load_dataset('./datasets/cifar100')
  File ""~/datasets/src/datasets/load.py"", line 668, in load_dataset
    **config_kwargs,
  File ""~/datasets/src/datasets/builder.py"", line 896, in __init__
    super(GeneratorBasedBuilder, self).__init__(*args, **kwargs)
  File ""~/datasets/src/datasets/builder.py"", line 247, in __init__
    info.update(self._info())
  File ""~/.cache/huggingface/modules/datasets_modules/datasets/cifar100/61d2489b2d4a4abc34201432541b7380984ec714e290817d9a1ee318e4b74e0f/cifar100.py"", line 79, in _info
    citation=_CITATION,
  File ""<string>"", line 19, in __init__
  File ""~/datasets/src/datasets/info.py"", line 136, in __post_init__
    self.supervised_keys = SupervisedKeysData(*self.supervised_keys)
TypeError: __init__() takes from 1 to 3 positional arguments but 4 were given
```
Is there a way I can fix this?

Also, what does adding `supervised_keys` do? Is it necessary? How would I specify `supervised_keys` for a multi-input, multi-label dataset?

Thanks,
Gunjan"
https://github.com/huggingface/datasets/issues/1810,Add Hateful Memes Dataset,"['I am not sure, but would `datasets.Sequence(datasets.Sequence(datasets.Sequence(datasets.Value(""int"")))` work?'
 'Also, I found the information for loading only subsets of the data [here](https://github.com/huggingface/datasets/blob/master/docs/source/splits.rst).'
 'Hi @lhoestq,\r\n\r\nRequest you to check this once.\r\n\r\nThanks,\r\nGunjan'
 ""Hi @gchhablani since Array2D doesn't support images of different sizes, I would suggest to store in the dataset the paths to the image file instead of the image data. This has the advantage of not decompressing the data (images are often compressed using jpeg, png etc.). Users can still apply `.map` to load the images if they want to. Though it would en up being Sequences features.\r\n\r\nIn the future we'll add support for ragged tensors for this case and update the relevant dataset with this feature.""]","## Add Hateful Memes Dataset
- **Name:** Hateful Memes
- **Description:** [https://ai.facebook.com/blog/hateful-memes-challenge-and-data-set]( https://ai.facebook.com/blog/hateful-memes-challenge-and-data-set)
- **Paper:** [https://arxiv.org/pdf/2005.04790.pdf](https://arxiv.org/pdf/2005.04790.pdf)
- **Data:** [This link](https://drivendata-competition-fb-hateful-memes-data.s3.amazonaws.com/XjiOc5ycDBRRNwbhRlgH.zip?AWSAccessKeyId=AKIARVBOBDCY4MWEDJKS&Signature=DaUuGgZWUgDHzEPPbyJ2PhSJ56Q%3D&Expires=1612816874)
- **Motivation:** Including multi-modal datasets to 🤗 datasets.

I will be adding this dataset. It requires the user to sign an agreement on DrivenData. So, it will be used with a manual download.

The issue with this dataset is that the images are of different sizes. The image datasets added so far (CIFAR-10 and MNIST) have a uniform shape throughout.
So something like 
```python
 datasets.Array2D(shape=(28, 28), dtype=""uint8"")
```
won't work for the images. How would I add image features then? I checked `datasets/features.py` but couldn't figure out the appropriate class for this. I'm assuming I would want to avoid re-sizing at all since we want the user to be able to access the original images.

Also, in case I want to load only a subset of the data, since the actual data is around 8.8GB, how would that be possible?

Thanks,
Gunjan"
https://github.com/huggingface/datasets/issues/1808,writing Datasets in a human readable format,"['AFAIK, there is currently no built-in method on the `Dataset` object to do this.\r\nHowever, a workaround is to directly use the Arrow table backing the dataset, **but it implies loading the whole dataset in memory** (correct me if I\'m mistaken @lhoestq).\r\n\r\nYou can convert the Arrow table to a pandas dataframe to save the data as csv as follows:\r\n```python\r\narrow_table = dataset.data\r\ndataframe = arrow_table.to_pandas()\r\ndataframe.to_csv(""/path/to/file.csv"")\r\n```\r\n\r\nSimilarly, you can convert the dataset to a Python dict and save it as JSON:\r\n```python\r\nimport json\r\narrow_table = dataset.data\r\npy_dict = arrow_table.to_pydict()\r\nwith open(""/path/to/file.json"", ""w+"") as f:\r\n    json.dump(py_dict, f)\r\n```'
 'Indeed this works as long as you have enough memory.\r\nIt would be amazing to have export options like csv, json etc. !\r\n\r\nIt should be doable to implement something that iterates through the dataset batch by batch to write to csv for example.\r\nThere is already an `export` method but currently the only export type that is supported is `tfrecords`.']","Hi
I see there is a save_to_disk function to save data, but this is not human readable format, is there a way I could save a Dataset object in a human readable  format to a file like json? thanks @lhoestq "
https://github.com/huggingface/datasets/issues/1805,can't pickle SwigPyObject objects when calling dataset.get_nearest_examples from FAISS index,"['Hi ! Indeed we used to require mapping functions to be picklable with `pickle` or `dill` in order to cache the resulting datasets. And FAISS indexes are not picklable unfortunately.\r\n\r\nBut since #1703 this is no longer required (the caching will simply be disabled). This change will be available in the next release of `datasets`, or you can also install `datasets` from source.'
 ""I totally forgot to answer this issue, I'm so sorry. \r\n\r\nI was able to get it working by installing `datasets` from source. Huge thanks!""]","So, I have the following instances in my dataset

```
{'question': 'An astronomer observes that a planet rotates faster after a meteorite impact. Which is the most likely effect of 
this increase in rotation?', 
'answer': 'C', 
'example_id': 'ARCCH_Mercury_7175875', 
'options':[{'option_context': 'One effect of increased amperage in the planetary world (..)', 'option_id': 'A', 'option_text': 'Planetary density will decrease.'},
 (...)]}
```

The `options` value is always an list with 4 options, each one is a dict with `option_context`; `option_id` and `option_text`.

I would like to overwrite the `option_context` of each instance of my dataset for a dpr result that I am developing. Then, I trained a model already and save it in a FAISS index
```
dpr_dataset = load_dataset(
            ""text"",
            data_files=ARC_CORPUS_TEXT,
            cache_dir=CACHE_DIR,
            split=""train[:100%]"",
        )
dpr_dataset.load_faiss_index(""embeddings"", f""{ARC_CORPUS_FAISS}"")
torch.set_grad_enabled(False)
```

Then, as a processor of my dataset, I created a map function that calls the `dpr_dataset` for each _option_

```
def generate_context(example):
    question_text = example['question']
    for option in example['options']:
        question_with_option = question_text + "" "" + option['option_text']
        tokenize_text =  question_tokenizer(question_with_option, return_tensors=""pt"").to(device)
        question_embed = (
            question_encoder(**tokenize_text)
        )[0][0].cpu().numpy()
        _, retrieved_examples = dpr_dataset.get_nearest_examples(
            ""embeddings"", question_embed, k=10
        )
    #    option[""option_context""] = retrieved_examples[""text""]
    #    option[""option_context""] = "" "".join(option[""option_context""]).strip()
    #result_dict = {
    #    'example_id': example['example_id'],
    #    'answer': example['answer'],
    #    'question': question_text,
        #options': example['options']
    # }
    return example
```

I intentionally commented on this portion of the code.

But when I call the `map` method, `ds_with_context = dataset.map(generate_context,load_from_cache_file=False)`

It calls the following error:

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-55-75a458ce205c> in <module>
----> 1 ds_with_context = dataset.map(generate_context,load_from_cache_file=False)

~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/datasets/dataset_dict.py in map(self, function, with_indices, input_columns, batched, batch_size, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc)
    301                     num_proc=num_proc,
    302                 )
--> 303                 for k, dataset in self.items()
    304             }
    305         )

~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/datasets/dataset_dict.py in <dictcomp>(.0)
    301                     num_proc=num_proc,
    302                 )
--> 303                 for k, dataset in self.items()
    304             }
    305         )

~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/datasets/arrow_dataset.py in map(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint)
   1257                 fn_kwargs=fn_kwargs,
   1258                 new_fingerprint=new_fingerprint,
-> 1259                 update_data=update_data,
   1260             )
   1261         else:

~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/datasets/arrow_dataset.py in wrapper(*args, **kwargs)
    155         }
    156         # apply actual function
--> 157         out: Union[""Dataset"", ""DatasetDict""] = func(self, *args, **kwargs)
    158         datasets: List[""Dataset""] = list(out.values()) if isinstance(out, dict) else [out]
    159         # re-apply format to the output

~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/datasets/fingerprint.py in wrapper(*args, **kwargs)
    156                         kwargs_for_fingerprint[""fingerprint_name""] = fingerprint_name
    157                         kwargs[fingerprint_name] = update_fingerprint(
--> 158                             self._fingerprint, transform, kwargs_for_fingerprint
    159                         )
    160 

~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/datasets/fingerprint.py in update_fingerprint(fingerprint, transform, transform_args)
    103     for key in sorted(transform_args):
    104         hasher.update(key)
--> 105         hasher.update(transform_args[key])
    106     return hasher.hexdigest()
    107 

~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/datasets/fingerprint.py in update(self, value)
     55     def update(self, value):
     56         self.m.update(f""=={type(value)}=="".encode(""utf8""))
---> 57         self.m.update(self.hash(value).encode(""utf-8""))
     58 
     59     def hexdigest(self):

~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/datasets/fingerprint.py in hash(cls, value)
     51             return cls.dispatch[type(value)](cls, value)
     52         else:
---> 53             return cls.hash_default(value)
     54 
     55     def update(self, value):

~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/datasets/fingerprint.py in hash_default(cls, value)
     44     @classmethod
     45     def hash_default(cls, value):
---> 46         return cls.hash_bytes(dumps(value))
     47 
     48     @classmethod

~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/datasets/utils/py_utils.py in dumps(obj)
    387     file = StringIO()
    388     with _no_cache_fields(obj):
--> 389         dump(obj, file)
    390     return file.getvalue()
    391 

~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/datasets/utils/py_utils.py in dump(obj, file)
    359 def dump(obj, file):
    360     """"""pickle an object to a file""""""
--> 361     Pickler(file, recurse=True).dump(obj)
    362     return
    363 

~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/dill/_dill.py in dump(self, obj)
    452             raise PicklingError(msg)
    453         else:
--> 454             StockPickler.dump(self, obj)
    455         stack.clear()  # clear record of 'recursion-sensitive' pickled objects
    456         return

/usr/lib/python3.7/pickle.py in dump(self, obj)
    435         if self.proto >= 4:
    436             self.framer.start_framing()
--> 437         self.save(obj)
    438         self.write(STOP)
    439         self.framer.end_framing()

/usr/lib/python3.7/pickle.py in save(self, obj, save_persistent_id)
    502         f = self.dispatch.get(t)
    503         if f is not None:
--> 504             f(self, obj) # Call unbound method with explicit self
    505             return
    506 

~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/datasets/utils/py_utils.py in save_function(pickler, obj)
    554                 dill._dill._create_function,
    555                 (obj.__code__, globs, obj.__name__, obj.__defaults__, obj.__closure__, obj.__dict__, fkwdefaults),
--> 556                 obj=obj,
    557             )
    558         else:

/usr/lib/python3.7/pickle.py in save_reduce(self, func, args, state, listitems, dictitems, obj)
    636         else:
    637             save(func)
--> 638             save(args)
    639             write(REDUCE)
    640 

/usr/lib/python3.7/pickle.py in save(self, obj, save_persistent_id)
    502         f = self.dispatch.get(t)
    503         if f is not None:
--> 504             f(self, obj) # Call unbound method with explicit self
    505             return
    506 

/usr/lib/python3.7/pickle.py in save_tuple(self, obj)
    784         write(MARK)
    785         for element in obj:
--> 786             save(element)
    787 
    788         if id(obj) in memo:

/usr/lib/python3.7/pickle.py in save(self, obj, save_persistent_id)
    502         f = self.dispatch.get(t)
    503         if f is not None:
--> 504             f(self, obj) # Call unbound method with explicit self
    505             return
    506 

~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/dill/_dill.py in save_module_dict(pickler, obj)
    939             # we only care about session the first pass thru
    940             pickler._session = False
--> 941         StockPickler.save_dict(pickler, obj)
    942         log.info(""# D2"")
    943     return

/usr/lib/python3.7/pickle.py in save_dict(self, obj)
    854 
    855         self.memoize(obj)
--> 856         self._batch_setitems(obj.items())
    857 
    858     dispatch[dict] = save_dict

/usr/lib/python3.7/pickle.py in _batch_setitems(self, items)
    880                 for k, v in tmp:
    881                     save(k)
--> 882                     save(v)
    883                 write(SETITEMS)
    884             elif n:

/usr/lib/python3.7/pickle.py in save(self, obj, save_persistent_id)
    547 
    548         # Save the reduce() output and finally memoize the object
--> 549         self.save_reduce(obj=obj, *rv)
    550 
    551     def persistent_id(self, obj):

/usr/lib/python3.7/pickle.py in save_reduce(self, func, args, state, listitems, dictitems, obj)
    660 
    661         if state is not None:
--> 662             save(state)
    663             write(BUILD)
    664 

/usr/lib/python3.7/pickle.py in save(self, obj, save_persistent_id)
    502         f = self.dispatch.get(t)
    503         if f is not None:
--> 504             f(self, obj) # Call unbound method with explicit self
    505             return
    506 

~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/dill/_dill.py in save_module_dict(pickler, obj)
    939             # we only care about session the first pass thru
    940             pickler._session = False
--> 941         StockPickler.save_dict(pickler, obj)
    942         log.info(""# D2"")
    943     return

/usr/lib/python3.7/pickle.py in save_dict(self, obj)
    854 
    855         self.memoize(obj)
--> 856         self._batch_setitems(obj.items())
    857 
    858     dispatch[dict] = save_dict

/usr/lib/python3.7/pickle.py in _batch_setitems(self, items)
    880                 for k, v in tmp:
    881                     save(k)
--> 882                     save(v)
    883                 write(SETITEMS)
    884             elif n:

/usr/lib/python3.7/pickle.py in save(self, obj, save_persistent_id)
    502         f = self.dispatch.get(t)
    503         if f is not None:
--> 504             f(self, obj) # Call unbound method with explicit self
    505             return
    506 

~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/dill/_dill.py in save_module_dict(pickler, obj)
    939             # we only care about session the first pass thru
    940             pickler._session = False
--> 941         StockPickler.save_dict(pickler, obj)
    942         log.info(""# D2"")
    943     return

/usr/lib/python3.7/pickle.py in save_dict(self, obj)
    854 
    855         self.memoize(obj)
--> 856         self._batch_setitems(obj.items())
    857 
    858     dispatch[dict] = save_dict

/usr/lib/python3.7/pickle.py in _batch_setitems(self, items)
    885                 k, v = tmp[0]
    886                 save(k)
--> 887                 save(v)
    888                 write(SETITEM)
    889             # else tmp is empty, and we're done

/usr/lib/python3.7/pickle.py in save(self, obj, save_persistent_id)
    547 
    548         # Save the reduce() output and finally memoize the object
--> 549         self.save_reduce(obj=obj, *rv)
    550 
    551     def persistent_id(self, obj):

/usr/lib/python3.7/pickle.py in save_reduce(self, func, args, state, listitems, dictitems, obj)
    660 
    661         if state is not None:
--> 662             save(state)
    663             write(BUILD)
    664 

/usr/lib/python3.7/pickle.py in save(self, obj, save_persistent_id)
    502         f = self.dispatch.get(t)
    503         if f is not None:
--> 504             f(self, obj) # Call unbound method with explicit self
    505             return
    506 

~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/dill/_dill.py in save_module_dict(pickler, obj)
    939             # we only care about session the first pass thru
    940             pickler._session = False
--> 941         StockPickler.save_dict(pickler, obj)
    942         log.info(""# D2"")
    943     return

/usr/lib/python3.7/pickle.py in save_dict(self, obj)
    854 
    855         self.memoize(obj)
--> 856         self._batch_setitems(obj.items())
    857 
    858     dispatch[dict] = save_dict

/usr/lib/python3.7/pickle.py in _batch_setitems(self, items)
    880                 for k, v in tmp:
    881                     save(k)
--> 882                     save(v)
    883                 write(SETITEMS)
    884             elif n:

/usr/lib/python3.7/pickle.py in save(self, obj, save_persistent_id)
    547 
    548         # Save the reduce() output and finally memoize the object
--> 549         self.save_reduce(obj=obj, *rv)
    550 
    551     def persistent_id(self, obj):

/usr/lib/python3.7/pickle.py in save_reduce(self, func, args, state, listitems, dictitems, obj)
    660 
    661         if state is not None:
--> 662             save(state)
    663             write(BUILD)
    664 

/usr/lib/python3.7/pickle.py in save(self, obj, save_persistent_id)
    502         f = self.dispatch.get(t)
    503         if f is not None:
--> 504             f(self, obj) # Call unbound method with explicit self
    505             return
    506 

~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/dill/_dill.py in save_module_dict(pickler, obj)
    939             # we only care about session the first pass thru
    940             pickler._session = False
--> 941         StockPickler.save_dict(pickler, obj)
    942         log.info(""# D2"")
    943     return

/usr/lib/python3.7/pickle.py in save_dict(self, obj)
    854 
    855         self.memoize(obj)
--> 856         self._batch_setitems(obj.items())
    857 
    858     dispatch[dict] = save_dict

/usr/lib/python3.7/pickle.py in _batch_setitems(self, items)
    885                 k, v = tmp[0]
    886                 save(k)
--> 887                 save(v)
    888                 write(SETITEM)
    889             # else tmp is empty, and we're done

/usr/lib/python3.7/pickle.py in save(self, obj, save_persistent_id)
    522             reduce = getattr(obj, ""__reduce_ex__"", None)
    523             if reduce is not None:
--> 524                 rv = reduce(self.proto)
    525             else:
    526                 reduce = getattr(obj, ""__reduce__"", None)

TypeError: can't pickle SwigPyObject objects
```

Which I have no idea how to solve/deal with it

"
https://github.com/huggingface/datasets/issues/1803,Querying examples from big datasets is slower than small datasets,"['Hello, @lhoestq / @gaceladri  : We have been seeing similar behavior with bigger datasets, where querying time increases. Are you folks aware of any solution that fixes this problem yet?  '
 ""Hi ! I'm pretty sure that it can be fixed by using the Arrow IPC file format instead of the raw streaming format but I haven't tested yet.\r\nI'll take a look at it soon and let you know""
 'My workaround is to shard the dataset into splits in my ssd disk and feed the data in different training sessions. But it is a bit of a pain when we need to reload the last training session with the rest of the split with the Trainer in transformers.\r\n\r\nI mean, when I split the training and then reloads the model and optimizer, it not gets the correct global_status of the optimizer, so I need to hardcode some things. I\'m planning to open an issue in transformers and think about it.\r\n```\r\nfrom datasets import load_dataset\r\n\r\nbook_corpus = load_dataset(""bookcorpus"", split=""train[:25%]"")\r\nwikicorpus = load_dataset(""wikicorpus"", split=""train[:25%]"")\r\nopenwebtext = load_dataset(""openwebtext"", split=""train[:25%]"")\r\n\r\nbig_dataset = datasets.concatenate_datasets([wikicorpus, openwebtext, book_corpus])\r\nbig_dataset.shuffle(seed=42)\r\nbig_dataset = big_dataset.map(encode, batched=True, num_proc=20, load_from_cache_file=True, writer_batch_size=5000)\r\nbig_dataset.set_format(type=\'torch\', columns=[""text"", ""input_ids"", ""attention_mask"", ""token_type_ids""])\r\n\r\n\r\ntraining_args = TrainingArguments(\r\n    output_dir=""./linear_bert"",\r\n    overwrite_output_dir=True,\r\n    per_device_train_batch_size=71,\r\n    save_steps=500,\r\n    save_total_limit=10,\r\n    logging_first_step=True,\r\n    logging_steps=100,\r\n    gradient_accumulation_steps=9,\r\n    fp16=True,\r\n    dataloader_num_workers=20,\r\n    warmup_steps=24000,\r\n    learning_rate=0.000545205002870214,\r\n    adam_epsilon=1e-6,\r\n    adam_beta2=0.98,\r\n    weight_decay=0.01,\r\n    max_steps=138974,  # the total number of steps after concatenating 100% datasets\r\n    max_grad_norm=1.0,\r\n)\r\n\r\ntrainer = Trainer(\r\n    model=model,\r\n    args=training_args,\r\n    data_collator=data_collator,\r\n    train_dataset=big_dataset,\r\n    tokenizer=tokenizer))\r\n```\r\n\r\nI do one training pass with the total steps of this shard and I use len(bbig)/batchsize to stop the training (hardcoded in the trainer.py) when I pass over all the examples in this split.\r\n\r\nNow Im working, I will edit the comment with a more elaborated answer when I left the work.'
 ""I just tested and using the Arrow File format doesn't improve the speed... This will need further investigation.\r\n\r\nMy guess is that it has to iterate over the record batches or chunks of a ChunkedArray in order to retrieve elements.\r\n\r\nHowever if we know in advance in which chunk the element is, and at what index it is, then we can access it instantaneously. But this requires dealing with the chunked arrays instead of the pyarrow Table directly which is not practical.""
 ""I have a dataset with about 2.7 million rows (which I'm loading via `load_from_disk`), and I need to fetch around 300k (particular) rows of it, by index. Currently this is taking a really long time (~8 hours). I tried sharding the large dataset but overall it doesn't change how long it takes to fetch the desired rows.\r\n\r\nI actually have enough RAM that I could fit the large dataset in memory. Would having the large dataset in memory speed up querying? To find out, I tried to load (a column of) the large dataset into memory like this:\r\n```\r\ncolumn_data = large_ds['column_name']\r\n```\r\nbut in itself this takes a really long time.\r\n\r\nI'm pretty stuck - do you have any ideas what I should do? ""
 ""Hi ! Feel free to post a message on the [forum](https://discuss.huggingface.co/c/datasets/10). I'd be happy to help you with this.\r\n\r\nIn your post on the forum, feel free to add more details about your setup:\r\nWhat are column names and types of your dataset ?\r\nHow was the dataset constructed ?\r\nIs the dataset shuffled ?\r\nIs the dataset tokenized ?\r\nAre you on a SSD or an HDD ?\r\n\r\nI'm sure we can figure something out.\r\nFor example on my laptop I can access the 6 millions articles from wikipedia in less than a minute.""
 ""Thanks @lhoestq, I've [posted on the forum](https://discuss.huggingface.co/t/fetching-rows-of-a-large-dataset-by-index/4271?u=abisee).""
 'Fixed by #2122.']","After some experiments with bookcorpus I noticed that querying examples from big datasets is slower than small datasets.
For example
```python
from datasets import load_dataset

b1 = load_dataset(""bookcorpus"", split=""train[:1%]"")
b50 = load_dataset(""bookcorpus"", split=""train[:50%]"")
b100 = load_dataset(""bookcorpus"", split=""train[:100%]"")

%timeit _ = b1[-1]                                                                     
# 12.2 µs ± 70.4 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)

%timeit _ = b50[-1]                                                                    
# 92.5 µs ± 1.24 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)

%timeit _ = b100[-1]                                                                      
# 177 µs ± 3.13 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)

```

It looks like the time to fetch the example increases with the size of the dataset.

This is maybe due to the use of the Arrow streaming format to store the data on disk. I guess pyarrow needs to iterate through the file as a stream to find the queried sample.

Maybe switching to the Arrow IPC file format could help fixing this issue.

Indeed according to the [documentation](https://arrow.apache.org/docs/format/Columnar.html?highlight=arrow1#ipc-file-format), it's identical to the streaming format except that it contains the memory offsets of each sample, which could fix the issue:
> We define a “file format” supporting random access that is build with the stream format. The file starts and ends with a magic string ARROW1 (plus padding). What follows in the file is identical to the stream format. At the end of the file, we write a footer containing a redundant copy of the schema (which is a part of the streaming format) plus memory offsets and sizes for each of the data blocks in the file. This enables random access any record batch in the file. See File.fbs for the precise details of the file footer.

cc @gaceladri since it can help speed up your training when this one is fixed."
https://github.com/huggingface/datasets/issues/1797,Connection error,['Hi ! For future references let me add a link to our discussion here : https://github.com/huggingface/datasets/issues/759#issuecomment-770684693\r\n\r\nLet me know if you manage to fix your proxy issue or if we can do something on our end to help you :)'],"Hi
I am hitting to the error, help me and thanks.

`train_data = datasets.load_dataset(""xsum"", split=""train"")`
`ConnectionError: Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/1.0.2/datasets/xsum/xsum.py`"
https://github.com/huggingface/datasets/issues/1796,Filter on dataset too much slowww,"[""When I use the filter on the arrow table directly, it works like butter. But I can't find a way to update the table in `Dataset` object.\r\n\r\n```\r\nds_table = dataset.data.filter(mask=dataset['flag'])\r\n```""
 '@thomwolf @lhoestq can you guys please take a look and recommend some solution.'
 ""Hi ! Currently the filter method reads the dataset batch by batch to write a new, filtered, arrow file on disk. Therefore all the reading + writing can take some time.\r\nUsing a mask directly on the arrow table doesn't do any read or write operation therefore it's way quicker.\r\n\r\nReplacing the old table by the new one should do the job:\r\n```python\r\ndataset._data = dataset._data.filter(...)\r\n```\r\n\r\nNote: this is a **workaround** and in general users shouldn't have to do that. In particular if you did some `shuffle` or `select` before that then it would not work correctly since the indices mapping (index from `__getitem__` -> index in the table) would not be valid anymore. But if you haven't done any `shuffle`, `select`, `shard`, `train_test_split` etc. then it should work.\r\n\r\nIdeally it would be awesome to update the filter function to allow masking this way !\r\nIf you would like to give it a shot I will be happy to help :) ""
 'Yes, would be happy to contribute. Thanks'
 ""Hi @lhoestq @ayubSubhaniya,\r\n\r\nIf there's no progress on this one, can I try working on it?\r\n\r\nThanks,\r\nGunjan""
 'Sure @gchhablani feel free to start working on it, this would be very appreciated :)\r\nThis feature is would be really awesome, especially since arrow allows to mask really quickly and without having to rewrite the dataset on disk']","I have a dataset with 50M rows.
For pre-processing, I need to tokenize this and filter rows with the large sequence.

My tokenization took roughly 12mins. I used `map()` with batch size 1024 and multi-process with 96 processes.

When I applied the `filter()` function it is taking too much time. I need to filter sequences based on a boolean column.
Below are the variants I tried.
1. filter() with batch size 1024, single process (takes roughly 3 hr)
2. filter() with batch size 1024, 96 processes (takes 5-6 hrs ¯\\\_(ツ)\_/¯)
3. filter() with loading all data in memory, only a single boolean column (never ends).

Can someone please help?

Below is a sample code for small dataset.

```
from datasets import load_dataset
dataset = load_dataset('glue', 'mrpc', split='train')
dataset = dataset.map(lambda x: {'flag': random.randint(0,1)==1})

def _amplify(data):
        return data

dataset = dataset.filter(_amplify, batch_size=1024, keep_in_memory=False, input_columns=['flag'])
```
"
https://github.com/huggingface/datasets/issues/1790,"ModuleNotFoundError: No module named 'apache_beam', when specific languages.","[""Hi !\r\n\r\nApache Beam is a framework used to define data transformation pipelines. These pipeline can then be run in many runtimes: DataFlow, Spark, Flink, etc. There also exist a local runner called the DirectRunner.\r\nWikipedia is a dataset that requires some parsing, so to allow the processing to be run on this kind of runtime we're using Apache Beam.\r\n\r\nAt Hugging Face we've already processed certain versions of wikipedia (the `20200501.en` one for example) so that users can directly download the processed version instead of using Apache Beam to process it.\r\nHowever for the japanese language we haven't processed it so you'll have to run the processing on your side.\r\nSo you do need Apache Beam to process `20200501.ja`.\r\n\r\nYou can install Apache Beam with\r\n```\r\npip install apache-beam\r\n```\r\n\r\nI think we can probably improve the error message to let users know of this subtlety.\r\nWhat #498 implied is that Apache Beam is not needed when you process a dataset that doesn't use Apache Beam.""
 'Thanks for your reply! \r\nI understood.\r\n\r\nI tried again with installing apache-beam, add ` beam_runner=""DirectRunner""` and an anther `mwparserfromhell` is also required so I installed it.\r\nbut, it also failed. It exited 1 without error message.\r\n\r\n```py\r\nimport datasets\r\n# BTW, 20200501.ja doesn\'t exist at wikipedia, so I specified date argument\r\nwiki = datasets.load_dataset(""wikipedia"", language=""ja"", date=""20210120"", cache_dir=""./datasets"", beam_runner=""DirectRunner"")\r\nprint(wiki)\r\n```\r\nand its log is below\r\n```\r\nUsing custom data configuration 20210120.ja\r\nDownloading and preparing dataset wikipedia/20210120.ja-date=20210120,language=ja (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to ./datasets/wikipedia/20210120.ja-date=20210120,language=ja/0.0.0/4021357e28509391eab2f8300d9b689e7e8f3a877ebb3d354b01577d497ebc63...\r\nKilled\r\n```\r\n\r\nI also tried on another machine because it may caused by insufficient resources.\r\n```\r\n$ python main.py\r\nUsing custom data configuration 20210120.ja\r\nDownloading and preparing dataset wikipedia/20210120.ja-date=20210120,language=ja (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to ./datasets/wikipedia/20210120.ja-date=20210120,language=ja/0.0.0/4021357e28509391eab2f8300d9b689e7e8f3a877ebb3d354b01577d497ebc63...\r\n\r\nTraceback (most recent call last):\r\n  File ""main.py"", line 3, in <module>\r\n    wiki = datasets.load_dataset(""wikipedia"", language=""ja"", date=""20210120"", cache_dir=""./datasets"", beam_runner=""DirectRunner"")\r\n  File ""/home/miyamonz/.cache/pypoetry/virtualenvs/try-datasets-4t4JWXxu-py3.8/lib/python3.8/site-packages/datasets/load.py"", line 609, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File ""/home/miyamonz/.cache/pypoetry/virtualenvs/try-datasets-4t4JWXxu-py3.8/lib/python3.8/site-packages/datasets/builder.py"", line 526, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File ""/home/miyamonz/.cache/pypoetry/virtualenvs/try-datasets-4t4JWXxu-py3.8/lib/python3.8/site-packages/datasets/builder.py"", line 1069, in _download_and_prepare\r\n    pipeline_results = pipeline.run()\r\n  File ""/home/miyamonz/.cache/pypoetry/virtualenvs/try-datasets-4t4JWXxu-py3.8/lib/python3.8/site-packages/apache_beam/pipeline.py"", line 561, in run\r\n    return self.runner.run_pipeline(self, self._options)\r\n  File ""/home/miyamonz/.cache/pypoetry/virtualenvs/try-datasets-4t4JWXxu-py3.8/lib/python3.8/site-packages/apache_beam/runners/direct/direct_runner.py"", line 126, in run_pipeline\r\n    return runner.run_pipeline(pipeline, options)\r\n  File ""/home/miyamonz/.cache/pypoetry/virtualenvs/try-datasets-4t4JWXxu-py3.8/lib/python3.8/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py"", line 182, in run_pipeline\r\n    self._latest_run_result = self.run_via_runner_api(\r\n  File ""/home/miyamonz/.cache/pypoetry/virtualenvs/try-datasets-4t4JWXxu-py3.8/lib/python3.8/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py"", line 193, in run_via_runner_api\r\n    return self.run_stages(stage_context, stages)\r\n  File ""/home/miyamonz/.cache/pypoetry/virtualenvs/try-datasets-4t4JWXxu-py3.8/lib/python3.8/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py"", line 358, in run_stages\r\n    stage_results = self._run_stage(\r\n  File ""/home/miyamonz/.cache/pypoetry/virtualenvs/try-datasets-4t4JWXxu-py3.8/lib/python3.8/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py"", line 549, in _run_stage\r\n    last_result, deferred_inputs, fired_timers = self._run_bundle(\r\n  File ""/home/miyamonz/.cache/pypoetry/virtualenvs/try-datasets-4t4JWXxu-py3.8/lib/python3.8/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py"", line 595, in _run_bundle\r\n    result, splits = bundle_manager.process_bundle(\r\n  File ""/home/miyamonz/.cache/pypoetry/virtualenvs/try-datasets-4t4JWXxu-py3.8/lib/python3.8/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py"", line 888, in process_bundle\r\n    self._send_input_to_worker(process_bundle_id, transform_id, elements)\r\n  File ""/home/miyamonz/.cache/pypoetry/virtualenvs/try-datasets-4t4JWXxu-py3.8/lib/python3.8/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py"", line 765, in _send_input_to_worker\r\n    data_out.write(byte_stream)\r\n  File ""apache_beam/coders/stream.pyx"", line 42, in apache_beam.coders.stream.OutputStream.write\r\n  File ""apache_beam/coders/stream.pyx"", line 47, in apache_beam.coders.stream.OutputStream.write\r\n  File ""apache_beam/coders/stream.pyx"", line 109, in apache_beam.coders.stream.OutputStream.extend\r\nAssertionError: OutputStream realloc failed.\r\n```\r\n\r\n'
 'Hi @miyamonz,\r\n\r\nI tried replicating this issue using the same snippet used by you. I am able to download the dataset without any issues, although I stopped it in the middle because the dataset is huge.\r\n\r\nBased on a similar issue [here](https://github.com/google-research/fixmatch/issues/23), it could be related to your environment setup, although I am just guessing here. Can you share these details?'
 'thanks for your reply and sorry for my late response.\r\n\r\n## environment\r\nmy local machine environment info\r\n- Ubuntu on WSL2\r\n\r\n`lsb_release -a`\r\n```\r\nNo LSB modules are available.\r\nDistributor ID: Ubuntu\r\nDescription:    Ubuntu 20.04.2 LTS\r\nRelease:        20.04\r\nCodename:       focal\r\n```\r\n\r\nRTX 2070 super\r\nInside WSL, there is no nvidia-msi command. I don\'t know why.\r\nBut, `torch.cuda.is_available()` is true and when I start something ML training code GPU usage is growing up, so I think it works.\r\n\r\nFrom PowerShell, there is nvidia-smi.exe and result is below.\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 470.05       Driver Version: 470.05       CUDA Version: 11.3     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  NVIDIA GeForce ... WDDM  | 00000000:09:00.0  On |                  N/A |\r\n|  0%   30C    P8    19W / 175W |    523MiB /  8192MiB |      3%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|    0   N/A  N/A      1728    C+G   Insufficient Permissions        N/A      |\r\n|    0   N/A  N/A      3672    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\r\n|    0   N/A  N/A      6304    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\r\n|    0   N/A  N/A      8648    C+G   C:\\Windows\\explorer.exe         N/A      |\r\n|    0   N/A  N/A      9536    C+G   ...y\\ShellExperienceHost.exe    N/A      |\r\n|    0   N/A  N/A     10668    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\r\n|    0   N/A  N/A     10948    C+G   ...artMenuExperienceHost.exe    N/A      |\r\n|    0   N/A  N/A     11988    C+G   ...8wekyb3d8bbwe\\Cortana.exe    N/A      |\r\n|    0   N/A  N/A     12464    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\r\n|    0   N/A  N/A     13280    C+G   ...upport\\CEF\\Max Helper.exe    N/A      |\r\n|    0   N/A  N/A     15948    C+G   ...t\\GoogleIMEJaRenderer.exe    N/A      |\r\n|    0   N/A  N/A     16128    C+G   ...ram Files\\Slack\\Slack.exe    N/A      |\r\n|    0   N/A  N/A     19096    C+G   ...8bbwe\\WindowsTerminal.exe    N/A      |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\nI don\'t know what should I show in such a case. If it\'s not enough, please tell me some commands.\r\n\r\n---\r\n## what I did\r\nI surveyed more and I found 2 issues.\r\n\r\nAbout the first one, I wrote it as a new issue.\r\nhttps://github.com/huggingface/datasets/issues/2031\r\n\r\nThe error I mentioned in the previous comment above, which occurred on my local machine, is no longer occurring.\r\n\r\nBut, it still failed. In the previous comment, I wrote `AssertionError: OutputStream realloc failed.` happen on another machine. It also happens on my local machine.\r\n\r\nHere\'s what I\'ve tried.\r\n\r\nthe wikipedia.py downloads these xml.bz2 files based on dumpstatus.json\r\nIn Japanese Wikipedia dataset that I specified, it will download these 6 files.\r\n\r\n\r\n`https://dumps.wikimedia.org/jawiki/20210120/dumpstatus.json`\r\nand filtered json based on wikipedia.py is below.\r\n```json\r\n {\r\n   ""jobs"": {\r\n     ""articlesmultistreamdump"": {\r\n       ""files"": {\r\n         ""jawiki-20210120-pages-articles-multistream1.xml-p1p114794.bz2"": {\r\n           ""url"": ""/jawiki/20210120/jawiki-20210120-pages-articles-multistream1.xml-p1p114794.bz2""\r\n         },\r\n         ""jawiki-20210120-pages-articles-multistream2.xml-p114795p390428.bz2"": {\r\n           ""url"": ""/jawiki/20210120/jawiki-20210120-pages-articles-multistream2.xml-p114795p390428.bz2""\r\n         },\r\n         ""jawiki-20210120-pages-articles-multistream3.xml-p390429p902407.bz2"": {\r\n           ""url"": ""/jawiki/20210120/jawiki-20210120-pages-articles-multistream3.xml-p390429p902407.bz2""\r\n         },\r\n         ""jawiki-20210120-pages-articles-multistream4.xml-p902408p1721646.bz2"": {\r\n           ""url"": ""/jawiki/20210120/jawiki-20210120-pages-articles-multistream4.xml-p902408p1721646.bz2""\r\n         },\r\n         ""jawiki-20210120-pages-articles-multistream5.xml-p1721647p2807947.bz2"": {\r\n           ""url"": ""/jawiki/20210120/jawiki-20210120-pages-articles-multistream5.xml-p1721647p2807947.bz2""\r\n         },\r\n         ""jawiki-20210120-pages-articles-multistream6.xml-p2807948p4290013.bz2"": {\r\n           ""url"": ""/jawiki/20210120/jawiki-20210120-pages-articles-multistream6.xml-p2807948p4290013.bz2""\r\n         }\r\n       }\r\n     }\r\n   }\r\n }\r\n```\r\n\r\nSo, I tried running with fewer resources by modifying this line.\r\nhttps://github.com/huggingface/datasets/blob/13a5b7db992ad5cf77895e4c0f76595314390418/datasets/wikipedia/wikipedia.py#L524\r\nI changed it like this. just change filepaths list.\r\n`            | ""Initialize"" >> beam.Create(filepaths[:1])`\r\n\r\nand I added a print line inside for the loop of _extract_content.\r\nlike this `if(i % 100000 == 0): print(i)`\r\n\r\nfirst, without modification, it always stops after all _extract_content is done.\r\n\r\n- `filepaths[:1]` then it succeeded.\r\n- `filepaths[:2]` then it failed.\r\nI don\'t try all patterns because each pattern takes a long time.\r\n\r\n### my opinion\r\nIt seems it\'s successful when the entire file size is small.\r\n  \r\nso, at least it doesn\'t file-specific issue.\r\n\r\n\r\nI don\'t know it\'s true but I think when beam_writter writes into a file, it consumes memory depends on its entire file.\r\nbut It\'s correct Apache Beam\'s behavior? I\'m not familiar with this library.\r\n'
 ""I don't know if this is related, but there is this issue on the wikipedia processing that you reported at #2031 (open PR is at #2037 ) .\r\nDoes the fix your proposed at #2037 helps in your case ?\r\n\r\nAnd for information, the DirectRunner of Apache Beam is not optimized for memory intensive tasks, so you must be right when you say that it uses the memory for the entire file.""
 'the #2037 doesn\'t solve my problem directly, but I found the point!\r\n\r\nhttps://github.com/huggingface/datasets/blob/349ac4398a3bcae6356f14c5754483383a60e8a4/datasets/wikipedia/wikipedia.py#L523\r\nthis `beam.transforms.Reshuffle()` cause the memory error.\r\n\r\nit makes sense if I consider the shuffle means. Beam\'s reshuffle seems need put all data in memory.\r\nPreviously I doubt that this line causes error, but at that time another bug showed in #2037 made error, so I can\'t found it.\r\n\r\nAnyway, I comment out this line, and run load_dataset, then it works!\r\n\r\n```python\r\nwiki = datasets.load_dataset(\r\n    ""./wikipedia.py"",\r\n    cache_dir=""./datasets"",\r\n    beam_runner=""DirectRunner"",\r\n    language=""ja"",\r\n    date=""20210120"",\r\n)[""train""]\r\n```\r\n![image](https://user-images.githubusercontent.com/6331508/112283369-6a9f3300-8ccb-11eb-82e5-827bf7fddfb9.png)\r\n\r\nDataset has already shuffle function. https://github.com/huggingface/datasets/blob/349ac4398a3bcae6356f14c5754483383a60e8a4/src/datasets/arrow_dataset.py#L2069\r\nSo, though I don\'t know it\'s difference correctly, but I think Beam\'s reshuffle isn\'t be needed. How do you think?'
 'The reshuffle is needed when you use parallelism.\r\nThe objective is to redistribute the articles evenly on the workers, since the `_extract_content` step generated many articles per file. By using reshuffle, we can split the processing of the articles of one file into several workers. Without reshuffle, all the articles of one file would be processed on the same worker that read the file, making the whole process take a very long time.'
 'Maybe the reshuffle step can be added only if the runner is not a DirectRunner ?']","```py
import datasets
wiki = datasets.load_dataset('wikipedia', '20200501.ja', cache_dir='./datasets')
```
then `ModuleNotFoundError: No module named 'apache_beam'` happend.

The error doesn't appear when it's '20200501.en'.
I don't know Apache Beam, but according to #498 it isn't necessary when it's saved to local. is it correct?"
https://github.com/huggingface/datasets/issues/1786,How to use split dataset ,"['By default, all 3 splits will be loaded if you run the following:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(""lambada"")\r\nprint(dataset[""train""])\r\nprint(dataset[""valid""])\r\n\r\n```\r\n\r\nIf you wanted to do load this manually, you could do this:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\ndata_files = {\r\n   ""train"": ""data/lambada/train.txt"",\r\n   ""valid"": ""data/lambada/valid.txt"",\r\n   ""test"": ""data/lambada/test.txt"",\r\n}\r\nds = load_dataset(""text"", data_files=data_files)\r\n```'
 'Thank you for the quick response! ']","![Capture1](https://user-images.githubusercontent.com/78090287/106057436-cb6a1f00-6111-11eb-8c9c-3658065b1fdf.PNG)

Hey,
I want to split the lambada dataset into corpus, test, train and valid txt files (like penn treebank) but I am not able to achieve this. What I am doing is, executing the lambada.py file in my project but its not giving desired results. Any help will be appreciated!"
https://github.com/huggingface/datasets/issues/1785,Not enough disk space (Needed: Unknown size) when caching on a cluster,"['Hi ! \r\n\r\nWhat do you mean by ""disk_usage(""."").free` can\'t compute on the cluster\'s shared disk"" exactly ?\r\nDoes it return 0 ?'
 'Yes, that\'s right. It shows 0 free space even though there is. I suspect it might have to do with permissions on the shared disk.\r\n\r\n```python\r\n>>> disk_usage(""."")\r\nusage(total=999999, used=999999, free=0)\r\n```'
 ""That's an interesting behavior...\r\nDo you know any other way to get the free space that works in your case ?\r\nAlso if it's a permission issue could you try fix the permissions and let mus know if that helped ?""
 'I think its an issue on the clusters end (unclear exactly why -- maybe something with docker containers?), will close the issue']","I'm running some experiments where I'm caching datasets on a cluster and accessing it through multiple compute nodes. However, I get an error when loading the cached dataset from the shared disk.

The exact error thrown:

```bash
>>> load_dataset(dataset, cache_dir=""/path/to/cluster/shared/path"")
OSError: Not enough disk space. Needed: Unknown size (download: Unknown size, generated: Unknown size, post-processed: Unknown size)
```


[`utils.has_sufficient_disk_space`](https://github.com/huggingface/datasets/blob/8a03ab7d123a76ee744304f21ce868c75f411214/src/datasets/utils/py_utils.py#L332) fails on each job because of how the cluster system is designed (`disk_usage(""."").free` can't compute on the cluster's shared disk).


This is exactly where the error gets thrown:
https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L502

```python
if not utils.has_sufficient_disk_space(self.info.size_in_bytes or 0, directory=self._cache_dir_root):
    raise IOError(
          ""Not enough disk space. Needed: {} (download: {}, generated: {}, post-processed: {})"".format(
          utils.size_str(self.info.size_in_bytes or 0),
          utils.size_str(self.info.download_size or 0),
          utils.size_str(self.info.dataset_size or 0),
          utils.size_str(self.info.post_processing_size or 0),
       )
    )

```

What would be a good way to circumvent this? my current fix is to manually comment out that part, but that is not ideal. 
Would it be possible to pass a flag to skip this check on disk space?"
https://github.com/huggingface/datasets/issues/1784,JSONDecodeError on JSON with multiple lines,"['Hi !\r\n\r\nThe `json` dataset script does support this format. For example loading a dataset with this format works on my side:\r\n```json\r\n{""key1"":11, ""key2"":12, ""key3"":13}\r\n{""key1"":21, ""key2"":22, ""key3"":23}\r\n```\r\n\r\nCan you show the full stacktrace please ? Also which version of datasets and pyarrow are you using ?\r\n\r\n'
 ""Hi Quentin!\r\n\r\nI apologize for bothering you. There was some issue with my pyarrow version as far as I understand. I don't remember the exact version I was using as I didn't check it.\r\n\r\nI repeated it with `datasets 1.2.1` and `pyarrow  2.0.0` and it worked.\r\n\r\nClosing this issue. Again, sorry for the bother.\r\n\r\nThanks,\r\nGunjan""]","Hello :),

I have been trying to load data using a JSON file. Based on the [docs](https://huggingface.co/docs/datasets/loading_datasets.html#json-files), the following format is supported:

```json
{""key1"":11, ""key2"":12, ""key3"":13}
{""key1"":21, ""key2"":22, ""key3"":23}
```
 But, when I try loading a dataset with the same format, I get a JSONDecodeError : `JSONDecodeError: Extra data: line 2 column 1 (char 7142)`. Now, this is expected when using `json` to load a JSON file. But I was wondering if there are any special arguments to pass when using `load_dataset` as the docs suggest that this format is supported.

When I convert the JSON file to a list of dictionaries format, I get AttributeError: `AttributeError: 'list' object has no attribute 'keys'`. So, I can't convert them to list of dictionaries either.

Please let me know :)

Thanks,
Gunjan"
https://github.com/huggingface/datasets/issues/1783,Dataset Examples Explorer,"[""Hi @ChewKokWah,\r\n\r\nWe're working on it! In the meantime, you can still find the dataset explorer at the following URL: https://huggingface.co/datasets/viewer/""
 'Glad to see that it still exist, this existing one is more than good enough for me, it is feature rich, simple to use and concise. \r\nHope similar feature can be retain in the future version.']","In the Older version of the Dataset, there are a useful Dataset Explorer that allow user to visualize the examples (training, test and validation) of a particular dataset, it is no longer there in current version.

Hope HuggingFace can re-enable the feature that at least allow viewing of  the first 20  examples of a particular dataset, or alternatively can extract 20 examples for each datasets and make those part of the Dataset Card Documentation."
https://github.com/huggingface/datasets/issues/1781,AttributeError: module 'pyarrow' has no attribute 'PyExtensionType' during import ,"[""Hi ! I'm not able to reproduce the issue. Can you try restarting your runtime ?\r\n\r\nThe PyExtensionType is available in pyarrow starting 0.17.1 iirc. If restarting your runtime doesn't fix this, can you try updating pyarrow ?\r\n```\r\npip install pyarrow --upgrade\r\n```""
 'We should bump up the version test of pyarrow maybe no?\r\n\r\nhttps://github.com/huggingface/datasets/blob/master/src/datasets/__init__.py#L60'
 ""Yes indeed.\r\n\r\nAlso it looks like Pyarrow 3.0.0 got released on pypi 10 hours ago. This might be related to the bug, I'll investigate\r\nEDIT: looks like the 3.0.0 release doesn't have unexpected breaking changes for us, so I don't think the issue comes from that""
 'Maybe colab moved to pyarrow 0.16 by default (instead of 0.14 before)?'
 ""Installing datasets installs pyarrow>=0.17.1 so in theory it doesn't matter which version of pyarrow colab has by default (which is currently pyarrow 0.14.1).\r\n\r\nAlso now the colab runtime refresh the pyarrow version automatically after the update from pip (previously you needed to restart your runtime).\r\n\r\nI guess what happened is that Colab didn't refresh pyarrow for some reason, and the AttributeError was raised *before* the pyarrow version check from `datasets` at https://github.com/huggingface/datasets/blob/master/src/datasets/__init__.py#L60""
 'Yes colab doesn’t reload preloaded library unless you restart the instance. Maybe we should move the check on top of the init '
 ""Yes I'll do that :)"" 'I updated the pyarrow version check in #1782']","I'm using Colab. And suddenly this morning, there is this error. Have a look below!

![screenshot-colab research google com-2021 01 26-08-15-36](https://user-images.githubusercontent.com/45964869/105799890-fdaf3b80-5fae-11eb-8f06-11b65cdccc30.png)
"
https://github.com/huggingface/datasets/issues/1776,[Question & Bug Report] Can we preprocess a dataset on the fly?,"['We are very actively working on this. How does your dataset look like in practice (number/size/type of files)?'
 ""It's a text file with many lines (about 1B) of Chinese sentences. I use it to train language model using https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm_wwm.py""
 'Indeed I will submit a PR in a fez days to enable processing on-the-fly :)\r\nThis can be useful in language modeling for tokenization, padding etc.\r\n'
 'any update on this issue? ...really look forward to use it '
 'Hi @acul3,\r\n\r\nPlease look at the discussion on a related Issue #1825. I think using `set_transform` after building from source should do.'
 '@gchhablani thank you so much\r\n\r\nwill try look at it']","I know we can use `Datasets.map` to preprocess a dataset, but I'm using it with very large corpus which generates huge cache file (several TB cache from a 400 GB text file). I have no disk large enough to save it.  Can we preprocess a dataset on the fly without generating cache?

BTW, I tried raising `writer_batch_size`. Seems that argument doesn't have any effect when it's larger than `batch_size`, because you are saving all the batch instantly after it's processed. Please check the following code:

https://github.com/huggingface/datasets/blob/0281f9d881f3a55c89aeaa642f1ba23444b64083/src/datasets/arrow_dataset.py#L1532"
https://github.com/huggingface/datasets/issues/1775,Efficient ways to iterate the dataset,"['It seems that selecting a subset of colums directly from the dataset, i.e., dataset[""column""], is slow.'
 'I was wrong, ```dataset[""column""]``` is fast.']","For a large dataset that does not fits the memory, how can I select only a subset of features from each example?

If I iterate over the dataset and then select the subset of features one by one, the resulted memory usage will be huge. Any ways to solve this?

Thanks"
https://github.com/huggingface/datasets/issues/1774,is it possible to make slice to be more compatible like python list and numpy?,"[""Hi ! Thanks for reporting.\r\nI am working on changes in the way data are sliced from arrow. I can probably fix your issue with the changes I'm doing.\r\nIf you have some code to reproduce the issue it would be nice so I can make sure that this case will be supported :)\r\nI'll make a PR in a few days ""
 'Good if you can take care at your side.\r\nHere is the [colab notebook](https://colab.research.google.com/drive/19c-abm87RTRYgW9G1D8ktfwRW95zDYBZ?usp=sharing)']","Hi,
see below error:
```
AssertionError: Requested slice [:10000000000000000] incompatible with 20 examples.
```"
https://github.com/huggingface/datasets/issues/1773,bug in loading datasets ,"['Looks like an issue with your csv file. Did you use the right delimiter ?\r\nApparently at line 37 the CSV reader from pandas reads 2 fields instead of 1.'
 'Note that you can pass any argument you would pass to `pandas.read_csv` as kwargs to `load_dataset`. For example you can do\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\'csv\', data_files=data_files, sep=""\\t"")\r\n```\r\n\r\nfor example to use a tab separator.\r\n\r\nYou can see the full list of arguments here: https://github.com/huggingface/datasets/blob/master/src/datasets/packaged_modules/csv/csv.py\r\n\r\n(I\'ve not found the list in the documentation though, we definitely must add them !)'
 'You can try to convert the file to (CSV UTF-8)']","Hi,
I need to load a dataset, I use these commands:

```
from datasets import load_dataset
dataset = load_dataset('csv', data_files={'train': 'sick/train.csv',
                                          'test':  'sick/test.csv',
                                          'validation': 'sick/validation.csv'})
print(dataset['validation'])
```
the dataset in sick/train.csv are simple csv files representing the data. I am getting this error, do you have an idea how I can solve this? thank you @lhoestq 

                            
```
Using custom data configuration default
Downloading and preparing dataset csv/default-61468fc71a743ec1 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /julia/cache_home_2/datasets/csv/default-61468fc71a743ec1/0.0.0/2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2...
Traceback (most recent call last):
  File ""/julia/libs/anaconda3/envs/success/lib/python3.7/site-packages/datasets-1.2.0-py3.7.egg/datasets/builder.py"", line 485, in incomplete_dir
    yield tmp_dir
  File ""/julia/libs/anaconda3/envs/success/lib/python3.7/site-packages/datasets-1.2.0-py3.7.egg/datasets/builder.py"", line 527, in download_and_prepare
    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
  File ""/julia/libs/anaconda3/envs/success/lib/python3.7/site-packages/datasets-1.2.0-py3.7.egg/datasets/builder.py"", line 604, in _download_and_prepare
    self._prepare_split(split_generator, **prepare_split_kwargs)
  File ""/julia/libs/anaconda3/envs/success/lib/python3.7/site-packages/datasets-1.2.0-py3.7.egg/datasets/builder.py"", line 959, in _prepare_split
    for key, table in utils.tqdm(generator, unit="" tables"", leave=False, disable=not_verbose):
  File ""/julia/libs/anaconda3/envs/success/lib/python3.7/site-packages/tqdm-4.49.0-py3.7.egg/tqdm/std.py"", line 1133, in __iter__
    for obj in iterable:
  File ""/julia/cache_home_2/modules/datasets_modules/datasets/csv/2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2/csv.py"", line 129, in _generate_tables
    for batch_idx, df in enumerate(csv_file_reader):
  File ""/julia/libs/anaconda3/envs/success/lib/python3.7/site-packages/pandas-1.2.0-py3.7-linux-x86_64.egg/pandas/io/parsers.py"", line 1029, in __next__
    return self.get_chunk()
  File ""/julia/libs/anaconda3/envs/success/lib/python3.7/site-packages/pandas-1.2.0-py3.7-linux-x86_64.egg/pandas/io/parsers.py"", line 1079, in get_chunk
    return self.read(nrows=size)
  File ""/julia/libs/anaconda3/envs/success/lib/python3.7/site-packages/pandas-1.2.0-py3.7-linux-x86_64.egg/pandas/io/parsers.py"", line 1052, in read
    index, columns, col_dict = self._engine.read(nrows)
  File ""/julia/libs/anaconda3/envs/success/lib/python3.7/site-packages/pandas-1.2.0-py3.7-linux-x86_64.egg/pandas/io/parsers.py"", line 2056, in read
    data = self._reader.read(nrows)
  File ""pandas/_libs/parsers.pyx"", line 756, in pandas._libs.parsers.TextReader.read
  File ""pandas/_libs/parsers.pyx"", line 783, in pandas._libs.parsers.TextReader._read_low_memory
  File ""pandas/_libs/parsers.pyx"", line 827, in pandas._libs.parsers.TextReader._read_rows
  File ""pandas/_libs/parsers.pyx"", line 814, in pandas._libs.parsers.TextReader._tokenize_rows
  File ""pandas/_libs/parsers.pyx"", line 1951, in pandas._libs.parsers.raise_parser_error
pandas.errors.ParserError: Error tokenizing data. C error: Expected 1 fields in line 37, saw 2


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""write_sick.py"", line 19, in <module>
    'validation': 'sick/validation.csv'})
  File ""/julia/libs/anaconda3/envs/success/lib/python3.7/site-packages/datasets-1.2.0-py3.7.egg/datasets/load.py"", line 612, in load_dataset
    ignore_verifications=ignore_verifications,
  File ""/julia/libs/anaconda3/envs/success/lib/python3.7/site-packages/datasets-1.2.0-py3.7.egg/datasets/builder.py"", line 534, in download_and_prepare
    self._save_info()
  File ""/julia/libs/anaconda3/envs/success/lib/python3.7/contextlib.py"", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/julia/libs/anaconda3/envs/success/lib/python3.7/site-packages/datasets-1.2.0-py3.7.egg/datasets/builder.py"", line 491, in incomplete_dir
    shutil.rmtree(tmp_dir)
  File ""/julia/libs/anaconda3/envs/success/lib/python3.7/shutil.py"", line 498, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File ""/julia/libs/anaconda3/envs/success/lib/python3.7/shutil.py"", line 496, in rmtree
    os.rmdir(path)
OSError: [Errno 39] Directory not empty: '/julia/cache_home_2/datasets/csv/default-61468fc71a743ec1/0.0.0/2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2.incomplete'
```

"
https://github.com/huggingface/datasets/issues/1771,Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/1.2.1/datasets/csv/csv.py,"['I temporary manually download csv.py as custom dataset loading script'
 ""Indeed in 1.2.1 the script to process csv file is downloaded. Starting from the next release though we include the csv processing directly in the library.\r\nSee PR #1726 \r\nWe'll do a new release soon :)""
 'Thanks.']","Hi,
When I load_dataset from local csv files, below error happened, looks raw.githubusercontent.com was blocked by the chinese government. But why it need to download csv.py? should it include when pip install the dataset?

```
Traceback (most recent call last):
  File ""/home/tom/pyenv/pystory/lib/python3.6/site-packages/datasets/load.py"", line 267, in prepare_module
    local_path = cached_path(file_path, download_config=download_config)
  File ""/home/tom/pyenv/pystory/lib/python3.6/site-packages/datasets/utils/file_utils.py"", line 343, in cached_path
    max_retries=download_config.max_retries,
  File ""/home/tom/pyenv/pystory/lib/python3.6/site-packages/datasets/utils/file_utils.py"", line 617, in get_from_cache
    raise ConnectionError(""Couldn't reach {}"".format(url))
ConnectionError: Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/1.2.1/datasets/csv/csv.py

```"
https://github.com/huggingface/datasets/issues/1770,how can I combine 2 dataset with different/same features?,"[""Hi ! Currently we don't have a way to `zip` datasets but we plan to add this soon :)\r\nFor now you'll need to use `map` to add the fields from one dataset to the other. See the comment here for more info : https://github.com/huggingface/datasets/issues/853#issuecomment-727872188""
 'Good to hear.\r\nCurrently I did not use map , just fetch src and tgt from the 2 dataset and merge them.\r\nIt will be a release if you can deal with it at the backend.\r\nThanks.']","to combine 2 dataset by one-one map like ds = zip(ds1, ds2):
ds1: {'text'}, ds2: {'text'}, combine ds:{'src', 'tgt'} 
or different feature:
ds1: {'src'}, ds2: {'tgt'}, combine ds:{'src', 'tgt'}"
https://github.com/huggingface/datasets/issues/1769,"_pickle.PicklingError: Can't pickle typing.Union[str, NoneType]: it's not the same object as typing.Union when calling datasets.map with num_proc=2","['More information: `run_mlm.py` will raise same error when `data_args.line_by_line==True`\r\n\r\nhttps://github.com/huggingface/transformers/blob/9152f16023b59d262b51573714b40325c8e49370/examples/language-modeling/run_mlm.py#L300\r\n'
 'Hi ! What version of python and datasets do you have ? And also what version of dill and pickle ?'
 '> Hi ! What version of python and datasets do you have ? And also what version of dill and pickle ?\r\n\r\npython==3.6.10\r\ndatasets==1.2.1\r\ndill==0.3.2\r\npickle.format_version==4.0'
 'Multiprocessing in python require all the functions to be picklable. More specifically, functions need to be picklable with `dill`.\r\n\r\nHowever objects like `typing.Union[str, NoneType]` are not picklable in python <3.7.\r\nCan you try to update your python version to python>=3.7 ?\r\n']","It may be a bug of multiprocessing with Datasets, when I disable the multiprocessing by set num_proc to None, everything works fine.

The script I use is https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm_wwm.py

Script args:

```
--model_name_or_path
../../../model/chinese-roberta-wwm-ext
--train_file
/nfs/volume-377-2/bert/data/test/train.txt
--output_dir
test
--do_train
--per_device_train_batch_size
2
--gradient_accumulation_steps
2
--learning_rate
1e-4
--max_steps
1000
--warmup_steps
10
--save_steps
1000
--save_total_limit
1
--seed
23333
--max_seq_length
512
--preprocessing_num_workers
2
--cache_dir
/nfs/volume-377-2/bert/data/test/cache
```

Where the `/nfs/volume-377-2/bert/data/test/train.txt` is just a toy example with 10000 lines of random string, you should be able to reproduce this error esaily.

Full Traceback:

```
Traceback (most recent call last):
  File ""/nfs/volume-377-2/bert/transformers/examples/language-modeling/run_mlm_wwm.py"", line 398, in <module>
    main()
  File ""/nfs/volume-377-2/bert/transformers/examples/language-modeling/run_mlm_wwm.py"", line 325, in main
    load_from_cache_file=not data_args.overwrite_cache,
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/site-packages/datasets/dataset_dict.py"", line 303, in map
    for k, dataset in self.items()
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/site-packages/datasets/dataset_dict.py"", line 303, in <dictcomp>
    for k, dataset in self.items()
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/site-packages/datasets/arrow_dataset.py"", line 1318, in map
    transformed_shards = [r.get() for r in results]
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/site-packages/datasets/arrow_dataset.py"", line 1318, in <listcomp>
    transformed_shards = [r.get() for r in results]
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/site-packages/multiprocess/pool.py"", line 644, in get
    raise self._value
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/site-packages/multiprocess/pool.py"", line 424, in _handle_tasks
    put(task)
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/site-packages/multiprocess/connection.py"", line 209, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/site-packages/multiprocess/reduction.py"", line 54, in dumps
    cls(buf, protocol, *args, **kwds).dump(obj)
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/site-packages/dill/_dill.py"", line 446, in dump
    StockPickler.dump(self, obj)
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py"", line 409, in dump
    self.save(obj)
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py"", line 476, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py"", line 751, in save_tuple
    save(element)
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py"", line 476, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/site-packages/dill/_dill.py"", line 933, in save_module_dict
    StockPickler.save_dict(pickler, obj)
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py"", line 821, in save_dict
    self._batch_setitems(obj.items())
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py"", line 847, in _batch_setitems
    save(v)
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py"", line 476, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/site-packages/dill/_dill.py"", line 1438, in save_function
    obj.__dict__, fkwdefaults), obj=obj)
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py"", line 610, in save_reduce
    save(args)
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py"", line 476, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py"", line 751, in save_tuple
    save(element)
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py"", line 476, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py"", line 736, in save_tuple
    save(element)
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py"", line 476, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/site-packages/dill/_dill.py"", line 1170, in save_cell
    pickler.save_reduce(_create_cell, (f,), obj=obj)
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py"", line 610, in save_reduce
    save(args)
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py"", line 476, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py"", line 736, in save_tuple
    save(element)
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py"", line 521, in save
    self.save_reduce(obj=obj, *rv)
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py"", line 605, in save_reduce
    save(cls)
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py"", line 476, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/site-packages/dill/_dill.py"", line 1365, in save_type
    obj.__bases__, _dict), obj=obj)
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py"", line 610, in save_reduce
    save(args)
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py"", line 476, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py"", line 751, in save_tuple
    save(element)
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py"", line 476, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/site-packages/dill/_dill.py"", line 933, in save_module_dict
    StockPickler.save_dict(pickler, obj)
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py"", line 821, in save_dict
    self._batch_setitems(obj.items())
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py"", line 847, in _batch_setitems
    save(v)
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py"", line 476, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/site-packages/dill/_dill.py"", line 933, in save_module_dict
    StockPickler.save_dict(pickler, obj)
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py"", line 821, in save_dict
    self._batch_setitems(obj.items())
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py"", line 847, in _batch_setitems
    save(v)
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py"", line 507, in save
    self.save_global(obj, rv)
  File ""/home/luban/miniconda3/envs/py36/lib/python3.6/pickle.py"", line 927, in save_global
    (obj, module_name, name))
_pickle.PicklingError: Can't pickle typing.Union[str, NoneType]: it's not the same object as typing.Union
```
"
https://github.com/huggingface/datasets/issues/1766,Issues when run two programs compute the same metrics,"['Hi ! To avoid collisions you can specify a `experiment_id` when instantiating your metric using `load_metric`. It will replace ""default_experiment"" with the experiment id that you provide in the arrow filename. \r\n\r\nAlso when two `experiment_id` collide we\'re supposed to detect it using our locking mechanism. Not sure why it didn\'t work in your case. Could you share some code that reproduces the issue ? This would help us investigate.'
 'Thank you for your response. I fixed the issue by set ""keep_in_memory=True"" when load_metric. \r\nI cannot share the entire source code but below is the wrapper I wrote:\r\n\r\n```python\r\nclass Evaluation:\r\n    def __init__(self, metric=\'sacrebleu\'):\r\n        # self.metric = load_metric(metric, keep_in_memory=True)\r\n        self.metric = load_metric(metric)\r\n\r\n    def add(self, predictions, references):\r\n        self.metric.add_batch(predictions=predictions, references=references)\r\n\r\n    def compute(self):\r\n        return self.metric.compute()[\'score\']\r\n```\r\n\r\nThen call the given wrapper as follows:\r\n\r\n```python\r\neval = Evaluation(metric=\'sacrebleu\')\r\nfor query, candidates,  labels in tqdm(dataset):\r\n    predictions = net.generate(query)\r\n    references = [[s] for s in labels]\r\n    eval.add(predictions, references)\r\n    if n % 100 == 0:\r\n        bleu += eval.compute()\r\n        eval = Evaluation(metric=\'sacrebleu\')']","I got the following error when running two different programs that both compute sacreblue metrics. It seems that both read/and/write to the same location (.cache/huggingface/metrics/sacrebleu/default/default_experiment-1-0.arrow) where it caches the batches:

```
File ""train_matching_min.py"", line 160, in <module>ch_9_label
    avg_loss = valid(epoch, args.batch, args.validation, args.with_label)
  File ""train_matching_min.py"", line 93, in valid
    bleu += eval.compute()
  File ""/u/tlhoang/projects/seal/match/models/eval.py"", line 23, in compute
    return self.metric.compute()['score']
  File ""/dccstor/know/anaconda3/lib/python3.7/site-packages/datasets/metric.py"", line 387, in compute
    self._finalize()
  File ""/dccstor/know/anaconda3/lib/python3.7/site-packages/datasets/metric.py"", line 355, in _finalize
    self.data = Dataset(**reader.read_files([{""filename"": f} for f in file_paths]))
  File ""/dccstor/know/anaconda3/lib/python3.7/site-packages/datasets/arrow_reader.py"", line 231, in read_files
    pa_table = self._read_files(files)
  File ""/dccstor/know/anaconda3/lib/python3.7/site-packages/datasets/arrow_reader.py"", line 170, in _read_files
    pa_table: pa.Table = self._get_dataset_from_filename(f_dict)
  File ""/dccstor/know/anaconda3/lib/python3.7/site-packages/datasets/arrow_reader.py"", line 299, in _get_dataset_from_filename
    pa_table = f.read_all()
  File ""pyarrow/ipc.pxi"", line 481, in pyarrow.lib.RecordBatchReader.read_all
  File ""pyarrow/error.pxi"", line 84, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: Expected to read 1819307375 metadata bytes, but only read 454396
``` "
https://github.com/huggingface/datasets/issues/1765,Error iterating over Dataset with DataLoader,"['Instead of:\r\n```python\r\ndataloader = torch.utils.data.DataLoader(encoded_dataset, batch_sampler=32)\r\n```\r\nIt should be:\r\n```python\r\ndataloader = torch.utils.data.DataLoader(encoded_dataset, batch_size=32)\r\n```\r\n\r\n`batch_sampler` accepts a Sampler object or an Iterable, so you get an error.'
 ""@mariosasko I thought that would fix it, but now I'm getting a different error:\r\n\r\n```\r\n/usr/local/lib/python3.6/dist-packages/datasets/arrow_dataset.py:851: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\r\n  return torch.tensor(x, **format_kwargs)\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-20-3af1d82bf93a> in <module>()\r\n      1 dataloader = torch.utils.data.DataLoader(encoded_dataset, batch_size=32)\r\n----> 2 next(iter(dataloader))\r\n\r\n5 frames\r\n/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/collate.py in default_collate(batch)\r\n     53             storage = elem.storage()._new_shared(numel)\r\n     54             out = elem.new(storage)\r\n---> 55         return torch.stack(batch, 0, out=out)\r\n     56     elif elem_type.__module__ == 'numpy' and elem_type.__name__ != 'str_' \\\r\n     57             and elem_type.__name__ != 'string_':\r\n\r\nRuntimeError: stack expects each tensor to be equal size, but got [7] at entry 0 and [10] at entry 1\r\n```\r\n\r\nAny thoughts what this means?I Do I need padding?""
 'Yes, padding is an answer. \r\n\r\nThis can be solved easily by passing a callable to the collate_fn arg of DataLoader that adds padding. '
 'Padding was the fix, thanks!']","I have a Dataset that I've mapped a tokenizer over:

```
encoded_dataset.set_format(type='torch',columns=['attention_mask','input_ids','token_type_ids'])
encoded_dataset[:1]
```
```
{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),
 'input_ids': tensor([[  101,   178,  1198,  1400,  1714, 22233, 21365,  4515,  8618,  1113,
            102]]),
 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}
```

When I try to iterate as in the docs, I get errors:

```
dataloader = torch.utils.data.DataLoader(encoded_dataset, batch_sampler=32)
next(iter(dataloader))
```

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-45-05180ba8aa35> in <module>()
      1 dataloader = torch.utils.data.DataLoader(encoded_dataset, batch_sampler=32)
----> 2 next(iter(dataloader))

3 frames
/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py in __init__(self, loader)
    411         self._timeout = loader.timeout
    412         self._collate_fn = loader.collate_fn
--> 413         self._sampler_iter = iter(self._index_sampler)
    414         self._base_seed = torch.empty((), dtype=torch.int64).random_(generator=loader.generator).item()
    415         self._persistent_workers = loader.persistent_workers

TypeError: 'int' object is not iterable


```"
https://github.com/huggingface/datasets/issues/1764,Connection Issues,['Academic WIFI was blocking.'],"Today, I am getting connection issues while loading a dataset and the metric.
```
Traceback (most recent call last):
  File ""src/train.py"", line 180, in <module>
    train_dataset, dev_dataset, test_dataset = create_race_dataset()
  File ""src/train.py"", line 130, in create_race_dataset
    train_dataset = load_dataset(""race"", ""all"", split=""train"")
  File ""/Users/saeed/Desktop/codes/repos/dreamscape-qa/env/lib/python3.7/site-packages/datasets/load.py"", line 591, in load_dataset
    path, script_version=script_version, download_config=download_config, download_mode=download_mode, dataset=True
  File ""/Users/saeed/Desktop/codes/repos/dreamscape-qa/env/lib/python3.7/site-packages/datasets/load.py"", line 267, in prepare_module
    local_path = cached_path(file_path, download_config=download_config)
  File ""/Users/saeed/Desktop/codes/repos/dreamscape-qa/env/lib/python3.7/site-packages/datasets/utils/file_utils.py"", line 343, in cached_path
    max_retries=download_config.max_retries,
  File ""/Users/saeed/Desktop/codes/repos/dreamscape-qa/env/lib/python3.7/site-packages/datasets/utils/file_utils.py"", line 617, in get_from_cache
    raise ConnectionError(""Couldn't reach {}"".format(url))
ConnectionError: Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/1.2.1/datasets/race/race.py
```

Or

```
Traceback (most recent call last):
  File ""src/train.py"", line 105, in <module>
    rouge = datasets.load_metric(""rouge"")
  File ""/Users/saeed/Desktop/codes/repos/dreamscape-qa/env/lib/python3.7/site-packages/datasets/load.py"", line 500, in load_metric
    dataset=False,
  File ""/Users/saeed/Desktop/codes/repos/dreamscape-qa/env/lib/python3.7/site-packages/datasets/load.py"", line 267, in prepare_module
    local_path = cached_path(file_path, download_config=download_config)
  File ""/Users/saeed/Desktop/codes/repos/dreamscape-qa/env/lib/python3.7/site-packages/datasets/utils/file_utils.py"", line 343, in cached_path
    max_retries=download_config.max_retries,
  File ""/Users/saeed/Desktop/codes/repos/dreamscape-qa/env/lib/python3.7/site-packages/datasets/utils/file_utils.py"", line 617, in get_from_cache
    raise ConnectionError(""Couldn't reach {}"".format(url))
ConnectionError: Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/1.2.1/metrics/rouge/rouge.py
```"
https://github.com/huggingface/datasets/issues/1762,Unable to format dataset to CUDA Tensors,"['Hi ! You can get CUDA tensors with\r\n\r\n```python\r\ndataset.set_format(""torch"", columns=columns, device=""cuda"")\r\n```\r\n\r\nIndeed `set_format` passes the `**kwargs` to `torch.tensor`'
 ""Hi @lhoestq,\r\n\r\nThanks a lot. Is this true for all format types?\r\n\r\nAs in, for 'torch', I can have `**kwargs` to `torch.tensor` and for 'tf' those args are passed to `tf.Tensor`, and the same for 'numpy' and 'pandas'?""
 ""Yes the keywords arguments are passed to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.\r\nWe don't support the kwargs for pandas on the other hand.""
 'Thanks @lhoestq,\r\nWould it be okay if I added this to the docs and made a PR?'
 'Sure ! Feel free to open a PR to improve the documentation :) '
 'Closing this issue as it has been resolved.']","Hi,

I came across this [link](https://huggingface.co/docs/datasets/torch_tensorflow.html) where the docs show show to convert a dataset to a particular format. I see that there is an option to convert it to tensors, but I don't see any option to convert it to CUDA tensors.

I tried this, but Dataset doesn't support assignment:
```
  columns=['input_ids', 'token_type_ids', 'attention_mask', 'start_positions','end_positions']

        samples.set_format(type='torch', columns = columns)
        for column in columns:
            samples[column].to(torch.device(self.config.device))
```
There should be an option to do so, or if there is already a way to do this, please let me know.

Thanks,
Gunjan"
https://github.com/huggingface/datasets/issues/1759,wikipedia dataset incomplete,"['Hi !\r\nFrom what pickle file fo you get this ?\r\nI guess you mean the dataset loaded using `load_dataset` ?'
 ""yes sorry, I used the `load_dataset`function and saved the data to a pickle file so I don't always have to reload it and are able to work offline. ""
 'The wikipedia articles are processed using the `mwparserfromhell` library. Even if it works well in most cases, such issues can happen unfortunately. You can find the repo here: https://github.com/earwig/mwparserfromhell\r\n\r\nThere also exist other datasets based on wikipedia that were processed differently (and are often cleaner) such as `wiki40b`.\r\n\r\n'
 'ok great. Thank you, @lhoestq. ']","Hey guys,

I am using the https://github.com/huggingface/datasets/tree/master/datasets/wikipedia dataset.
Unfortunately, I found out that there is an incompleteness for the German dataset.
 For reasons unknown to me, the number of inhabitants has been removed from many pages:
Thorey-sur-Ouche has 128 inhabitants according to the webpage (https://de.wikipedia.org/wiki/Thorey-sur-Ouche).
The pickle file however shows: französische Gemeinde mit  Einwohnern (Stand).
 Is it possible to fix this?

Best regards 
Chris
"
https://github.com/huggingface/datasets/issues/1758,dataset.search() (elastic) cannot reliably retrieve search results,"['Hi !\r\nI tried your code on my side and I was able to workaround this issue by waiting a few seconds before querying the index.\r\nMaybe this is because the index is not updated yet on the ElasticSearch side ?'
 'Thanks for the feedback! I added a 30 second ""sleep"" and that seemed to work well!']","I am trying to use elastic search to retrieve the indices of items in the dataset in their precise order, given shuffled training indices.

The problem I have is that I cannot retrieve reliable results with my data on my first search. I have to run the search **twice** to get the right answer.

I am indexing data that looks like the following from the HF SQuAD 2.0 data set:

```
['57318658e6313a140071d02b',
 '56f7165e3d8e2e1400e3733a',
 '570e2f6e0b85d914000d7d21',
 '5727e58aff5b5019007d97d0',
 '5a3b5a503ff257001ab8441f',
 '57262fab271a42140099d725']
```



To reproduce the issue, try:

```
from datasets import load_dataset, load_metric
from transformers import BertTokenizerFast, BertForQuestionAnswering
from elasticsearch import Elasticsearch
import numpy as np
import collections
from tqdm.auto import tqdm
import torch

# from https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/question_answering.ipynb#scrollTo=941LPhDWeYv-
tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')
max_length = 384 # The maximum length of a feature (question and context)
doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed.
pad_on_right = tokenizer.padding_side == ""right""
squad_v2 = True

# from https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/question_answering.ipynb#scrollTo=941LPhDWeYv-
def prepare_validation_features(examples):
    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results
    # in one example possible giving several features when a context is long, each of those features having a
    # context that overlaps a bit the context of the previous feature.
    tokenized_examples = tokenizer(
        examples[""question"" if pad_on_right else ""context""],
        examples[""context"" if pad_on_right else ""question""],
        truncation=""only_second"" if pad_on_right else ""only_first"",
        max_length=max_length,
        stride=doc_stride,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding=""max_length"",
    )

    # Since one example might give us several features if it has a long context, we need a map from a feature to
    # its corresponding example. This key gives us just that.
    sample_mapping = tokenized_examples.pop(""overflow_to_sample_mapping"")

    # We keep the example_id that gave us this feature and we will store the offset mappings.
    tokenized_examples[""example_id""] = []

    for i in range(len(tokenized_examples[""input_ids""])):
        # Grab the sequence corresponding to that example (to know what is the context and what is the question).
        sequence_ids = tokenized_examples.sequence_ids(i)
        context_index = 1 if pad_on_right else 0

        # One example can give several spans, this is the index of the example containing this span of text.
        sample_index = sample_mapping[i]
        tokenized_examples[""example_id""].append(examples[""id""][sample_index])

        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token
        # position is part of the context or not.
        tokenized_examples[""offset_mapping""][i] = [
            (list(o) if sequence_ids[k] == context_index else None)
            for k, o in enumerate(tokenized_examples[""offset_mapping""][i])
        ]

    return tokenized_examples





# build base examples, features set of training data
shuffled_idx = pd.read_csv('https://raw.githubusercontent.com/afogarty85/temp/main/idx.csv')['idx'].to_list()
examples = load_dataset(""squad_v2"").shuffle(seed=1)['train']
features = load_dataset(""squad_v2"").shuffle(seed=1)['train'].map(
    prepare_validation_features,
    batched=True,
    remove_columns=['answers', 'context', 'id', 'question', 'title'])
# reorder features by the training process
features = features.select(indices=shuffled_idx)
# get the example ids to match with the ""example"" data; get unique entries
id_list = list(dict.fromkeys(features['example_id']))
# now search for their index positions in the examples data set; load elastic search
es = Elasticsearch([{'host': 'localhost'}]).ping()
# add an index to the id column for the examples
examples.add_elasticsearch_index(column='id')
# retrieve the example index
example_idx_k1 = [examples.search(index_name='id', query=i, k=1).indices for i in id_list]
example_idx_k1 = [item for sublist in example_idx_k1 for item in sublist]

example_idx_k2 = [examples.search(index_name='id', query=i, k=3).indices for i in id_list]
example_idx_k2 = [item for sublist in example_idx_k2 for item in sublist]

len(example_idx_k1)  # should be 130319
len(example_idx_k2)  # should be 130319

#trial 1 lengths:
# k=1: 130314
# k=3: 130319

# trial 2:
# just run k=3 first: 130310
# try k=1 after k=3: 130319
```

"
https://github.com/huggingface/datasets/issues/1757,FewRel,"['+1'
 '@dspoka Please check the following link : https://github.com/thunlp/FewRel\r\nThis link mentions two versions of the datasets. Also, this one seems to be the official link.\r\n\r\nI am assuming this is the correct link and implementing based on the same.'
 'Hi @lhoestq,\r\n\r\nThis issue can be closed, I guess.'
 'Yes :) closing\r\nThanks again for adding FewRel !'
 ""Thanks for adding this @gchhablani ! Sorry didn't see the email notifications sooner!""]","## Adding a Dataset
- **Name:** FewRel
- **Description:**  Large-Scale Supervised Few-Shot Relation Classification Dataset
- **Paper:** @inproceedings{han2018fewrel,
               title={FewRel:A Large-Scale Supervised Few-Shot Relation Classification Dataset with State-of-the-Art Evaluation},
               author={Han, Xu and Zhu, Hao and Yu, Pengfei and Wang, Ziyun and Yao, Yuan and Liu, Zhiyuan and Sun, Maosong},
               booktitle={EMNLP},
               year={2018}}
- **Data:** https://github.com/ProKil/FewRel
- **Motivation:** relationship extraction dataset that's been used by some state of the art systems that should be incorporated.

Instructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).
"
https://github.com/huggingface/datasets/issues/1755,Using select/reordering datasets slows operations down immensely,"['You can use `Dataset.flatten_indices()` to make it fast after a select or shuffle.'
 'Thanks for the input! I gave that a try by adding this after my selection / reordering operations, but before the big computation task of `score_squad`\r\n\r\n```\r\nexamples = examples.flatten_indices()\r\nfeatures = features.flatten_indices()\r\n```\r\n\r\nThat helped quite a bit!']","I am using portions of HF's helpful work in preparing / scoring the SQuAD 2.0 data. The problem I have is that after using `select` to re-ordering the dataset, computations slow down immensely where the total scoring process on 131k training examples would take maybe 3 minutes, now take over an hour.

The below example should be reproducible and I have ran myself down this path because I want to use HF's scoring functions and helpful data preparation, but use my own trainer. The training process uses shuffle and therefore the order I trained on no longer matches the original data set order. So, to score my results correctly, the original data set needs to match the order of the training. This requires that I: (1) collect the index for each row of data emitted during training, and (2) use this index information to re-order the datasets correctly so the orders match when I go to score.


The problem is, the dataset class starts performing very poorly as soon as you start manipulating its order by immense magnitudes.



```
from datasets import load_dataset, load_metric
from transformers import BertTokenizerFast, BertForQuestionAnswering
from elasticsearch import Elasticsearch
import numpy as np
import collections
from tqdm.auto import tqdm
import torch

# from https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/question_answering.ipynb#scrollTo=941LPhDWeYv-
tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')
max_length = 384 # The maximum length of a feature (question and context)
doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed.
pad_on_right = tokenizer.padding_side == ""right""
squad_v2 = True

# from https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/question_answering.ipynb#scrollTo=941LPhDWeYv-
def prepare_validation_features(examples):
    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results
    # in one example possible giving several features when a context is long, each of those features having a
    # context that overlaps a bit the context of the previous feature.
    tokenized_examples = tokenizer(
        examples[""question"" if pad_on_right else ""context""],
        examples[""context"" if pad_on_right else ""question""],
        truncation=""only_second"" if pad_on_right else ""only_first"",
        max_length=max_length,
        stride=doc_stride,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding=""max_length"",
    )

    # Since one example might give us several features if it has a long context, we need a map from a feature to
    # its corresponding example. This key gives us just that.
    sample_mapping = tokenized_examples.pop(""overflow_to_sample_mapping"")

    # We keep the example_id that gave us this feature and we will store the offset mappings.
    tokenized_examples[""example_id""] = []

    for i in range(len(tokenized_examples[""input_ids""])):
        # Grab the sequence corresponding to that example (to know what is the context and what is the question).
        sequence_ids = tokenized_examples.sequence_ids(i)
        context_index = 1 if pad_on_right else 0

        # One example can give several spans, this is the index of the example containing this span of text.
        sample_index = sample_mapping[i]
        tokenized_examples[""example_id""].append(examples[""id""][sample_index])

        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token
        # position is part of the context or not.
        tokenized_examples[""offset_mapping""][i] = [
            (list(o) if sequence_ids[k] == context_index else None)
            for k, o in enumerate(tokenized_examples[""offset_mapping""][i])
        ]

    return tokenized_examples

# from https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/question_answering.ipynb#scrollTo=941LPhDWeYv-
def postprocess_qa_predictions(examples, features, starting_logits, ending_logits, n_best_size = 20, max_answer_length = 30):
    all_start_logits, all_end_logits = starting_logits, ending_logits
    # Build a map example to its corresponding features.
    example_id_to_index = {k: i for i, k in enumerate(examples[""id""])}
    features_per_example = collections.defaultdict(list)

    for i, feature in enumerate(features):
        features_per_example[example_id_to_index[feature[""example_id""]]].append(i)

    # The dictionaries we have to fill.
    predictions = collections.OrderedDict()

    # Logging.
    print(f""Post-processing {len(examples)} example predictions split into {len(features)} features."")

    # Let's loop over all the examples!
    for example_index, example in enumerate(tqdm(examples)):
        # Those are the indices of the features associated to the current example.
        feature_indices = features_per_example[example_index]

        min_null_score = None # Only used if squad_v2 is True.
        valid_answers = []

        context = example[""context""]
        # Looping through all the features associated to the current example.
        for feature_index in feature_indices:

            # We grab the predictions of the model for this feature.
            start_logits = all_start_logits[feature_index]
            end_logits = all_end_logits[feature_index]
            # This is what will allow us to map some the positions in our logits to span of texts in the original
            # context.
            offset_mapping = features[feature_index][""offset_mapping""]

            # Update minimum null prediction.
            cls_index = features[feature_index][""input_ids""].index(tokenizer.cls_token_id)
            feature_null_score = start_logits[cls_index] + end_logits[cls_index]
            if min_null_score is None or min_null_score < feature_null_score:
                min_null_score = feature_null_score

            # Go through all possibilities for the `n_best_size` greater start and end logits.
            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()
            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()
            for start_index in start_indexes:
                for end_index in end_indexes:
                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond
                    # to part of the input_ids that are not in the context.
                    if (
                        start_index >= len(offset_mapping)
                        or end_index >= len(offset_mapping)
                        or offset_mapping[start_index] is None
                        or offset_mapping[end_index] is None
                    ):
                        continue
                    # Don't consider answers with a length that is either < 0 or > max_answer_length.
                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:
                        continue

                    start_char = offset_mapping[start_index][0]
                    end_char = offset_mapping[end_index][1]
                    valid_answers.append(
                        {
                            ""score"": start_logits[start_index] + end_logits[end_index],
                            ""text"": context[start_char: end_char]
                        }
                    )


        if len(valid_answers) > 0:
            best_answer = sorted(valid_answers, key=lambda x: x[""score""], reverse=True)[0]
        else:
            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid
            # failure.
            best_answer = {""text"": """", ""score"": 0.0}

        # Let's pick our final answer: the best one or the null answer (only for squad_v2)
        if not squad_v2:
            predictions[example[""id""]] = best_answer[""text""]
        else:
            answer = best_answer[""text""] if best_answer[""score""] > min_null_score else """"
            predictions[example[""id""]] = answer

    return predictions



# build base examples, features from training data
examples = load_dataset(""squad_v2"").shuffle(seed=5)['train']
features = load_dataset(""squad_v2"").shuffle(seed=5)['train'].map(
    prepare_validation_features,
    batched=True,
    remove_columns=['answers', 'context', 'id', 'question', 'title'])

# sim some shuffled training indices that we want to use to re-order the data to compare how we did
shuffle_idx = np.arange(0, 131754)
np.random.shuffle(shuffle_idx)
# create a new dataset with rows selected following the training shuffle
features = features.select(indices=shuffle_idx)
# get unique example ids to match with the ""example"" data
id_list = list(dict.fromkeys(features['example_id']))
# now search for their index positions; load elastic search
es = Elasticsearch([{'host': 'localhost'}]).ping()
# add an index to the id column for the examples
examples.add_elasticsearch_index(column='id')
# search the examples for their index position
example_idx = [examples.search(index_name='id', query=i, k=1).indices for i in id_list]
# drop the elastic search
examples.drop_index(index_name='id')
# put examples in the right order
examples = examples.select(indices=example_idx)

# generate some fake data
logits = {'starting_logits': torch.randn(131754, 384), 'ending_logits': torch.randn(131754, 384)}


def score_squad(logits, n_best_size, max_answer):
    # proceed with QA calculation
    final_predictions = postprocess_qa_predictions(examples=examples,
                                                   features=features,
                                                   starting_logits=logits['starting_logits'],
                                                   ending_logits=logits['ending_logits'],
                                                   n_best_size=20,
                                                   max_answer_length=30)
    metric = load_metric(""squad_v2"")
    formatted_predictions = [{""id"": k, ""prediction_text"": v, ""no_answer_probability"": 0.0} for k, v in final_predictions.items()]
    references = [{""id"": ex[""id""], ""answers"": ex[""answers""]} for ex in examples]
    metrics = metric.compute(predictions=formatted_predictions, references=references)
    return metrics

metrics = score_squad(logits, n_best_size=20, max_answer=30)
```













"
https://github.com/huggingface/datasets/issues/1747,datasets slicing with seed ,"[""Hi :) \r\nThe slicing API from https://huggingface.co/docs/datasets/splits.html doesn't shuffle the data.\r\nYou can shuffle and then take a subset of your dataset with\r\n```python\r\n# shuffle and take the first 100 examples\r\ndataset = dataset.shuffle(seed=42).select(range(100))\r\n```\r\n\r\nYou can find more information about shuffling and selecting rows in the documentation: https://huggingface.co/docs/datasets/processing.html#selecting-sorting-shuffling-splitting-rows""
 ""thank you so much\n\nOn Mon, Jan 18, 2021 at 3:17 PM Quentin Lhoest <notifications@github.com>\nwrote:\n\n> Hi :)\n> The slicing API doesn't shuffle the data.\n> You can shuffle and then take a subset of your dataset with\n>\n> # shuffle and take the first 100 examplesdataset = dataset.shuffle(seed=42).select(range(100))\n>\n> You can find more information about shuffling and selecting rows in the\n> documentation:\n> https://huggingface.co/docs/datasets/processing.html#selecting-sorting-shuffling-splitting-rows\n>\n> —\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/huggingface/datasets/issues/1747#issuecomment-762278134>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AM3GZM5D5MDPLJGI4IG3UADS2Q7GPANCNFSM4WHLOZJQ>\n> .\n>\n""]","Hi
I need to slice a dataset with random seed, I looked into documentation here https://huggingface.co/docs/datasets/splits.html 
I could not find a seed option, could you assist me please how I can get a slice for different seeds?
thank you.
@lhoestq  "
https://github.com/huggingface/datasets/issues/1745,difference between wsc and wsc.fixed for superglue,['From the description given in the dataset script for `wsc.fixed`:\r\n```\r\nThis version fixes issues where the spans are not actually substrings of the text.\r\n```'],"Hi
I see two versions of wsc in superglue, and I am not sure what is the differences and which one is the original one. could you help to discuss the differences? thanks @lhoestq "
https://github.com/huggingface/datasets/issues/1743,Issue while Creating Custom Metric,"['Currently it\'s only possible to define the features for the two columns `references` and `predictions`.\r\nThe data for these columns can then be passed to `metric.add_batch` and `metric.compute`.\r\nInstead of defining more columns `text`, `offset_mapping` and `ground` you must include them in either references and predictions.\r\n\r\nFor example \r\n```python\r\nfeatures = datasets.Features({\r\n    \'predictions\':datasets.Sequence(datasets.Value(""int32"")),\r\n    ""references"": datasets.Sequence({\r\n        ""references_ids"": datasets.Value(""int32""),\r\n        ""offset_mapping"": datasets.Value(""int32""),\r\n        \'text\': datasets.Value(\'string\'),\r\n        ""ground"": datasets.Value(""int32"")\r\n    }),\r\n})\r\n```\r\n\r\nAnother option would be to simply have the two features like \r\n```python\r\nfeatures = datasets.Features({\r\n    \'predictions\':datasets.Sequence(datasets.Value(""int32"")),\r\n    ""references"": datasets.Sequence(datasets.Value(""int32"")),\r\n})\r\n```\r\nand keep `offset_mapping`, `text` and `ground` as as parameters for the computation (i.e. kwargs when calling `metric.compute`).\r\n\r\n\r\nWhat is the metric you would like to implement ?\r\n\r\nI\'m asking since we consider allowing additional fields as requested in the `Comet` metric (see PR and discussion [here](https://github.com/huggingface/datasets/pull/1577)) and I\'d like to know if it\'s something that can be interesting for users.\r\n\r\nWhat do you think ?'
 ""Hi @lhoestq,\r\n\r\nI am doing text segmentation and the metric is effectively dice score on character offsets. So I need to pass the actual spans and I want to be able to get the spans based on predictions using offset_mapping.\r\n\r\nIncluding them in references seems like a good idea. I'll try it out and get back to you. If there's a better way to write a metric function for the same, please let me know.""]","Hi Team,

I am trying to create a custom metric for my training as follows, where f1 is my own metric:

```python
 def _info(self):
        # TODO: Specifies the datasets.MetricInfo object
        return datasets.MetricInfo(
            # This is the description that will appear on the metrics page.
            description=_DESCRIPTION,
            citation=_CITATION,
            inputs_description=_KWARGS_DESCRIPTION,
            # This defines the format of each prediction and reference
            features = datasets.Features({'predictions':datasets.Sequence(datasets.Value(""int32"")), ""references"": datasets.Sequence(datasets.Value(""int32"")),""offset_mapping"":datasets.Sequence(datasets.Value(""int32"")),'text':datasets.Sequence(datasets.Value('string')),""ground"":datasets.Sequence(datasets.Value(""int32"")),}),
            # Homepage of the metric for documentation
            homepage=""http://metric.homepage"",
            # Additional links to the codebase or references
            codebase_urls=[""http://github.com/path/to/codebase/of/new_metric""],
            reference_urls=[""http://path.to.reference.url/new_metric""]
        )

    def _compute(self,predictions,references,text,offset_mapping,spans):

        pred_spans = []

        for i,preds in enumerate(predictions):
            current_preds = []
            for j,token_preds in enumerate(preds):
                if (preds>0.5):
                    current_preds+=list(range(offset_mapping[i][j][0],offset_mapping[i][j][1]))
            pred_spans.append(current_spans)
        
        return {
            ""Token Wise F1"": f1_score(references,predictions,labels=[0,1]),
            ""Offset Wise F1"": np.mean([f1(preds,gold) for preds,fold in zip(pred_spans,ground)])
        }

```

I believe this is not correct. But that's not the issue I am facing right now. I get this error :
```python
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-144-ed7349b50821> in <module>()
----> 1 new_metric.compute(predictions=inputs[""labels""],references=inputs[""labels""], text=inputs[""text""], offset_mapping=inputs[""offset_mapping""],ground=inputs[""ground""] )

2 frames
/usr/local/lib/python3.6/dist-packages/datasets/features.py in encode_batch(self, batch)
    802         encoded_batch = {}
    803         if set(batch) != set(self):
--> 804             print(batch)
    805             print(self)
    806             raise ValueError(""Column mismatch between batch {} and features {}"".format(set(batch), set(self)))

ValueError: Column mismatch between batch {'references', 'predictions'} and features {'ground', 'predictions', 'offset_mapping', 'text', 'references'}
```
On checking the features.py file, I see the call is made from add_batch() in metrics.py which only takes in predictions and references.

How do I make my custom metric work? Will it work with a trainer even if I am able to make this metric work?

Thanks,
Gunjan"
https://github.com/huggingface/datasets/issues/1741,error when run fine_tuning on text_classification,['none'],"dataset:sem_eval_2014_task_1
pretrained_model:bert-base-uncased

error description:
when i use these resoruce to train fine_tuning a text_classification on sem_eval_2014_task_1,there always be some problem(when i use other dataset ,there exist the error too). And i followed the colab code (url:https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/text_classification.ipynb#scrollTo=TlqNaB8jIrJW).


the error is like this :
`File ""train.py"", line 69, in <module>
    trainer.train()
  File ""/home/projects/anaconda3/envs/calibration/lib/python3.7/site-packages/transformers/trainer.py"", line 784, in train
    for step, inputs in enumerate(epoch_iterator):
  File ""/home/projects/anaconda3/envs/calibration/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 435, in __next__
    data = self._next_data()
  File ""/home/projects/anaconda3/envs/calibration/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 475, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File ""/home/projects/anaconda3/envs/calibration/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py"", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File ""/home/projects/anaconda3/envs/calibration/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py"", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
KeyError: 2`

this is my code :
```dataset_name = 'sem_eval_2014_task_1'
num_labels_size = 3
batch_size = 4
model_checkpoint = 'bert-base-uncased'
number_train_epoch = 5

def tokenize(batch):
return tokenizer(batch['premise'], batch['hypothesis'], truncation=True, )

def compute_metrics(pred):
labels = pred.label_ids
preds = pred.predictions.argmax(-1)
precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='micro')
acc = accuracy_score(labels, preds)
return {
'accuracy': acc,
'f1': f1,
'precision': precision,
'recall': recall
}

model = BertForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels_size)
tokenizer = BertTokenizerFast.from_pretrained(model_checkpoint, use_fast=True)

train_dataset = load_dataset(dataset_name, split='train')
test_dataset = load_dataset(dataset_name, split='test')

train_encoded_dataset = train_dataset.map(tokenize, batched=True)
test_encoded_dataset = test_dataset.map(tokenize, batched=True)

args = TrainingArguments(
output_dir='./results',
evaluation_strategy=""epoch"",
learning_rate=2e-5,
per_device_train_batch_size=batch_size,
per_device_eval_batch_size=batch_size,
num_train_epochs=number_train_epoch,
weight_decay=0.01,
do_predict=True,
)
trainer = Trainer(
model=model,
args=args,
compute_metrics=compute_metrics,
train_dataset=train_encoded_dataset,
eval_dataset=test_encoded_dataset,
tokenizer=tokenizer
)

trainer.train()
trainer.evaluate()

"
https://github.com/huggingface/datasets/issues/1733,"connection issue with glue, what is the data url for glue? ","['Hello @juliahane, which config of GLUE causes you trouble?\r\nThe URLs are defined in the dataset script source code: https://github.com/huggingface/datasets/blob/master/datasets/glue/glue.py']","Hi
my codes sometimes fails due to connection issue with glue, could you tell me how I can have the URL datasets library is trying to read GLUE from to test the machines I am working on if there is an issue on my side or not
thanks "
https://github.com/huggingface/datasets/issues/1731,Couldn't reach swda.py,"['Hi @yangp725,\r\nThe SWDA has been added very recently and has not been released yet, thus it is not available in the `1.2.0` version of 🤗`datasets`.\r\nYou can still access it by installing the latest version of the library (master branch), by following instructions in [this issue](https://github.com/huggingface/datasets/issues/1641#issuecomment-751571471).\r\nLet me know if this helps !'
 'Thanks @SBrandeis ,\r\nProblem solved by downloading and installing the latest version datasets.']","ConnectionError: Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/1.2.0/datasets/swda/swda.py
"
https://github.com/huggingface/datasets/issues/1729,Is there support for Deep learning datasets?,"['Hi @ZurMaD!\r\nThanks for your interest in 🤗 `datasets`. Support for image datasets is at an early stage, with CIFAR-10 added in #1617 \r\nMNIST is also on the way: #1730 \r\n\r\nIf you feel like adding another image dataset, I would advise starting by reading the [ADD_NEW_DATASET.md](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md) guide. New datasets are always very much appreciated 🚀\r\n']",I looked around this repository and looking the datasets I think that there's no support for images-datasets. Or am I missing something? For example to add a repo like this https://github.com/DZPeru/fish-datasets
https://github.com/huggingface/datasets/issues/1728,Add an entry to an arrow dataset,"['Hi @ameet-1997,\r\nI think what you are looking for is the `concatenate_datasets` function: https://huggingface.co/docs/datasets/processing.html?highlight=concatenate#concatenate-several-datasets\r\n\r\nFor your use case, I would use the [`map` method](https://huggingface.co/docs/datasets/processing.html?highlight=concatenate#processing-data-with-map) to transform the SQuAD sentences and the `concatenate` the original and mapped dataset.\r\n\r\nLet me know If this helps!'
 ""That's a great idea! Thank you so much!\r\n\r\nWhen I try that solution, I get the following error when I try to concatenate `datasets` and `modified_dataset`. I have also attached the output I get when I print out those two variables. Am I missing something?\r\n\r\nCode:\r\n``` python\r\ncombined_dataset = concatenate_datasets([datasets, modified_dataset])\r\n```\r\n\r\nError:\r\n```\r\nAttributeError: 'DatasetDict' object has no attribute 'features'\r\n```\r\n\r\nOutput:\r\n```\r\n(Pdb) datasets\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['attention_mask', 'input_ids', 'special_tokens_mask'],\r\n        num_rows: 493\r\n    })\r\n})\r\n(Pdb) modified_dataset\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['attention_mask', 'input_ids', 'special_tokens_mask'],\r\n        num_rows: 493\r\n    })\r\n})\r\n```\r\n\r\nThe error is stemming from the fact that the attribute `datasets.features` does not exist. Would it not be possible to use `concatenate_datasets` in such a case? Is there an alternate solution?""
 ""You should do `combined_dataset = concatenate_datasets([datasets['train'], modified_dataset['train']])`\r\n\r\nDidn't we talk about returning a Dataset instead of a DatasetDict with load_dataset and no split provided @lhoestq? Not sure it's the way to go but I'm wondering if it's not simpler for some use-cases.""
 '> Didn\'t we talk about returning a Dataset instead of a DatasetDict with load_dataset and no split provided @lhoestq? Not sure it\'s the way to go but I\'m wondering if it\'s not simpler for some use-cases.\r\n\r\nMy opinion is that users should always know in advance what type of objects they\'re going to get. Otherwise the development workflow on their side is going to be pretty chaotic with sometimes unexpected behaviors.\r\nFor instance is `split=` is not specified it\'s currently always returning a DatasetDict. And if `split=""train""` is given for example it\'s always returning a Dataset.'
 'Thanks @thomwolf. Your solution worked!']","Is it possible to add an entry to a dataset object?

**Motivation: I want to transform the sentences in the dataset and add them to the original dataset**

For example, say we have the following code:

``` python
from datasets import load_dataset

# Load a dataset and print the first examples in the training set
squad_dataset = load_dataset('squad')
print(squad_dataset['train'][0])
```

Is it possible to add an entry to `squad_dataset`? Something like the following?

``` python
squad_dataset.append({'text': ""This is a new sentence""})
```

The motivation for doing this is that I want to transform the sentences in the squad dataset and add them to the original dataset.

If the above doesn't work, is there any other way of achieving the motivation mentioned above? Perhaps by creating a new arrow dataset by using the older one and the transformer sentences?
"
https://github.com/huggingface/datasets/issues/1727,BLEURT score calculation raises UnrecognizedFlagError,"['Upgrading tensorflow to version 2.4.0 solved the issue.'
 'I still have the same error even with TF 2.4.0.'
 'And I have the same error with TF 2.4.1. I believe this issue should be reopened. Any ideas?!'
 'I\'m seeing the same issue with TF 2.4.1 when running the following in https://colab.research.google.com/github/huggingface/datasets/blob/master/notebooks/Overview.ipynb:\r\n```\r\n!pip install git+https://github.com/google-research/bleurt.git\r\nreferences = [""foo bar baz"", ""one two three""]\r\nbleurt_metric = load_metric(\'bleurt\')\r\npredictions =  [""foo bar"", ""four five six""]\r\nbleurt_metric.compute(predictions=predictions, references=references)\r\n```'
 ""@aleSuglia @oscartackstrom - Are you getting the error when running your code in a Jupyter notebook ?\r\n\r\nI tried reproducing this error again, and was unable to do so from the python command line console in a virtual environment similar to the one I originally used (and unfortunately no longer have access to) when I first got the error. \r\nHowever, I've managed to reproduce the error by running the same code in a Jupyter notebook running a kernel from the same virtual environment.\r\nThis made me suspect that the problem is somehow related to the Jupyter notebook.\r\n\r\nMore environment details:\r\n```\r\nOS: Ubuntu Linux 18.04\r\nconda==4.8.3\r\npython==3.8.5\r\ndatasets==1.3.0\r\ntensorflow==2.4.0\r\nBLEURT==0.0.1\r\nnotebook==6.2.0\r\n```""
 ""This happens when running the notebook on colab. The issue seems to be that colab populates sys.argv with arguments not handled by bleurt.\r\n\r\nRunning this before calling bleurt fixes it:\r\n```\r\nimport sys\r\nsys.argv = sys.argv[:1]\r\n```\r\n\r\nNot the most elegant solution. Perhaps it needs to be fixed in the bleurt code itself rather than huggingface?\r\n\r\nThis is the output of `print(sys.argv)` when running on colab:\r\n```\r\n['/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py', '-f', '/root/.local/share/jupyter/runtime/kernel-a857a78c-44d6-4b9d-b18a-030b858ee327.json']\r\n```""
 'I got the error when running it from the command line. It looks more like an error that should be fixed in the BLEURT codebase.'
 'Seems to be a known issue in the bleurt codebase: https://github.com/google-research/bleurt/issues/24.'
 'Hi, the problem should be solved now.']","Calling the `compute` method for **bleurt** metric fails with an `UnrecognizedFlagError` for `FLAGS.bleurt_batch_size`. 

My environment:
```
python==3.8.5
datasets==1.2.0
tensorflow==2.3.1
cudatoolkit==11.0.221
```

Test code for reproducing the error:
```
from datasets import load_metric
bleurt = load_metric('bleurt')
gen_text = ""I am walking on the promenade today""
ref_text = ""I am walking along the promenade on this sunny day""
bleurt.compute(predictions=[test_text], references=[test_text])
```

Error Output:
```
Using default BLEURT-Base checkpoint for sequence maximum length 128. You can use a bigger model for better results with e.g.: datasets.load_metric('bleurt', 'bleurt-large-512').
INFO:tensorflow:Reading checkpoint /home/ubuntu/.cache/huggingface/metrics/bleurt/default/downloads/extracted/9aee35580225730ac5422599f35c4986e4c49cafd08082123342b1019720dac4/bleurt-base-128.
INFO:tensorflow:Config file found, reading.
INFO:tensorflow:Will load checkpoint bert_custom
INFO:tensorflow:Performs basic checks...
INFO:tensorflow:... name:bert_custom
INFO:tensorflow:... vocab_file:vocab.txt
INFO:tensorflow:... bert_config_file:bert_config.json
INFO:tensorflow:... do_lower_case:True
INFO:tensorflow:... max_seq_length:128
INFO:tensorflow:Creating BLEURT scorer.
INFO:tensorflow:Loading model...
INFO:tensorflow:BLEURT initialized.
---------------------------------------------------------------------------
UnrecognizedFlagError                     Traceback (most recent call last)
<ipython-input-12-8b3f4322318a> in <module>
      2 gen_text = ""I am walking on the promenade today""
      3 ref_text = ""I am walking along the promenade on this sunny day""
----> 4 bleurt.compute(predictions=[gen_text], references=[ref_text])

~/anaconda3/envs/noved/lib/python3.8/site-packages/datasets/metric.py in compute(self, *args, **kwargs)
    396             references = self.data[""references""]
    397             with temp_seed(self.seed):
--> 398                 output = self._compute(predictions=predictions, references=references, **kwargs)
    399 
    400             if self.buf_writer is not None:

~/.cache/huggingface/modules/datasets_modules/metrics/bleurt/b1de33e1cbbcb1dbe276c887efa1fad68c6aff913885108078fa1ad408908778/bleurt.py in _compute(self, predictions, references)
    103 
    104     def _compute(self, predictions, references):
--> 105         scores = self.scorer.score(references=references, candidates=predictions)
    106         return {""scores"": scores}

~/anaconda3/envs/noved/lib/python3.8/site-packages/bleurt/score.py in score(self, references, candidates, batch_size)
    164     """"""
    165     if not batch_size:
--> 166       batch_size = FLAGS.bleurt_batch_size
    167 
    168     candidates, references = list(candidates), list(references)

~/anaconda3/envs/noved/lib/python3.8/site-packages/tensorflow/python/platform/flags.py in __getattr__(self, name)
     83     # a flag.
     84     if not wrapped.is_parsed():
---> 85       wrapped(_sys.argv)
     86     return wrapped.__getattr__(name)
     87 

~/anaconda3/envs/noved/lib/python3.8/site-packages/absl/flags/_flagvalues.py in __call__(self, argv, known_only)
    643     for name, value in unknown_flags:
    644       suggestions = _helpers.get_flag_suggestions(name, list(self))
--> 645       raise _exceptions.UnrecognizedFlagError(
    646           name, value, suggestions=suggestions)
    647 

UnrecognizedFlagError: Unknown command line flag 'f'
```

Possible Fix:
Modify `_compute` method https://github.com/huggingface/datasets/blob/7e64851a12263dc74d41c668167918484c8000ab/metrics/bleurt/bleurt.py#L104
to receive a `batch_size` argument, for example:
```
def _compute(self, predictions, references, batch_size=1):
    scores = self.scorer.score(references=references, candidates=predictions, batch_size=batch_size)
    return {""scores"": scores}
```"
https://github.com/huggingface/datasets/issues/1725,load the local dataset,"['You should rephrase your question or give more examples and details on what you want to do.\r\n\r\nit’s not possible to understand it and help you with only this information.'
 'sorry for that.\r\ni want to know how could i load the train set and the test set from the local ,which api or function should i use .\r\n'
 'Did you try to follow the instructions in the documentation?\r\nHere: https://huggingface.co/docs/datasets/loading_datasets.html#from-local-files'
 'thanks a lot \r\ni find that the problem is i dont use vpn...\r\nso i have to keep my net work even if i want to load the local data ?'
 'We will solve this soon (cf #1724)' 'thanks a lot']","your guidebook's example is like
>>>from datasets import load_dataset
>>> dataset = load_dataset('json', data_files='my_file.json')
but the first arg is path...
so how should i do if i want to load the local dataset for model training?
i will be grateful if you can help me handle this problem!
thanks a lot!"
https://github.com/huggingface/datasets/issues/1724,could not run models on a offline server successfully,"['Transferred to `datasets` based on the stack trace.'
 ""Hi @lkcao !\r\nYour issue is indeed related to `datasets`. In addition to installing the package manually, you will need to download the `text.py` script on your server. You'll find it (under `datasets/datasets/text`: https://github.com/huggingface/datasets/blob/master/datasets/text/text.py.\r\nThen you can change the line 221 of `run_mlm_new.py` into:\r\n```python\r\n  datasets = load_dataset('/path/to/text.py', data_files=data_files)\r\n```\r\nWhere `/path/to/text.py` is the path on the server where you saved the `text.py` script.""
 ""We're working on including the local dataset builders (csv, text, json etc.) directly in the `datasets` package so that they can be used offline""
 ""The local dataset builders (csv, text , json and pandas) are now part of the `datasets` package since #1726 :)\r\nYou can now use them offline\r\n```python\r\ndatasets = load_dataset('text', data_files=data_files)\r\n```\r\n\r\nWe'll do a new release soon""
 ""> The local dataset builders (csv, text , json and pandas) are now part of the `datasets` package since #1726 :)\r\n> You can now use them offline\r\n> \r\n> ```python\r\n> datasets = load_dataset('text', data_files=data_files)\r\n> ```\r\n> \r\n> We'll do a new release soon\r\n\r\nso the new version release now?""
 ""Yes it's been available since datasets 1.3.0 !""]","Hi, I really need your help about this.
I am trying to fine-tuning a RoBERTa on a remote server, which is strictly banning internet. I try to install all the packages by hand and try to run run_mlm.py on the server. It works well on colab, but when I try to run it on this offline server, it shows:
![image](https://user-images.githubusercontent.com/49967236/104276256-25a88600-546a-11eb-9776-8ec695dfa24e.png)

is there anything I can do? Is it possible to download all the things in cache and upload it to the server? Please help me out..."
https://github.com/huggingface/datasets/issues/1718,Possible cache miss in datasets,"[""Thanks for reporting !\r\nI was able to reproduce thanks to your code and find the origin of the bug.\r\nThe cache was not reusing the same file because one object was not deterministic. It comes from a conversion from `set` to `list` in the `datasets.arrrow_dataset.transmit_format` function, where the resulting list would not always be in the same order and therefore the function that computes the hash used by the cache would not always return the same result.\r\nI'm opening a PR to fix this.\r\n\r\nAlso we plan to do a new release in the coming days so you can expect the fix to be available soon.\r\nNote that you can still specify `cache_file_name=` in the second `map()` call to name the cache file yourself if you want to.""
 'Thanks for the fast reply, waiting for the fix :)\r\n\r\nI tried to use `cache_file_names` and wasn\'t sure how, I tried to give it the following:\r\n```\r\ntokenized_datasets = tokenized_datasets.map(\r\n    group_texts,\r\n    batched=True,\r\n    num_proc=60,\r\n    load_from_cache_file=True,\r\n    cache_file_names={k: f\'.cache/{str(k)}\' for k in tokenized_datasets}\r\n)\r\n```\r\n\r\nand got an error:\r\n```\r\nmultiprocess.pool.RemoteTraceback:\r\n""""""\r\nTraceback (most recent call last):\r\n  File ""/venv/lib/python3.6/site-packages/multiprocess/pool.py"", line 119, in worker\r\n    result = (True, func(*args, **kwds))\r\n  File ""/venv/lib/python3.6/site-packages/datasets/arrow_dataset.py"", line 157, in wrapper\r\n    out: Union[""Dataset"", ""DatasetDict""] = func(self, *args, **kwargs)\r\n  File ""/venv/lib/python3.6/site-packages/datasets/fingerprint.py"", line 163, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File ""/venv/lib/python3.6/site-packages/datasets/arrow_dataset.py"", line 1491, in _map_single\r\n    tmp_file = tempfile.NamedTemporaryFile(""wb"", dir=os.path.dirname(cache_file_name), delete=False)\r\n  File ""/usr/lib/python3.6/tempfile.py"", line 690, in NamedTemporaryFile\r\n    (fd, name) = _mkstemp_inner(dir, prefix, suffix, flags, output_type)\r\n  File ""/usr/lib/python3.6/tempfile.py"", line 401, in _mkstemp_inner\r\n    fd = _os.open(file, flags, 0o600)\r\nFileNotFoundError: [Errno 2] No such file or directory: \'_00000_of_00060.cache/tmpsvszxtop\'\r\n""""""\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File ""test.py"", line 48, in <module>\r\n    cache_file_names={k: f\'.cache/{str(k)}\' for k in tokenized_datasets}\r\n  File ""/venv/lib/python3.6/site-packages/datasets/dataset_dict.py"", line 303, in map\r\n    for k, dataset in self.items()\r\n  File ""/venv/lib/python3.6/site-packages/datasets/dataset_dict.py"", line 303, in <dictcomp>\r\n    for k, dataset in self.items()\r\n  File ""/venv/lib/python3.6/site-packages/datasets/arrow_dataset.py"", line 1317, in map\r\n    transformed_shards = [r.get() for r in results]\r\n  File ""/venv/lib/python3.6/site-packages/datasets/arrow_dataset.py"", line 1317, in <listcomp>\r\n    transformed_shards = [r.get() for r in results]\r\n  File ""/venv/lib/python3.6/site-packages/multiprocess/pool.py"", line 644, in get\r\n    raise self._value\r\nFileNotFoundError: [Errno 2] No such file or directory: \'_00000_of_00060.cache/tmpsvszxtop\'\r\n```\r\n'
 ""The documentation says\r\n```\r\ncache_file_names (`Optional[Dict[str, str]]`, defaults to `None`): Provide the name of a cache file to use to store the\r\n    results of the computation instead of the automatically generated cache file name.\r\n    You have to provide one :obj:`cache_file_name` per dataset in the dataset dictionary.\r\n```\r\nWhat is expected is simply the name of a file, not a path. The file will be located in the cache directory of the `wikitext` dataset. You can try again with something like\r\n```python\r\ncache_file_names = {k: f'tokenized_and_grouped_{str(k)}' for k in tokenized_datasets}\r\n```""
 ""Managed to get `cache_file_names` working and caching works well with it\r\nHad to make a small modification for it to work:\r\n```\r\ncache_file_names = {k: f'tokenized_and_grouped_{str(k)}.arrow' for k in tokenized_datasets}\r\n```""
 ""Another comment on `cache_file_names`, it doesn't save the produced cached files in the dataset's cache folder, it requires to give a path to an existing directory for it to work.\r\nI can confirm that this is how it works in `datasets==1.1.3`""
 'Oh yes indeed ! Maybe we need to update the docstring to mention that it is a path'
 'I fixed the docstring. Hopefully this is less confusing now: https://github.com/huggingface/datasets/commit/42ccc0012ba8864e6db1392430100f350236183a'
 'I upgraded to the latest version and I encountered some strange behaviour, the script I posted in the OP doesn\'t trigger recalculation, however, if I add the following change it does trigger partial recalculation, I am not sure if its something wrong on my machine or a bug:\r\n```\r\nfrom datasets import load_dataset\r\nfrom transformers import AutoTokenizer\r\n\r\ndatasets = load_dataset(\'wikitext\', \'wikitext-103-raw-v1\')\r\ntokenizer = AutoTokenizer.from_pretrained(\'bert-base-uncased\', use_fast=True)\r\n\r\ncolumn_names = datasets[""train""].column_names\r\ntext_column_name = ""text"" if ""text"" in column_names else column_names[0]\r\ndef tokenize_function(examples):\r\n    return tokenizer(examples[text_column_name], return_special_tokens_mask=True)\r\n# CHANGE\r\nprint(\'hello\')\r\n# CHANGE\r\n\r\ntokenized_datasets = datasets.map(\r\n    tokenize_function,\r\n    batched=True,\r\n...\r\n```\r\nI am using datasets in the `run_mlm.py` script in the transformers examples and I found that if I change the script without touching any of the preprocessing. it still triggers recalculation which is very weird\r\n\r\nEdit: accidently clicked the close issue button '
 ""This is because the `group_texts` line definition changes (it is defined 3 lines later than in the previous call). Currently if a function is moved elsewhere in a script we consider it to be different.\r\n\r\nNot sure this is actually a good idea to keep this behavior though. We had this as a security in the early development of the lib but now the recursive hashing of objects is robust so we can probably remove that.\r\nMoreover we're already ignoring the line definition for lambda functions.""
 'I opened a PR to change this, let me know what you think.'
 'Sounds great, thank you for your quick responses and help! Looking forward for the next release.'
 'I am having a similar issue where only the grouped files are loaded from cache while the tokenized ones aren\'t. I can confirm both datasets are being stored to file, but only the grouped version is loaded from cache. Not sure what might be going on. But I\'ve tried to remove all kinds of non deterministic behaviour, but still no luck. Thanks for the help!\r\n\r\n\r\n```python\r\n    # Datasets\r\n    train = sorted(glob(args.data_dir + \'*.{}\'.format(args.ext)))\r\n    if args.dev_split >= len(train):\r\n        raise ValueError(""Not enough dev files"")\r\n    dev = []\r\n    state = random.Random(1001)\r\n    for _ in range(args.dev_split):\r\n        dev.append(train.pop(state.randint(0, len(train) - 1)))\r\n\r\n    max_seq_length = min(args.max_seq_length, tokenizer.model_max_length)\r\n\r\n    def tokenize_function(examples):\r\n        return tokenizer(examples[\'text\'], return_special_tokens_mask=True)\r\n\r\n    def group_texts(examples):\r\n        # Concatenate all texts from our dataset and generate chunks of max_seq_length\r\n        concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\r\n        total_length = len(concatenated_examples[list(examples.keys())[0]])\r\n        # Truncate (not implementing padding)\r\n        total_length = (total_length // max_seq_length) * max_seq_length\r\n        # Split by chunks of max_seq_length\r\n        result = {\r\n            k: [t[i : i + max_seq_length] for i in range(0, total_length, max_seq_length)]\r\n            for k, t in concatenated_examples.items()\r\n        }\r\n        return result\r\n\r\n    datasets = load_dataset(\r\n        \'text\', name=\'DBNL\', data_files={\'train\': train[:10], \'dev\': dev[:5]}, \r\n        cache_dir=args.data_cache_dir)\r\n    datasets = datasets.map(tokenize_function, \r\n        batched=True, remove_columns=[\'text\'], \r\n        cache_file_names={k: os.path.join(args.data_cache_dir, f\'{k}-tokenized\') for k in datasets},\r\n        load_from_cache_file=not args.overwrite_cache)\r\n    datasets = datasets.map(group_texts, \r\n        batched=True,\r\n        cache_file_names={k: os.path.join(args.data_cache_dir, f\'{k}-grouped\') for k in datasets},\r\n        load_from_cache_file=not args.overwrite_cache)\r\n```\r\n\r\nAnd this is the log\r\n\r\n```\r\n04/26/2021 10:26:59 - WARNING - datasets.builder -   Using custom data configuration DBNL-f8d988ad33ccf2c1\r\n04/26/2021 10:26:59 - WARNING - datasets.builder -   Reusing dataset text (/home/manjavacasema/data/.cache/text/DBNL-f8d988ad33ccf2c1/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\r\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:00<00:00, 21.07ba/s]\r\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:01<00:00, 24.28ba/s]\r\n04/26/2021 10:27:01 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/manjavacasema/data/.cache/train-grouped\r\n04/26/2021 10:27:01 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/manjavacasema/data/.cache/dev-grouped\r\n```\r\n'
 'Hi ! What tokenizer are you using ?' ""It's the ByteLevelBPETokenizer""]","Hi,

I am using the datasets package and even though I run the same data processing functions, datasets always recomputes the function instead of using cache.
I have attached an example script that for me reproduces the problem.
In the attached example the second map function always recomputes instead of loading from cache.
Is this a bug or am I doing something wrong?
Is there a way for fix this and avoid all the recomputation?

Thanks

Edit:
transformers==3.5.1
datasets==1.2.0

```
from datasets import load_dataset
from transformers import AutoTokenizer

datasets = load_dataset('wikitext', 'wikitext-103-raw-v1')
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', use_fast=True)


column_names = datasets[""train""].column_names
text_column_name = ""text"" if ""text"" in column_names else column_names[0]
def tokenize_function(examples):
    return tokenizer(examples[text_column_name], return_special_tokens_mask=True)

tokenized_datasets = datasets.map(
    tokenize_function,
    batched=True,
    num_proc=60,
    remove_columns=[text_column_name],
    load_from_cache_file=True,
)
max_seq_length = tokenizer.model_max_length
def group_texts(examples):
    # Concatenate all texts.
    concatenated_examples = {
        k: sum(examples[k], []) for k in examples.keys()}
    total_length = len(concatenated_examples[list(examples.keys())[0]])
    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can
    # customize this part to your needs.
    total_length = (total_length // max_seq_length) * max_seq_length
    # Split by chunks of max_len.
    result = {
        k: [t[i: i + max_seq_length]
            for i in range(0, total_length, max_seq_length)]
        for k, t in concatenated_examples.items()
    }
    return result

tokenized_datasets = tokenized_datasets.map(
    group_texts,
    batched=True,
    num_proc=60,
    load_from_cache_file=True,
)
print(tokenized_datasets)

print('finished')
```"
https://github.com/huggingface/datasets/issues/1717,SciFact dataset - minor changes,"['Hi Dave,\r\nYou are more than welcome to open a PR to make these changes! 🤗\r\nYou will find the relevant information about opening a PR in the [contributing guide](https://github.com/huggingface/datasets/blob/master/CONTRIBUTING.md) and in the [dataset addition guide](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).\r\n\r\nPinging also @lhoestq for the Google cloud matter.'
 ""> I'd like to make a few minor changes, including the citation information and the `_URL` from which to download the dataset. Can I submit a PR for this?\r\n\r\nSure ! Also feel free to ping us for reviews or if we can help :)\r\n\r\n> It also looks like the dataset is being downloaded directly from Huggingface's Google cloud account rather than via the `_URL` in [scifact.py](https://github.com/huggingface/datasets/blob/master/datasets/scifact/scifact.py). Can you help me update the version on gcloud?\r\n\r\nWhat makes you think that ?\r\nAfaik there's no scifact on our google storage\r\n""
 ""\r\n\r\n> > I'd like to make a few minor changes, including the citation information and the `_URL` from which to download the dataset. Can I submit a PR for this?\r\n> \r\n> Sure ! Also feel free to ping us for reviews or if we can help :)\r\n> \r\nOK! We're organizing a [shared task](https://sdproc.org/2021/sharedtasks.html#sciver) based on the dataset, and I made some updates and changed the download URL - so the current code points to a dead URL. I'll update appropriately once the task is finalized and make a PR.\r\n\r\n> > It also looks like the dataset is being downloaded directly from Huggingface's Google cloud account rather than via the `_URL` in [scifact.py](https://github.com/huggingface/datasets/blob/master/datasets/scifact/scifact.py). Can you help me update the version on gcloud?\r\n> \r\n> What makes you think that ?\r\n> Afaik there's no scifact on our google storage\r\n\r\nYou're right, I had the data cached on my machine somewhere. \r\n\r\n""
 'I opened a PR about this: https://github.com/huggingface/datasets/pull/1780. Closing this issue, will continue there.']","Hi,

SciFact dataset creator here. First of all, thanks for adding the dataset to Huggingface, much appreciated!

I'd like to make a few minor changes, including the citation information and the `_URL` from which to download the dataset. Can I submit a PR for this?

It also looks like the dataset is being downloaded directly from Huggingface's Google cloud account rather than via the `_URL` in [scifact.py](https://github.com/huggingface/datasets/blob/master/datasets/scifact/scifact.py). Can you help me update the version on gcloud?

Thanks,

Dave"
https://github.com/huggingface/datasets/issues/1713,Installation using conda,"['Yes indeed the idea is to have the next release on conda cc @LysandreJik '
 'Great! Did you guys have a timeframe in mind for the next release?\r\n\r\nThank you for all the great work in developing this library.'
 'I think we can have `datasets` on conda by next week. Will see what I can do!'
 'Thank you. Looking forward to it.'
 '`datasets` has been added to the huggingface channel thanks to @LysandreJik :)\r\nIt depends on conda-forge though\r\n\r\n```\r\nconda install -c huggingface -c conda-forge datasets\r\n```']",Will a conda package for installing datasets be added to the huggingface conda channel? I have installed transformers using conda and would like to use the datasets library to use some of the scripts in the transformers/examples folder but am unable to do so at the moment as datasets can only be installed using pip and using pip in a conda environment is generally a bad idea in my experience.
https://github.com/huggingface/datasets/issues/1710,IsADirectoryError when trying to download C4,"[""I haven't tested C4 on my side so there so there may be a few bugs in the code/adjustments to make.\r\nHere it looks like in c4.py, line 190 one of the `files_to_download` is `'/'` which is invalid.\r\nValid files are paths to local files or URLs to remote files.""]","**TLDR**:

I fail to download C4 and see a stacktrace originating in `IsADirectoryError` as an explanation for failure.

How can the problem be fixed? 

**VERBOSE**:

I use Python version 3.7 and have the following dependencies listed in my project:

```
datasets==1.2.0
apache-beam==2.26.0
```

When running the following code, where `/data/huggingface/unpacked/` contains a single unzipped `wet.paths` file manually downloaded as per the instructions for C4:

```
from datasets import load_dataset

load_dataset(""c4"", ""en"", data_dir=""/data/huggingface/unpacked"", beam_runner='DirectRunner')
```

I get the following stacktrace:

```
/Users/fredriko/venv/misc/bin/python /Users/fredriko/source/misc/main.py
Downloading and preparing dataset c4/en (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /Users/fredriko/.cache/huggingface/datasets/c4/en/2.3.0/8304cf264cc42bdebcb13fca4b9cb36368a96f557d36f9dc969bebbe2568b283...
Traceback (most recent call last):
  File ""/Users/fredriko/source/misc/main.py"", line 3, in <module>
    load_dataset(""c4"", ""en"", data_dir=""/data/huggingface/unpacked"", beam_runner='DirectRunner')
  File ""/Users/fredriko/venv/misc/lib/python3.7/site-packages/datasets/load.py"", line 612, in load_dataset
    ignore_verifications=ignore_verifications,
  File ""/Users/fredriko/venv/misc/lib/python3.7/site-packages/datasets/builder.py"", line 527, in download_and_prepare
    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
  File ""/Users/fredriko/venv/misc/lib/python3.7/site-packages/datasets/builder.py"", line 1066, in _download_and_prepare
    pipeline=pipeline,
  File ""/Users/fredriko/venv/misc/lib/python3.7/site-packages/datasets/builder.py"", line 582, in _download_and_prepare
    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)
  File ""/Users/fredriko/.cache/huggingface/modules/datasets_modules/datasets/c4/8304cf264cc42bdebcb13fca4b9cb36368a96f557d36f9dc969bebbe2568b283/c4.py"", line 190, in _split_generators
    file_paths = dl_manager.download_and_extract(files_to_download)
  File ""/Users/fredriko/venv/misc/lib/python3.7/site-packages/datasets/utils/download_manager.py"", line 258, in download_and_extract
    return self.extract(self.download(url_or_urls))
  File ""/Users/fredriko/venv/misc/lib/python3.7/site-packages/datasets/utils/download_manager.py"", line 189, in download
    self._record_sizes_checksums(url_or_urls, downloaded_path_or_paths)
  File ""/Users/fredriko/venv/misc/lib/python3.7/site-packages/datasets/utils/download_manager.py"", line 117, in _record_sizes_checksums
    self._recorded_sizes_checksums[str(url)] = get_size_checksum_dict(path)
  File ""/Users/fredriko/venv/misc/lib/python3.7/site-packages/datasets/utils/info_utils.py"", line 80, in get_size_checksum_dict
    with open(path, ""rb"") as f:
IsADirectoryError: [Errno 21] Is a directory: '/'

Process finished with exit code 1
```"
https://github.com/huggingface/datasets/issues/1706,Error when downloading a large dataset on slow connection.,"['Hi ! Is this an issue you have with `openwebtext` specifically or also with other datasets ?\r\n\r\nIt looks like the downloaded file is corrupted and can\'t be extracted using `tarfile`.\r\nCould you try loading it again with \r\n```python\r\nimport datasets\r\ndatasets.load_dataset(""openwebtext"", download_mode=""force_redownload"")\r\n```']","I receive the following error after about an hour trying to download the `openwebtext` dataset.

The code used is:
```python
import datasets
datasets.load_dataset(""openwebtext"")
```

> Traceback (most recent call last):                                                                                                                                                                                                                             [4/28]
>   File ""<stdin>"", line 1, in <module>
>   File ""/home/lucadiliello/anaconda3/envs/nlp/lib/python3.7/site-packages/datasets/load.py"", line 610, in load_dataset
>     ignore_verifications=ignore_verifications,
>   File ""/home/lucadiliello/anaconda3/envs/nlp/lib/python3.7/site-packages/datasets/builder.py"", line 515, in download_and_prepare
>     dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
>   File ""/home/lucadiliello/anaconda3/envs/nlp/lib/python3.7/site-packages/datasets/builder.py"", line 570, in _download_and_prepare
>     split_generators = self._split_generators(dl_manager, **split_generators_kwargs)
>   File ""/home/lucadiliello/.cache/huggingface/modules/datasets_modules/datasets/openwebtext/5c636399c7155da97c982d0d70ecdce30fbca66a4eb4fc768ad91f8331edac02/openwebtext.py"", line 62, in _split_generators
>     dl_dir = dl_manager.download_and_extract(_URL)
>   File ""/home/lucadiliello/anaconda3/envs/nlp/lib/python3.7/site-packages/datasets/utils/download_manager.py"", line 254, in download_and_extract
>     return self.extract(self.download(url_or_urls))
>   File ""/home/lucadiliello/anaconda3/envs/nlp/lib/python3.7/site-packages/datasets/utils/download_manager.py"", line 235, in extract
>     num_proc=num_proc,
>   File ""/home/lucadiliello/anaconda3/envs/nlp/lib/python3.7/site-packages/datasets/utils/py_utils.py"", line 225, in map_nested
>     return function(data_struct)
>   File ""/home/lucadiliello/anaconda3/envs/nlp/lib/python3.7/site-packages/datasets/utils/file_utils.py"", line 343, in cached_path
>     tar_file.extractall(output_path_extracted)
>   File ""/home/lucadiliello/anaconda3/envs/nlp/lib/python3.7/tarfile.py"", line 2000, in extractall
>     numeric_owner=numeric_owner)
>   File ""/home/lucadiliello/anaconda3/envs/nlp/lib/python3.7/tarfile.py"", line 2042, in extract
>     numeric_owner=numeric_owner)
>   File ""/home/lucadiliello/anaconda3/envs/nlp/lib/python3.7/tarfile.py"", line 2112, in _extract_member
>     self.makefile(tarinfo, targetpath)
>   File ""/home/lucadiliello/anaconda3/envs/nlp/lib/python3.7/tarfile.py"", line 2161, in makefile
>     copyfileobj(source, target, tarinfo.size, ReadError, bufsize)
>   File ""/home/lucadiliello/anaconda3/envs/nlp/lib/python3.7/tarfile.py"", line 253, in copyfileobj
>     buf = src.read(remainder)
>   File ""/home/lucadiliello/anaconda3/envs/nlp/lib/python3.7/lzma.py"", line 200, in read
>     return self._buffer.read(size)
>   File ""/home/lucadiliello/anaconda3/envs/nlp/lib/python3.7/_compression.py"", line 68, in readinto
>     data = self.read(len(byte_view))
>   File ""/home/lucadiliello/anaconda3/envs/nlp/lib/python3.7/_compression.py"", line 99, in read
>     raise EOFError(""Compressed file ended before the ""
> EOFError: Compressed file ended before the end-of-stream marker was reached
"
https://github.com/huggingface/datasets/issues/1701,Some datasets miss dataset_infos.json or dummy_data.zip,"[""Thanks for reporting.\r\nWe should indeed add all the missing dummy_data.zip and also the dataset_infos.json at least for lm1b, reclor and wikihow.\r\n\r\nFor c4 I haven't tested the script and I think we'll require some optimizations regarding beam datasets before processing it.\r\n""]","While working on dataset REAME generation script at https://github.com/madlag/datasets_readme_generator , I noticed that some datasets miss a dataset_infos.json : 

```
c4
lm1b
reclor
wikihow
```

And some does not have a dummy_data.zip : 

```
kor_nli
math_dataset
mlqa
ms_marco
newsgroup
qa4mre
qangaroo
reddit_tifu
super_glue
trivia_qa
web_of_science
wmt14
wmt15
wmt16
wmt17
wmt18
wmt19
xtreme
```

But it seems that some of those last do have a ""dummy"" directory .

"
https://github.com/huggingface/datasets/issues/1696,Unable to install datasets,"['Maybe try to create a virtual env with python 3.8 or 3.7'
 'Thanks, @thomwolf! I fixed the issue by downgrading python to 3.7. '
 'Damn sorry' 'Damn sorry']","** Edit **
I believe there's a bug with the package when you're installing it with Python 3.9. I recommend sticking with previous versions. Thanks, @thomwolf for the insight! 

**Short description**

I followed the instructions for installing datasets (https://huggingface.co/docs/datasets/installation.html). However, while I tried to download datasets using `pip install datasets` I got a massive error message after getting stuck at ""Installing build dependencies..."" 

I was wondering if this problem can be fixed by creating a virtual environment, but it didn't help. Can anyone offer some advice on how to fix this issue? 

Here's an error message: 

`(env) Gas-MacBook-Pro:Downloads destiny$ pip install datasets
Collecting datasets
  Using cached datasets-1.2.0-py3-none-any.whl (159 kB)
Collecting numpy>=1.17
  Using cached numpy-1.19.5-cp39-cp39-macosx_10_9_x86_64.whl (15.6 MB)
Collecting pyarrow>=0.17.1
  Using cached pyarrow-2.0.0.tar.gz (58.9 MB)
....

      _configtest.c:9:5: warning: incompatible redeclaration of library function 'ceilf' [-Wincompatible-library-redeclaration]
      int ceilf (void);
          ^
      _configtest.c:9:5: note: 'ceilf' is a builtin with type 'float (float)'
      _configtest.c:10:5: warning: incompatible redeclaration of library function 'rintf' [-Wincompatible-library-redeclaration]
      int rintf (void);
          ^
      _configtest.c:10:5: note: 'rintf' is a builtin with type 'float (float)'
      _configtest.c:11:5: warning: incompatible redeclaration of library function 'truncf' [-Wincompatible-library-redeclaration]
      int truncf (void);
          ^
      _configtest.c:11:5: note: 'truncf' is a builtin with type 'float (float)'
      _configtest.c:12:5: warning: incompatible redeclaration of library function 'sqrtf' [-Wincompatible-library-redeclaration]
      int sqrtf (void);
          ^
      _configtest.c:12:5: note: 'sqrtf' is a builtin with type 'float (float)'
      _configtest.c:13:5: warning: incompatible redeclaration of library function 'log10f' [-Wincompatible-library-redeclaration]
      int log10f (void);
          ^
      _configtest.c:13:5: note: 'log10f' is a builtin with type 'float (float)'
      _configtest.c:14:5: warning: incompatible redeclaration of library function 'logf' [-Wincompatible-library-redeclaration]
      int logf (void);
          ^
      _configtest.c:14:5: note: 'logf' is a builtin with type 'float (float)'
      _configtest.c:15:5: warning: incompatible redeclaration of library function 'log1pf' [-Wincompatible-library-redeclaration]
      int log1pf (void);
          ^
      _configtest.c:15:5: note: 'log1pf' is a builtin with type 'float (float)'
      _configtest.c:16:5: warning: incompatible redeclaration of library function 'expf' [-Wincompatible-library-redeclaration]
      int expf (void);
          ^
      _configtest.c:16:5: note: 'expf' is a builtin with type 'float (float)'
      _configtest.c:17:5: warning: incompatible redeclaration of library function 'expm1f' [-Wincompatible-library-redeclaration]
      int expm1f (void);
          ^
      _configtest.c:17:5: note: 'expm1f' is a builtin with type 'float (float)'
      _configtest.c:18:5: warning: incompatible redeclaration of library function 'asinf' [-Wincompatible-library-redeclaration]
      int asinf (void);
          ^
      _configtest.c:18:5: note: 'asinf' is a builtin with type 'float (float)'
      _configtest.c:19:5: warning: incompatible redeclaration of library function 'acosf' [-Wincompatible-library-redeclaration]
      int acosf (void);
          ^
      _configtest.c:19:5: note: 'acosf' is a builtin with type 'float (float)'
      _configtest.c:20:5: warning: incompatible redeclaration of library function 'atanf' [-Wincompatible-library-redeclaration]
      int atanf (void);
          ^
      _configtest.c:20:5: note: 'atanf' is a builtin with type 'float (float)'
      _configtest.c:21:5: warning: incompatible redeclaration of library function 'asinhf' [-Wincompatible-library-redeclaration]
      int asinhf (void);
          ^
      _configtest.c:21:5: note: 'asinhf' is a builtin with type 'float (float)'
      _configtest.c:22:5: warning: incompatible redeclaration of library function 'acoshf' [-Wincompatible-library-redeclaration]
      int acoshf (void);
          ^
      _configtest.c:22:5: note: 'acoshf' is a builtin with type 'float (float)'
      _configtest.c:23:5: warning: incompatible redeclaration of library function 'atanhf' [-Wincompatible-library-redeclaration]
      int atanhf (void);
          ^
      _configtest.c:23:5: note: 'atanhf' is a builtin with type 'float (float)'
      _configtest.c:24:5: warning: incompatible redeclaration of library function 'hypotf' [-Wincompatible-library-redeclaration]
      int hypotf (void);
          ^
      _configtest.c:24:5: note: 'hypotf' is a builtin with type 'float (float, float)'
      _configtest.c:25:5: warning: incompatible redeclaration of library function 'atan2f' [-Wincompatible-library-redeclaration]
      int atan2f (void);
          ^
      _configtest.c:25:5: note: 'atan2f' is a builtin with type 'float (float, float)'
      _configtest.c:26:5: warning: incompatible redeclaration of library function 'powf' [-Wincompatible-library-redeclaration]
      int powf (void);
          ^
      _configtest.c:26:5: note: 'powf' is a builtin with type 'float (float, float)'
      _configtest.c:27:5: warning: incompatible redeclaration of library function 'fmodf' [-Wincompatible-library-redeclaration]
      int fmodf (void);
          ^
      _configtest.c:27:5: note: 'fmodf' is a builtin with type 'float (float, float)'
      _configtest.c:28:5: warning: incompatible redeclaration of library function 'modff' [-Wincompatible-library-redeclaration]
      int modff (void);
          ^
      _configtest.c:28:5: note: 'modff' is a builtin with type 'float (float, float *)'
      _configtest.c:29:5: warning: incompatible redeclaration of library function 'frexpf' [-Wincompatible-library-redeclaration]
      int frexpf (void);
          ^
      _configtest.c:29:5: note: 'frexpf' is a builtin with type 'float (float, int *)'
      _configtest.c:30:5: warning: incompatible redeclaration of library function 'ldexpf' [-Wincompatible-library-redeclaration]
      int ldexpf (void);
          ^
      _configtest.c:30:5: note: 'ldexpf' is a builtin with type 'float (float, int)'
      _configtest.c:31:5: warning: incompatible redeclaration of library function 'exp2f' [-Wincompatible-library-redeclaration]
      int exp2f (void);
          ^
      _configtest.c:31:5: note: 'exp2f' is a builtin with type 'float (float)'
      _configtest.c:32:5: warning: incompatible redeclaration of library function 'log2f' [-Wincompatible-library-redeclaration]
      int log2f (void);
          ^
      _configtest.c:32:5: note: 'log2f' is a builtin with type 'float (float)'
      _configtest.c:33:5: warning: incompatible redeclaration of library function 'copysignf' [-Wincompatible-library-redeclaration]
      int copysignf (void);
          ^
      _configtest.c:33:5: note: 'copysignf' is a builtin with type 'float (float, float)'
      _configtest.c:34:5: warning: incompatible redeclaration of library function 'nextafterf' [-Wincompatible-library-redeclaration]
      int nextafterf (void);
          ^
      _configtest.c:34:5: note: 'nextafterf' is a builtin with type 'float (float, float)'
      _configtest.c:35:5: warning: incompatible redeclaration of library function 'cbrtf' [-Wincompatible-library-redeclaration]
      int cbrtf (void);
          ^
      _configtest.c:35:5: note: 'cbrtf' is a builtin with type 'float (float)'
      35 warnings generated.
      clang _configtest.o -o _configtest
      success!
      removing: _configtest.c _configtest.o _configtest.o.d _configtest
      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers
  
      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'
      clang: _configtest.c
      _configtest.c:1:5: warning: incompatible redeclaration of library function 'sinl' [-Wincompatible-library-redeclaration]
      int sinl (void);
          ^
      _configtest.c:1:5: note: 'sinl' is a builtin with type 'long double (long double)'
      _configtest.c:2:5: warning: incompatible redeclaration of library function 'cosl' [-Wincompatible-library-redeclaration]
      int cosl (void);
          ^
      _configtest.c:2:5: note: 'cosl' is a builtin with type 'long double (long double)'
      _configtest.c:3:5: warning: incompatible redeclaration of library function 'tanl' [-Wincompatible-library-redeclaration]
      int tanl (void);
          ^
      _configtest.c:3:5: note: 'tanl' is a builtin with type 'long double (long double)'
      _configtest.c:4:5: warning: incompatible redeclaration of library function 'sinhl' [-Wincompatible-library-redeclaration]
      int sinhl (void);
          ^
      _configtest.c:4:5: note: 'sinhl' is a builtin with type 'long double (long double)'
      _configtest.c:5:5: warning: incompatible redeclaration of library function 'coshl' [-Wincompatible-library-redeclaration]
      int coshl (void);
          ^
      _configtest.c:5:5: note: 'coshl' is a builtin with type 'long double (long double)'
      _configtest.c:6:5: warning: incompatible redeclaration of library function 'tanhl' [-Wincompatible-library-redeclaration]
      int tanhl (void);
          ^
      _configtest.c:6:5: note: 'tanhl' is a builtin with type 'long double (long double)'
      _configtest.c:7:5: warning: incompatible redeclaration of library function 'fabsl' [-Wincompatible-library-redeclaration]
      int fabsl (void);
          ^
      _configtest.c:7:5: note: 'fabsl' is a builtin with type 'long double (long double)'
      _configtest.c:8:5: warning: incompatible redeclaration of library function 'floorl' [-Wincompatible-library-redeclaration]
      int floorl (void);
          ^
      _configtest.c:8:5: note: 'floorl' is a builtin with type 'long double (long double)'
      _configtest.c:9:5: warning: incompatible redeclaration of library function 'ceill' [-Wincompatible-library-redeclaration]
      int ceill (void);
          ^
      _configtest.c:9:5: note: 'ceill' is a builtin with type 'long double (long double)'
      _configtest.c:10:5: warning: incompatible redeclaration of library function 'rintl' [-Wincompatible-library-redeclaration]
      int rintl (void);
          ^
      _configtest.c:10:5: note: 'rintl' is a builtin with type 'long double (long double)'
      _configtest.c:11:5: warning: incompatible redeclaration of library function 'truncl' [-Wincompatible-library-redeclaration]
      int truncl (void);
          ^
      _configtest.c:11:5: note: 'truncl' is a builtin with type 'long double (long double)'
      _configtest.c:12:5: warning: incompatible redeclaration of library function 'sqrtl' [-Wincompatible-library-redeclaration]
      int sqrtl (void);
          ^
      _configtest.c:12:5: note: 'sqrtl' is a builtin with type 'long double (long double)'
      _configtest.c:13:5: warning: incompatible redeclaration of library function 'log10l' [-Wincompatible-library-redeclaration]
      int log10l (void);
          ^
      _configtest.c:13:5: note: 'log10l' is a builtin with type 'long double (long double)'
      _configtest.c:14:5: warning: incompatible redeclaration of library function 'logl' [-Wincompatible-library-redeclaration]
      int logl (void);
          ^
      _configtest.c:14:5: note: 'logl' is a builtin with type 'long double (long double)'
      _configtest.c:15:5: warning: incompatible redeclaration of library function 'log1pl' [-Wincompatible-library-redeclaration]
      int log1pl (void);
          ^
      _configtest.c:15:5: note: 'log1pl' is a builtin with type 'long double (long double)'
      _configtest.c:16:5: warning: incompatible redeclaration of library function 'expl' [-Wincompatible-library-redeclaration]
      int expl (void);
          ^
      _configtest.c:16:5: note: 'expl' is a builtin with type 'long double (long double)'
      _configtest.c:17:5: warning: incompatible redeclaration of library function 'expm1l' [-Wincompatible-library-redeclaration]
      int expm1l (void);
          ^
      _configtest.c:17:5: note: 'expm1l' is a builtin with type 'long double (long double)'
      _configtest.c:18:5: warning: incompatible redeclaration of library function 'asinl' [-Wincompatible-library-redeclaration]
      int asinl (void);
          ^
      _configtest.c:18:5: note: 'asinl' is a builtin with type 'long double (long double)'
      _configtest.c:19:5: warning: incompatible redeclaration of library function 'acosl' [-Wincompatible-library-redeclaration]
      int acosl (void);
          ^
      _configtest.c:19:5: note: 'acosl' is a builtin with type 'long double (long double)'
      _configtest.c:20:5: warning: incompatible redeclaration of library function 'atanl' [-Wincompatible-library-redeclaration]
      int atanl (void);
          ^
      _configtest.c:20:5: note: 'atanl' is a builtin with type 'long double (long double)'
      _configtest.c:21:5: warning: incompatible redeclaration of library function 'asinhl' [-Wincompatible-library-redeclaration]
      int asinhl (void);
          ^
      _configtest.c:21:5: note: 'asinhl' is a builtin with type 'long double (long double)'
      _configtest.c:22:5: warning: incompatible redeclaration of library function 'acoshl' [-Wincompatible-library-redeclaration]
      int acoshl (void);
          ^
      _configtest.c:22:5: note: 'acoshl' is a builtin with type 'long double (long double)'
      _configtest.c:23:5: warning: incompatible redeclaration of library function 'atanhl' [-Wincompatible-library-redeclaration]
      int atanhl (void);
          ^
      _configtest.c:23:5: note: 'atanhl' is a builtin with type 'long double (long double)'
      _configtest.c:24:5: warning: incompatible redeclaration of library function 'hypotl' [-Wincompatible-library-redeclaration]
      int hypotl (void);
          ^
      _configtest.c:24:5: note: 'hypotl' is a builtin with type 'long double (long double, long double)'
      _configtest.c:25:5: warning: incompatible redeclaration of library function 'atan2l' [-Wincompatible-library-redeclaration]
      int atan2l (void);
          ^
      _configtest.c:25:5: note: 'atan2l' is a builtin with type 'long double (long double, long double)'
      _configtest.c:26:5: warning: incompatible redeclaration of library function 'powl' [-Wincompatible-library-redeclaration]
      int powl (void);
          ^
      _configtest.c:26:5: note: 'powl' is a builtin with type 'long double (long double, long double)'
      _configtest.c:27:5: warning: incompatible redeclaration of library function 'fmodl' [-Wincompatible-library-redeclaration]
      int fmodl (void);
          ^
      _configtest.c:27:5: note: 'fmodl' is a builtin with type 'long double (long double, long double)'
      _configtest.c:28:5: warning: incompatible redeclaration of library function 'modfl' [-Wincompatible-library-redeclaration]
      int modfl (void);
          ^
      _configtest.c:28:5: note: 'modfl' is a builtin with type 'long double (long double, long double *)'
      _configtest.c:29:5: warning: incompatible redeclaration of library function 'frexpl' [-Wincompatible-library-redeclaration]
      int frexpl (void);
          ^
      _configtest.c:29:5: note: 'frexpl' is a builtin with type 'long double (long double, int *)'
      _configtest.c:30:5: warning: incompatible redeclaration of library function 'ldexpl' [-Wincompatible-library-redeclaration]
      int ldexpl (void);
          ^
      _configtest.c:30:5: note: 'ldexpl' is a builtin with type 'long double (long double, int)'
      _configtest.c:31:5: warning: incompatible redeclaration of library function 'exp2l' [-Wincompatible-library-redeclaration]
      int exp2l (void);
          ^
      _configtest.c:31:5: note: 'exp2l' is a builtin with type 'long double (long double)'
      _configtest.c:32:5: warning: incompatible redeclaration of library function 'log2l' [-Wincompatible-library-redeclaration]
      int log2l (void);
          ^
      _configtest.c:32:5: note: 'log2l' is a builtin with type 'long double (long double)'
      _configtest.c:33:5: warning: incompatible redeclaration of library function 'copysignl' [-Wincompatible-library-redeclaration]
      int copysignl (void);
          ^
      _configtest.c:33:5: note: 'copysignl' is a builtin with type 'long double (long double, long double)'
      _configtest.c:34:5: warning: incompatible redeclaration of library function 'nextafterl' [-Wincompatible-library-redeclaration]
      int nextafterl (void);
          ^
      _configtest.c:34:5: note: 'nextafterl' is a builtin with type 'long double (long double, long double)'
      _configtest.c:35:5: warning: incompatible redeclaration of library function 'cbrtl' [-Wincompatible-library-redeclaration]
      int cbrtl (void);
          ^
      _configtest.c:35:5: note: 'cbrtl' is a builtin with type 'long double (long double)'
      35 warnings generated.
      clang _configtest.o -o _configtest
      success!
      removing: _configtest.c _configtest.o _configtest.o.d _configtest
      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers
  
      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'
      clang: _configtest.c
      success!
      removing: _configtest.c _configtest.o _configtest.o.d
      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers
  
      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'
      clang: _configtest.c
      success!
      removing: _configtest.c _configtest.o _configtest.o.d
      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers
  
      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'
      clang: _configtest.c
      success!
      removing: _configtest.c _configtest.o _configtest.o.d
      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers
  
      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'
      clang: _configtest.c
      success!
      removing: _configtest.c _configtest.o _configtest.o.d
      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers
  
      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'
      clang: _configtest.c
      _configtest.c:8:12: error: use of undeclared identifier 'HAVE_DECL_SIGNBIT'
          (void) HAVE_DECL_SIGNBIT;
                 ^
      1 error generated.
      failure.
      removing: _configtest.c _configtest.o
      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers
  
      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'
      clang: _configtest.c
      success!
      removing: _configtest.c _configtest.o _configtest.o.d
      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers
  
      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'
      clang: _configtest.c
      success!
      removing: _configtest.c _configtest.o _configtest.o.d
      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers
  
      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'
      clang: _configtest.c
      success!
      removing: _configtest.c _configtest.o _configtest.o.d
      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers
  
      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'
      clang: _configtest.c
      success!
      removing: _configtest.c _configtest.o _configtest.o.d
      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers
  
      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'
      clang: _configtest.c
      removing: _configtest.c _configtest.o _configtest.o.d
      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers
  
      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'
      clang: _configtest.c
      removing: _configtest.c _configtest.o _configtest.o.d
      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers
  
      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'
      clang: _configtest.c
      removing: _configtest.c _configtest.o _configtest.o.d
      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers
  
      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'
      clang: _configtest.c
      _configtest.c:1:5: warning: incompatible redeclaration of library function 'cabs' [-Wincompatible-library-redeclaration]
      int cabs (void);
          ^
      _configtest.c:1:5: note: 'cabs' is a builtin with type 'double (_Complex double)'
      _configtest.c:2:5: warning: incompatible redeclaration of library function 'cacos' [-Wincompatible-library-redeclaration]
      int cacos (void);
          ^
      _configtest.c:2:5: note: 'cacos' is a builtin with type '_Complex double (_Complex double)'
      _configtest.c:3:5: warning: incompatible redeclaration of library function 'cacosh' [-Wincompatible-library-redeclaration]
      int cacosh (void);
          ^
      _configtest.c:3:5: note: 'cacosh' is a builtin with type '_Complex double (_Complex double)'
      _configtest.c:4:5: warning: incompatible redeclaration of library function 'carg' [-Wincompatible-library-redeclaration]
      int carg (void);
          ^
      _configtest.c:4:5: note: 'carg' is a builtin with type 'double (_Complex double)'
      _configtest.c:5:5: warning: incompatible redeclaration of library function 'casin' [-Wincompatible-library-redeclaration]
      int casin (void);
          ^
      _configtest.c:5:5: note: 'casin' is a builtin with type '_Complex double (_Complex double)'
      _configtest.c:6:5: warning: incompatible redeclaration of library function 'casinh' [-Wincompatible-library-redeclaration]
      int casinh (void);
          ^
      _configtest.c:6:5: note: 'casinh' is a builtin with type '_Complex double (_Complex double)'
      _configtest.c:7:5: warning: incompatible redeclaration of library function 'catan' [-Wincompatible-library-redeclaration]
      int catan (void);
          ^
      _configtest.c:7:5: note: 'catan' is a builtin with type '_Complex double (_Complex double)'
      _configtest.c:8:5: warning: incompatible redeclaration of library function 'catanh' [-Wincompatible-library-redeclaration]
      int catanh (void);
          ^
      _configtest.c:8:5: note: 'catanh' is a builtin with type '_Complex double (_Complex double)'
      _configtest.c:9:5: warning: incompatible redeclaration of library function 'ccos' [-Wincompatible-library-redeclaration]
      int ccos (void);
          ^
      _configtest.c:9:5: note: 'ccos' is a builtin with type '_Complex double (_Complex double)'
      _configtest.c:10:5: warning: incompatible redeclaration of library function 'ccosh' [-Wincompatible-library-redeclaration]
      int ccosh (void);
          ^
      _configtest.c:10:5: note: 'ccosh' is a builtin with type '_Complex double (_Complex double)'
      _configtest.c:11:5: warning: incompatible redeclaration of library function 'cexp' [-Wincompatible-library-redeclaration]
      int cexp (void);
          ^
      _configtest.c:11:5: note: 'cexp' is a builtin with type '_Complex double (_Complex double)'
      _configtest.c:12:5: warning: incompatible redeclaration of library function 'cimag' [-Wincompatible-library-redeclaration]
      int cimag (void);
          ^
      _configtest.c:12:5: note: 'cimag' is a builtin with type 'double (_Complex double)'
      _configtest.c:13:5: warning: incompatible redeclaration of library function 'clog' [-Wincompatible-library-redeclaration]
      int clog (void);
          ^
      _configtest.c:13:5: note: 'clog' is a builtin with type '_Complex double (_Complex double)'
      _configtest.c:14:5: warning: incompatible redeclaration of library function 'conj' [-Wincompatible-library-redeclaration]
      int conj (void);
          ^
      _configtest.c:14:5: note: 'conj' is a builtin with type '_Complex double (_Complex double)'
      _configtest.c:15:5: warning: incompatible redeclaration of library function 'cpow' [-Wincompatible-library-redeclaration]
      int cpow (void);
          ^
      _configtest.c:15:5: note: 'cpow' is a builtin with type '_Complex double (_Complex double, _Complex double)'
      _configtest.c:16:5: warning: incompatible redeclaration of library function 'cproj' [-Wincompatible-library-redeclaration]
      int cproj (void);
          ^
      _configtest.c:16:5: note: 'cproj' is a builtin with type '_Complex double (_Complex double)'
      _configtest.c:17:5: warning: incompatible redeclaration of library function 'creal' [-Wincompatible-library-redeclaration]
      int creal (void);
          ^
      _configtest.c:17:5: note: 'creal' is a builtin with type 'double (_Complex double)'
      _configtest.c:18:5: warning: incompatible redeclaration of library function 'csin' [-Wincompatible-library-redeclaration]
      int csin (void);
          ^
      _configtest.c:18:5: note: 'csin' is a builtin with type '_Complex double (_Complex double)'
      _configtest.c:19:5: warning: incompatible redeclaration of library function 'csinh' [-Wincompatible-library-redeclaration]
      int csinh (void);
          ^
      _configtest.c:19:5: note: 'csinh' is a builtin with type '_Complex double (_Complex double)'
      _configtest.c:20:5: warning: incompatible redeclaration of library function 'csqrt' [-Wincompatible-library-redeclaration]
      int csqrt (void);
          ^
      _configtest.c:20:5: note: 'csqrt' is a builtin with type '_Complex double (_Complex double)'
      _configtest.c:21:5: warning: incompatible redeclaration of library function 'ctan' [-Wincompatible-library-redeclaration]
      int ctan (void);
          ^
      _configtest.c:21:5: note: 'ctan' is a builtin with type '_Complex double (_Complex double)'
      _configtest.c:22:5: warning: incompatible redeclaration of library function 'ctanh' [-Wincompatible-library-redeclaration]
      int ctanh (void);
          ^
      _configtest.c:22:5: note: 'ctanh' is a builtin with type '_Complex double (_Complex double)'
      22 warnings generated.
      clang _configtest.o -o _configtest
      success!
      removing: _configtest.c _configtest.o _configtest.o.d _configtest
      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers
  
      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'
      clang: _configtest.c
      _configtest.c:1:5: warning: incompatible redeclaration of library function 'cabsf' [-Wincompatible-library-redeclaration]
      int cabsf (void);
          ^
      _configtest.c:1:5: note: 'cabsf' is a builtin with type 'float (_Complex float)'
      _configtest.c:2:5: warning: incompatible redeclaration of library function 'cacosf' [-Wincompatible-library-redeclaration]
      int cacosf (void);
          ^
      _configtest.c:2:5: note: 'cacosf' is a builtin with type '_Complex float (_Complex float)'
      _configtest.c:3:5: warning: incompatible redeclaration of library function 'cacoshf' [-Wincompatible-library-redeclaration]
      int cacoshf (void);
          ^
      _configtest.c:3:5: note: 'cacoshf' is a builtin with type '_Complex float (_Complex float)'
      _configtest.c:4:5: warning: incompatible redeclaration of library function 'cargf' [-Wincompatible-library-redeclaration]
      int cargf (void);
          ^
      _configtest.c:4:5: note: 'cargf' is a builtin with type 'float (_Complex float)'
      _configtest.c:5:5: warning: incompatible redeclaration of library function 'casinf' [-Wincompatible-library-redeclaration]
      int casinf (void);
          ^
      _configtest.c:5:5: note: 'casinf' is a builtin with type '_Complex float (_Complex float)'
      _configtest.c:6:5: warning: incompatible redeclaration of library function 'casinhf' [-Wincompatible-library-redeclaration]
      int casinhf (void);
          ^
      _configtest.c:6:5: note: 'casinhf' is a builtin with type '_Complex float (_Complex float)'
      _configtest.c:7:5: warning: incompatible redeclaration of library function 'catanf' [-Wincompatible-library-redeclaration]
      int catanf (void);
          ^
      _configtest.c:7:5: note: 'catanf' is a builtin with type '_Complex float (_Complex float)'
      _configtest.c:8:5: warning: incompatible redeclaration of library function 'catanhf' [-Wincompatible-library-redeclaration]
      int catanhf (void);
          ^
      _configtest.c:8:5: note: 'catanhf' is a builtin with type '_Complex float (_Complex float)'
      _configtest.c:9:5: warning: incompatible redeclaration of library function 'ccosf' [-Wincompatible-library-redeclaration]
      int ccosf (void);
          ^
      _configtest.c:9:5: note: 'ccosf' is a builtin with type '_Complex float (_Complex float)'
      _configtest.c:10:5: warning: incompatible redeclaration of library function 'ccoshf' [-Wincompatible-library-redeclaration]
      int ccoshf (void);
          ^
      _configtest.c:10:5: note: 'ccoshf' is a builtin with type '_Complex float (_Complex float)'
      _configtest.c:11:5: warning: incompatible redeclaration of library function 'cexpf' [-Wincompatible-library-redeclaration]
      int cexpf (void);
          ^
      _configtest.c:11:5: note: 'cexpf' is a builtin with type '_Complex float (_Complex float)'
      _configtest.c:12:5: warning: incompatible redeclaration of library function 'cimagf' [-Wincompatible-library-redeclaration]
      int cimagf (void);
          ^
      _configtest.c:12:5: note: 'cimagf' is a builtin with type 'float (_Complex float)'
      _configtest.c:13:5: warning: incompatible redeclaration of library function 'clogf' [-Wincompatible-library-redeclaration]
      int clogf (void);
          ^
      _configtest.c:13:5: note: 'clogf' is a builtin with type '_Complex float (_Complex float)'
      _configtest.c:14:5: warning: incompatible redeclaration of library function 'conjf' [-Wincompatible-library-redeclaration]
      int conjf (void);
          ^
      _configtest.c:14:5: note: 'conjf' is a builtin with type '_Complex float (_Complex float)'
      _configtest.c:15:5: warning: incompatible redeclaration of library function 'cpowf' [-Wincompatible-library-redeclaration]
      int cpowf (void);
          ^
      _configtest.c:15:5: note: 'cpowf' is a builtin with type '_Complex float (_Complex float, _Complex float)'
      _configtest.c:16:5: warning: incompatible redeclaration of library function 'cprojf' [-Wincompatible-library-redeclaration]
      int cprojf (void);
          ^
      _configtest.c:16:5: note: 'cprojf' is a builtin with type '_Complex float (_Complex float)'
      _configtest.c:17:5: warning: incompatible redeclaration of library function 'crealf' [-Wincompatible-library-redeclaration]
      int crealf (void);
          ^
      _configtest.c:17:5: note: 'crealf' is a builtin with type 'float (_Complex float)'
      _configtest.c:18:5: warning: incompatible redeclaration of library function 'csinf' [-Wincompatible-library-redeclaration]
      int csinf (void);
          ^
      _configtest.c:18:5: note: 'csinf' is a builtin with type '_Complex float (_Complex float)'
      _configtest.c:19:5: warning: incompatible redeclaration of library function 'csinhf' [-Wincompatible-library-redeclaration]
      int csinhf (void);
          ^
      _configtest.c:19:5: note: 'csinhf' is a builtin with type '_Complex float (_Complex float)'
      _configtest.c:20:5: warning: incompatible redeclaration of library function 'csqrtf' [-Wincompatible-library-redeclaration]
      int csqrtf (void);
          ^
      _configtest.c:20:5: note: 'csqrtf' is a builtin with type '_Complex float (_Complex float)'
      _configtest.c:21:5: warning: incompatible redeclaration of library function 'ctanf' [-Wincompatible-library-redeclaration]
      int ctanf (void);
          ^
      _configtest.c:21:5: note: 'ctanf' is a builtin with type '_Complex float (_Complex float)'
      _configtest.c:22:5: warning: incompatible redeclaration of library function 'ctanhf' [-Wincompatible-library-redeclaration]
      int ctanhf (void);
          ^
      _configtest.c:22:5: note: 'ctanhf' is a builtin with type '_Complex float (_Complex float)'
      22 warnings generated.
      clang _configtest.o -o _configtest
      success!
      removing: _configtest.c _configtest.o _configtest.o.d _configtest
      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers
  
      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'
      clang: _configtest.c
      _configtest.c:1:5: warning: incompatible redeclaration of library function 'cabsl' [-Wincompatible-library-redeclaration]
      int cabsl (void);
          ^
      _configtest.c:1:5: note: 'cabsl' is a builtin with type 'long double (_Complex long double)'
      _configtest.c:2:5: warning: incompatible redeclaration of library function 'cacosl' [-Wincompatible-library-redeclaration]
      int cacosl (void);
          ^
      _configtest.c:2:5: note: 'cacosl' is a builtin with type '_Complex long double (_Complex long double)'
      _configtest.c:3:5: warning: incompatible redeclaration of library function 'cacoshl' [-Wincompatible-library-redeclaration]
      int cacoshl (void);
          ^
      _configtest.c:3:5: note: 'cacoshl' is a builtin with type '_Complex long double (_Complex long double)'
      _configtest.c:4:5: warning: incompatible redeclaration of library function 'cargl' [-Wincompatible-library-redeclaration]
      int cargl (void);
          ^
      _configtest.c:4:5: note: 'cargl' is a builtin with type 'long double (_Complex long double)'
      _configtest.c:5:5: warning: incompatible redeclaration of library function 'casinl' [-Wincompatible-library-redeclaration]
      int casinl (void);
          ^
      _configtest.c:5:5: note: 'casinl' is a builtin with type '_Complex long double (_Complex long double)'
      _configtest.c:6:5: warning: incompatible redeclaration of library function 'casinhl' [-Wincompatible-library-redeclaration]
      int casinhl (void);
          ^
      _configtest.c:6:5: note: 'casinhl' is a builtin with type '_Complex long double (_Complex long double)'
      _configtest.c:7:5: warning: incompatible redeclaration of library function 'catanl' [-Wincompatible-library-redeclaration]
      int catanl (void);
          ^
      _configtest.c:7:5: note: 'catanl' is a builtin with type '_Complex long double (_Complex long double)'
      _configtest.c:8:5: warning: incompatible redeclaration of library function 'catanhl' [-Wincompatible-library-redeclaration]
      int catanhl (void);
          ^
      _configtest.c:8:5: note: 'catanhl' is a builtin with type '_Complex long double (_Complex long double)'
      _configtest.c:9:5: warning: incompatible redeclaration of library function 'ccosl' [-Wincompatible-library-redeclaration]
      int ccosl (void);
          ^
      _configtest.c:9:5: note: 'ccosl' is a builtin with type '_Complex long double (_Complex long double)'
      _configtest.c:10:5: warning: incompatible redeclaration of library function 'ccoshl' [-Wincompatible-library-redeclaration]
      int ccoshl (void);
          ^
      _configtest.c:10:5: note: 'ccoshl' is a builtin with type '_Complex long double (_Complex long double)'
      _configtest.c:11:5: warning: incompatible redeclaration of library function 'cexpl' [-Wincompatible-library-redeclaration]
      int cexpl (void);
          ^
      _configtest.c:11:5: note: 'cexpl' is a builtin with type '_Complex long double (_Complex long double)'
      _configtest.c:12:5: warning: incompatible redeclaration of library function 'cimagl' [-Wincompatible-library-redeclaration]
      int cimagl (void);
          ^
      _configtest.c:12:5: note: 'cimagl' is a builtin with type 'long double (_Complex long double)'
      _configtest.c:13:5: warning: incompatible redeclaration of library function 'clogl' [-Wincompatible-library-redeclaration]
      int clogl (void);
          ^
      _configtest.c:13:5: note: 'clogl' is a builtin with type '_Complex long double (_Complex long double)'
      _configtest.c:14:5: warning: incompatible redeclaration of library function 'conjl' [-Wincompatible-library-redeclaration]
      int conjl (void);
          ^
      _configtest.c:14:5: note: 'conjl' is a builtin with type '_Complex long double (_Complex long double)'
      _configtest.c:15:5: warning: incompatible redeclaration of library function 'cpowl' [-Wincompatible-library-redeclaration]
      int cpowl (void);
          ^
      _configtest.c:15:5: note: 'cpowl' is a builtin with type '_Complex long double (_Complex long double, _Complex long double)'
      _configtest.c:16:5: warning: incompatible redeclaration of library function 'cprojl' [-Wincompatible-library-redeclaration]
      int cprojl (void);
          ^
      _configtest.c:16:5: note: 'cprojl' is a builtin with type '_Complex long double (_Complex long double)'
      _configtest.c:17:5: warning: incompatible redeclaration of library function 'creall' [-Wincompatible-library-redeclaration]
      int creall (void);
          ^
      _configtest.c:17:5: note: 'creall' is a builtin with type 'long double (_Complex long double)'
      _configtest.c:18:5: warning: incompatible redeclaration of library function 'csinl' [-Wincompatible-library-redeclaration]
      int csinl (void);
          ^
      _configtest.c:18:5: note: 'csinl' is a builtin with type '_Complex long double (_Complex long double)'
      _configtest.c:19:5: warning: incompatible redeclaration of library function 'csinhl' [-Wincompatible-library-redeclaration]
      int csinhl (void);
          ^
      _configtest.c:19:5: note: 'csinhl' is a builtin with type '_Complex long double (_Complex long double)'
      _configtest.c:20:5: warning: incompatible redeclaration of library function 'csqrtl' [-Wincompatible-library-redeclaration]
      int csqrtl (void);
          ^
      _configtest.c:20:5: note: 'csqrtl' is a builtin with type '_Complex long double (_Complex long double)'
      _configtest.c:21:5: warning: incompatible redeclaration of library function 'ctanl' [-Wincompatible-library-redeclaration]
      int ctanl (void);
          ^
      _configtest.c:21:5: note: 'ctanl' is a builtin with type '_Complex long double (_Complex long double)'
      _configtest.c:22:5: warning: incompatible redeclaration of library function 'ctanhl' [-Wincompatible-library-redeclaration]
      int ctanhl (void);
          ^
      _configtest.c:22:5: note: 'ctanhl' is a builtin with type '_Complex long double (_Complex long double)'
      22 warnings generated.
      clang _configtest.o -o _configtest
      success!
      removing: _configtest.c _configtest.o _configtest.o.d _configtest
      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers
  
      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'
      clang: _configtest.c
      _configtest.c:2:12: warning: unused function 'static_func' [-Wunused-function]
      static int static_func (char * restrict a)
                 ^
      1 warning generated.
      success!
      removing: _configtest.c _configtest.o _configtest.o.d
      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers
  
      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'
      clang: _configtest.c
      _configtest.c:3:19: warning: unused function 'static_func' [-Wunused-function]
      static inline int static_func (void)
                        ^
      1 warning generated.
      success!
      removing: _configtest.c _configtest.o _configtest.o.d
      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers
  
      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'
      clang: _configtest.c
      removing: _configtest.c _configtest.o _configtest.o.d
      File: build/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy/config.h
      #define SIZEOF_PY_INTPTR_T 8
      #define SIZEOF_OFF_T 8
      #define SIZEOF_PY_LONG_LONG 8
      #define MATHLIB
      #define HAVE_SIN 1
      #define HAVE_COS 1
      #define HAVE_TAN 1
      #define HAVE_SINH 1
      #define HAVE_COSH 1
      #define HAVE_TANH 1
      #define HAVE_FABS 1
      #define HAVE_FLOOR 1
      #define HAVE_CEIL 1
      #define HAVE_SQRT 1
      #define HAVE_LOG10 1
      #define HAVE_LOG 1
      #define HAVE_EXP 1
      #define HAVE_ASIN 1
      #define HAVE_ACOS 1
      #define HAVE_ATAN 1
      #define HAVE_FMOD 1
      #define HAVE_MODF 1
      #define HAVE_FREXP 1
      #define HAVE_LDEXP 1
      #define HAVE_RINT 1
      #define HAVE_TRUNC 1
      #define HAVE_EXP2 1
      #define HAVE_LOG2 1
      #define HAVE_ATAN2 1
      #define HAVE_POW 1
      #define HAVE_NEXTAFTER 1
      #define HAVE_STRTOLL 1
      #define HAVE_STRTOULL 1
      #define HAVE_CBRT 1
      #define HAVE_STRTOLD_L 1
      #define HAVE_BACKTRACE 1
      #define HAVE_MADVISE 1
      #define HAVE_XMMINTRIN_H 1
      #define HAVE_EMMINTRIN_H 1
      #define HAVE_XLOCALE_H 1
      #define HAVE_DLFCN_H 1
      #define HAVE_SYS_MMAN_H 1
      #define HAVE___BUILTIN_ISNAN 1
      #define HAVE___BUILTIN_ISINF 1
      #define HAVE___BUILTIN_ISFINITE 1
      #define HAVE___BUILTIN_BSWAP32 1
      #define HAVE___BUILTIN_BSWAP64 1
      #define HAVE___BUILTIN_EXPECT 1
      #define HAVE___BUILTIN_MUL_OVERFLOW 1
      #define HAVE___BUILTIN_CPU_SUPPORTS 1
      #define HAVE__M_FROM_INT64 1
      #define HAVE__MM_LOAD_PS 1
      #define HAVE__MM_PREFETCH 1
      #define HAVE__MM_LOAD_PD 1
      #define HAVE___BUILTIN_PREFETCH 1
      #define HAVE_LINK_AVX 1
      #define HAVE_LINK_AVX2 1
      #define HAVE_XGETBV 1
      #define HAVE_ATTRIBUTE_NONNULL 1
      #define HAVE_ATTRIBUTE_TARGET_AVX 1
      #define HAVE_ATTRIBUTE_TARGET_AVX2 1
      #define HAVE___THREAD 1
      #define HAVE_SINF 1
      #define HAVE_COSF 1
      #define HAVE_TANF 1
      #define HAVE_SINHF 1
      #define HAVE_COSHF 1
      #define HAVE_TANHF 1
      #define HAVE_FABSF 1
      #define HAVE_FLOORF 1
      #define HAVE_CEILF 1
      #define HAVE_RINTF 1
      #define HAVE_TRUNCF 1
      #define HAVE_SQRTF 1
      #define HAVE_LOG10F 1
      #define HAVE_LOGF 1
      #define HAVE_LOG1PF 1
      #define HAVE_EXPF 1
      #define HAVE_EXPM1F 1
      #define HAVE_ASINF 1
      #define HAVE_ACOSF 1
      #define HAVE_ATANF 1
      #define HAVE_ASINHF 1
      #define HAVE_ACOSHF 1
      #define HAVE_ATANHF 1
      #define HAVE_HYPOTF 1
      #define HAVE_ATAN2F 1
      #define HAVE_POWF 1
      #define HAVE_FMODF 1
      #define HAVE_MODFF 1
      #define HAVE_FREXPF 1
      #define HAVE_LDEXPF 1
      #define HAVE_EXP2F 1
      #define HAVE_LOG2F 1
      #define HAVE_COPYSIGNF 1
      #define HAVE_NEXTAFTERF 1
      #define HAVE_CBRTF 1
      #define HAVE_SINL 1
      #define HAVE_COSL 1
      #define HAVE_TANL 1
      #define HAVE_SINHL 1
      #define HAVE_COSHL 1
      #define HAVE_TANHL 1
      #define HAVE_FABSL 1
      #define HAVE_FLOORL 1
      #define HAVE_CEILL 1
      #define HAVE_RINTL 1
      #define HAVE_TRUNCL 1
      #define HAVE_SQRTL 1
      #define HAVE_LOG10L 1
      #define HAVE_LOGL 1
      #define HAVE_LOG1PL 1
      #define HAVE_EXPL 1
      #define HAVE_EXPM1L 1
      #define HAVE_ASINL 1
      #define HAVE_ACOSL 1
      #define HAVE_ATANL 1
      #define HAVE_ASINHL 1
      #define HAVE_ACOSHL 1
      #define HAVE_ATANHL 1
      #define HAVE_HYPOTL 1
      #define HAVE_ATAN2L 1
      #define HAVE_POWL 1
      #define HAVE_FMODL 1
      #define HAVE_MODFL 1
      #define HAVE_FREXPL 1
      #define HAVE_LDEXPL 1
      #define HAVE_EXP2L 1
      #define HAVE_LOG2L 1
      #define HAVE_COPYSIGNL 1
      #define HAVE_NEXTAFTERL 1
      #define HAVE_CBRTL 1
      #define HAVE_DECL_SIGNBIT
      #define HAVE_COMPLEX_H 1
      #define HAVE_CABS 1
      #define HAVE_CACOS 1
      #define HAVE_CACOSH 1
      #define HAVE_CARG 1
      #define HAVE_CASIN 1
      #define HAVE_CASINH 1
      #define HAVE_CATAN 1
      #define HAVE_CATANH 1
      #define HAVE_CCOS 1
      #define HAVE_CCOSH 1
      #define HAVE_CEXP 1
      #define HAVE_CIMAG 1
      #define HAVE_CLOG 1
      #define HAVE_CONJ 1
      #define HAVE_CPOW 1
      #define HAVE_CPROJ 1
      #define HAVE_CREAL 1
      #define HAVE_CSIN 1
      #define HAVE_CSINH 1
      #define HAVE_CSQRT 1
      #define HAVE_CTAN 1
      #define HAVE_CTANH 1
      #define HAVE_CABSF 1
      #define HAVE_CACOSF 1
      #define HAVE_CACOSHF 1
      #define HAVE_CARGF 1
      #define HAVE_CASINF 1
      #define HAVE_CASINHF 1
      #define HAVE_CATANF 1
      #define HAVE_CATANHF 1
      #define HAVE_CCOSF 1
      #define HAVE_CCOSHF 1
      #define HAVE_CEXPF 1
      #define HAVE_CIMAGF 1
      #define HAVE_CLOGF 1
      #define HAVE_CONJF 1
      #define HAVE_CPOWF 1
      #define HAVE_CPROJF 1
      #define HAVE_CREALF 1
      #define HAVE_CSINF 1
      #define HAVE_CSINHF 1
      #define HAVE_CSQRTF 1
      #define HAVE_CTANF 1
      #define HAVE_CTANHF 1
      #define HAVE_CABSL 1
      #define HAVE_CACOSL 1
      #define HAVE_CACOSHL 1
      #define HAVE_CARGL 1
      #define HAVE_CASINL 1
      #define HAVE_CASINHL 1
      #define HAVE_CATANL 1
      #define HAVE_CATANHL 1
      #define HAVE_CCOSL 1
      #define HAVE_CCOSHL 1
      #define HAVE_CEXPL 1
      #define HAVE_CIMAGL 1
      #define HAVE_CLOGL 1
      #define HAVE_CONJL 1
      #define HAVE_CPOWL 1
      #define HAVE_CPROJL 1
      #define HAVE_CREALL 1
      #define HAVE_CSINL 1
      #define HAVE_CSINHL 1
      #define HAVE_CSQRTL 1
      #define HAVE_CTANL 1
      #define HAVE_CTANHL 1
      #define NPY_RESTRICT restrict
      #define NPY_RELAXED_STRIDES_CHECKING 1
      #define HAVE_LDOUBLE_INTEL_EXTENDED_16_BYTES_LE 1
      #define NPY_PY3K 1
      #ifndef __cplusplus
      /* #undef inline */
      #endif
  
      #ifndef _NPY_NPY_CONFIG_H_
      #error config.h should never be included directly, include npy_config.h instead
      #endif
  
      EOF
        adding 'build/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy/config.h' to sources.
      Generating build/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy/_numpyconfig.h
      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers
  
      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'
      clang: _configtest.c
      _configtest.c:1:5: warning: incompatible redeclaration of library function 'exp' [-Wincompatible-library-redeclaration]
      int exp (void);
          ^
      _configtest.c:1:5: note: 'exp' is a builtin with type 'double (double)'
      1 warning generated.
      clang _configtest.o -o _configtest
      success!
      removing: _configtest.c _configtest.o _configtest.o.d _configtest
      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers
  
      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'
      clang: _configtest.c
      success!
      removing: _configtest.c _configtest.o _configtest.o.d
      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers
  
      compile options: '-Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -c'
      clang: _configtest.c
      success!
      removing: _configtest.c _configtest.o _configtest.o.d
      File: build/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy/_numpyconfig.h
      #define NPY_SIZEOF_SHORT SIZEOF_SHORT
      #define NPY_SIZEOF_INT SIZEOF_INT
      #define NPY_SIZEOF_LONG SIZEOF_LONG
      #define NPY_SIZEOF_FLOAT 4
      #define NPY_SIZEOF_COMPLEX_FLOAT 8
      #define NPY_SIZEOF_DOUBLE 8
      #define NPY_SIZEOF_COMPLEX_DOUBLE 16
      #define NPY_SIZEOF_LONGDOUBLE 16
      #define NPY_SIZEOF_COMPLEX_LONGDOUBLE 32
      #define NPY_SIZEOF_PY_INTPTR_T 8
      #define NPY_SIZEOF_OFF_T 8
      #define NPY_SIZEOF_PY_LONG_LONG 8
      #define NPY_SIZEOF_LONGLONG 8
      #define NPY_NO_SMP 0
      #define NPY_HAVE_DECL_ISNAN
      #define NPY_HAVE_DECL_ISINF
      #define NPY_HAVE_DECL_ISFINITE
      #define NPY_HAVE_DECL_SIGNBIT
      #define NPY_USE_C99_COMPLEX 1
      #define NPY_HAVE_COMPLEX_DOUBLE 1
      #define NPY_HAVE_COMPLEX_FLOAT 1
      #define NPY_HAVE_COMPLEX_LONG_DOUBLE 1
      #define NPY_RELAXED_STRIDES_CHECKING 1
      #define NPY_USE_C99_FORMATS 1
      #define NPY_VISIBILITY_HIDDEN __attribute__((visibility(""hidden"")))
      #define NPY_ABI_VERSION 0x01000009
      #define NPY_API_VERSION 0x0000000D
  
      #ifndef __STDC_FORMAT_MACROS
      #define __STDC_FORMAT_MACROS 1
      #endif
  
      EOF
        adding 'build/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy/_numpyconfig.h' to sources.
      executing numpy/core/code_generators/generate_numpy_api.py
        adding 'build/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy/__multiarray_api.h' to sources.
      numpy.core - nothing done with h_files = ['build/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy/config.h', 'build/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy/_numpyconfig.h', 'build/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy/__multiarray_api.h']
      building extension ""numpy.core._multiarray_tests"" sources
      creating build/src.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray
      conv_template:> build/src.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/_multiarray_tests.c
      building extension ""numpy.core._multiarray_umath"" sources
        adding 'build/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy/config.h' to sources.
        adding 'build/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy/_numpyconfig.h' to sources.
      executing numpy/core/code_generators/generate_numpy_api.py
        adding 'build/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy/__multiarray_api.h' to sources.
      executing numpy/core/code_generators/generate_ufunc_api.py
        adding 'build/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy/__ufunc_api.h' to sources.
      conv_template:> build/src.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/arraytypes.c
      conv_template:> build/src.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/einsum.c
      conv_template:> build/src.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/lowlevel_strided_loops.c
      conv_template:> build/src.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/nditer_templ.c
      conv_template:> build/src.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/scalartypes.c
      creating build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath
      conv_template:> build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/funcs.inc
        adding 'build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath' to include_dirs.
      conv_template:> build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/simd.inc
      conv_template:> build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/loops.h
      conv_template:> build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/loops.c
      conv_template:> build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/matmul.h
      conv_template:> build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/matmul.c
      conv_template:> build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/scalarmath.c
        adding 'build/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath' to include_dirs.
      conv_template:> build/src.macosx-10.15-x86_64-3.9/numpy/core/src/common/templ_common.h
        adding 'build/src.macosx-10.15-x86_64-3.9/numpy/core/src/common' to include_dirs.
      numpy.core - nothing done with h_files = ['build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/funcs.inc', 'build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/simd.inc', 'build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/loops.h', 'build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/matmul.h', 'build/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath/npy_math_internal.h', 'build/src.macosx-10.15-x86_64-3.9/numpy/core/src/common/templ_common.h', 'build/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy/config.h', 'build/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy/_numpyconfig.h', 'build/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy/__multiarray_api.h', 'build/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy/__ufunc_api.h']
      building extension ""numpy.core._umath_tests"" sources
      conv_template:> build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/_umath_tests.c
      building extension ""numpy.core._rational_tests"" sources
      conv_template:> build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/_rational_tests.c
      building extension ""numpy.core._struct_ufunc_tests"" sources
      conv_template:> build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/_struct_ufunc_tests.c
      building extension ""numpy.core._operand_flag_tests"" sources
      conv_template:> build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/_operand_flag_tests.c
      building extension ""numpy.fft.fftpack_lite"" sources
      building extension ""numpy.linalg.lapack_lite"" sources
      creating build/src.macosx-10.15-x86_64-3.9/numpy/linalg
        adding 'numpy/linalg/lapack_lite/python_xerbla.c' to sources.
      building extension ""numpy.linalg._umath_linalg"" sources
        adding 'numpy/linalg/lapack_lite/python_xerbla.c' to sources.
      conv_template:> build/src.macosx-10.15-x86_64-3.9/numpy/linalg/umath_linalg.c
      building extension ""numpy.random.mtrand"" sources
      creating build/src.macosx-10.15-x86_64-3.9/numpy/random
      building data_files sources
      build_src: building npy-pkg config files
      running build_py
      creating build/lib.macosx-10.15-x86_64-3.9
      creating build/lib.macosx-10.15-x86_64-3.9/numpy
      copying numpy/conftest.py -> build/lib.macosx-10.15-x86_64-3.9/numpy
      copying numpy/version.py -> build/lib.macosx-10.15-x86_64-3.9/numpy
      copying numpy/_globals.py -> build/lib.macosx-10.15-x86_64-3.9/numpy
      copying numpy/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy
      copying numpy/dual.py -> build/lib.macosx-10.15-x86_64-3.9/numpy
      copying numpy/_distributor_init.py -> build/lib.macosx-10.15-x86_64-3.9/numpy
      copying numpy/setup.py -> build/lib.macosx-10.15-x86_64-3.9/numpy
      copying numpy/ctypeslib.py -> build/lib.macosx-10.15-x86_64-3.9/numpy
      copying numpy/matlib.py -> build/lib.macosx-10.15-x86_64-3.9/numpy
      copying numpy/_pytesttester.py -> build/lib.macosx-10.15-x86_64-3.9/numpy
      copying build/src.macosx-10.15-x86_64-3.9/numpy/__config__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy
      creating build/lib.macosx-10.15-x86_64-3.9/numpy/compat
      copying numpy/compat/py3k.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/compat
      copying numpy/compat/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/compat
      copying numpy/compat/setup.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/compat
      copying numpy/compat/_inspect.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/compat
      creating build/lib.macosx-10.15-x86_64-3.9/numpy/core
      copying numpy/core/umath.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core
      copying numpy/core/fromnumeric.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core
      copying numpy/core/_dtype.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core
      copying numpy/core/_add_newdocs.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core
      copying numpy/core/_methods.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core
      copying numpy/core/_internal.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core
      copying numpy/core/_string_helpers.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core
      copying numpy/core/multiarray.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core
      copying numpy/core/records.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core
      copying numpy/core/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core
      copying numpy/core/setup_common.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core
      copying numpy/core/_aliased_types.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core
      copying numpy/core/memmap.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core
      copying numpy/core/overrides.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core
      copying numpy/core/getlimits.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core
      copying numpy/core/_dtype_ctypes.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core
      copying numpy/core/defchararray.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core
      copying numpy/core/shape_base.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core
      copying numpy/core/machar.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core
      copying numpy/core/setup.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core
      copying numpy/core/numeric.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core
      copying numpy/core/function_base.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core
      copying numpy/core/einsumfunc.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core
      copying numpy/core/umath_tests.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core
      copying numpy/core/info.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core
      copying numpy/core/numerictypes.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core
      copying numpy/core/_type_aliases.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core
      copying numpy/core/cversions.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core
      copying numpy/core/arrayprint.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core
      copying numpy/core/code_generators/generate_numpy_api.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/core
      creating build/lib.macosx-10.15-x86_64-3.9/numpy/distutils
      copying numpy/distutils/unixccompiler.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils
      copying numpy/distutils/numpy_distribution.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils
      copying numpy/distutils/conv_template.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils
      copying numpy/distutils/cpuinfo.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils
      copying numpy/distutils/ccompiler.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils
      copying numpy/distutils/msvc9compiler.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils
      copying numpy/distutils/npy_pkg_config.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils
      copying numpy/distutils/compat.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils
      copying numpy/distutils/misc_util.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils
      copying numpy/distutils/log.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils
      copying numpy/distutils/line_endings.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils
      copying numpy/distutils/lib2def.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils
      copying numpy/distutils/pathccompiler.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils
      copying numpy/distutils/system_info.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils
      copying numpy/distutils/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils
      copying numpy/distutils/core.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils
      copying numpy/distutils/__version__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils
      copying numpy/distutils/exec_command.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils
      copying numpy/distutils/from_template.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils
      copying numpy/distutils/mingw32ccompiler.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils
      copying numpy/distutils/setup.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils
      copying numpy/distutils/extension.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils
      copying numpy/distutils/msvccompiler.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils
      copying numpy/distutils/intelccompiler.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils
      copying numpy/distutils/info.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils
      copying build/src.macosx-10.15-x86_64-3.9/numpy/distutils/__config__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils
      creating build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/command
      copying numpy/distutils/command/build.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/command
      copying numpy/distutils/command/config_compiler.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/command
      copying numpy/distutils/command/build_ext.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/command
      copying numpy/distutils/command/config.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/command
      copying numpy/distutils/command/install_headers.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/command
      copying numpy/distutils/command/build_py.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/command
      copying numpy/distutils/command/build_src.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/command
      copying numpy/distutils/command/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/command
      copying numpy/distutils/command/sdist.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/command
      copying numpy/distutils/command/build_scripts.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/command
      copying numpy/distutils/command/bdist_rpm.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/command
      copying numpy/distutils/command/install_clib.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/command
      copying numpy/distutils/command/build_clib.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/command
      copying numpy/distutils/command/autodist.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/command
      copying numpy/distutils/command/egg_info.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/command
      copying numpy/distutils/command/install.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/command
      copying numpy/distutils/command/develop.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/command
      copying numpy/distutils/command/install_data.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/command
      creating build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/fcompiler
      copying numpy/distutils/fcompiler/gnu.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/fcompiler
      copying numpy/distutils/fcompiler/compaq.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/fcompiler
      copying numpy/distutils/fcompiler/intel.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/fcompiler
      copying numpy/distutils/fcompiler/none.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/fcompiler
      copying numpy/distutils/fcompiler/nag.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/fcompiler
      copying numpy/distutils/fcompiler/pg.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/fcompiler
      copying numpy/distutils/fcompiler/ibm.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/fcompiler
      copying numpy/distutils/fcompiler/sun.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/fcompiler
      copying numpy/distutils/fcompiler/lahey.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/fcompiler
      copying numpy/distutils/fcompiler/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/fcompiler
      copying numpy/distutils/fcompiler/g95.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/fcompiler
      copying numpy/distutils/fcompiler/mips.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/fcompiler
      copying numpy/distutils/fcompiler/hpux.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/fcompiler
      copying numpy/distutils/fcompiler/environment.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/fcompiler
      copying numpy/distutils/fcompiler/pathf95.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/fcompiler
      copying numpy/distutils/fcompiler/absoft.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/fcompiler
      copying numpy/distutils/fcompiler/vast.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/distutils/fcompiler
      creating build/lib.macosx-10.15-x86_64-3.9/numpy/doc
      copying numpy/doc/misc.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/doc
      copying numpy/doc/internals.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/doc
      copying numpy/doc/creation.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/doc
      copying numpy/doc/constants.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/doc
      copying numpy/doc/ufuncs.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/doc
      copying numpy/doc/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/doc
      copying numpy/doc/broadcasting.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/doc
      copying numpy/doc/basics.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/doc
      copying numpy/doc/subclassing.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/doc
      copying numpy/doc/indexing.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/doc
      copying numpy/doc/byteswapping.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/doc
      copying numpy/doc/structured_arrays.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/doc
      copying numpy/doc/glossary.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/doc
      creating build/lib.macosx-10.15-x86_64-3.9/numpy/f2py
      copying numpy/f2py/cfuncs.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/f2py
      copying numpy/f2py/common_rules.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/f2py
      copying numpy/f2py/crackfortran.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/f2py
      copying numpy/f2py/cb_rules.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/f2py
      copying numpy/f2py/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/f2py
      copying numpy/f2py/rules.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/f2py
      copying numpy/f2py/f2py2e.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/f2py
      copying numpy/f2py/func2subr.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/f2py
      copying numpy/f2py/__version__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/f2py
      copying numpy/f2py/diagnose.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/f2py
      copying numpy/f2py/setup.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/f2py
      copying numpy/f2py/capi_maps.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/f2py
      copying numpy/f2py/f90mod_rules.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/f2py
      copying numpy/f2py/f2py_testing.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/f2py
      copying numpy/f2py/use_rules.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/f2py
      copying numpy/f2py/info.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/f2py
      copying numpy/f2py/auxfuncs.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/f2py
      copying numpy/f2py/__main__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/f2py
      creating build/lib.macosx-10.15-x86_64-3.9/numpy/fft
      copying numpy/fft/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/fft
      copying numpy/fft/setup.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/fft
      copying numpy/fft/helper.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/fft
      copying numpy/fft/fftpack.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/fft
      copying numpy/fft/info.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/fft
      creating build/lib.macosx-10.15-x86_64-3.9/numpy/lib
      copying numpy/lib/_iotools.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib
      copying numpy/lib/mixins.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib
      copying numpy/lib/nanfunctions.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib
      copying numpy/lib/recfunctions.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib
      copying numpy/lib/histograms.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib
      copying numpy/lib/scimath.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib
      copying numpy/lib/_version.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib
      copying numpy/lib/user_array.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib
      copying numpy/lib/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib
      copying numpy/lib/format.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib
      copying numpy/lib/twodim_base.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib
      copying numpy/lib/financial.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib
      copying numpy/lib/index_tricks.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib
      copying numpy/lib/npyio.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib
      copying numpy/lib/shape_base.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib
      copying numpy/lib/setup.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib
      copying numpy/lib/stride_tricks.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib
      copying numpy/lib/utils.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib
      copying numpy/lib/arrayterator.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib
      copying numpy/lib/function_base.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib
      copying numpy/lib/arraysetops.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib
      copying numpy/lib/arraypad.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib
      copying numpy/lib/type_check.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib
      copying numpy/lib/info.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib
      copying numpy/lib/polynomial.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib
      copying numpy/lib/_datasource.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib
      copying numpy/lib/ufunclike.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/lib
      creating build/lib.macosx-10.15-x86_64-3.9/numpy/linalg
      copying numpy/linalg/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/linalg
      copying numpy/linalg/setup.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/linalg
      copying numpy/linalg/linalg.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/linalg
      copying numpy/linalg/info.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/linalg
      creating build/lib.macosx-10.15-x86_64-3.9/numpy/ma
      copying numpy/ma/extras.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/ma
      copying numpy/ma/version.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/ma
      copying numpy/ma/testutils.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/ma
      copying numpy/ma/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/ma
      copying numpy/ma/core.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/ma
      copying numpy/ma/bench.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/ma
      copying numpy/ma/setup.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/ma
      copying numpy/ma/timer_comparison.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/ma
      copying numpy/ma/mrecords.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/ma
      creating build/lib.macosx-10.15-x86_64-3.9/numpy/matrixlib
      copying numpy/matrixlib/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/matrixlib
      copying numpy/matrixlib/setup.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/matrixlib
      copying numpy/matrixlib/defmatrix.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/matrixlib
      creating build/lib.macosx-10.15-x86_64-3.9/numpy/polynomial
      copying numpy/polynomial/laguerre.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/polynomial
      copying numpy/polynomial/_polybase.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/polynomial
      copying numpy/polynomial/polyutils.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/polynomial
      copying numpy/polynomial/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/polynomial
      copying numpy/polynomial/setup.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/polynomial
      copying numpy/polynomial/hermite_e.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/polynomial
      copying numpy/polynomial/chebyshev.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/polynomial
      copying numpy/polynomial/polynomial.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/polynomial
      copying numpy/polynomial/legendre.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/polynomial
      copying numpy/polynomial/hermite.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/polynomial
      creating build/lib.macosx-10.15-x86_64-3.9/numpy/random
      copying numpy/random/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/random
      copying numpy/random/setup.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/random
      copying numpy/random/info.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/random
      creating build/lib.macosx-10.15-x86_64-3.9/numpy/testing
      copying numpy/testing/nosetester.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/testing
      copying numpy/testing/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/testing
      copying numpy/testing/noseclasses.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/testing
      copying numpy/testing/setup.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/testing
      copying numpy/testing/utils.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/testing
      copying numpy/testing/print_coercion_tables.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/testing
      copying numpy/testing/decorators.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/testing
      creating build/lib.macosx-10.15-x86_64-3.9/numpy/testing/_private
      copying numpy/testing/_private/nosetester.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/testing/_private
      copying numpy/testing/_private/__init__.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/testing/_private
      copying numpy/testing/_private/noseclasses.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/testing/_private
      copying numpy/testing/_private/utils.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/testing/_private
      copying numpy/testing/_private/parameterized.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/testing/_private
      copying numpy/testing/_private/decorators.py -> build/lib.macosx-10.15-x86_64-3.9/numpy/testing/_private
      running build_clib
      customize UnixCCompiler
      customize UnixCCompiler using build_clib
      building 'npymath' library
      compiling C sources
      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers
  
      creating build/temp.macosx-10.15-x86_64-3.9
      creating build/temp.macosx-10.15-x86_64-3.9/numpy
      creating build/temp.macosx-10.15-x86_64-3.9/numpy/core
      creating build/temp.macosx-10.15-x86_64-3.9/numpy/core/src
      creating build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/npymath
      creating build/temp.macosx-10.15-x86_64-3.9/build
      creating build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9
      creating build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy
      creating build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core
      creating build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core/src
      creating build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath
      compile options: '-Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -Inumpy/core/include -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -c'
      clang: numpy/core/src/npymath/npy_math.c
      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath/npy_math_complex.c
      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath/ieee754.c
      clang: numpy/core/src/npymath/halffloat.c
      numpy/core/src/npymath/npy_math_complex.c.src:48:33: warning: unused variable 'tiny' [-Wunused-const-variable]
      static const volatile npy_float tiny = 3.9443045e-31f;
                                      ^
      numpy/core/src/npymath/npy_math_complex.c.src:67:25: warning: unused variable 'c_halff' [-Wunused-const-variable]
      static const npy_cfloat c_halff = {0.5F, 0.0};
                              ^
      numpy/core/src/npymath/npy_math_complex.c.src:68:25: warning: unused variable 'c_if' [-Wunused-const-variable]
      static const npy_cfloat c_if = {0.0, 1.0F};
                              ^
      numpy/core/src/npymath/npy_math_complex.c.src:69:25: warning: unused variable 'c_ihalff' [-Wunused-const-variable]
      static const npy_cfloat c_ihalff = {0.0, 0.5F};
                              ^
      numpy/core/src/npymath/npy_math_complex.c.src:79:1: warning: unused function 'caddf' [-Wunused-function]
      caddf(npy_cfloat a, npy_cfloat b)
      ^
      numpy/core/src/npymath/npy_math_complex.c.src:87:1: warning: unused function 'csubf' [-Wunused-function]
      csubf(npy_cfloat a, npy_cfloat b)
      ^
      numpy/core/src/npymath/npy_math_complex.c.src:137:1: warning: unused function 'cnegf' [-Wunused-function]
      cnegf(npy_cfloat a)
      ^
      numpy/core/src/npymath/npy_math_complex.c.src:144:1: warning: unused function 'cmulif' [-Wunused-function]
      cmulif(npy_cfloat a)
      ^
      numpy/core/src/npymath/npy_math_complex.c.src:67:26: warning: unused variable 'c_half' [-Wunused-const-variable]
      static const npy_cdouble c_half = {0.5, 0.0};
                               ^
      numpy/core/src/npymath/npy_math_complex.c.src:68:26: warning: unused variable 'c_i' [-Wunused-const-variable]
      static const npy_cdouble c_i = {0.0, 1.0};
                               ^
      numpy/core/src/npymath/npy_math_complex.c.src:69:26: warning: unused variable 'c_ihalf' [-Wunused-const-variable]
      static const npy_cdouble c_ihalf = {0.0, 0.5};
                               ^
      numpy/core/src/npymath/npy_math_complex.c.src:79:1: warning: unused function 'cadd' [-Wunused-function]
      cadd(npy_cdouble a, npy_cdouble b)
      ^
      numpy/core/src/npymath/npy_math_complex.c.src:87:1: warning: unused function 'csub' [-Wunused-function]
      csub(npy_cdouble a, npy_cdouble b)
      ^
      numpy/core/src/npymath/npy_math_complex.c.src:137:1: warning: unused function 'cneg' [-Wunused-function]
      cneg(npy_cdouble a)
      ^
      numpy/core/src/npymath/npy_math_complex.c.src:144:1: warning: unused function 'cmuli' [-Wunused-function]
      cmuli(npy_cdouble a)
      ^
      numpy/core/src/npymath/npy_math_complex.c.src:67:30: warning: unused variable 'c_halfl' [-Wunused-const-variable]
      static const npy_clongdouble c_halfl = {0.5L, 0.0};
                                   ^
      numpy/core/src/npymath/npy_math_complex.c.src:68:30: warning: unused variable 'c_il' [-Wunused-const-variable]
      static const npy_clongdouble c_il = {0.0, 1.0L};
                                   ^
      numpy/core/src/npymath/npy_math_complex.c.src:69:30: warning: unused variable 'c_ihalfl' [-Wunused-const-variable]
      static const npy_clongdouble c_ihalfl = {0.0, 0.5L};
                                   ^
      numpy/core/src/npymath/npy_math_complex.c.src:79:1: warning: unused function 'caddl' [-Wunused-function]
      caddl(npy_clongdouble a, npy_clongdouble b)
      ^
      numpy/core/src/npymath/npy_math_complex.c.src:87:1: warning: unused function 'csubl' [-Wunused-function]
      csubl(npy_clongdouble a, npy_clongdouble b)
      ^
      numpy/core/src/npymath/npy_math_complex.c.src:137:1: warning: unused function 'cnegl' [-Wunused-function]
      cnegl(npy_clongdouble a)
      ^
      numpy/core/src/npymath/npy_math_complex.c.src:144:1: warning: unused function 'cmulil' [-Wunused-function]
      cmulil(npy_clongdouble a)
      ^
      22 warnings generated.
      ar: adding 4 object files to build/temp.macosx-10.15-x86_64-3.9/libnpymath.a
      ranlib:@ build/temp.macosx-10.15-x86_64-3.9/libnpymath.a
      building 'npysort' library
      compiling C sources
      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers
  
      creating build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core/src/npysort
      compile options: '-Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Inumpy/core/include -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -c'
      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/npysort/quicksort.c
      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/npysort/mergesort.c
      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/npysort/heapsort.c
      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/npysort/selection.c
      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/npysort/binsearch.c
      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
              npy_intp k;
              ^~~~~~~~~~~
      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
          else if (0 && kth == num - 1) {
                   ^
                   /* DISABLES CODE */ ( )
      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
              npy_intp k;
              ^~~~~~~~~~~
      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
          else if (0 && kth == num - 1) {
                   ^
                   /* DISABLES CODE */ ( )
      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
              npy_intp k;
              ^~~~~~~~~~~
      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
          else if (0 && kth == num - 1) {
                   ^
                   /* DISABLES CODE */ ( )
      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
              npy_intp k;
              ^~~~~~~~~~~
      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
          else if (0 && kth == num - 1) {
                   ^
                   /* DISABLES CODE */ ( )
      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
              npy_intp k;
              ^~~~~~~~~~~
      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
          else if (0 && kth == num - 1) {
                   ^
                   /* DISABLES CODE */ ( )
      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
              npy_intp k;
              ^~~~~~~~~~~
      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
          else if (0 && kth == num - 1) {
                   ^
                   /* DISABLES CODE */ ( )
      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
              npy_intp k;
              ^~~~~~~~~~~
      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
          else if (0 && kth == num - 1) {
                   ^
                   /* DISABLES CODE */ ( )
      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
              npy_intp k;
              ^~~~~~~~~~~
      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
          else if (0 && kth == num - 1) {
                   ^
                   /* DISABLES CODE */ ( )
      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
              npy_intp k;
              ^~~~~~~~~~~
      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
          else if (0 && kth == num - 1) {
                   ^
                   /* DISABLES CODE */ ( )
      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
              npy_intp k;
              ^~~~~~~~~~~
      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
          else if (0 && kth == num - 1) {
                   ^
                   /* DISABLES CODE */ ( )
      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
              npy_intp k;
              ^~~~~~~~~~~
      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
          else if (0 && kth == num - 1) {
                   ^
                   /* DISABLES CODE */ ( )
      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
              npy_intp k;
              ^~~~~~~~~~~
      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
          else if (0 && kth == num - 1) {
                   ^
                   /* DISABLES CODE */ ( )
      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
              npy_intp k;
              ^~~~~~~~~~~
      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
          else if (0 && kth == num - 1) {
                   ^
                   /* DISABLES CODE */ ( )
      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
              npy_intp k;
              ^~~~~~~~~~~
      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
          else if (0 && kth == num - 1) {
                   ^
                   /* DISABLES CODE */ ( )
      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
              npy_intp k;
              ^~~~~~~~~~~
      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
          else if (0 && kth == num - 1) {
                   ^
                   /* DISABLES CODE */ ( )
      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
              npy_intp k;
              ^~~~~~~~~~~
      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
          else if (0 && kth == num - 1) {
                   ^
                   /* DISABLES CODE */ ( )
      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
              npy_intp k;
              ^~~~~~~~~~~
      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
          else if (0 && kth == num - 1) {
                   ^
                   /* DISABLES CODE */ ( )
      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
              npy_intp k;
              ^~~~~~~~~~~
      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
          else if (0 && kth == num - 1) {
                   ^
                   /* DISABLES CODE */ ( )
      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
              npy_intp k;
              ^~~~~~~~~~~
      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
          else if (0 && kth == num - 1) {
                   ^
                   /* DISABLES CODE */ ( )
      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
              npy_intp k;
              ^~~~~~~~~~~
      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
          else if (0 && kth == num - 1) {
                   ^
                   /* DISABLES CODE */ ( )
      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
              npy_intp k;
              ^~~~~~~~~~~
      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
          else if (0 && kth == num - 1) {
                   ^
                   /* DISABLES CODE */ ( )
      numpy/core/src/npysort/selection.c.src:328:9: warning: code will never be executed [-Wunreachable-code]
              npy_intp k;
              ^~~~~~~~~~~
      numpy/core/src/npysort/selection.c.src:326:14: note: silence by adding parentheses to mark code as explicitly dead
          else if (0 && kth == num - 1) {
                   ^
                   /* DISABLES CODE */ ( )
      22 warnings generated.
      ar: adding 5 object files to build/temp.macosx-10.15-x86_64-3.9/libnpysort.a
      ranlib:@ build/temp.macosx-10.15-x86_64-3.9/libnpysort.a
      running build_ext
      customize UnixCCompiler
      customize UnixCCompiler using build_ext
      building 'numpy.core._dummy' extension
      compiling C sources
      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers
  
      compile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -Inumpy/core/include -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -c'
      clang: numpy/core/src/dummymodule.c
      clang -bundle -undefined dynamic_lookup -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/dummymodule.o -L/usr/local/lib -L/usr/local/opt/openssl@1.1/lib -L/usr/local/opt/sqlite/lib -Lbuild/temp.macosx-10.15-x86_64-3.9 -o build/lib.macosx-10.15-x86_64-3.9/numpy/core/_dummy.cpython-39-darwin.so
      building 'numpy.core._multiarray_tests' extension
      compiling C sources
      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers
  
      creating build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray
      creating build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/common
      compile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -Inumpy/core/include -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -c'
      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/_multiarray_tests.c
      clang: numpy/core/src/common/mem_overlap.c
      clang -bundle -undefined dynamic_lookup -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/_multiarray_tests.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/common/mem_overlap.o -L/usr/local/lib -L/usr/local/opt/openssl@1.1/lib -L/usr/local/opt/sqlite/lib -Lbuild/temp.macosx-10.15-x86_64-3.9 -lnpymath -o build/lib.macosx-10.15-x86_64-3.9/numpy/core/_multiarray_tests.cpython-39-darwin.so
      building 'numpy.core._multiarray_umath' extension
      compiling C sources
      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers
  
      creating build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray
      creating build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/umath
      creating build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath
      creating build/temp.macosx-10.15-x86_64-3.9/private
      creating build/temp.macosx-10.15-x86_64-3.9/private/var
      creating build/temp.macosx-10.15-x86_64-3.9/private/var/folders
      creating build/temp.macosx-10.15-x86_64-3.9/private/var/folders/fz
      creating build/temp.macosx-10.15-x86_64-3.9/private/var/folders/fz/0j719tys48x7jlnjnwc69smr0000gn
      creating build/temp.macosx-10.15-x86_64-3.9/private/var/folders/fz/0j719tys48x7jlnjnwc69smr0000gn/T
      creating build/temp.macosx-10.15-x86_64-3.9/private/var/folders/fz/0j719tys48x7jlnjnwc69smr0000gn/T/pip-install-ufzck51l
      creating build/temp.macosx-10.15-x86_64-3.9/private/var/folders/fz/0j719tys48x7jlnjnwc69smr0000gn/T/pip-install-ufzck51l/numpy_b0e8a3953a1d4b46801f12bcea55536e
      creating build/temp.macosx-10.15-x86_64-3.9/private/var/folders/fz/0j719tys48x7jlnjnwc69smr0000gn/T/pip-install-ufzck51l/numpy_b0e8a3953a1d4b46801f12bcea55536e/numpy
      creating build/temp.macosx-10.15-x86_64-3.9/private/var/folders/fz/0j719tys48x7jlnjnwc69smr0000gn/T/pip-install-ufzck51l/numpy_b0e8a3953a1d4b46801f12bcea55536e/numpy/_build_utils
      creating build/temp.macosx-10.15-x86_64-3.9/private/var/folders/fz/0j719tys48x7jlnjnwc69smr0000gn/T/pip-install-ufzck51l/numpy_b0e8a3953a1d4b46801f12bcea55536e/numpy/_build_utils/src
      compile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -DNO_ATLAS_INFO=3 -DHAVE_CBLAS -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Inumpy/core/include -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -c'
      extra options: '-msse3 -I/System/Library/Frameworks/vecLib.framework/Headers'
      clang: numpy/core/src/multiarray/alloc.c
      clang: numpy/core/src/multiarray/calculation.cclang: numpy/core/src/multiarray/array_assign_scalar.c
      clang: numpy/core/src/multiarray/convert.c
  
      clang: numpy/core/src/multiarray/ctors.c
      clang: numpy/core/src/multiarray/datetime_busday.c
      clang: numpy/core/src/multiarray/dragon4.cclang: numpy/core/src/multiarray/flagsobject.c
  
      numpy/core/src/multiarray/ctors.c:2261:36: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]
          if (!(PyUString_Check(name) && PyUString_GET_SIZE(name) == 0)) {
                                         ^
      numpy/core/include/numpy/npy_3kcompat.h:110:28: note: expanded from macro 'PyUString_GET_SIZE'
      #define PyUString_GET_SIZE PyUnicode_GET_SIZE
                                 ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'
            PyUnicode_WSTR_LENGTH(op) :                    \
            ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'
      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)
                                        ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3)
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/core/src/multiarray/ctors.c:2261:36: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]
          if (!(PyUString_Check(name) && PyUString_GET_SIZE(name) == 0)) {
                                         ^
      numpy/core/include/numpy/npy_3kcompat.h:110:28: note: expanded from macro 'PyUString_GET_SIZE'
      #define PyUString_GET_SIZE PyUnicode_GET_SIZE
                                 ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'
            ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\
                   ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/core/src/multiarray/ctors.c:2261:36: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]
          if (!(PyUString_Check(name) && PyUString_GET_SIZE(name) == 0)) {
                                         ^
      numpy/core/include/numpy/npy_3kcompat.h:110:28: note: expanded from macro 'PyUString_GET_SIZE'
      #define PyUString_GET_SIZE PyUnicode_GET_SIZE
                                 ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'
             PyUnicode_WSTR_LENGTH(op)))
             ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'
      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)
                                        ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3)
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      clang: numpy/core/src/multiarray/arrayobject.c
      clang: numpy/core/src/multiarray/array_assign_array.c
      clang: numpy/core/src/multiarray/convert_datatype.c
      clang: numpy/core/src/multiarray/getset.c
      clang: numpy/core/src/multiarray/datetime_busdaycal.c
      clang: numpy/core/src/multiarray/buffer.c
      clang: numpy/core/src/multiarray/compiled_base.c
      clang: numpy/core/src/multiarray/hashdescr.c
      clang: numpy/core/src/multiarray/descriptor.c
      numpy/core/src/multiarray/descriptor.c:453:13: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]
              if (PyUString_GET_SIZE(name) == 0) {
                  ^
      numpy/core/include/numpy/npy_3kcompat.h:110:28: note: expanded from macro 'PyUString_GET_SIZE'
      #define PyUString_GET_SIZE PyUnicode_GET_SIZE
                                 ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'
            PyUnicode_WSTR_LENGTH(op) :                    \
            ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'
      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)
                                        ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3)
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/core/src/multiarray/descriptor.c:453:13: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]
              if (PyUString_GET_SIZE(name) == 0) {
                  ^
      numpy/core/include/numpy/npy_3kcompat.h:110:28: note: expanded from macro 'PyUString_GET_SIZE'
      #define PyUString_GET_SIZE PyUnicode_GET_SIZE
                                 ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'
            ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\
                   ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/core/src/multiarray/descriptor.c:453:13: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]
              if (PyUString_GET_SIZE(name) == 0) {
                  ^
      numpy/core/include/numpy/npy_3kcompat.h:110:28: note: expanded from macro 'PyUString_GET_SIZE'
      #define PyUString_GET_SIZE PyUnicode_GET_SIZE
                                 ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'
             PyUnicode_WSTR_LENGTH(op)))
             ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'
      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)
                                        ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3)
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/core/src/multiarray/descriptor.c:460:48: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]
                  else if (PyUString_Check(title) && PyUString_GET_SIZE(title) > 0) {
                                                     ^
      numpy/core/include/numpy/npy_3kcompat.h:110:28: note: expanded from macro 'PyUString_GET_SIZE'
      #define PyUString_GET_SIZE PyUnicode_GET_SIZE
                                 ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'
            PyUnicode_WSTR_LENGTH(op) :                    \
            ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'
      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)
                                        ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3)
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/core/src/multiarray/descriptor.c:460:48: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]
                  else if (PyUString_Check(title) && PyUString_GET_SIZE(title) > 0) {
                                                     ^
      numpy/core/include/numpy/npy_3kcompat.h:110:28: note: expanded from macro 'PyUString_GET_SIZE'
      #define PyUString_GET_SIZE PyUnicode_GET_SIZE
                                 ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'
            ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\
                   ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/core/src/multiarray/descriptor.c:460:48: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]
                  else if (PyUString_Check(title) && PyUString_GET_SIZE(title) > 0) {
                                                     ^
      numpy/core/include/numpy/npy_3kcompat.h:110:28: note: expanded from macro 'PyUString_GET_SIZE'
      #define PyUString_GET_SIZE PyUnicode_GET_SIZE
                                 ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'
             PyUnicode_WSTR_LENGTH(op)))
             ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'
      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)
                                        ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3)
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      clang: numpy/core/src/multiarray/conversion_utils.c
      clang: numpy/core/src/multiarray/item_selection.c
      clang: numpy/core/src/multiarray/dtype_transfer.c
      clang: numpy/core/src/multiarray/mapping.c
      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/arraytypes.c
      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/nditer_templ.c
      3 warnings generated.
      clang: numpy/core/src/multiarray/datetime.c
      numpy/core/src/multiarray/arraytypes.c.src:477:11: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]
          ptr = PyUnicode_AS_UNICODE(temp);
                ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:279:7: note: expanded from macro 'PyUnicode_AS_UNICODE'
            PyUnicode_AsUnicode(_PyObject_CAST(op)))
            ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/core/src/multiarray/arraytypes.c.src:482:15: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]
          datalen = PyUnicode_GET_DATA_SIZE(temp);
                    ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:268:6: note: expanded from macro 'PyUnicode_GET_DATA_SIZE'
          (PyUnicode_GET_SIZE(op) * Py_UNICODE_SIZE)
           ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'
            PyUnicode_WSTR_LENGTH(op) :                    \
            ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'
      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)
                                        ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3)
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/core/src/multiarray/arraytypes.c.src:482:15: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]
          datalen = PyUnicode_GET_DATA_SIZE(temp);
                    ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:268:6: note: expanded from macro 'PyUnicode_GET_DATA_SIZE'
          (PyUnicode_GET_SIZE(op) * Py_UNICODE_SIZE)
           ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'
            ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\
                   ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/core/src/multiarray/arraytypes.c.src:482:15: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]
          datalen = PyUnicode_GET_DATA_SIZE(temp);
                    ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:268:6: note: expanded from macro 'PyUnicode_GET_DATA_SIZE'
          (PyUnicode_GET_SIZE(op) * Py_UNICODE_SIZE)
           ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'
             PyUnicode_WSTR_LENGTH(op)))
             ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'
      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)
                                        ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3)
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      clang: numpy/core/src/multiarray/common.c
      numpy/core/src/multiarray/common.c:187:28: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]
                      itemsize = PyUnicode_GET_DATA_SIZE(temp);
                                 ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:268:6: note: expanded from macro 'PyUnicode_GET_DATA_SIZE'
          (PyUnicode_GET_SIZE(op) * Py_UNICODE_SIZE)
           ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'
            PyUnicode_WSTR_LENGTH(op) :                    \
            ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'
      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)
                                        ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3)
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/core/src/multiarray/common.c:187:28: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]
                      itemsize = PyUnicode_GET_DATA_SIZE(temp);
                                 ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:268:6: note: expanded from macro 'PyUnicode_GET_DATA_SIZE'
          (PyUnicode_GET_SIZE(op) * Py_UNICODE_SIZE)
           ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'
            ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\
                   ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/core/src/multiarray/common.c:187:28: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]
                      itemsize = PyUnicode_GET_DATA_SIZE(temp);
                                 ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:268:6: note: expanded from macro 'PyUnicode_GET_DATA_SIZE'
          (PyUnicode_GET_SIZE(op) * Py_UNICODE_SIZE)
           ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'
             PyUnicode_WSTR_LENGTH(op)))
             ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'
      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)
                                        ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3)
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/core/src/multiarray/common.c:239:28: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]
                      itemsize = PyUnicode_GET_DATA_SIZE(temp);
                                 ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:268:6: note: expanded from macro 'PyUnicode_GET_DATA_SIZE'
          (PyUnicode_GET_SIZE(op) * Py_UNICODE_SIZE)
           ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'
            PyUnicode_WSTR_LENGTH(op) :                    \
            ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'
      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)
                                        ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3)
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/core/src/multiarray/common.c:239:28: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]
                      itemsize = PyUnicode_GET_DATA_SIZE(temp);
                                 ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:268:6: note: expanded from macro 'PyUnicode_GET_DATA_SIZE'
          (PyUnicode_GET_SIZE(op) * Py_UNICODE_SIZE)
           ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'
            ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\
                   ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/core/src/multiarray/common.c:239:28: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]
                      itemsize = PyUnicode_GET_DATA_SIZE(temp);
                                 ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:268:6: note: expanded from macro 'PyUnicode_GET_DATA_SIZE'
          (PyUnicode_GET_SIZE(op) * Py_UNICODE_SIZE)
           ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'
             PyUnicode_WSTR_LENGTH(op)))
             ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'
      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)
                                        ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3)
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/core/src/multiarray/common.c:282:24: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]
              int itemsize = PyUnicode_GET_DATA_SIZE(obj);
                             ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:268:6: note: expanded from macro 'PyUnicode_GET_DATA_SIZE'
          (PyUnicode_GET_SIZE(op) * Py_UNICODE_SIZE)
           ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'
            PyUnicode_WSTR_LENGTH(op) :                    \
            ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'
      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)
                                        ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3)
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/core/src/multiarray/common.c:282:24: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]
              int itemsize = PyUnicode_GET_DATA_SIZE(obj);
                             ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:268:6: note: expanded from macro 'PyUnicode_GET_DATA_SIZE'
          (PyUnicode_GET_SIZE(op) * Py_UNICODE_SIZE)
           ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'
            ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\
                   ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/core/src/multiarray/common.c:282:24: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]
              int itemsize = PyUnicode_GET_DATA_SIZE(obj);
                             ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:268:6: note: expanded from macro 'PyUnicode_GET_DATA_SIZE'
          (PyUnicode_GET_SIZE(op) * Py_UNICODE_SIZE)
           ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'
             PyUnicode_WSTR_LENGTH(op)))
             ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'
      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)
                                        ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3)
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      6 warnings generated.
      clang: numpy/core/src/multiarray/nditer_pywrap.c
      9 warnings generated.
      clang: numpy/core/src/multiarray/sequence.c
      clang: numpy/core/src/multiarray/shape.c
      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/einsum.c
      clang: numpy/core/src/multiarray/methods.c
      clang: numpy/core/src/multiarray/iterators.c
      clang: numpy/core/src/multiarray/datetime_strings.c
      clang: numpy/core/src/multiarray/number.c
      clang: numpy/core/src/multiarray/scalarapi.c
      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/scalartypes.c
      numpy/core/src/multiarray/scalarapi.c:74:28: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]
                  return (void *)PyUnicode_AS_DATA(scalar);
                                 ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:283:21: note: expanded from macro 'PyUnicode_AS_DATA'
          ((const char *)(PyUnicode_AS_UNICODE(op)))
                          ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:279:7: note: expanded from macro 'PyUnicode_AS_UNICODE'
            PyUnicode_AsUnicode(_PyObject_CAST(op)))
            ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/core/src/multiarray/scalarapi.c:135:28: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]
                  return (void *)PyUnicode_AS_DATA(scalar);
                                 ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:283:21: note: expanded from macro 'PyUnicode_AS_DATA'
          ((const char *)(PyUnicode_AS_UNICODE(op)))
                          ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:279:7: note: expanded from macro 'PyUnicode_AS_UNICODE'
            PyUnicode_AsUnicode(_PyObject_CAST(op)))
            ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/core/src/multiarray/scalarapi.c:568:29: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]
                  descr->elsize = PyUnicode_GET_DATA_SIZE(sc);
                                  ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:268:6: note: expanded from macro 'PyUnicode_GET_DATA_SIZE'
          (PyUnicode_GET_SIZE(op) * Py_UNICODE_SIZE)
           ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'
            PyUnicode_WSTR_LENGTH(op) :                    \
            ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'
      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)
                                        ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3)
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/core/src/multiarray/scalarapi.c:568:29: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]
                  descr->elsize = PyUnicode_GET_DATA_SIZE(sc);
                                  ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:268:6: note: expanded from macro 'PyUnicode_GET_DATA_SIZE'
          (PyUnicode_GET_SIZE(op) * Py_UNICODE_SIZE)
           ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'
            ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\
                   ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/core/src/multiarray/scalarapi.c:568:29: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]
                  descr->elsize = PyUnicode_GET_DATA_SIZE(sc);
                                  ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:268:6: note: expanded from macro 'PyUnicode_GET_DATA_SIZE'
          (PyUnicode_GET_SIZE(op) * Py_UNICODE_SIZE)
           ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'
             PyUnicode_WSTR_LENGTH(op)))
             ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'
      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)
                                        ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3)
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/core/src/multiarray/scalartypes.c.src:475:17: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]
          ip = dptr = PyUnicode_AS_UNICODE(self);
                      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:279:7: note: expanded from macro 'PyUnicode_AS_UNICODE'
            PyUnicode_AsUnicode(_PyObject_CAST(op)))
            ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/core/src/multiarray/scalartypes.c.src:476:11: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]
          len = PyUnicode_GET_SIZE(self);
                ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'
            PyUnicode_WSTR_LENGTH(op) :                    \
            ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'
      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)
                                        ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3)
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/core/src/multiarray/scalartypes.c.src:476:11: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]
          len = PyUnicode_GET_SIZE(self);
                ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'
            ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\
                   ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/core/src/multiarray/scalartypes.c.src:476:11: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]
          len = PyUnicode_GET_SIZE(self);
                ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'
             PyUnicode_WSTR_LENGTH(op)))
             ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'
      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)
                                        ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3)
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/core/src/multiarray/scalartypes.c.src:481:11: warning: 'PyUnicode_FromUnicode' is deprecated [-Wdeprecated-declarations]
          new = PyUnicode_FromUnicode(ip, len);
                ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:551:1: note: 'PyUnicode_FromUnicode' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3) PyAPI_FUNC(PyObject*) PyUnicode_FromUnicode(
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/core/src/multiarray/scalartypes.c.src:475:17: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]
          ip = dptr = PyUnicode_AS_UNICODE(self);
                      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:279:7: note: expanded from macro 'PyUnicode_AS_UNICODE'
            PyUnicode_AsUnicode(_PyObject_CAST(op)))
            ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/core/src/multiarray/scalartypes.c.src:476:11: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]
          len = PyUnicode_GET_SIZE(self);
                ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'
            PyUnicode_WSTR_LENGTH(op) :                    \
            ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'
      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)
                                        ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3)
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/core/src/multiarray/scalartypes.c.src:476:11: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]
          len = PyUnicode_GET_SIZE(self);
                ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'
            ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\
                   ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/core/src/multiarray/scalartypes.c.src:476:11: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]
          len = PyUnicode_GET_SIZE(self);
                ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'
             PyUnicode_WSTR_LENGTH(op)))
             ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'
      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)
                                        ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3)
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/core/src/multiarray/scalartypes.c.src:481:11: warning: 'PyUnicode_FromUnicode' is deprecated [-Wdeprecated-declarations]
          new = PyUnicode_FromUnicode(ip, len);
                ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:551:1: note: 'PyUnicode_FromUnicode' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3) PyAPI_FUNC(PyObject*) PyUnicode_FromUnicode(
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/core/src/multiarray/scalartypes.c.src:1849:18: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]
              buffer = PyUnicode_AS_DATA(self);
                       ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:283:21: note: expanded from macro 'PyUnicode_AS_DATA'
          ((const char *)(PyUnicode_AS_UNICODE(op)))
                          ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:279:7: note: expanded from macro 'PyUnicode_AS_UNICODE'
            PyUnicode_AsUnicode(_PyObject_CAST(op)))
            ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/core/src/multiarray/scalartypes.c.src:1850:18: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]
              buflen = PyUnicode_GET_DATA_SIZE(self);
                       ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:268:6: note: expanded from macro 'PyUnicode_GET_DATA_SIZE'
          (PyUnicode_GET_SIZE(op) * Py_UNICODE_SIZE)
           ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'
            PyUnicode_WSTR_LENGTH(op) :                    \
            ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'
      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)
                                        ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3)
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/core/src/multiarray/scalartypes.c.src:1850:18: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]
              buflen = PyUnicode_GET_DATA_SIZE(self);
                       ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:268:6: note: expanded from macro 'PyUnicode_GET_DATA_SIZE'
          (PyUnicode_GET_SIZE(op) * Py_UNICODE_SIZE)
           ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'
            ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\
                   ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/core/src/multiarray/scalartypes.c.src:1850:18: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]
              buflen = PyUnicode_GET_DATA_SIZE(self);
                       ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:268:6: note: expanded from macro 'PyUnicode_GET_DATA_SIZE'
          (PyUnicode_GET_SIZE(op) * Py_UNICODE_SIZE)
           ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'
             PyUnicode_WSTR_LENGTH(op)))
             ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'
      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)
                                        ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3)
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      5 warnings generated.
      clang: numpy/core/src/multiarray/typeinfo.c
      clang: numpy/core/src/multiarray/refcount.c
      clang: numpy/core/src/multiarray/usertypes.c
      clang: numpy/core/src/multiarray/multiarraymodule.c
      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/lowlevel_strided_loops.c
      clang: numpy/core/src/multiarray/vdot.c
      clang: numpy/core/src/umath/umathmodule.c
      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/matmul.c
      clang: numpy/core/src/umath/reduction.c
      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/loops.c
      clang: numpy/core/src/multiarray/nditer_api.c
      14 warnings generated.
      clang: numpy/core/src/multiarray/strfuncs.c
      numpy/core/src/umath/loops.c.src:655:18: warning: 'PyEval_CallObjectWithKeywords' is deprecated [-Wdeprecated-declarations]
              result = PyEval_CallObject(tocall, arglist);
                       ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/ceval.h:24:5: note: expanded from macro 'PyEval_CallObject'
          PyEval_CallObjectWithKeywords(callable, arg, (PyObject *)NULL)
          ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/ceval.h:17:1: note: 'PyEval_CallObjectWithKeywords' has been explicitly marked deprecated here
      Py_DEPRECATED(3.9) PyAPI_FUNC(PyObject *) PyEval_CallObjectWithKeywords(
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/core/src/multiarray/strfuncs.c:178:13: warning: 'PyEval_CallObjectWithKeywords' is deprecated [-Wdeprecated-declarations]
              s = PyEval_CallObject(PyArray_ReprFunction, arglist);
                  ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/ceval.h:24:5: note: expanded from macro 'PyEval_CallObject'
          PyEval_CallObjectWithKeywords(callable, arg, (PyObject *)NULL)
          ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/ceval.h:17:1: note: 'PyEval_CallObjectWithKeywords' has been explicitly marked deprecated here
      Py_DEPRECATED(3.9) PyAPI_FUNC(PyObject *) PyEval_CallObjectWithKeywords(
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/core/src/multiarray/strfuncs.c:195:13: warning: 'PyEval_CallObjectWithKeywords' is deprecated [-Wdeprecated-declarations]
              s = PyEval_CallObject(PyArray_StrFunction, arglist);
                  ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/ceval.h:24:5: note: expanded from macro 'PyEval_CallObject'
          PyEval_CallObjectWithKeywords(callable, arg, (PyObject *)NULL)
          ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/ceval.h:17:1: note: 'PyEval_CallObjectWithKeywords' has been explicitly marked deprecated here
      Py_DEPRECATED(3.9) PyAPI_FUNC(PyObject *) PyEval_CallObjectWithKeywords(
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      2 warnings generated.
      clang: numpy/core/src/multiarray/temp_elide.c
      clang: numpy/core/src/umath/cpuid.c
      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/scalarmath.c
      clang: numpy/core/src/umath/ufunc_object.c
      numpy/core/src/umath/scalarmath.c.src:1449:1: warning: unused function 'byte_long' [-Wunused-function]
      byte_long(PyObject *obj)
      ^
      numpy/core/src/umath/scalarmath.c.src:1449:1: warning: unused function 'ubyte_long' [-Wunused-function]
      ubyte_long(PyObject *obj)
      ^
      numpy/core/src/umath/scalarmath.c.src:1449:1: warning: unused function 'short_long' [-Wunused-function]
      short_long(PyObject *obj)
      ^
      numpy/core/src/umath/scalarmath.c.src:1449:1: warning: unused function 'ushort_long' [-Wunused-function]
      ushort_long(PyObject *obj)
      ^
      numpy/core/src/umath/scalarmath.c.src:1449:1: warning: unused function 'int_long' [-Wunused-function]
      int_long(PyObject *obj)
      ^
      numpy/core/src/umath/scalarmath.c.src:1449:1: warning: unused function 'uint_long' [-Wunused-function]
      uint_long(PyObject *obj)
      ^
      numpy/core/src/umath/scalarmath.c.src:1449:1: warning: unused function 'long_long' [-Wunused-function]
      long_long(PyObject *obj)
      ^
      numpy/core/src/umath/scalarmath.c.src:1449:1: warning: unused function 'ulong_long' [-Wunused-function]
      ulong_long(PyObject *obj)
      ^
      numpy/core/src/umath/scalarmath.c.src:1449:1: warning: unused function 'longlong_long' [-Wunused-function]
      longlong_long(PyObject *obj)
      ^
      numpy/core/src/umath/scalarmath.c.src:1449:1: warning: unused function 'ulonglong_long' [-Wunused-function]
      ulonglong_long(PyObject *obj)
      ^
      numpy/core/src/umath/scalarmath.c.src:1449:1: warning: unused function 'half_long' [-Wunused-function]
      half_long(PyObject *obj)
      ^
      numpy/core/src/umath/scalarmath.c.src:1449:1: warning: unused function 'float_long' [-Wunused-function]
      float_long(PyObject *obj)
      ^
      numpy/core/src/umath/scalarmath.c.src:1449:1: warning: unused function 'double_long' [-Wunused-function]
      double_long(PyObject *obj)
      ^
      numpy/core/src/umath/scalarmath.c.src:1449:1: warning: unused function 'longdouble_long' [-Wunused-function]
      longdouble_long(PyObject *obj)
      ^
      numpy/core/src/umath/scalarmath.c.src:1449:1: warning: unused function 'cfloat_long' [-Wunused-function]
      cfloat_long(PyObject *obj)
      ^
      numpy/core/src/umath/scalarmath.c.src:1449:1: warning: unused function 'cdouble_long' [-Wunused-function]
      cdouble_long(PyObject *obj)
      ^
      numpy/core/src/umath/scalarmath.c.src:1449:1: warning: unused function 'clongdouble_long' [-Wunused-function]
      clongdouble_long(PyObject *obj)
      ^
      clang: numpy/core/src/multiarray/nditer_constr.c
      numpy/core/src/umath/ufunc_object.c:657:19: warning: comparison of integers of different signs: 'int' and 'size_t' (aka 'unsigned long') [-Wsign-compare]
          for (i = 0; i < len; i++) {
                      ~ ^ ~~~
      clang: numpy/core/src/umath/override.c
      clang: numpy/core/src/npymath/npy_math.c
      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath/ieee754.c
      numpy/core/src/umath/loops.c.src:2527:22: warning: code will never be executed [-Wunreachable-code]
              npy_intp n = dimensions[0];
                           ^~~~~~~~~~
      numpy/core/src/umath/loops.c.src:2526:29: note: silence by adding parentheses to mark code as explicitly dead
          if (IS_BINARY_REDUCE && 0) {
                                  ^
                                  /* DISABLES CODE */ ( )
      numpy/core/src/umath/loops.c.src:2527:22: warning: code will never be executed [-Wunreachable-code]
              npy_intp n = dimensions[0];
                           ^~~~~~~~~~
      numpy/core/src/umath/loops.c.src:2526:29: note: silence by adding parentheses to mark code as explicitly dead
          if (IS_BINARY_REDUCE && 0) {
                                  ^
                                  /* DISABLES CODE */ ( )
      numpy/core/src/umath/loops.c.src:2527:22: warning: code will never be executed [-Wunreachable-code]
              npy_intp n = dimensions[0];
                           ^~~~~~~~~~
      numpy/core/src/umath/loops.c.src:2526:29: note: silence by adding parentheses to mark code as explicitly dead
          if (IS_BINARY_REDUCE && 0) {
                                  ^
                                  /* DISABLES CODE */ ( )
      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath/npy_math_complex.c
      numpy/core/src/npymath/npy_math_complex.c.src:48:33: warning: unused variable 'tiny' [-Wunused-const-variable]
      static const volatile npy_float tiny = 3.9443045e-31f;
                                      ^
      numpy/core/src/npymath/npy_math_complex.c.src:67:25: warning: unused variable 'c_halff' [-Wunused-const-variable]
      static const npy_cfloat c_halff = {0.5F, 0.0};
                              ^
      numpy/core/src/npymath/npy_math_complex.c.src:68:25: warning: unused variable 'c_if' [-Wunused-const-variable]
      static const npy_cfloat c_if = {0.0, 1.0F};
                              ^
      numpy/core/src/npymath/npy_math_complex.c.src:69:25: warning: unused variable 'c_ihalff' [-Wunused-const-variable]
      static const npy_cfloat c_ihalff = {0.0, 0.5F};
                              ^
      numpy/core/src/npymath/npy_math_complex.c.src:79:1: warning: unused function 'caddf' [-Wunused-function]
      caddf(npy_cfloat a, npy_cfloat b)
      ^
      numpy/core/src/npymath/npy_math_complex.c.src:87:1: warning: unused function 'csubf' [-Wunused-function]
      csubf(npy_cfloat a, npy_cfloat b)
      ^
      numpy/core/src/npymath/npy_math_complex.c.src:137:1: warning: unused function 'cnegf' [-Wunused-function]
      cnegf(npy_cfloat a)
      ^
      numpy/core/src/npymath/npy_math_complex.c.src:144:1: warning: unused function 'cmulif' [-Wunused-function]
      cmulif(npy_cfloat a)
      ^
      numpy/core/src/npymath/npy_math_complex.c.src:67:26: warning: unused variable 'c_half' [-Wunused-const-variable]
      static const npy_cdouble c_half = {0.5, 0.0};
                               ^
      numpy/core/src/npymath/npy_math_complex.c.src:68:26: warning: unused variable 'c_i' [-Wunused-const-variable]
      static const npy_cdouble c_i = {0.0, 1.0};
                               ^
      numpy/core/src/npymath/npy_math_complex.c.src:69:26: warning: unused variable 'c_ihalf' [-Wunused-const-variable]
      static const npy_cdouble c_ihalf = {0.0, 0.5};
                               ^
      numpy/core/src/npymath/npy_math_complex.c.src:79:1: warning: unused function 'cadd' [-Wunused-function]
      cadd(npy_cdouble a, npy_cdouble b)
      ^
      numpy/core/src/npymath/npy_math_complex.c.src:87:1: warning: unused function 'csub' [-Wunused-function]
      csub(npy_cdouble a, npy_cdouble b)
      ^
      numpy/core/src/npymath/npy_math_complex.c.src:137:1: warning: unused function 'cneg' [-Wunused-function]
      cneg(npy_cdouble a)
      ^
      numpy/core/src/npymath/npy_math_complex.c.src:144:1: warning: unused function 'cmuli' [-Wunused-function]
      cmuli(npy_cdouble a)
      ^
      numpy/core/src/npymath/npy_math_complex.c.src:67:30: warning: unused variable 'c_halfl' [-Wunused-const-variable]
      static const npy_clongdouble c_halfl = {0.5L, 0.0};
                                   ^
      numpy/core/src/npymath/npy_math_complex.c.src:68:30: warning: unused variable 'c_il' [-Wunused-const-variable]
      static const npy_clongdouble c_il = {0.0, 1.0L};
                                   ^
      numpy/core/src/npymath/npy_math_complex.c.src:69:30: warning: unused variable 'c_ihalfl' [-Wunused-const-variable]
      static const npy_clongdouble c_ihalfl = {0.0, 0.5L};
                                   ^
      numpy/core/src/npymath/npy_math_complex.c.src:79:1: warning: unused function 'caddl' [-Wunused-function]
      caddl(npy_clongdouble a, npy_clongdouble b)
      ^
      numpy/core/src/npymath/npy_math_complex.c.src:87:1: warning: unused function 'csubl' [-Wunused-function]
      csubl(npy_clongdouble a, npy_clongdouble b)
      ^
      numpy/core/src/npymath/npy_math_complex.c.src:137:1: warning: unused function 'cnegl' [-Wunused-function]
      cnegl(npy_clongdouble a)
      ^
      numpy/core/src/npymath/npy_math_complex.c.src:144:1: warning: unused function 'cmulil' [-Wunused-function]
      cmulil(npy_clongdouble a)
      ^
      22 warnings generated.
      clang: numpy/core/src/common/mem_overlap.c
      clang: numpy/core/src/npymath/halffloat.c
      clang: numpy/core/src/common/array_assign.c
      clang: numpy/core/src/common/ufunc_override.c
      clang: numpy/core/src/common/npy_longdouble.c
      clang: numpy/core/src/common/numpyos.c
      clang: numpy/core/src/common/ucsnarrow.c
      1 warning generated.
      clang: numpy/core/src/umath/extobj.c
      numpy/core/src/common/ucsnarrow.c:139:34: warning: 'PyUnicode_FromUnicode' is deprecated [-Wdeprecated-declarations]
              ret = (PyUnicodeObject *)PyUnicode_FromUnicode((Py_UNICODE*)buf,
                                       ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:551:1: note: 'PyUnicode_FromUnicode' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3) PyAPI_FUNC(PyObject*) PyUnicode_FromUnicode(
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      1 warning generated.
      clang: numpy/core/src/common/python_xerbla.c
      clang: numpy/core/src/common/cblasfuncs.c
      clang: /private/var/folders/fz/0j719tys48x7jlnjnwc69smr0000gn/T/pip-install-ufzck51l/numpy_b0e8a3953a1d4b46801f12bcea55536e/numpy/_build_utils/src/apple_sgemv_fix.c
      In file included from /private/var/folders/fz/0j719tys48x7jlnjnwc69smr0000gn/T/pip-install-ufzck51l/numpy_b0e8a3953a1d4b46801f12bcea55536e/numpy/_build_utils/src/apple_sgemv_fix.c:26:
      In file included from numpy/core/include/numpy/arrayobject.h:4:
      In file included from numpy/core/include/numpy/ndarrayobject.h:21:
      build/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy/__multiarray_api.h:1463:1: warning: unused function '_import_array' [-Wunused-function]
      _import_array(void)
      ^
      1 warning generated.
      17 warnings generated.
      clang: numpy/core/src/umath/ufunc_type_resolution.c
      4 warnings generated.
      4 warnings generated.
      clang -bundle -undefined dynamic_lookup -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/alloc.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/arrayobject.o build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/arraytypes.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/array_assign_scalar.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/array_assign_array.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/buffer.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/calculation.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/compiled_base.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/common.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/convert.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/convert_datatype.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/conversion_utils.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/ctors.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/datetime.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/datetime_strings.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/datetime_busday.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/datetime_busdaycal.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/descriptor.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/dragon4.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/dtype_transfer.o build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/einsum.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/flagsobject.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/getset.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/hashdescr.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/item_selection.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/iterators.o build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/lowlevel_strided_loops.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/mapping.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/methods.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/multiarraymodule.o build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/nditer_templ.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/nditer_api.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/nditer_constr.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/nditer_pywrap.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/number.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/refcount.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/sequence.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/shape.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/scalarapi.o build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/scalartypes.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/strfuncs.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/temp_elide.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/typeinfo.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/usertypes.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/multiarray/vdot.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/umath/umathmodule.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/umath/reduction.o build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/loops.o build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/matmul.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/umath/ufunc_object.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/umath/extobj.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/umath/cpuid.o build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/scalarmath.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/umath/ufunc_type_resolution.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/umath/override.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/npymath/npy_math.o build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath/ieee754.o build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath/npy_math_complex.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/npymath/halffloat.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/common/array_assign.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/common/mem_overlap.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/common/npy_longdouble.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/common/ucsnarrow.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/common/ufunc_override.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/common/numpyos.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/common/cblasfuncs.o build/temp.macosx-10.15-x86_64-3.9/numpy/core/src/common/python_xerbla.o build/temp.macosx-10.15-x86_64-3.9/private/var/folders/fz/0j719tys48x7jlnjnwc69smr0000gn/T/pip-install-ufzck51l/numpy_b0e8a3953a1d4b46801f12bcea55536e/numpy/_build_utils/src/apple_sgemv_fix.o -L/usr/local/lib -L/usr/local/opt/openssl@1.1/lib -L/usr/local/opt/sqlite/lib -Lbuild/temp.macosx-10.15-x86_64-3.9 -lnpymath -lnpysort -o build/lib.macosx-10.15-x86_64-3.9/numpy/core/_multiarray_umath.cpython-39-darwin.so -Wl,-framework -Wl,Accelerate
      building 'numpy.core._umath_tests' extension
      compiling C sources
      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers
  
      compile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -Inumpy/core/include -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -c'
      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/_umath_tests.c
      clang -bundle -undefined dynamic_lookup -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/_umath_tests.o -L/usr/local/lib -L/usr/local/opt/openssl@1.1/lib -L/usr/local/opt/sqlite/lib -Lbuild/temp.macosx-10.15-x86_64-3.9 -o build/lib.macosx-10.15-x86_64-3.9/numpy/core/_umath_tests.cpython-39-darwin.so
      building 'numpy.core._rational_tests' extension
      compiling C sources
      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers
  
      compile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -Inumpy/core/include -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -c'
      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/_rational_tests.c
      clang -bundle -undefined dynamic_lookup -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/_rational_tests.o -L/usr/local/lib -L/usr/local/opt/openssl@1.1/lib -L/usr/local/opt/sqlite/lib -Lbuild/temp.macosx-10.15-x86_64-3.9 -o build/lib.macosx-10.15-x86_64-3.9/numpy/core/_rational_tests.cpython-39-darwin.so
      building 'numpy.core._struct_ufunc_tests' extension
      compiling C sources
      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers
  
      compile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -Inumpy/core/include -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -c'
      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/_struct_ufunc_tests.c
      clang -bundle -undefined dynamic_lookup -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/_struct_ufunc_tests.o -L/usr/local/lib -L/usr/local/opt/openssl@1.1/lib -L/usr/local/opt/sqlite/lib -Lbuild/temp.macosx-10.15-x86_64-3.9 -o build/lib.macosx-10.15-x86_64-3.9/numpy/core/_struct_ufunc_tests.cpython-39-darwin.so
      building 'numpy.core._operand_flag_tests' extension
      compiling C sources
      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers
  
      compile options: '-DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -Inumpy/core/include -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -c'
      clang: build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/_operand_flag_tests.c
      clang -bundle -undefined dynamic_lookup -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/core/src/umath/_operand_flag_tests.o -L/usr/local/lib -L/usr/local/opt/openssl@1.1/lib -L/usr/local/opt/sqlite/lib -Lbuild/temp.macosx-10.15-x86_64-3.9 -o build/lib.macosx-10.15-x86_64-3.9/numpy/core/_operand_flag_tests.cpython-39-darwin.so
      building 'numpy.fft.fftpack_lite' extension
      compiling C sources
      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers
  
      creating build/temp.macosx-10.15-x86_64-3.9/numpy/fft
      compile options: '-Inumpy/core/include -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -c'
      clang: numpy/fft/fftpack_litemodule.c
      clang: numpy/fft/fftpack.c
      clang -bundle -undefined dynamic_lookup -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk build/temp.macosx-10.15-x86_64-3.9/numpy/fft/fftpack_litemodule.o build/temp.macosx-10.15-x86_64-3.9/numpy/fft/fftpack.o -L/usr/local/lib -L/usr/local/opt/openssl@1.1/lib -L/usr/local/opt/sqlite/lib -Lbuild/temp.macosx-10.15-x86_64-3.9 -o build/lib.macosx-10.15-x86_64-3.9/numpy/fft/fftpack_lite.cpython-39-darwin.so
      building 'numpy.linalg.lapack_lite' extension
      compiling C sources
      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers
  
      creating build/temp.macosx-10.15-x86_64-3.9/numpy/linalg
      creating build/temp.macosx-10.15-x86_64-3.9/numpy/linalg/lapack_lite
      compile options: '-DNO_ATLAS_INFO=3 -DHAVE_CBLAS -Inumpy/core/include -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -c'
      extra options: '-msse3 -I/System/Library/Frameworks/vecLib.framework/Headers'
      clang: numpy/linalg/lapack_litemodule.c
      clang: numpy/linalg/lapack_lite/python_xerbla.c
      clang -bundle -undefined dynamic_lookup -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk build/temp.macosx-10.15-x86_64-3.9/numpy/linalg/lapack_litemodule.o build/temp.macosx-10.15-x86_64-3.9/numpy/linalg/lapack_lite/python_xerbla.o -L/usr/local/lib -L/usr/local/opt/openssl@1.1/lib -L/usr/local/opt/sqlite/lib -Lbuild/temp.macosx-10.15-x86_64-3.9 -o build/lib.macosx-10.15-x86_64-3.9/numpy/linalg/lapack_lite.cpython-39-darwin.so -Wl,-framework -Wl,Accelerate
      building 'numpy.linalg._umath_linalg' extension
      compiling C sources
      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers
  
      creating build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/linalg
      compile options: '-DNO_ATLAS_INFO=3 -DHAVE_CBLAS -Inumpy/core/include -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -c'
      extra options: '-msse3 -I/System/Library/Frameworks/vecLib.framework/Headers'
      clang: build/src.macosx-10.15-x86_64-3.9/numpy/linalg/umath_linalg.c
      numpy/linalg/umath_linalg.c.src:735:32: warning: unknown warning group '-Wmaybe-uninitialized', ignored [-Wunknown-warning-option]
      #pragma GCC diagnostic ignored ""-Wmaybe-uninitialized""
                                     ^
      numpy/linalg/umath_linalg.c.src:541:1: warning: unused function 'dump_ufunc_object' [-Wunused-function]
      dump_ufunc_object(PyUFuncObject* ufunc)
      ^
      numpy/linalg/umath_linalg.c.src:566:1: warning: unused function 'dump_linearize_data' [-Wunused-function]
      dump_linearize_data(const char* name, const LINEARIZE_DATA_t* params)
      ^
      numpy/linalg/umath_linalg.c.src:602:1: warning: unused function 'dump_FLOAT_matrix' [-Wunused-function]
      dump_FLOAT_matrix(const char* name,
      ^
      numpy/linalg/umath_linalg.c.src:602:1: warning: unused function 'dump_DOUBLE_matrix' [-Wunused-function]
      dump_DOUBLE_matrix(const char* name,
      ^
      numpy/linalg/umath_linalg.c.src:602:1: warning: unused function 'dump_CFLOAT_matrix' [-Wunused-function]
      dump_CFLOAT_matrix(const char* name,
      ^
      numpy/linalg/umath_linalg.c.src:602:1: warning: unused function 'dump_CDOUBLE_matrix' [-Wunused-function]
      dump_CDOUBLE_matrix(const char* name,
      ^
      numpy/linalg/umath_linalg.c.src:865:1: warning: unused function 'zero_FLOAT_matrix' [-Wunused-function]
      zero_FLOAT_matrix(void *dst_in, const LINEARIZE_DATA_t* data)
      ^
      numpy/linalg/umath_linalg.c.src:865:1: warning: unused function 'zero_DOUBLE_matrix' [-Wunused-function]
      zero_DOUBLE_matrix(void *dst_in, const LINEARIZE_DATA_t* data)
      ^
      numpy/linalg/umath_linalg.c.src:865:1: warning: unused function 'zero_CFLOAT_matrix' [-Wunused-function]
      zero_CFLOAT_matrix(void *dst_in, const LINEARIZE_DATA_t* data)
      ^
      numpy/linalg/umath_linalg.c.src:865:1: warning: unused function 'zero_CDOUBLE_matrix' [-Wunused-function]
      zero_CDOUBLE_matrix(void *dst_in, const LINEARIZE_DATA_t* data)
      ^
      numpy/linalg/umath_linalg.c.src:1862:1: warning: unused function 'dump_geev_params' [-Wunused-function]
      dump_geev_params(const char *name, GEEV_PARAMS_t* params)
      ^
      numpy/linalg/umath_linalg.c.src:2132:1: warning: unused function 'init_cgeev' [-Wunused-function]
      init_cgeev(GEEV_PARAMS_t* params,
      ^
      numpy/linalg/umath_linalg.c.src:2213:1: warning: unused function 'process_cgeev_results' [-Wunused-function]
      process_cgeev_results(GEEV_PARAMS_t *NPY_UNUSED(params))
      ^
      numpy/linalg/umath_linalg.c.src:2376:1: warning: unused function 'dump_gesdd_params' [-Wunused-function]
      dump_gesdd_params(const char *name,
      ^
      numpy/linalg/umath_linalg.c.src:2864:1: warning: unused function 'dump_gelsd_params' [-Wunused-function]
      dump_gelsd_params(const char *name,
      ^
      16 warnings generated.
      clang -bundle -undefined dynamic_lookup -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk build/temp.macosx-10.15-x86_64-3.9/build/src.macosx-10.15-x86_64-3.9/numpy/linalg/umath_linalg.o build/temp.macosx-10.15-x86_64-3.9/numpy/linalg/lapack_lite/python_xerbla.o -L/usr/local/lib -L/usr/local/opt/openssl@1.1/lib -L/usr/local/opt/sqlite/lib -Lbuild/temp.macosx-10.15-x86_64-3.9 -lnpymath -o build/lib.macosx-10.15-x86_64-3.9/numpy/linalg/_umath_linalg.cpython-39-darwin.so -Wl,-framework -Wl,Accelerate
      building 'numpy.random.mtrand' extension
      compiling C sources
      C compiler: clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers
  
      creating build/temp.macosx-10.15-x86_64-3.9/numpy/random
      creating build/temp.macosx-10.15-x86_64-3.9/numpy/random/mtrand
      compile options: '-D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -Inumpy/core/include -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -c'
      clang: numpy/random/mtrand/mtrand.c
      clang: numpy/random/mtrand/initarray.cclang: numpy/random/mtrand/randomkit.c
  
      clang: numpy/random/mtrand/distributions.c
      numpy/random/mtrand/mtrand.c:40400:34: error: no member named 'tp_print' in 'struct _typeobject'
        __pyx_type_6mtrand_RandomState.tp_print = 0;
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ^
      numpy/random/mtrand/mtrand.c:42673:22: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]
                          (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :
                           ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'
            PyUnicode_WSTR_LENGTH(op) :                    \
            ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'
      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)
                                        ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3)
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/random/mtrand/mtrand.c:42673:22: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]
                          (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :
                           ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'
            ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\
                   ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/random/mtrand/mtrand.c:42673:22: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]
                          (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :
                           ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'
             PyUnicode_WSTR_LENGTH(op)))
             ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'
      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)
                                        ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3)
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/random/mtrand/mtrand.c:42673:52: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]
                          (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :
                                                         ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'
            PyUnicode_WSTR_LENGTH(op) :                    \
            ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'
      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)
                                        ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3)
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/random/mtrand/mtrand.c:42673:52: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]
                          (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :
                                                         ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'
            ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\
                   ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/random/mtrand/mtrand.c:42673:52: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]
                          (PyUnicode_GET_SIZE(**name) != PyUnicode_GET_SIZE(key)) ? 1 :
                                                         ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'
             PyUnicode_WSTR_LENGTH(op)))
             ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'
      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)
                                        ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3)
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/random/mtrand/mtrand.c:42689:26: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]
                              (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :
                               ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'
            PyUnicode_WSTR_LENGTH(op) :                    \
            ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'
      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)
                                        ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3)
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/random/mtrand/mtrand.c:42689:26: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]
                              (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :
                               ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'
            ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\
                   ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/random/mtrand/mtrand.c:42689:26: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]
                              (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :
                               ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'
             PyUnicode_WSTR_LENGTH(op)))
             ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'
      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)
                                        ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3)
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/random/mtrand/mtrand.c:42689:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]
                              (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :
                                                                ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:261:7: note: expanded from macro 'PyUnicode_GET_SIZE'
            PyUnicode_WSTR_LENGTH(op) :                    \
            ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'
      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)
                                        ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3)
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/random/mtrand/mtrand.c:42689:59: warning: 'PyUnicode_AsUnicode' is deprecated [-Wdeprecated-declarations]
                              (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :
                                                                ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:262:14: note: expanded from macro 'PyUnicode_GET_SIZE'
            ((void)PyUnicode_AsUnicode(_PyObject_CAST(op)),\
                   ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:580:1: note: 'PyUnicode_AsUnicode' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3) PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      numpy/random/mtrand/mtrand.c:42689:59: warning: '_PyUnicode_get_wstr_length' is deprecated [-Wdeprecated-declarations]
                              (PyUnicode_GET_SIZE(**argname) != PyUnicode_GET_SIZE(key)) ? 1 :
                                                                ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:264:8: note: expanded from macro 'PyUnicode_GET_SIZE'
             PyUnicode_WSTR_LENGTH(op)))
             ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:451:35: note: expanded from macro 'PyUnicode_WSTR_LENGTH'
      #define PyUnicode_WSTR_LENGTH(op) _PyUnicode_get_wstr_length((PyObject*)op)
                                        ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/cpython/unicodeobject.h:445:1: note: '_PyUnicode_get_wstr_length' has been explicitly marked deprecated here
      Py_DEPRECATED(3.3)
      ^
      /usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9/pyport.h:508:54: note: expanded from macro 'Py_DEPRECATED'
      #define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
                                                           ^
      12 warnings and 1 error generated.
      error: Command ""clang -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -isysroot /Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX10.15.sdk/System/Library/Frameworks/Tk.framework/Versions/8.5/Headers -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -Inumpy/core/include -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/usr/local/include -I/usr/local/opt/openssl@1.1/include -I/usr/local/opt/sqlite/include -I/Users/destiny/Downloads/env/include -I/usr/local/Cellar/python@3.9/3.9.0_1/Frameworks/Python.framework/Versions/3.9/include/python3.9 -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/common -Ibuild/src.macosx-10.15-x86_64-3.9/numpy/core/src/npymath -c numpy/random/mtrand/mtrand.c -o build/temp.macosx-10.15-x86_64-3.9/numpy/random/mtrand/mtrand.o -MMD -MF build/temp.macosx-10.15-x86_64-3.9/numpy/random/mtrand/mtrand.o.d"" failed with exit status 1"
https://github.com/huggingface/datasets/issues/1687,Question: Shouldn't .info be a part of DatasetDict?,"['We could do something. There is a part of `.info` which is split specific (cache files, split instructions) but maybe if could be made to work.'
 'Yes this was kinda the idea I was going for. DatasetDict.info would be the shared info amongs the datasets (maybe even some info on how they differ). ']","Currently, only `Dataset` contains the .info or .features, but as many datasets contains standard splits (train, test) and thus the underlying information is the same (or at least should be) across the datasets. 

For instance:
```
>>> ds = datasets.load_dataset(""conll2002"", ""es"")
>>> ds.info
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: 'DatasetDict' object has no attribute 'info'
```

I could imagine that this wouldn't work for datasets dicts which hold entirely different datasets (multimodal datasets), but it seems odd that splits of the same dataset is treated the same as what is essentially different datasets. 

Intuitively it would also make sense that if a dataset is supplied via. the load_dataset that is have a common .info which covers the entire dataset.

It is entirely possible that I am missing another perspective"
https://github.com/huggingface/datasets/issues/1686,Dataset Error: DaNE contains empty samples at the end,"['Thanks for reporting, I opened a PR to fix that'
 'One the PR is merged the fix will be available in the next release of `datasets`.\r\n\r\nIf you don\'t want to wait the next release you can still load the script from the master branch with\r\n\r\n```python\r\nload_dataset(""dane"", script_version=""master"")\r\n```'
 'If you have other questions feel free to reopen :) ']","The dataset DaNE, contains empty samples at the end. It is naturally easy to remove using a filter but should probably not be there, to begin with as it can cause errors.

```python
>>> import datasets
[...]
>>> dataset = datasets.load_dataset(""dane"")
[...]
>>> dataset[""test""][-1]
{'dep_ids': [], 'dep_labels': [], 'lemmas': [], 'morph_tags': [], 'ner_tags': [], 'pos_tags': [], 'sent_id': '', 'text': '', 'tok_ids': [], 'tokens': []}
>>> dataset[""train""][-1]
{'dep_ids': [], 'dep_labels': [], 'lemmas': [], 'morph_tags': [], 'ner_tags': [], 'pos_tags': [], 'sent_id': '', 'text': '', 'tok_ids': [], 'tokens': []}
```

Best,
Kenneth"
https://github.com/huggingface/datasets/issues/1683,`ArrowInvalid` occurs while running `Dataset.map()` function for DPRContext,"['Looks like the mapping function returns a dictionary with a 768-dim array in the `embeddings` field. Since the map is batched, we actually expect the `embeddings` field to be an array of shape (batch_size, 768) to have one embedding per example in the batch.\r\n\r\nTo fix that can you try to remove one of the `[0]` ? In my opinion you only need one of them, not two.'
 'It makes sense :D\r\n\r\nIt seems to work! Thanks a lot :))\r\n\r\nClosing the issue']","It seems to fail the final batch ):

steps to reproduce:
```
from datasets import load_dataset
from elasticsearch import Elasticsearch
import torch
from transformers import file_utils, set_seed
from transformers import DPRContextEncoder, DPRContextEncoderTokenizerFast
MAX_SEQ_LENGTH = 256
ctx_encoder = DPRContextEncoder.from_pretrained(""facebook/dpr-ctx_encoder-single-nq-base"", cache_dir=""../datasets/"")
ctx_tokenizer = DPRContextEncoderTokenizerFast.from_pretrained(
    ""facebook/dpr-ctx_encoder-single-nq-base"", 
    cache_dir=""..datasets/""
)

dataset = load_dataset('text', 
                data_files='data/raw/ARC_Corpus.txt',
                cache_dir='../datasets')

torch.set_grad_enabled(False)
ds_with_embeddings = dataset.map(
    lambda example: {
        'embeddings': ctx_encoder(
            **ctx_tokenizer(
                example[""text""], 
                padding='max_length', 
                truncation=True, 
                max_length=MAX_SEQ_LENGTH,
                return_tensors=""pt""
            )
        )[0][0].numpy(),
    },
    batched=True,
    load_from_cache_file=False,
    batch_size=1000
)
```
ARC Corpus can be obtained from [here](https://ai2-datasets.s3-us-west-2.amazonaws.com/arc/ARC-V1-Feb2018.zip)

And then the error:

```
---------------------------------------------------------------------------
ArrowInvalid                              Traceback (most recent call last)
<ipython-input-13-67d139bb2ed3> in <module>
     14     batched=True,
     15     load_from_cache_file=False,
---> 16     batch_size=1000
     17 )

~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/datasets/dataset_dict.py in map(self, function, with_indices, input_columns, batched, batch_size, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc)
    301                     num_proc=num_proc,
    302                 )
--> 303                 for k, dataset in self.items()
    304             }
    305         )

~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/datasets/dataset_dict.py in <dictcomp>(.0)
    301                     num_proc=num_proc,
    302                 )
--> 303                 for k, dataset in self.items()
    304             }
    305         )

~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/datasets/arrow_dataset.py in map(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint)
   1257                 fn_kwargs=fn_kwargs,
   1258                 new_fingerprint=new_fingerprint,
-> 1259                 update_data=update_data,
   1260             )
   1261         else:

~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/datasets/arrow_dataset.py in wrapper(*args, **kwargs)
    155         }
    156         # apply actual function
--> 157         out: Union[""Dataset"", ""DatasetDict""] = func(self, *args, **kwargs)
    158         datasets: List[""Dataset""] = list(out.values()) if isinstance(out, dict) else [out]
    159         # re-apply format to the output

~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/datasets/fingerprint.py in wrapper(*args, **kwargs)
    161             # Call actual function
    162 
--> 163             out = func(self, *args, **kwargs)
    164 
    165             # Update fingerprint of in-place transforms + update in-place history of transforms

~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/datasets/arrow_dataset.py in _map_single(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, update_data)
   1526                     if update_data:
   1527                         batch = cast_to_python_objects(batch)
-> 1528                         writer.write_batch(batch)
   1529             if update_data:
   1530                 writer.finalize()  # close_stream=bool(buf_writer is None))  # We only close if we are writing in a file

~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/datasets/arrow_writer.py in write_batch(self, batch_examples, writer_batch_size)
    276             typed_sequence = TypedSequence(batch_examples[col], type=col_type, try_type=col_try_type)
    277             typed_sequence_examples[col] = typed_sequence
--> 278         pa_table = pa.Table.from_pydict(typed_sequence_examples)
    279         self.write_table(pa_table)
    280 

~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/pyarrow/table.pxi in pyarrow.lib.Table.from_pydict()

~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/pyarrow/table.pxi in pyarrow.lib.Table.from_arrays()

~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/pyarrow/table.pxi in pyarrow.lib.Table.validate()

~/.cache/pypoetry/virtualenvs/masters-utTTC0p8-py3.7/lib/python3.7/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()

ArrowInvalid: Column 1 named text expected length 768 but got length 1000
```"
https://github.com/huggingface/datasets/issues/1681,"Dataset ""dane"" missing","['Hi @KennethEnevoldsen ,\r\nI think the issue might be that this dataset was added during the community sprint and has not been released yet. It will be available with the v2 of datasets.\r\nFor now, you should be able to load the datasets after installing the latest (master) version of datasets using pip:\r\npip install git+https://github.com/huggingface/datasets.git@master'
 'The `dane` dataset was added recently, that\'s why it wasn\'t available yet. We did an intermediate release today just before the v2.0.\r\n\r\nTo load it you can just update `datasets`\r\n```\r\npip install --upgrade datasets\r\n```\r\n\r\nand then you can load `dane` with\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(""dane"")\r\n```'
 'Thanks. Solved the problem.']","the `dane` dataset appear to be missing in the latest version (1.1.3).

```python
>>> import datasets
>>> datasets.__version__
'1.1.3'
>>> ""dane"" in datasets.list_datasets()
True
```

As we can see it should be present, but doesn't seem to be findable when using `load_dataset`.

```python
>>> datasets.load_dataset(""dane"")
Traceback (most recent call last):
  File ""/home/kenneth/.Envs/EDP/lib/python3.8/site-packages/datasets/load.py"", line 267, in prepare_module
    local_path = cached_path(file_path, download_config=download_config)
  File ""/home/kenneth/.Envs/EDP/lib/python3.8/site-packages/datasets/utils/file_utils.py"", line 300, in cached_path
    output_path = get_from_cache(
  File ""/home/kenneth/.Envs/EDP/lib/python3.8/site-packages/datasets/utils/file_utils.py"", line 486, in get_from_cache
    raise FileNotFoundError(""Couldn't find file at {}"".format(url))
FileNotFoundError: Couldn't find file at https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/dane/dane.py

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/kenneth/.Envs/EDP/lib/python3.8/site-packages/datasets/load.py"", line 278, in prepare_module
    local_path = cached_path(file_path, download_config=download_config)
  File ""/home/kenneth/.Envs/EDP/lib/python3.8/site-packages/datasets/utils/file_utils.py"", line 300, in cached_path
    output_path = get_from_cache(
  File ""/home/kenneth/.Envs/EDP/lib/python3.8/site-packages/datasets/utils/file_utils.py"", line 486, in get_from_cache
    raise FileNotFoundError(""Couldn't find file at {}"".format(url))
FileNotFoundError: Couldn't find file at https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/dane/dane.py

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/kenneth/.Envs/EDP/lib/python3.8/site-packages/datasets/load.py"", line 588, in load_dataset
    module_path, hash = prepare_module(
  File ""/home/kenneth/.Envs/EDP/lib/python3.8/site-packages/datasets/load.py"", line 280, in prepare_module
    raise FileNotFoundError(
FileNotFoundError: Couldn't find file locally at dane/dane.py, or remotely at https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/dane/dane.py or https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/dane/dane.py
```

This issue might be relevant to @ophelielacroix from the Alexandra Institut whom created the data."
https://github.com/huggingface/datasets/issues/1679,Can't import cc100 dataset,"['cc100 was added recently, that\'s why it wasn\'t available yet.\r\n\r\nTo load it you can just update `datasets`\r\n```\r\npip install --upgrade datasets\r\n```\r\n\r\nand then you can load `cc100` with\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nlang = ""en""\r\ndataset = load_dataset(""cc100"", lang=lang, split=""train"")\r\n```']","There is some issue to import cc100 dataset.

```
from datasets import load_dataset
dataset = load_dataset(""cc100"")
```

FileNotFoundError: Couldn't find file at https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/cc100/cc100.py

During handling of the above exception, another exception occurred:

FileNotFoundError                         Traceback (most recent call last)
FileNotFoundError: Couldn't find file at https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/cc100/cc100.py

During handling of the above exception, another exception occurred:

FileNotFoundError                         Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/datasets/load.py in prepare_module(path, script_version, download_config, download_mode, dataset, force_local_path, **download_kwargs)
    280                 raise FileNotFoundError(
    281                     ""Couldn't find file locally at {}, or remotely at {} or {}"".format(
--> 282                         combined_path, github_file_path, file_path
    283                     )
    284                 )

FileNotFoundError: Couldn't find file locally at cc100/cc100.py, or remotely at https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/cc100/cc100.py or https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/cc100/cc100.py"
https://github.com/huggingface/datasets/issues/1675,Add the 800GB Pile dataset?,"['The pile dataset would be very nice.\r\nBenchmarks show that pile trained models achieve better results than most of actually trained models'
 ""The pile can very easily be added and adapted using this [tfds implementation](https://github.com/EleutherAI/The-Pile/blob/master/the_pile/tfds_pile.py) from the repo. \r\n\r\nHowever, the question is whether you'd be ok with 800GB+ cached in your local disk, since the tfds implementation was designed to offload the storage to Google Cloud Storage.""
 ""With the dataset streaming feature (see #2375) it will be more convenient to play with such big datasets :)\r\nI'm currently adding C4 (see #2511 ) but I can probably start working on this afterwards""
 'Hi folks! Just wanted to follow up on this -- would be really nice to get the Pile on HF Datasets... unclear if it would be easy to also add partitions of the Pile subject to the original 22 datasets used, but that would be nice too!'
 'Hi folks, thanks to some awesome work by @lhoestq and @albertvillanova you can now stream the Pile as follows:\r\n\r\n```python\r\n# Install master branch of `datasets`\r\npip install git+https://github.com/huggingface/datasets.git#egg=datasets[streaming]\r\npip install zstandard\r\n\r\nfrom datasets import load_dataset\r\n\r\ndset = load_dataset(""json"", data_files=""https://the-eye.eu/public/AI/pile/train/00.jsonl.zst"", streaming=True, split=""train"")\r\nnext(iter(dset))\r\n# {\'meta\': {\'pile_set_name\': \'Pile-CC\'},\r\n# \'text\': \'It is done, and submitted. You can play “Survival of the Tastiest” on Android, and on the web ... \'}\r\n```\r\n\r\nNext step is to add the Pile as a ""canonical"" dataset that can be streamed without specifying the file names explicitly :)'
 '> Hi folks! Just wanted to follow up on this -- would be really nice to get the Pile on HF Datasets... unclear if it would be easy to also add partitions of the Pile subject to the original 22 datasets used, but that would be nice too!\r\n\r\nHi @siddk thanks to a tip from @richarddwang it seems we can access some of the partitions that EleutherAI created for the Pile [here](https://the-eye.eu/public/AI/pile_preliminary_components/). What\'s missing are links to the preprocessed versions of pre-existing datasets like DeepMind Mathematics and OpenSubtitles, but worst case we do the processing ourselves and host these components on the Hub.\r\n\r\nMy current idea is that we could provide 23 configs: one for each of the 22 datasets and an `all` config that links to the train / dev / test splits that EleutherAI released [here](https://the-eye.eu/public/AI/pile/), e.g.\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\n# Load a single component\r\nyoutube_subtitles = load_dataset(""the_pile"", ""youtube_subtitles"")\r\n# Load the train / dev / test splits of the whole corpus\r\ndset = load_dataset(""the_pile"", ""all"")\r\n```\r\n\r\nIdeally we\'d like everything to be compatible with the streaming API and there\'s ongoing work by @albertvillanova to make this happen for the various compression algorithms.\r\n\r\ncc @lhoestq '
 'Ah I just saw that @lhoestq is already thinking about the specifying of one or more subsets in [this PR](https://github.com/huggingface/datasets/pull/2817#issuecomment-901874049) :)']","## Adding a Dataset
- **Name:** The Pile
- **Description:** The Pile is a 825 GiB diverse, open source language modelling data set that consists of 22 smaller, high-quality datasets combined together. See [here](https://twitter.com/nabla_theta/status/1345130408170541056?s=20) for the Twitter announcement
- **Paper:** https://pile.eleuther.ai/paper.pdf
- **Data:** https://pile.eleuther.ai/
- **Motivation:** Enables hardcore  (GPT-3 scale!) language modelling

## Remarks
Given the extreme size of this dataset, I'm not sure how feasible this will be to include in `datasets` 🤯  . I'm also unsure how many `datasets` users are pretraining LMs, so the usage of this dataset may not warrant the effort to integrate it.
"
https://github.com/huggingface/datasets/issues/1674,dutch_social can't be loaded,"['exactly the same issue in some other datasets.\r\nDid you find any solution??\r\n'
 ""Hi @koenvandenberge and @alighofrani95!\r\nThe datasets you're experiencing issues with were most likely added recently to the `datasets` library, meaning they have not been released yet. They will be released with the v2 of the library.\r\nMeanwhile, you can still load the datasets using one of the techniques described in this issue: #1641 \r\nLet me know if this helps!""
 'Maybe we should do a small release on Monday in the meantime @lhoestq ?'
 'Yes sure !'
 'I just did the release :)\r\n\r\nTo load it you can just update `datasets`\r\n```\r\npip install --upgrade datasets\r\n```\r\n\r\nand then you can load `dutch_social` with\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(""dutch_social"")\r\n```'
 '@lhoestq could you also shed light on the Hindi Wikipedia Dataset for issue number #1673. Will this also be available in the new release that you committed recently?'
 'The issue is different for this one, let me give more details in the issue'
 'Okay. Could you comment on the #1673 thread? Actually @thomwolf had commented that if i use datasets library from source, it would allow me to download the Hindi Wikipedia Dataset but even the version 1.1.3 gave me the same issue. The details are there in the issue #1673 thread.']","Hi all,

I'm trying to import the `dutch_social` dataset described [here](https://huggingface.co/datasets/dutch_social).

However, the code that should load the data doesn't seem to be working, in particular because the corresponding files can't be found at the provided links.

```
(base) Koens-MacBook-Pro:~ koenvandenberge$ python
Python 3.7.4 (default, Aug 13 2019, 15:17:50) 
[Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from datasets import load_dataset
dataset = load_dataset(
   'dutch_social')
>>> dataset = load_dataset(
...    'dutch_social')
Traceback (most recent call last):
  File ""/Users/koenvandenberge/opt/anaconda3/lib/python3.7/site-packages/datasets/load.py"", line 267, in prepare_module
    local_path = cached_path(file_path, download_config=download_config)
  File ""/Users/koenvandenberge/opt/anaconda3/lib/python3.7/site-packages/datasets/utils/file_utils.py"", line 308, in cached_path
    use_etag=download_config.use_etag,
  File ""/Users/koenvandenberge/opt/anaconda3/lib/python3.7/site-packages/datasets/utils/file_utils.py"", line 486, in get_from_cache
    raise FileNotFoundError(""Couldn't find file at {}"".format(url))
FileNotFoundError: Couldn't find file at https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/dutch_social/dutch_social.py

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/koenvandenberge/opt/anaconda3/lib/python3.7/site-packages/datasets/load.py"", line 278, in prepare_module
    local_path = cached_path(file_path, download_config=download_config)
  File ""/Users/koenvandenberge/opt/anaconda3/lib/python3.7/site-packages/datasets/utils/file_utils.py"", line 308, in cached_path
    use_etag=download_config.use_etag,
  File ""/Users/koenvandenberge/opt/anaconda3/lib/python3.7/site-packages/datasets/utils/file_utils.py"", line 486, in get_from_cache
    raise FileNotFoundError(""Couldn't find file at {}"".format(url))
FileNotFoundError: Couldn't find file at https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/dutch_social/dutch_social.py

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 2, in <module>
  File ""/Users/koenvandenberge/opt/anaconda3/lib/python3.7/site-packages/datasets/load.py"", line 589, in load_dataset
    path, script_version=script_version, download_config=download_config, download_mode=download_mode, dataset=True
  File ""/Users/koenvandenberge/opt/anaconda3/lib/python3.7/site-packages/datasets/load.py"", line 282, in prepare_module
    combined_path, github_file_path, file_path
FileNotFoundError: Couldn't find file locally at dutch_social/dutch_social.py, or remotely at https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/dutch_social/dutch_social.py or https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/dutch_social/dutch_social.py
```"
https://github.com/huggingface/datasets/issues/1673,Unable to Download Hindi Wikipedia Dataset,"[""Currently this dataset is only available when the library is installed from source since it was added after the last release.\r\n\r\nWe pin the dataset version with the library version so that people can have a reproducible dataset and processing when pinning the library.\r\n\r\nWe'll see if we can provide access to newer datasets with a warning that they are newer than your library version, that would help in cases like yours.""
 'So for now, should i try and install the library from source and then try out the same piece of code? Will it work then, considering both the versions will match then?'
 'Yes'
 'Hey, so i tried installing the library from source using the commands : **git clone https://github.com/huggingface/datasets**,  **cd datasets** and then **pip3 install -e .**. But i still am facing the same error that file is not found. Please advise.\r\n\r\nThe Datasets library version now is 1.1.3 by installing from source as compared to the earlier 1.0.3 that i had loaded using pip command but I am still getting same error\r\n\r\n![Error](https://user-images.githubusercontent.com/30871963/103479005-69f3b080-4df0-11eb-83ae-58d7bb56a90e.png)\r\n'
 'Looks like the wikipedia dump for hindi at the date of 05/05/2020 is not available anymore.\r\nYou can try to load a more recent version of wikipedia\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nd = load_dataset(""wikipedia"", language=""hi"", date=""20210101"", split=""train"", beam_runner=""DirectRunner"")\r\n```'
 'Okay, thank you so much']","I used the Dataset Library in Python to load the wikipedia dataset with the Hindi Config 20200501.hi along with something called beam_runner='DirectRunner' and it keeps giving me the error that the file is not found. I have attached the screenshot of the error and the code both. Please help me to understand how to resolve this issue.

![Code](https://user-images.githubusercontent.com/30871963/103437466-1f3a3300-4c4e-11eb-9d54-fc9601abfeec.png)

![Error](https://user-images.githubusercontent.com/30871963/103437407-7ee40e80-4c4d-11eb-8151-a86eb664e6be.png)
"
https://github.com/huggingface/datasets/issues/1672,load_dataset hang on file_lock,"['Can you try to upgrade to a more recent version of datasets?'
 'Thank, upgrading to 1.1.3 resolved the issue.'
 'Having the same issue with `datasets 1.1.3` of `1.5.0` (both tracebacks look the same) and `kilt_wikipedia`, Ubuntu 20.04\r\n\r\n```py\r\nIn [1]: from datasets import load_dataset                                                                                                                                                                          \r\n\r\nIn [2]: wikipedia = load_dataset(\'kilt_wikipedia\')[\'full\']                                                                                                                   \r\nDownloading: 7.37kB [00:00, 2.74MB/s]                                                                                                                                                                              \r\nDownloading: 3.33kB [00:00, 1.44MB/s]                                                                                                                                                                              \r\n^C---------------------------------------------------------------------------\r\nOSError                                   Traceback (most recent call last)\r\n~/anaconda3/envs/transformers2/lib/python3.7/site-packages/datasets/utils/filelock.py in _acquire(self)\r\n    380         try:\r\n--> 381             fcntl.flock(fd, fcntl.LOCK_EX | fcntl.LOCK_NB)\r\n    382         except (IOError, OSError):\r\n\r\nOSError: [Errno 37] No locks available\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nKeyboardInterrupt                         Traceback (most recent call last)\r\n<ipython-input-2-f412d3d46ec9> in <module>\r\n----> 1 wikipedia = load_dataset(\'kilt_wikipedia\')[\'full\']\r\n\r\n~/anaconda3/envs/transformers2/lib/python3.7/site-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, sav\r\ne_infos, script_version, **config_kwargs)\r\n    601         hash=hash,\r\n    602         features=features,\r\n--> 603         **config_kwargs,\r\n    604     )\r\n    605 \r\n\r\n~/anaconda3/envs/transformers2/lib/python3.7/site-packages/datasets/builder.py in __init__(self, *args, **kwargs)\r\n    841     def __init__(self, *args, **kwargs):\r\n    842         self._writer_batch_size = kwargs.pop(""writer_batch_size"", self._writer_batch_size)\r\n--> 843         super(GeneratorBasedBuilder, self).__init__(*args, **kwargs)\r\n    844 \r\n    845     @abc.abstractmethod\r\n\r\n~/anaconda3/envs/transformers2/lib/python3.7/site-packages/datasets/builder.py in __init__(self, cache_dir, name, hash, features, **config_kwargs)\r\n    174             os.makedirs(self._cache_dir_root, exist_ok=True)\r\n    175         lock_path = os.path.join(self._cache_dir_root, self._cache_dir.replace(os.sep, ""_"") + "".lock"")\r\n--> 176         with FileLock(lock_path):\r\n    177             if os.path.exists(self._cache_dir):  # check if data exist\r\n    178                 if len(os.listdir(self._cache_dir)) > 0:\r\n\r\n~/anaconda3/envs/transformers2/lib/python3.7/site-packages/datasets/utils/filelock.py in __enter__(self)\r\n    312 \r\n    313     def __enter__(self):\r\n--> 314         self.acquire()\r\n    315         return self\r\n    316 \r\n\r\n~/anaconda3/envs/transformers2/lib/python3.7/site-packages/datasets/utils/filelock.py in acquire(self, timeout, poll_intervall)\r\n    261                     if not self.is_locked:\r\n    262                         logger().debug(""Attempting to acquire lock %s on %s"", lock_id, lock_filename)\r\n--> 263                         self._acquire()\r\n    264 \r\n    265                 if self.is_locked:\r\n\r\n~/anaconda3/envs/transformers2/lib/python3.7/site-packages/datasets/utils/filelock.py in _acquire(self)\r\n    379 \r\n    380         try:\r\n--> 381             fcntl.flock(fd, fcntl.LOCK_EX | fcntl.LOCK_NB)\r\n    382         except (IOError, OSError):\r\n    383             os.close(fd)\r\n\r\nKeyboardInterrupt: \r\n\r\n```']","I am trying to load the squad dataset. Fails on Windows 10 but succeeds in Colab.
Transformers:  3.3.1
Datasets:  1.0.2
Windows 10 (also tested in WSL)

```
datasets.logging.set_verbosity_debug()
datasets.
train_dataset = load_dataset('squad', split='train')
valid_dataset = load_dataset('squad', split='validation')

train_dataset.features
```

```
https://raw.githubusercontent.com/huggingface/datasets/1.0.2/datasets/squad/squad.py not found in cache or force_download set to True, downloading to C:\Users\simpl\.cache\huggingface\datasets\tmpzj_o_6u7
Downloading:
5.24k/? [00:00<00:00, 134kB/s]
storing https://raw.githubusercontent.com/huggingface/datasets/1.0.2/datasets/squad/squad.py in cache at C:\Users\simpl\.cache\huggingface\datasets\f6877c8d2e01e8fcb60dc101be28b54a7522feac756deb9ac5c39c6d8ebef1ce.85f43de978b9b25921cb78d7a2f2b350c04acdbaedb9ecb5f7101cd7c0950e68.py
creating metadata file for C:\Users\simpl\.cache\huggingface\datasets\f6877c8d2e01e8fcb60dc101be28b54a7522feac756deb9ac5c39c6d8ebef1ce.85f43de978b9b25921cb78d7a2f2b350c04acdbaedb9ecb5f7101cd7c0950e68.py

Checking C:\Users\simpl\.cache\huggingface\datasets\f6877c8d2e01e8fcb60dc101be28b54a7522feac756deb9ac5c39c6d8ebef1ce.85f43de978b9b25921cb78d7a2f2b350c04acdbaedb9ecb5f7101cd7c0950e68.py for additional imports.
Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.2/datasets/squad/squad.py at C:\Users\simpl\.cache\huggingface\modules\datasets_modules\datasets\squad
Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.2/datasets/squad/squad.py at C:\Users\simpl\.cache\huggingface\modules\datasets_modules\datasets\squad\1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41
Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.0.2/datasets/squad/squad.py to C:\Users\simpl\.cache\huggingface\modules\datasets_modules\datasets\squad\1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41\squad.py
Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.0.2/datasets/squad\dataset_infos.json
Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.2/datasets/squad/squad.py at C:\Users\simpl\.cache\huggingface\modules\datasets_modules\datasets\squad\1244d044b266a5e4dbd4174d23cb995eead372fbca31a03edc3f8a132787af41\squad.json
No config specified, defaulting to first: squad/plain_text
```

Interrupting the jupyter kernel we are in a file lock.

In Google Colab the download is ok. In contrast to a local run in colab dataset_infos.json is downloaded
```
https://raw.githubusercontent.com/huggingface/datasets/1.0.2/datasets/squad/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/tmptl9ha_ad

Downloading:
2.19k/? [00:00<00:00, 26.2kB/s]
```"
https://github.com/huggingface/datasets/issues/1671,connection issue ,"['Also, mayjor issue for me is the format issue, even if I go through changing the whole code to use load_from_disk, then if I do \r\n\r\nd = datasets.load_from_disk(""imdb"")\r\nd = d[""train""][:10] => the format of this is no more in datasets format\r\nthis is different from you call load_datasets(""train[10]"")\r\n\r\ncould you tell me how I can make the two datastes the same format @lhoestq \r\n\r\n'
 '> `\r\nrequests.exceptions.ConnectTimeout: HTTPSConnectionPool(host=\'s3.amazonaws.com\', port=443): Max retries exceeded with url: /datasets.huggingface.co/datasets/datasets/glue/glue.py (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7ff6d6c60a20>, \'Connection to s3.amazonaws.com timed out. (connect timeout=10)\'))`\r\n\r\nDo you have an internet connection on the machine ? Is there a proxy that might block requests to aws ?\r\n\r\n> I tried to do read the data, save it to a path and then set HF_HOME, which does not work and this is still not reading from the old set path, could you assist me how to save the datasets in a path, and let dataset library read from this path to avoid connection issue. thanks\r\n\r\nHF_HOME is used to specify the directory for the cache files of this library.\r\nYou can use save_to_disk and load_from_disk without changing the HF_HOME:\r\n```python\r\nimdb = datasets.load_dataset(""imdb"")\r\nimdb.save_to_disk(""/idiap/temp/rkarimi/hf_datasets/imdb"")\r\nimdb = datasets.load_from_disk(""/idiap/temp/rkarimi/hf_datasets/imdb"")\r\n```\r\n\r\n> could you tell me how I can make the two datastes the same format\r\n\r\nIndeed they returns different things:\r\n- `load_dataset` returns a `Dataset` object if the split is specified, or a `DatasetDict` if no split is given. Therefore `load_datasets(""imdb"", split=""train[10]"")` returns a `Dataset` object containing 10 elements.\r\n- doing `d[""train""][:10]` on a DatasetDict ""d"" gets the train split `d[""train""]` as a `Dataset` object and then gets the first 10 elements as a dictionary']","Hi
I am getting this connection issue, resulting in large failure on cloud, @lhoestq  I appreciate your help on this.

If I want to keep the codes the same, so not using save_to_disk, load_from_disk, but save the datastes in the way load_dataset reads from and copy the files in the same folder the datasets library reads from, could you assist me how this can be done, thanks

I tried to do read the data, save it to a path and then set HF_HOME, which does not work and this is still not reading from the old set path, could you assist me how to save the datasets in a path, and let dataset library read from this path to avoid connection issue. thanks

```
imdb = datasets.load_dataset(""imdb"")
imdb.save_to_disk(""/idiap/temp/rkarimi/hf_datasets/imdb"")
>>> os.environ[""HF_HOME""]=""/idiap/temp/rkarimi/hf_datasets/""
>>> imdb = datasets.load_dataset(""imdb"")
Reusing dataset imdb (/idiap/temp/rkarimi/cache_home_2/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)
```

I tried afterwards to set HF_HOME in bash, this makes it read from it, but it cannot let dataset library load from the saved path and still  downloading data. could you tell me how to fix this issue @lhoestq  thanks 

Also this is on cloud, so I save them in a path, copy it to ""another machine"" to load the data

### Error stack

```
Traceback (most recent call last):
  File ""./finetune_t5_trainer.py"", line 344, in <module>
    main()
  File ""./finetune_t5_trainer.py"", line 232, in main
    for task in data_args.eval_tasks} if training_args.do_test else None
  File ""./finetune_t5_trainer.py"", line 232, in <dictcomp>
    for task in data_args.eval_tasks} if training_args.do_test else None
  File ""/workdir/seq2seq/data/tasks.py"", line 136, in get_dataset
    split = self.get_sampled_split(split, n_obs)
  File ""/workdir/seq2seq/data/tasks.py"", line 64, in get_sampled_split
    dataset = self.load_dataset(split)
  File ""/workdir/seq2seq/data/tasks.py"", line 454, in load_dataset
    split=split, script_version=""master"")
  File ""/usr/local/lib/python3.6/dist-packages/datasets/load.py"", line 589, in load_dataset
    path, script_version=script_version, download_config=download_config, download_mode=download_mode, dataset=True
  File ""/usr/local/lib/python3.6/dist-packages/datasets/load.py"", line 263, in prepare_module
    head_hf_s3(path, filename=name, dataset=dataset)
  File ""/usr/local/lib/python3.6/dist-packages/datasets/utils/file_utils.py"", line 200, in head_hf_s3
    return http_head(hf_bucket_url(identifier=identifier, filename=filename, use_cdn=use_cdn, dataset=dataset))
  File ""/usr/local/lib/python3.6/dist-packages/datasets/utils/file_utils.py"", line 403, in http_head
    url, proxies=proxies, headers=headers, cookies=cookies, allow_redirects=allow_redirects, timeout=timeout
  File ""/usr/local/lib/python3.6/dist-packages/requests/api.py"", line 104, in head
    return request('head', url, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/requests/api.py"", line 61, in request
    return session.request(method=method, url=url, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/requests/sessions.py"", line 542, in request
    resp = self.send(prep, **send_kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/requests/sessions.py"", line 655, in send
    r = adapter.send(request, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/requests/adapters.py"", line 504, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='s3.amazonaws.com', port=443): Max retries exceeded with url: /datasets.huggingface.co/datasets/datasets/glue/glue.py (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7ff6d6c60a20>, 'Connection to s3.amazonaws.com timed out. (connect timeout=10)'))
```
"
https://github.com/huggingface/datasets/issues/1670,wiki_dpr pre-processing performance,"['Hi ! And thanks for the tips :) \r\n\r\nIndeed currently `wiki_dpr` takes some time to be processed.\r\nMultiprocessing for dataset generation is definitely going to speed up things.\r\n\r\nRegarding the index note that for the default configurations, the index is downloaded instead of being built, which avoid spending time on constructing the index. However in other cases it would be awesome to make the construction faster.\r\n\r\nAny contribution that can help things faster are welcome. In particular in you have some code that can build a wiki_dpr IVF PQ index in a sharded GPU setup and would like to share it, we can add it to an `examples` folder. In particular since faiss is becoming the library of reference for dataset indexing for tasks like Open Domain Question Answering.\r\n\r\n'
 ""I'd be happy to contribute something when I get the time, probably adding multiprocessing and / or cython support to wiki_dpr. I've written cythonized apache beam code before as well.\r\n\r\nFor sharded index building, I used the FAISS example code for indexing 1 billion vectors as a start. I'm sure you're aware that the documentation isn't great, but the source code is fairly easy to follow.""
 'Nice thanks ! That would be awesome to make its construction faster :) ']","I've been working with wiki_dpr and noticed that the dataset processing is seriously impaired in performance [1]. It takes about 12h to process the entire dataset. Most of this time is simply loading and processing the data, but the actual indexing is also quite slow (3h).

I won't repeat the concerns around multiprocessing as they are addressed in other issues (#786), but this is the first obvious thing to do. Using cython to speed up the text manipulation may be also help. Loading and processing a dataset of this size in under 15 minutes does not seem unreasonable on a modern multi-core machine. I have hit such targets myself on similar tasks. Would love to see this improve.

The other issue is that it takes 3h to construct the FAISS index. If only we could use GPUs with HNSW, but we can't. My sharded GPU indexing code can build an IVF + PQ index in 10 minutes on 20 million vectors. Still, 3h seems slow even for the CPU.

It looks like HF is adding only 1000 vectors at a time by default [2], whereas the faiss benchmarks adds 1 million vectors at a time (effectively) [3]. It's possible the runtime could be reduced with a larger batch. Also, it looks like project dependencies ultimately use OpenBLAS, but this is known to have issues when combined with OpenMP, which HNSW does [3]. A workaround is to set the environment variable `OMP_WAIT_POLICY=PASSIVE` via `os.environ` or similar.

References:
[1] https://github.com/huggingface/datasets/blob/master/datasets/wiki_dpr/wiki_dpr.py
[2] https://github.com/huggingface/datasets/blob/master/src/datasets/search.py
[3] https://github.com/facebookresearch/faiss/blob/master/benchs/bench_hnsw.py
[4] https://github.com/facebookresearch/faiss/issues/422"
https://github.com/huggingface/datasets/issues/1669,wiki_dpr dataset pre-processesing performance,"['Sorry, double posted.']","I've been working with wiki_dpr and noticed that the dataset processing is seriously impaired in performance [1]. It takes about 12h to process the entire dataset. Most of this time is simply loading and processing the data, but the actual indexing is also quite slow (3h).

I won't repeat the concerns around multiprocessing as they are addressed in other issues (#786), but this is the first obvious thing to do. Using cython to speed up the text manipulation may be also help. Loading and processing a dataset of this size in under 15 minutes does not seem unreasonable on a modern multi-core machine. I have hit such targets myself on similar tasks. Would love to see this improve.

The other issue is that it takes 3h to construct the FAISS index. If only we could use GPUs with HNSW, but we can't. My sharded GPU indexing code can build an IVF + PQ index in 10 minutes on 20 million vectors. Still, 3h seems slow even for the CPU.

It looks like HF is adding only 1000 vectors at a time by default [2], whereas the faiss benchmarks adds 1 million vectors at a time (effectively) [3]. It's possible the runtime could be reduced with a larger batch. Also, it looks like project dependencies ultimately use OpenBLAS, but this is known to have issues when combined with OpenMP, which HNSW does [3]. A workaround is to set the environment variable `OMP_WAIT_POLICY=PASSIVE` via `os.environ` or similar.

References:
[1] https://github.com/huggingface/datasets/blob/master/datasets/wiki_dpr/wiki_dpr.py
[2] https://github.com/huggingface/datasets/blob/master/src/datasets/search.py
[3] https://github.com/facebookresearch/faiss/blob/master/benchs/bench_hnsw.py
[4] https://github.com/facebookresearch/faiss/issues/422"
https://github.com/huggingface/datasets/issues/1662,Arrow file is too large when saving vector data,"[""Hi !\r\nThe arrow file size is due to the embeddings. Indeed if they're stored as float32 then the total size of the embeddings is\r\n\r\n20 000 000 vectors * 768 dimensions * 4 bytes per dimension ~= 60GB\r\n\r\nIf you want to reduce the size you can consider using quantization for example, or maybe using dimension reduction techniques.\r\n""
 'Thanks for your reply @lhoestq.\r\nI want to save original embedding for these sentences for subsequent calculations. So does arrow have a way to save in a compressed format to reduce the size of the file?'
 ""Arrow doesn't have compression since it is designed to have no serialization overhead""
 'I see. Thank you.']",I computed the sentence embedding of each sentence of bookcorpus data using bert base and saved them to disk. I used 20M sentences and the obtained arrow file is about 59GB while the original text file is only about 1.3GB. Are there any ways to reduce the size of the arrow file?
https://github.com/huggingface/datasets/issues/1647,NarrativeQA fails to load with `load_dataset`,"['Hi @eric-mitchell,\r\nI think the issue might be that this dataset was added during the community sprint and has not been released yet. It will be available with the v2 of `datasets`.\r\nFor now, you should be able to load the datasets after installing the latest (master) version of `datasets` using pip:\r\n`pip install git+https://github.com/huggingface/datasets.git@master`'
 '@bhavitvyamalik Great, thanks for this! Confirmed that the problem is resolved on master at [cbbda53](https://github.com/huggingface/datasets/commit/cbbda53ac1520b01f0f67ed6017003936c41ec59).'
 'Update: HuggingFace did an intermediate release yesterday just before the v2.0.\r\n\r\nTo load it you can just update `datasets`\r\n\r\n`pip install --upgrade datasets`']","When loading the NarrativeQA dataset with `load_dataset('narrativeqa')` as given in the documentation [here](https://huggingface.co/datasets/narrativeqa), I receive a cascade of exceptions, ending with

    FileNotFoundError: Couldn't find file locally at narrativeqa/narrativeqa.py, or remotely at 
        https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/narrativeqa/narrativeqa.py or 
        https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/narrativeqa/narrativeqa.py

Workaround: manually copy the `narrativeqa.py` builder into my local directory with 

    curl https://raw.githubusercontent.com/huggingface/datasets/master/datasets/narrativeqa/narrativeqa.py -o narrativeqa.py

and load the dataset as `load_dataset('narrativeqa.py')` everything works fine. I'm on datasets v1.1.3 using Python 3.6.10."
https://github.com/huggingface/datasets/issues/1644,HoVeR dataset fails to load,"['Hover was added recently, that\'s why it wasn\'t available yet.\r\n\r\nTo load it you can just update `datasets`\r\n```\r\npip install --upgrade datasets\r\n```\r\n\r\nand then you can load `hover` with\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(""hover"")\r\n```']","Hi! I'm getting an error when trying to load **HoVeR** dataset. Another one (**SQuAD**) does work for me. I'm using the latest (1.1.3) version of the library.

Steps to reproduce the error:

```python
>>> from datasets import load_dataset
>>> dataset = load_dataset(""hover"")
Traceback (most recent call last):
  File ""/Users/urikz/anaconda/envs/mentionmemory/lib/python3.7/site-packages/datasets/load.py"", line 267, in prepare_module
    local_path = cached_path(file_path, download_config=download_config)
  File ""/Users/urikz/anaconda/envs/mentionmemory/lib/python3.7/site-packages/datasets/utils/file_utils.py"", line 308, in cached_path
    use_etag=download_config.use_etag,
  File ""/Users/urikz/anaconda/envs/mentionmemory/lib/python3.7/site-packages/datasets/utils/file_utils.py"", line 486, in get_from_cache
    raise FileNotFoundError(""Couldn't find file at {}"".format(url))
FileNotFoundError: Couldn't find file at https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/hover/hover.py

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/urikz/anaconda/envs/mentionmemory/lib/python3.7/site-packages/datasets/load.py"", line 278, in prepare_module
    local_path = cached_path(file_path, download_config=download_config)
  File ""/Users/urikz/anaconda/envs/mentionmemory/lib/python3.7/site-packages/datasets/utils/file_utils.py"", line 308, in cached_path
    use_etag=download_config.use_etag,
  File ""/Users/urikz/anaconda/envs/mentionmemory/lib/python3.7/site-packages/datasets/utils/file_utils.py"", line 486, in get_from_cache
    raise FileNotFoundError(""Couldn't find file at {}"".format(url))
FileNotFoundError: Couldn't find file at https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/hover/hover.py

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/urikz/anaconda/envs/mentionmemory/lib/python3.7/site-packages/datasets/load.py"", line 589, in load_dataset
    path, script_version=script_version, download_config=download_config, download_mode=download_mode, dataset=True
  File ""/Users/urikz/anaconda/envs/mentionmemory/lib/python3.7/site-packages/datasets/load.py"", line 282, in prepare_module
    combined_path, github_file_path, file_path
FileNotFoundError: Couldn't find file locally at hover/hover.py, or remotely at https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/hover/hover.py or https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/hover/hover.py
```"
https://github.com/huggingface/datasets/issues/1643,Dataset social_bias_frames 404,"['I see, master is already fixed in https://github.com/huggingface/datasets/commit/9e058f098a0919efd03a136b9b9c3dec5076f626']","```
>>> from datasets import load_dataset
>>> dataset = load_dataset(""social_bias_frames"")
...
Downloading and preparing dataset social_bias_frames/default
...
~/.pyenv/versions/3.7.6/lib/python3.7/site-packages/datasets/utils/file_utils.py in get_from_cache(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag)
    484             )
    485         elif response is not None and response.status_code == 404:
--> 486             raise FileNotFoundError(""Couldn't find file at {}"".format(url))
    487         raise ConnectionError(""Couldn't reach {}"".format(url))
    488 

FileNotFoundError: Couldn't find file at https://homes.cs.washington.edu/~msap/social-bias-frames/SocialBiasFrames_v2.tgz
```
[Here](https://homes.cs.washington.edu/~msap/social-bias-frames/) we find button `Download data` with the correct URL for the data: https://homes.cs.washington.edu/~msap/social-bias-frames/SBIC.v2.tgz"
https://github.com/huggingface/datasets/issues/1641,muchocine dataset cannot be dowloaded,"['I have encountered the same error with `v1.0.1` and `v1.0.2` on both Windows and Linux environments. However, cloning the repo and using the path to the dataset\'s root directory worked for me. Even after having the dataset cached - passing the path is the only way (for now) to load the dataset.\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(""squad"")                      # Works\r\ndataset = load_dataset(""code_search_net"", ""python"")  # Error\r\ndataset = load_dataset(""covid_qa_deepset"")           # Error\r\n\r\npath = ""/huggingface/datasets/datasets/{}/""\r\ndataset = load_dataset(path.format(""code_search_net""), ""python"")  # Works\r\ndataset = load_dataset(path.format(""covid_qa_deepset""))           # Works\r\n```\r\n\r\n'
 'Hi @mrm8488 and @amoux!\r\n The datasets you are trying to load have been added to the library during the community sprint for v2 last month. They will be available with the v2 release!\r\nFor now, there are still a couple of solutions to load the datasets:\r\n1. As suggested by @amoux, you can clone the git repo and pass the local path to the script\r\n2. You can also install the latest (master) version of `datasets` using pip: `pip install git+https://github.com/huggingface/datasets.git@master`'
 ""If you don't want to clone entire `datasets` repo, just download the `muchocine` directory and pass the local path to the directory. Cheers!""
 'Muchocine was added recently, that\'s why it wasn\'t available yet.\r\n\r\nTo load it you can just update `datasets`\r\n```\r\npip install --upgrade datasets\r\n```\r\n\r\nand then you can load `muchocine` with\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(""muchocine"", split=""train"")\r\n```'
 'Thanks @lhoestq ']","```python
---------------------------------------------------------------------------
FileNotFoundError                         Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/datasets/load.py in prepare_module(path, script_version, download_config, download_mode, dataset, force_local_path, **download_kwargs)
    267         try:
--> 268             local_path = cached_path(file_path, download_config=download_config)
    269         except FileNotFoundError:

7 frames
FileNotFoundError: Couldn't find file at https://raw.githubusercontent.com/huggingface/datasets/1.0.2/datasets/muchocine/muchocine.py

During handling of the above exception, another exception occurred:

FileNotFoundError                         Traceback (most recent call last)
FileNotFoundError: Couldn't find file at https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/muchocine/muchocine.py

During handling of the above exception, another exception occurred:

FileNotFoundError                         Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/datasets/load.py in prepare_module(path, script_version, download_config, download_mode, dataset, force_local_path, **download_kwargs)
    281                 raise FileNotFoundError(
    282                     ""Couldn't find file locally at {}, or remotely at {} or {}"".format(
--> 283                         combined_path, github_file_path, file_path
    284                     )
    285                 )

FileNotFoundError: Couldn't find file locally at muchocine/muchocine.py, or remotely at https://raw.githubusercontent.com/huggingface/datasets/1.0.2/datasets/muchocine/muchocine.py or https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/muchocine/muchocine.py
```"
https://github.com/huggingface/datasets/issues/1639,bug with sst2 in glue ,"['Maybe you can use nltk\'s treebank detokenizer ?\r\n```python\r\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer\r\n\r\nTreebankWordDetokenizer().detokenize(""it \'s a charming and often affecting journey . "".split())\r\n# ""it\'s a charming and often affecting journey.""\r\n```'
 'I am looking for alternative file URL here instead of adding extra processing code: https://github.com/huggingface/datasets/blob/171f2bba9dd8b92006b13cf076a5bf31d67d3e69/datasets/glue/glue.py#L174'
 ""I don't know if there exists a detokenized version somewhere. Even the version on kaggle is tokenized""]","Hi
I am getting very low accuracy on SST2 I investigate this and observe that for this dataset sentences are tokenized, while this is correct for the other datasets in GLUE, please see below.
Is there any alternatives I could get untokenized sentences? I am unfortunately under time pressure to report some results on this dataset. thank you for your help. @lhoestq 
 
```
>>> a =  datasets.load_dataset('glue', 'sst2', split=""validation"", script_version=""master"")
Reusing dataset glue (/julia/datasets/glue/sst2/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)
>>> a[:10]
{'idx': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 'label': [1, 0, 1, 1, 0, 1, 0, 0, 1, 0], 'sentence': [""it 's a charming and often affecting journey . "", 'unflinchingly bleak and desperate ', 'allows us to hope that nolan is poised to embark a major career as a commercial yet inventive filmmaker . ', ""the acting , costumes , music , cinematography and sound are all astounding given the production 's austere locales . "", ""it 's slow -- very , very slow . "", 'although laced with humor and a few fanciful touches , the film is a refreshingly serious look at young women . ', 'a sometimes tedious film . ', ""or doing last year 's taxes with your ex-wife . "", ""you do n't have to know about music to appreciate the film 's easygoing blend of comedy and romance . "", ""in exactly 89 minutes , most of which passed as slowly as if i 'd been sitting naked on an igloo , formula 51 sank from quirky to jerky to utter turkey . ""]}

```"
https://github.com/huggingface/datasets/issues/1636,winogrande cannot be dowloaded ,"['I have same issue for other datasets (`myanmar_news` in my case).\r\n\r\nA version of `datasets` runs correctly on my local machine (**without GPU**) which looking for the dataset at \r\n```\r\nhttps://raw.githubusercontent.com/huggingface/datasets/master/datasets/myanmar_news/myanmar_news.py\r\n```\r\n\r\nMeanwhile, other version runs on Colab (**with GPU**) failed to download the dataset. It try to find the dataset at `1.1.3` instead of `master` . If I disable GPU on my Colab, the code can load the dataset without any problem.\r\n\r\nMaybe there is some version missmatch with the GPU and CPU version of code for these datasets?'
 ""It looks like they're two different issues\r\n\r\n----------\r\n\r\nFirst for `myanmar_news`: \r\n\r\nIt must come from the way you installed `datasets`.\r\nIf you install `datasets` from source, then the `myanmar_news` script will be loaded from `master`.\r\nHowever if you install from `pip` it will get it using the version of the lib (here `1.1.3`) and `myanmar_news` is not available in `1.1.3`.\r\n\r\nThe difference between your GPU and CPU executions must be the environment, one seems to have installed `datasets` from source and not the other.\r\n\r\n----------\r\n\r\nThen for `winogrande`:\r\n\r\nThe errors says that the url https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/winogrande/winogrande.py is not reachable.\r\nHowever it works fine on my side.\r\n\r\nDoes your machine have an internet connection ? Are connections to github blocked by some sort of proxy ?\r\nCan you also try again in case github had issues when you tried the first time ?\r\n""]","Hi,
I am getting this error when trying to run the codes on the cloud.  Thank you for any suggestion and help on this @lhoestq 

```
 File ""./finetune_trainer.py"", line 318, in <module>
    main()
  File ""./finetune_trainer.py"", line 148, in main
    for task in data_args.tasks]
  File ""./finetune_trainer.py"", line 148, in <listcomp>
    for task in data_args.tasks]
  File ""/workdir/seq2seq/data/tasks.py"", line 65, in get_dataset
    dataset = self.load_dataset(split=split)
  File ""/workdir/seq2seq/data/tasks.py"", line 466, in load_dataset
    return datasets.load_dataset('winogrande', 'winogrande_l', split=split)
  File ""/usr/local/lib/python3.6/dist-packages/datasets/load.py"", line 589, in load_dataset
    path, script_version=script_version, download_config=download_config, download_mode=download_mode, dataset=True
  File ""/usr/local/lib/python3.6/dist-packages/datasets/load.py"", line 267, in prepare_module
    local_path = cached_path(file_path, download_config=download_config)
  File ""/usr/local/lib/python3.6/dist-packages/datasets/utils/file_utils.py"", line 308, in cached_path
    use_etag=download_config.use_etag,
  File ""/usr/local/lib/python3.6/dist-packages/datasets/utils/file_utils.py"", line 487, in get_from_cache
    raise ConnectionError(""Couldn't reach {}"".format(url))
ConnectionError: Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/winogrande/winogrande.py
yo/0 I1224 14:17:46.419031 31226 main shadow.py:122 > Traceback (most recent call last):
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/usr/local/lib/python3.6/dist-packages/torch/distributed/launch.py"", line 260, in <module>
    main()
  File ""/usr/local/lib/python3.6/dist-packages/torch/distributed/launch.py"", line 256, in main
    cmd=cmd)
```"
https://github.com/huggingface/datasets/issues/1634,Inspecting datasets per category,"[""That's interesting, can you tell me what you think would be useful to access to inspect a dataset?\r\n\r\nYou can filter them in the hub with the search by the way: https://huggingface.co/datasets have you seen it?""
 'Hi @thomwolf \r\nthank you, I was not aware of this, I was looking into the data viewer linked into readme page. \r\n\r\nThis is exactly what I was looking for, but  this does not work currently, please see the attached \r\nI am selecting to see all nli datasets in english and it retrieves none. thanks\r\n\r\n![5tarDHn9CP6ngeM](https://user-images.githubusercontent.com/53898419/103107612-1509aa80-4638-11eb-85b5-0c995a189969.png)\r\n\r\n\r\n\r\n'
 'I see 4 results for NLI in English but indeed some are not tagged yet and missing (GLUE), we will focus on that in January (cc @yjernite): https://huggingface.co/datasets?filter=task_ids:natural-language-inference,languages:en']","Hi
Is there a way I could get all NLI datasets/all QA datasets to get some understanding of available datasets per category? this is hard for me to inspect the datasets one by one in the webpage, thanks for the suggestions @lhoestq "
https://github.com/huggingface/datasets/issues/1633,social_i_qa wrong format of labels,"['@lhoestq, should I raise a PR for this? Just a minor change while reading labels text file'
 'Sure feel free to open a PR thanks !']","Hi,
there is extra ""\n"" in labels of social_i_qa datasets, no big deal, but I was wondering if you could remove it to make it consistent.
so label is 'label': '1\n', not '1'
thanks

```
>>> import datasets 
>>> from datasets import load_dataset
>>> dataset = load_dataset(
...    'social_i_qa')
cahce dir  /julia/cache/datasets
Downloading: 4.72kB [00:00, 3.52MB/s]                                                                                                  
cahce dir /julia/cache/datasets
Downloading: 2.19kB [00:00, 1.81MB/s]                                                                                                  
Using custom data configuration default
Reusing dataset social_i_qa (/julia/datasets/social_i_qa/default/0.1.0/4a4190cc2d2482d43416c2167c0c5dccdd769d4482e84893614bd069e5c3ba06)
>>> dataset['train'][0]
{'answerA': 'like attending', 'answerB': 'like staying home', 'answerC': 'a good friend to have', 'context': 'Cameron decided to have a barbecue and gathered her friends together.', 'label': '1\n', 'question': 'How would Others feel as a result?'}

```

"
https://github.com/huggingface/datasets/issues/1630,Adding UKP Argument Aspect Similarity Corpus,"['Adding a link to the guide on adding a dataset if someone want to give it a try: https://github.com/huggingface/datasets#add-a-new-dataset-to-the-hub\r\n\r\nwe should add this guide to the issue template @lhoestq '
 'thanks @thomwolf , this is added now. The template is correct, sorry my mistake not to include it. ']","Hi, this would be great to have this dataset included.

## Adding a Dataset
- **Name:** UKP Argument Aspect Similarity Corpus
- **Description:** The UKP Argument Aspect Similarity Corpus (UKP ASPECT) includes 3,595 sentence pairs over 28 controversial topics. Each sentence pair was annotated via crowdsourcing as either “high similarity”, “some similarity”, “no similarity” or “not related” with respect to the topic.
- **Paper:** https://www.aclweb.org/anthology/P19-1054/
- **Data:** https://tudatalib.ulb.tu-darmstadt.de/handle/tudatalib/1998
- **Motivation:** this is one of the datasets currently used frequently in recent adapter papers like https://arxiv.org/pdf/2005.00247.pdf 

Instructions to add a new dataset can be found [here](https://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md).

Thank you"
https://github.com/huggingface/datasets/issues/1627,`Dataset.map` disable progress bar,['Progress bar can be disabled like this:\r\n```python\r\nfrom datasets.utils.logging import set_verbosity_error\r\nset_verbosity_error()\r\n```\r\n\r\nThere is this line in `Dataset.map`:\r\n```python\r\nnot_verbose = bool(logger.getEffectiveLevel() > WARNING)\r\n```\r\n\r\nSo any logging level higher than `WARNING` turns off the progress bar.'],I can't find anything to turn off the `tqdm` progress bars while running a preprocessing function using `Dataset.map`. I want to do akin to `disable_tqdm=True` in the case of `transformers`. Is there something like that?
https://github.com/huggingface/datasets/issues/1624,Cannot download ade_corpus_v2,"['Hi @him1411, the dataset you are trying to load has been added during the community sprint and has not been released yet. It will be available with the v2 of `datasets`.\r\nFor now, you should be able to load the datasets after installing the latest (master) version of `datasets` using pip:\r\n`pip install git+https://github.com/huggingface/datasets.git@master`'
 '`ade_corpus_v2` was added recently, that\'s why it wasn\'t available yet.\r\n\r\nTo load it you can just update `datasets`\r\n```\r\npip install --upgrade datasets\r\n```\r\n\r\nand then you can load `ade_corpus_v2` with\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(""ade_corpus_v2"", ""Ade_corpos_v2_drug_ade_relation"")\r\n```\r\n\r\n(looks like there is a typo in the configuration name, we\'ll fix it for the v2.0 release of `datasets` soon)']","I tried this to get the dataset following this url : https://huggingface.co/datasets/ade_corpus_v2

but received this error : 

`Traceback (most recent call last):
  File ""/opt/anaconda3/lib/python3.7/site-packages/datasets/load.py"", line 267, in prepare_module
    local_path = cached_path(file_path, download_config=download_config)
  File ""/opt/anaconda3/lib/python3.7/site-packages/datasets/utils/file_utils.py"", line 308, in cached_path
    use_etag=download_config.use_etag,
  File ""/opt/anaconda3/lib/python3.7/site-packages/datasets/utils/file_utils.py"", line 486, in get_from_cache
    raise FileNotFoundError(""Couldn't find file at {}"".format(url))
FileNotFoundError: Couldn't find file at https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/ade_corpus_v2/ade_corpus_v2.py

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/anaconda3/lib/python3.7/site-packages/datasets/load.py"", line 278, in prepare_module
    local_path = cached_path(file_path, download_config=download_config)
  File ""/opt/anaconda3/lib/python3.7/site-packages/datasets/utils/file_utils.py"", line 308, in cached_path
    use_etag=download_config.use_etag,
  File ""/opt/anaconda3/lib/python3.7/site-packages/datasets/utils/file_utils.py"", line 486, in get_from_cache
    raise FileNotFoundError(""Couldn't find file at {}"".format(url))
FileNotFoundError: Couldn't find file at https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/ade_corpus_v2/ade_corpus_v2.py

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/opt/anaconda3/lib/python3.7/site-packages/datasets/load.py"", line 589, in load_dataset
    path, script_version=script_version, download_config=download_config, download_mode=download_mode, dataset=True
  File ""/opt/anaconda3/lib/python3.7/site-packages/datasets/load.py"", line 282, in prepare_module
    combined_path, github_file_path, file_path
FileNotFoundError: Couldn't find file locally at ade_corpus_v2/ade_corpus_v2.py, or remotely at https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/ade_corpus_v2/ade_corpus_v2.py or https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/ade_corpus_v2/ade_corpus_v2.py`


"
https://github.com/huggingface/datasets/issues/1622,Can't call shape on the output of select(),"[""Indeed that's a typo, do you want to open a PR to fix it?""
 'Yes, created a PR']","I get the error `TypeError: tuple expected at most 1 argument, got 2` when calling `shape` on the output of `select()`.
It's line 531 in shape in arrow_dataset.py that causes the problem:
``return tuple(self._indices.num_rows, self._data.num_columns)``
This makes sense, since `tuple(num1, num2)` is not a valid call.
 
Full code to reproduce:

```python
dataset = load_dataset(""cnn_dailymail"", ""3.0.0"")
train_set = dataset[""train""]
t = train_set.select(range(10))
print(t.shape)"
https://github.com/huggingface/datasets/issues/1618,Can't filter language:EN on https://huggingface.co/datasets,"[""cc'ing @mapmeld ""
 'Full language list is now deployed to https://huggingface.co/datasets ! Recommend close'
 'Cool @mapmeld ! My 2 cents (for a next iteration), it would be cool to have a small search widget in the filter dropdown as you have a ton of languages now here! Closing this in the meantime.']","When visiting https://huggingface.co/datasets, I don't see an obvious way to filter only English datasets. This is unexpected for me, am I missing something? I'd expect English to be selectable in the language widget. This problem reproduced on Mozilla Firefox and MS Edge:

![screenshot](https://user-images.githubusercontent.com/4547987/102792244-892e1f00-43a8-11eb-9e89-4826ca201a87.png)
"
https://github.com/huggingface/datasets/issues/1615,Bug: Can't download TriviaQA with `load_dataset` - custom `cache_dir`,"[""Hi @SapirWeissbuch,\r\nWhen you are saying it freezes, at that time it is unzipping the file from the zip file it downloaded. Since it's a very heavy file it'll take some time. It was taking ~11GB after unzipping when it started reading examples for me. Hope that helps!\r\n![Screenshot 2020-12-21 at 23 40 52](https://user-images.githubusercontent.com/19718818/102808355-3b380c00-43e6-11eb-81ab-c31019ae6322.png)\r\n""
 'Hi @bhavitvyamalik \r\nThanks for the reply!\r\nActually I let it run for 30 minutes before I killed the process. In this time, 30GB were extracted (much more than 11GB), I checked the size of the destination directory.\r\n\r\nWhat version of Datasets are you using?\r\n'
 'I\'m using datasets version: 1.1.3. I think you should drop `cache_dir` and use only\r\n`dataset = datasets.load_dataset(""trivia_qa"", ""rc"")`\r\n\r\nTried that on colab and it\'s working there too\r\n![image](https://user-images.githubusercontent.com/19718818/102814269-4db74300-43f0-11eb-8f26-ecfcf4632002.png)\r\n'
 'Train, Validation, and Test splits contain 138384, 18669, and 17210 samples respectively. It takes some time to read the samples. Even in your colab notebook it was reading the samples before you killed the process. Let me know if it works now!'
 ""Hi, it works on colab but it still doesn't work on my computer, same problem as before - overly large and long extraction process.\r\nI have to use a custom 'cache_dir' because I don't have any space left in my home directory where it is defaulted, maybe this could be the issue?""
 'I tried running this again - More details of the problem:\r\nCode:\r\n```\r\ndatasets.load_dataset(""trivia_qa"", ""rc"", cache_dir=""/path/to/cache"")\r\n```\r\n\r\nThe output:\r\n```\r\nDownloading and preparing dataset trivia_qa/rc (download: 2.48 GiB, generated: 14.92 GiB, post-processed: Unknown size, total: 17.40 GiB) to path/to/cache/trivia_qa/rc/1.1.0/e734e28133f4d9a353af322aa52b9f266f6f27cbf2f072690a1694e577546b0d...                                                                          \r\nDownloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2.67G/2.67G [03:38<00:00, 12.2MB/s]\r\n\r\n```\r\nThe process continues (no progress bar is visible).\r\nI tried `du -sh .` in `path/to/cache`, and the size keeps increasing, reached 35G before I killed the process.\r\n\r\nGoogle Colab with custom `cache_dir` has same issue.\r\nhttps://colab.research.google.com/drive/1nn1Lw02GhfGFylzbS2j6yksGjPo7kkN-?usp=sharing#scrollTo=2G2O0AeNIXan'
 '1) You can clear the huggingface folder in your `.cache` directory to use default directory for datasets. Speed of extraction and loading of samples depends a lot on your machine\'s configurations too.\r\n\r\n2) I tried on colab `dataset = datasets.load_dataset(""trivia_qa"", ""rc"", cache_dir = ""./datasets"")`. After memory usage reached around 42GB (starting from 32GB used already), the dataset was loaded in the memory. Even Your colab notebook shows \r\n![image](https://user-images.githubusercontent.com/19718818/102852229-c7c4e780-4443-11eb-91d6-bf21024358a3.png)\r\nwhich means it\'s loaded now.'
 'Facing the same issue.\r\nI am able to download datasets without `cache_dir`, however, when I specify the `cache_dir`, the process hangs indefinitely after partial download. \r\nTried for `data = load_dataset(""cnn_dailymail"", ""3.0.0"")`'
 'Hi @ashutoshml,\r\nI tried this and it worked for me:\r\n`data = load_dataset(""cnn_dailymail"", ""3.0.0"", cache_dir=""./dummy"")`\r\n\r\nI\'m using datasets==1.8.0. It took around 3-4 mins for dataset to unpack and start loading examples.'
 'Ok. I waited for 20-30 mins, and it still is stuck.\r\nI am using datasets==1.8.0.\r\n\r\nIs there anyway to check what is happening? like a` --verbose` flag?\r\n\r\n![Screenshot 2021-06-25 at 6 37 43 PM](https://user-images.githubusercontent.com/2375919/123429653-cdfb7280-d5e4-11eb-9fa7-ff295800cc86.png)\r\n']","Hello,
I'm having issue downloading TriviaQA dataset with `load_dataset`.

## Environment info
- `datasets` version: 1.1.3
- Platform: Linux-4.19.129-aufs-1-x86_64-with-debian-10.1
- Python version: 3.7.3

## The code I'm running:
```python
import datasets
dataset = datasets.load_dataset(""trivia_qa"", ""rc"", cache_dir = ""./datasets"")
```

## The output:
1. Download begins:
```
Downloading and preparing dataset trivia_qa/rc (download: 2.48 GiB, generated: 14.92 GiB, post-processed: Unknown size, total: 17.40 GiB) to /cs/labs/gabis/sapirweissbuch/tr
ivia_qa/rc/1.1.0/e734e28133f4d9a353af322aa52b9f266f6f27cbf2f072690a1694e577546b0d...                                                                                         
Downloading:  17%|███████████████████▉                                                                                                   | 446M/2.67G [00:37<04:45, 7.77MB/s]
```
2. 100% is reached
3. It got stuck here for about an hour, and added additional 30G of data to ""./datasets"" directory. I killed the process eventually.

A similar issue can be observed in Google Colab:

https://colab.research.google.com/drive/1nn1Lw02GhfGFylzbS2j6yksGjPo7kkN-?usp=sharing

## Expected behaviour:
The dataset ""TriviaQA"" should be successfully downloaded.
"
https://github.com/huggingface/datasets/issues/1611,shuffle with torch generator ,"['Is there a way one can convert the two generator? not sure overall what alternatives I could have to shuffle the datasets with a torch generator, thanks '
 '@lhoestq  let me please expalin in more details, maybe you could help me suggesting an alternative to solve the issue for now, I have multiple large datasets using huggingface library, then I need to define a distributed sampler on top of it, for this I need to shard the datasets and give each shard to each core, but before sharding I need to shuffle the dataset, if you are familiar with distributed sampler in pytorch, this needs to be done based on seed+epoch generator to make it consistent across the cores they do it through defining a torch generator, I was wondering if you could tell me how I can shuffle the data for now, I am unfortunately blocked by this and have a limited time left, and I greatly appreciate your help on this. thanks '
 '@lhoestq  Is there a way I could shuffle the datasets from this library with a custom defined shuffle function? thanks for your help on this. '
 ""Right now the shuffle method only accepts the `seed` (optional int) or `generator` (optional `np.random.Generator`) parameters.\r\n\r\nHere is a suggestion to shuffle the data using your own shuffle method using `select`.\r\n`select` can be used to re-order the dataset samples or simply pick a few ones if you want.\r\nIt's what is used under the hood when you call `dataset.shuffle`.\r\n\r\nTo use `select` you must have the list of re-ordered indices of your samples.\r\n\r\nLet's say you have a `shuffle` methods that you want to use. Then you can first build your shuffled list of indices:\r\n```python\r\nshuffled_indices = shuffle(range(len(dataset)))\r\n```\r\n\r\nThen you can shuffle your dataset using the shuffled indices with \r\n```python\r\nshuffled_dataset = dataset.select(shuffled_indices)\r\n```\r\n\r\nHope that helps""
 'thank you @lhoestq  thank you very much for responding to my question, this greatly helped me and remove the blocking for continuing my work, thanks. '
 '@lhoestq  could you confirm the method proposed does not bring the whole data into memory? thanks '
 'Yes the dataset is not loaded into memory' 'great. thanks a lot.']","Hi
I need to shuffle mutliple large datasets with `generator = torch.Generator()` for a distributed sampler which needs to make sure datasets are consistent across different cores, for this, this is really necessary for me to use  torch generator, based on documentation this generator is not supported with datasets, I really need to make shuffle work with this generator and I was wondering what I can do about this issue, thanks for your help 

@lhoestq "
https://github.com/huggingface/datasets/issues/1610,shuffle does not accept seed ,"['Hi, did you check the doc on `shuffle`?\r\nhttps://huggingface.co/docs/datasets/package_reference/main_classes.html?datasets.Dataset.shuffle#datasets.Dataset.shuffle'
 'Hi Thomas\r\nthanks for reponse, yes, I did checked it, but this does not work for me please see \r\n\r\n```\r\n(internship) rkarimi@italix17:/idiap/user/rkarimi/dev$ python \r\nPython 3.7.9 (default, Aug 31 2020, 12:42:55) \r\n[GCC 7.3.0] :: Anaconda, Inc. on linux\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n>>> import datasets \r\n2020-12-20 01:48:50.766004: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library \'libcudart.so.11.0\'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n2020-12-20 01:48:50.766029: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n>>> data = datasets.load_dataset(""scitail"", ""snli_format"")\r\ncahce dir  /idiap/temp/rkarimi/cache_home_1/datasets\r\ncahce dir  /idiap/temp/rkarimi/cache_home_1/datasets\r\nReusing dataset scitail (/idiap/temp/rkarimi/cache_home_1/datasets/scitail/snli_format/1.1.0/fd8ccdfc3134ce86eb4ef10ba7f21ee2a125c946e26bb1dd3625fe74f48d3b90)\r\n>>> data.shuffle(seed=2)\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\nTypeError: shuffle() got an unexpected keyword argument \'seed\'\r\n\r\n```\r\n\r\ndatasets version\r\n`datasets                  1.1.2                     <pip>\r\n`\r\n'
 'Thanks for reporting ! \r\n\r\nIndeed it looks like an issue with `suffle` on `DatasetDict`. We\'re going to fix that.\r\nIn the meantime you can shuffle each split (train, validation, test) separately:\r\n```python\r\nshuffled_train_dataset = data[""train""].shuffle(seed=42)\r\n```\r\n']","Hi
I need to shuffle the dataset, but this needs to be based on epoch+seed to be consistent across the cores, when I pass seed to shuffle, this does not accept seed, could you assist me with this? thanks  @lhoestq
 "
https://github.com/huggingface/datasets/issues/1609,Not able to use 'jigsaw_toxicity_pred' dataset,"['Hi @jassimran,\r\nThe `jigsaw_toxicity_pred` dataset has not been released yet, it will be available with version 2 of `datasets`, coming soon.\r\nYou can still access it by installing the master (unreleased) version of datasets directly :\r\n`pip install git+https://github.com/huggingface/datasets.git@master`\r\nPlease let me know if this helps'
 'Thanks.That works for now.']"," When trying to use jigsaw_toxicity_pred dataset, like this in a [colab](https://colab.research.google.com/drive/1LwO2A5M2X5dvhkAFYE4D2CUT3WUdWnkn?usp=sharing):
```
from datasets import list_datasets, list_metrics, load_dataset, load_metric

ds = load_dataset(""jigsaw_toxicity_pred"")
```
 
I see below error:

> FileNotFoundError: Couldn't find file at https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/jigsaw_toxicity_pred/jigsaw_toxicity_pred.py

During handling of the above exception, another exception occurred:

FileNotFoundError                         Traceback (most recent call last)
FileNotFoundError: Couldn't find file at https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/jigsaw_toxicity_pred/jigsaw_toxicity_pred.py

During handling of the above exception, another exception occurred:

FileNotFoundError                         Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/datasets/load.py in prepare_module(path, script_version, download_config, download_mode, dataset, force_local_path, **download_kwargs)
    280                 raise FileNotFoundError(
    281                     ""Couldn't find file locally at {}, or remotely at {} or {}"".format(
--> 282                         combined_path, github_file_path, file_path
    283                     )
    284                 )

FileNotFoundError: Couldn't find file locally at jigsaw_toxicity_pred/jigsaw_toxicity_pred.py, or remotely at https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/jigsaw_toxicity_pred/jigsaw_toxicity_pred.py or https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/jigsaw_toxicity_pred/jigsaw_toxicity_pred.py"
https://github.com/huggingface/datasets/issues/1600,AttributeError: 'DatasetDict' object has no attribute 'train_test_split',"['Hi @david-waterworth!\r\n\r\nAs indicated in the error message, `load_dataset(""csv"")` returns a `DatasetDict` object, which is mapping of `str` to `Dataset` objects. I believe in this case the behavior is to return a `train` split with all the data.\r\n`train_test_split` is a method of the `Dataset` object, so you will need to do something like this:\r\n```python\r\ndataset_dict = load_dataset(`\'csv\', data_files=\'data.txt\')\r\ndataset = dataset_dict[\'split name, eg train\']\r\ndataset.train_test_split(test_size=0.1)\r\n```\r\n\r\nPlease let me know if this helps. 🙂 '
 ""Thanks, that's working - the same issue also tripped me up with training. \r\n\r\nI also agree https://github.com/huggingface/datasets/issues/767 would be a useful addition. ""
 'Closing this now'
 ""> ```python\r\n> dataset_dict = load_dataset(`'csv', data_files='data.txt')\r\n> dataset = dataset_dict['split name, eg train']\r\n> dataset.train_test_split(test_size=0.1)\r\n> ```\r\n\r\nI am getting error like\r\nKeyError: 'split name, eg train'\r\nCould you please tell me how to solve this?""
 ""dataset = load_dataset('csv', data_files=['files/datasets/dataset.csv'])\r\ndataset = dataset['train']\r\ndataset = dataset.train_test_split(test_size=0.1)""]","The following code fails with ""'DatasetDict' object has no attribute 'train_test_split'"" - am I doing something wrong?
```
from datasets import load_dataset
dataset = load_dataset('csv', data_files='data.txt')
dataset = dataset.train_test_split(test_size=0.1)
```

> AttributeError: 'DatasetDict' object has no attribute 'train_test_split'"
https://github.com/huggingface/datasets/issues/1594,connection error ,"['This happen quite often when they are too many concurrent requests to github.\r\n\r\ni can understand it’s a bit cumbersome to handle on the user side. Maybe we should try a few times in the lib (eg with timeout) before failing, what do you think @lhoestq ?'
 ""Yes currently there's no retry afaik. We should add retries""
 'Retries were added in #1603 :) \r\nIt will be available in the next release'
 'Hi @lhoestq thank you for the modification, I will use`script_version=""master""` for now  :), to my experience, also setting timeout to a larger number like 3*60 which I normally use helps a lot on this.\r\n']","Hi
I am hitting to this error, thanks 

```
> Traceback (most recent call last):
  File ""finetune_t5_trainer.py"", line 379, in <module>
    main()
  File ""finetune_t5_trainer.py"", line 208, in main
    if training_args.do_eval or training_args.evaluation_strategy != EvaluationStrategy.NO
  File ""finetune_t5_trainer.py"", line 207, in <dictcomp>
    for task in data_args.eval_tasks}
  File ""/workdir/seq2seq/data/tasks.py"", line 70, in get_dataset
    dataset = self.load_dataset(split=split)
  File ""/workdir/seq2seq/data/tasks.py"", line 66, in load_dataset
    return datasets.load_dataset(self.task.name, split=split, script_version=""master"")
  File ""/usr/local/lib/python3.6/dist-packages/datasets/load.py"", line 589, in load_dataset
    path, script_version=script_version, download_config=download_config, download_mode=download_mode, dataset=True
  File ""/usr/local/lib/python3.6/dist-packages/datasets/load.py"", line 267, in prepare_module
    local_path = cached_path(file_path, download_config=download_config)
  File ""/usr/local/lib/python3.6/dist-packages/datasets/utils/file_utils.py"", line 308, in cached_path
    use_etag=download_config.use_etag,
  File ""/usr/local/lib/python3.6/dist-packages/datasets/utils/file_utils.py"", line 487, in get_from_cache
    raise ConnectionError(""Couldn't reach {}"".format(url))
ConnectionError: Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/master/datasets/boolq/boolq.py
el/0 I1217 01:11:33.898849 354161 main shadow.py:210 Current job status: FINISHED
```"
https://github.com/huggingface/datasets/issues/1593,Access to key in DatasetDict map,"['Indeed that would be cool\r\n\r\nAlso FYI right now the easiest way to do this is\r\n```python\r\ndataset_dict[""train""] = dataset_dict[""train""].map(my_transform_for_the_train_set)\r\ndataset_dict[""test""] = dataset_dict[""test""].map(my_transform_for_the_test_set)\r\n```']","It is possible that we want to do different things in the `map` function (and possibly other functions too) of a `DatasetDict`, depending on the key. I understand that `DatasetDict.map` is a really thin wrapper of `Dataset.map`, so it is easy to directly implement this functionality in the client code. Still, it'd be nice if there can be a flag, similar to `with_indices`, that allows the callable to know the key inside `DatasetDict`."
https://github.com/huggingface/datasets/issues/1592,Using datasets.Metric with Trainer(),['We are indeed working on the integration with `Trainer` :)'],"## Using datasets.Metric with Trainer()

Hi team, I was quite surprised in the [Metric documentation](https://huggingface.co/docs/datasets/using_metrics.html) I don't see how it can be used with `Trainer()`. That would be the most intuitive use case instead of having to iterate the batches and add predictions and references to the metric, then compute the metric manually. Ideally, any pre-built metrics can be added to `compute_metrics` argument of `Trainer()` and they will be calculated at an interval specified by `TrainingArguments.evaluation_strategy`. 

Is this option available but just not mentioned in the documentation or it's not possible at the moment? I notice in the [Transformer | Training and fine-tuning](https://huggingface.co/transformers/training.html) tutorial, you are using custom scripts to calculate the accuracy, P/R/F, which are already in the pre-built metrics."
https://github.com/huggingface/datasets/issues/1591,IWSLT-17 Link Broken,"[""Sorry, this is a duplicate of #1287. Not sure why it didn't come up when I searched `iwslt` in the issues list.""
 'Closing this since its a duplicate']","```
FileNotFoundError: Couldn't find file at https://wit3.fbk.eu/archive/2017-01-trnmted//texts/DeEnItNlRo/DeEnItNlRo/DeEnItNlRo-DeEnItNlRo.tgz
```"
https://github.com/huggingface/datasets/issues/1590,Add helper to resolve namespace collision,"['Do you have an example?'
 'I was thinking about using something like [importlib](https://docs.python.org/3/library/importlib.html#importing-a-source-file-directly) to over-ride the collision. \r\n\r\n**Reason requested**: I use the [following template](https://github.com/jramapuram/ml_base/) repo where I house all my datasets as a submodule.'
 'Alternatively huggingface could consider some submodule type structure like:\r\n\r\n`import huggingface.datasets`\r\n`import huggingface.transformers`\r\n\r\n`datasets` is a very common module in ML and should be an end-user decision and not scope all of python ¯\\_(ツ)_/¯ \r\n'
 ""That's a interesting option indeed. We'll think about it.""
 ""It also wasn't initially obvious to me that the samples which contain `import datasets` were in fact importing a huggingface library (in fact all the huggingface imports are very generic - transformers, tokenizers, datasets...)""]","Many projects use a module called `datasets`, however this is incompatible with huggingface datasets. It would be great if there if there was some helper or similar function to resolve such a common conflict. "
https://github.com/huggingface/datasets/issues/1585,FileNotFoundError for `amazon_polarity`,"['Hi @phtephanx , the `amazon_polarity` dataset has not been released yet. It will be available in the coming soon v2of `datasets` :) \r\n\r\nYou can still access it now if you want, but you will need to install datasets via the master branch:\r\n`pip install git+https://github.com/huggingface/datasets.git@master`']","Version: `datasets==v1.1.3`

### Reproduction
```python
from datasets import load_dataset
data = load_dataset(""amazon_polarity"")
```
crashes with
```bash
FileNotFoundError: Couldn't find file at https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/amazon_polarity/amazon_polarity.py
```
and 
```bash
FileNotFoundError: Couldn't find file at https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/amazon_polarity/amazon_polarity.py
```
and
```bash
FileNotFoundError: Couldn't find file locally at amazon_polarity/amazon_polarity.py, or remotely at https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/amazon_polarity/amazon_polarity.py or https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/amazon_polarity/amazon_polarity.py
```"
https://github.com/huggingface/datasets/issues/1581,Installing datasets and transformers in a tensorflow docker image throws Permission Error on 'import transformers',"['Thanks for reporting !\r\nYou can override the directory in which cache file are stored using for example\r\n```\r\nENV HF_HOME=""/root/cache/hf_cache_home""\r\n```\r\n\r\nThis way both `transformers` and `datasets` will use this directory instead of the default `.cache`'
 ""Great, thanks. I didn't see documentation about than ENV variable, looks like an obvious solution. ""
 '> Thanks for reporting !\r\n> You can override the directory in which cache file are stored using for example\r\n> \r\n> ```\r\n> ENV HF_HOME=""/root/cache/hf_cache_home""\r\n> ```\r\n> \r\n> This way both `transformers` and `datasets` will use this directory instead of the default `.cache`\r\n\r\ncan we disable caching directly?'
 'Hi ! Unfortunately no since we need this directory to load datasets.\r\nWhen you load a dataset, it downloads the raw data files in the cache directory inside <cache_dir>/downloads. Then it builds the dataset and saves it as arrow data inside <cache_dir>/<dataset_name>.\r\n\r\nHowever you can specify the directory of your choice, and it can be a temporary directory if you want to clean everything up at one point.'
 ""I'm closing this to keep issues a bit cleaner""]","I am using a docker container, based on latest tensorflow-gpu image, to run transformers and datasets (4.0.1 and 1.1.3 respectively - Dockerfile attached below). Importing transformers throws a Permission Error to access `/.cache`:

```
$ docker run --gpus=all --rm -it -u $(id -u):$(id -g) -v $(pwd)/data:/root/data -v $(pwd):/root -v $(pwd)/models/:/root/models -v $(pwd)/saved_models/:/root/saved_models -e ""HOST_HOSTNAME=$(hostname)"" hf-error:latest /bin/bash

________                               _______________                
___  __/__________________________________  ____/__  /________      __
__  /  _  _ \_  __ \_  ___/  __ \_  ___/_  /_   __  /_  __ \_ | /| / /
_  /   /  __/  / / /(__  )/ /_/ /  /   _  __/   _  / / /_/ /_ |/ |/ / 
/_/    \___//_/ /_//____/ \____//_/    /_/      /_/  \____/____/|__/


You are running this container as user with ID 1000 and group 1000,
which should map to the ID and group for your user on the Docker host. Great!

tf-docker /root > python
Python 3.6.9 (default, Oct  8 2020, 12:12:24) 
[GCC 8.4.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import transformers
2020-12-15 23:53:21.165827: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.6/dist-packages/transformers/__init__.py"", line 22, in <module>
    from .integrations import (  # isort:skip
  File ""/usr/local/lib/python3.6/dist-packages/transformers/integrations.py"", line 5, in <module>
    from .trainer_utils import EvaluationStrategy
  File ""/usr/local/lib/python3.6/dist-packages/transformers/trainer_utils.py"", line 25, in <module>
    from .file_utils import is_tf_available, is_torch_available, is_torch_tpu_available
  File ""/usr/local/lib/python3.6/dist-packages/transformers/file_utils.py"", line 88, in <module>
    import datasets  # noqa: F401
  File ""/usr/local/lib/python3.6/dist-packages/datasets/__init__.py"", line 26, in <module>
    from .arrow_dataset import Dataset, concatenate_datasets
  File ""/usr/local/lib/python3.6/dist-packages/datasets/arrow_dataset.py"", line 40, in <module>
    from .arrow_reader import ArrowReader
  File ""/usr/local/lib/python3.6/dist-packages/datasets/arrow_reader.py"", line 31, in <module>
    from .utils import cached_path, logging
  File ""/usr/local/lib/python3.6/dist-packages/datasets/utils/__init__.py"", line 20, in <module>
    from .download_manager import DownloadManager, GenerateMode
  File ""/usr/local/lib/python3.6/dist-packages/datasets/utils/download_manager.py"", line 25, in <module>
    from .file_utils import HF_DATASETS_CACHE, cached_path, get_from_cache, hash_url_to_filename
  File ""/usr/local/lib/python3.6/dist-packages/datasets/utils/file_utils.py"", line 118, in <module>
    os.makedirs(HF_MODULES_CACHE, exist_ok=True)
  File ""/usr/lib/python3.6/os.py"", line 210, in makedirs
    makedirs(head, mode, exist_ok)
  File ""/usr/lib/python3.6/os.py"", line 210, in makedirs
    makedirs(head, mode, exist_ok)
  File ""/usr/lib/python3.6/os.py"", line 220, in makedirs
    mkdir(name, mode)
PermissionError: [Errno 13] Permission denied: '/.cache'
```
I've pinned the problem to `RUN pip install datasets`, and by commenting it you can actually import transformers correctly. Another workaround I've found is creating the directory and giving permissions to it directly on the Dockerfile.

```
FROM tensorflow/tensorflow:latest-gpu-jupyter
WORKDIR /root

EXPOSE 80
EXPOSE 8888
EXPOSE 6006

ENV SHELL /bin/bash
ENV PATH=""/root/.local/bin:${PATH}""

ENV CUDA_CACHE_PATH=""/root/cache/cuda""
ENV CUDA_CACHE_MAXSIZE=""4294967296""

ENV TFHUB_CACHE_DIR=""/root/cache/tfhub""

RUN pip install --upgrade pip

RUN apt update -y && apt upgrade -y

RUN pip install transformers

#Installing datasets will throw the error, try commenting and rebuilding
RUN pip install datasets

#Another workaround is creating the directory and give permissions explicitly
#RUN mkdir /.cache
#RUN chmod 777 /.cache
```
"
https://github.com/huggingface/datasets/issues/1541,connection issue while downloading data,"['could you tell me how I can avoid download, by pre-downloading the data first, put them in a folder so the code does not try to redownload? could you tell me the path to put the downloaded data, and how to do it? thanks\r\n@lhoestq '
 'Does your instance have an internet connection ?\r\n\r\nIf you don\'t have an internet connection you\'ll need to have the dataset on the instance disk.\r\nTo do so first download the dataset on another machine using `load_dataset` and then you can save it in a folder using `my_dataset.save_to_disk(""path/to/folder"")`. Once the folder is copied on your instance you can reload the dataset with `datasets.load_from_disk(""path/to/folder"")`']","Hi
I am running my codes on google cloud, and I am getting this error resulting in the failure of the codes when trying to download the data, could you assist me to solve this? also as a temporary solution, could you tell me how I can increase the number of retries and timeout to at least let the models run for now. thanks 

```
Traceback (most recent call last):
  File ""finetune_t5_trainer.py"", line 361, in <module>
    main()
  File ""finetune_t5_trainer.py"", line 269, in main
    add_prefix=False if training_args.train_adapters else True)
  File ""/workdir/seq2seq/data/tasks.py"", line 70, in get_dataset
    dataset = self.load_dataset(split=split)
  File ""/workdir/seq2seq/data/tasks.py"", line 306, in load_dataset
    return datasets.load_dataset('glue', 'cola', split=split)
  File ""/usr/local/lib/python3.6/dist-packages/datasets/load.py"", line 589, in load_dataset
    path, script_version=script_version, download_config=download_config, download_mode=download_mode, dataset=True
  File ""/usr/local/lib/python3.6/dist-packages/datasets/load.py"", line 263, in prepare_module
    head_hf_s3(path, filename=name, dataset=dataset)
  File ""/usr/local/lib/python3.6/dist-packages/datasets/utils/file_utils.py"", line 200, in head_hf_s3
    return http_head(hf_bucket_url(identifier=identifier, filename=filename, use_cdn=use_cdn, dataset=dataset))
  File ""/usr/local/lib/python3.6/dist-packages/datasets/utils/file_utils.py"", line 403, in http_head
    url, proxies=proxies, headers=headers, cookies=cookies, allow_redirects=allow_redirects, timeout=timeout
  File ""/usr/local/lib/python3.6/dist-packages/requests/api.py"", line 104, in head
    return request('head', url, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/requests/api.py"", line 61, in request
    return session.request(method=method, url=url, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/requests/sessions.py"", line 542, in request
    resp = self.send(prep, **send_kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/requests/sessions.py"", line 655, in send
    r = adapter.send(request, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/requests/adapters.py"", line 504, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='s3.amazonaws.com', port=443): Max retries exceeded with url: /datasets.huggingface.co/datasets/datasets/glue/glue.py (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f47db511e80>, 'Connection to s3.amazonaws.com timed out. (connect timeout=10)'))
```"
https://github.com/huggingface/datasets/issues/1514,how to get all the options of a property in datasets ,"['In a dataset, labels correspond to the `ClassLabel` feature that has the `names` property that returns string represenation of the integer classes (or `num_classes` to get the number of different classes).'
 'I think the `features` attribute of the dataset object is what you are looking for:\r\n```\r\n>>> dataset.features\r\n{\'sentence1\': Value(dtype=\'string\', id=None),\r\n \'sentence2\': Value(dtype=\'string\', id=None),\r\n \'label\': ClassLabel(num_classes=2, names=[\'not_equivalent\', \'equivalent\'], names_file=None, id=None),\r\n \'idx\': Value(dtype=\'int32\', id=None)\r\n}\r\n>>> dataset.features[""label""].names\r\n[\'not_equivalent\', \'equivalent\']\r\n```\r\n\r\nFor reference: https://huggingface.co/docs/datasets/exploring.html']","Hi
could you tell me how I can get all unique options of a property of dataset?
for instance in case of boolq, if the user wants to know which unique labels it has, is there a way to access unique labels without getting all training data lables and then forming a set i mean? thanks"
https://github.com/huggingface/datasets/issues/1478,Inconsistent argument names.,"['Also for the `Accuracy` metric the `accuracy_score` method should have its args in the opposite order so `accuracy_score(predictions, references,,,)`.'
 'Thanks for pointing this out ! 🕵🏻 \r\nPredictions and references should indeed be swapped in the docstring.\r\nHowever, the call to `accuracy_score` should not be changed, it [signature](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score) being:\r\n```\r\nsklearn.metrics.accuracy_score(y_true, y_pred, *, normalize=True, sample_weight=None)\r\n```\r\n\r\nFeel free to open a PR if you want to fix this :)']","Just find it a wee bit odd that in the transformers library `predictions` are those made by the model:
https://github.com/huggingface/transformers/blob/master/src/transformers/trainer_utils.py#L51-L61

While in many datasets metrics they are the ground truth labels:
https://github.com/huggingface/datasets/blob/c3f53792a744ede18d748a1133b6597fdd2d8d18/metrics/accuracy/accuracy.py#L31-L40

Do you think predictions & references should be swapped? I'd be willing to do some refactoring here if you agree."
https://github.com/huggingface/datasets/issues/1452,SNLI dataset contains labels with value -1,"['I believe the `-1` label is used for missing/NULL data as per HuggingFace Dataset conventions. If I recall correctly SNLI has some entries with no (gold) labels in the dataset.'
 ""Ah, you're right. The dataset has some pairs with missing labels. Thanks for reminding me.""]","```
import datasets
nli_data = datasets.load_dataset(""snli"")
train_data = nli_data['train']
train_labels = train_data['label']
label_set = set(train_labels)
print(label_set)
```

**Output:**
`{0, 1, 2, -1}`"
https://github.com/huggingface/datasets/issues/1444,"FileNotFound remotly, can't load a dataset","['This dataset will be available in version-2 of the library. If you want to use this dataset now, install datasets from `master` branch rather.\r\n\r\nCommand to install datasets from `master` branch:\r\n`!pip install git+https://github.com/huggingface/datasets.git@master`'
 'Closing this, thanks @VasudevGupta7 ']","```py
!pip install datasets
import datasets as ds

corpus = ds.load_dataset('large_spanish_corpus')
```
gives the error

> FileNotFoundError: Couldn't find file locally at large_spanish_corpus/large_spanish_corpus.py, or remotely at https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/large_spanish_corpus/large_spanish_corpus.py or https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/large_spanish_corpus/large_spanish_corpus.py

not just `large_spanish_corpus`,  `zest` too, but `squad` is available. 

this was using colab and localy "
https://github.com/huggingface/datasets/issues/1422,Can't map dataset (loaded from csv),"[""Please could you post the whole script? I can't reproduce your issue. After updating the feature names/labels to match with the data, everything works fine for me. Try to update datasets/transformers to the newest version.""
 'Actually, the problem was how `tokenize` function was defined. This was completely my side mistake, so there are really no needs in this issue anymore']","Hello! I am trying to load single csv file with two columns: ('label': str, 'text' str), where is label is str of two possible classes.

Below steps are similar with [this notebook](https://colab.research.google.com/drive/1-JIJlao4dI-Ilww_NnTc0rxtp-ymgDgM?usp=sharing), where bert model and tokenizer are used to classify lmdb loaded dataset. Only one difference it is the dataset loaded from .csv file.
Here is how I load it:

```python
data_path = 'data.csv'
data = pd.read_csv(data_path)

# process class name to indices
classes = ['neg', 'pos']
class_to_idx = { cl: i for i, cl in enumerate(classes) }

# now data is like {'label': int, 'text' str}
data['label'] = data['label'].apply(lambda x: class_to_idx[x])

# load dataset and map it with defined `tokenize` function
features = Features({
  target: ClassLabel(num_classes=2, names=['neg', 'pos'], names_file=None, id=None),
  feature: Value(dtype='string', id=None),
})
dataset = Dataset.from_pandas(data, features=features)
dataset.map(tokenize, batched=True, batch_size=len(dataset))
```

It ruins on the last line with following error:
```
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-112-32b6275ce418> in <module>()
      9 })
     10 dataset = Dataset.from_pandas(data, features=features)
---> 11 dataset.map(tokenizer, batched=True, batch_size=len(dataset))

2 frames
/usr/local/lib/python3.6/dist-packages/datasets/arrow_dataset.py in map(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint)
   1237         test_inputs = self[:2] if batched else self[0]
   1238         test_indices = [0, 1] if batched else 0
-> 1239         update_data = does_function_return_dict(test_inputs, test_indices)
   1240         logger.info(""Testing finished, running the mapping function on the dataset"")
   1241 

/usr/local/lib/python3.6/dist-packages/datasets/arrow_dataset.py in does_function_return_dict(inputs, indices)
   1208             fn_args = [inputs] if input_columns is None else [inputs[col] for col in input_columns]
   1209             processed_inputs = (
-> 1210                 function(*fn_args, indices, **fn_kwargs) if with_indices else function(*fn_args, **fn_kwargs)
   1211             )
   1212             does_return_dict = isinstance(processed_inputs, Mapping)

/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py in __call__(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)
   2281             )
   2282         ), (
-> 2283             ""text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) ""
   2284             ""or `List[List[str]]` (batch of pretokenized examples).""
   2285         )

AssertionError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).
```

which I think is not expected. I also tried the same steps using `Dataset.from_csv` which resulted in the same error.

For reproducing this, I used [this dataset from kaggle](https://www.kaggle.com/team-ai/spam-text-message-classification)"
https://github.com/huggingface/datasets/issues/1324,❓ Sharing ElasticSearch indexed dataset ,"['Hello @pietrolesci , I am not sure to understand what you are trying to do here.\r\n\r\nIf you\'re looking for ways to save a dataset on disk, you can you the `save_to_disk` method:\r\n```python\r\n>>> import datasets\r\n>>> loaded_dataset = datasets.load(""dataset_name"")\r\n>>> loaded_dataset.save_to_disk(""/path/on/your/disk"")\r\n```\r\n\r\nThe saved dataset can later be retrieved using:\r\n```python\r\n>>> loaded_dataset = datasets.Dataset.load_from_disk(""/path/on/your/disk"")\r\n```\r\n\r\nAlso, I\'d recommend posting your question directly in the issue section of the [elasticsearch repo](https://github.com/elastic/elasticsearch)'
 'Hi @SBrandeis,\n\nThanks a lot for picking up my request. \n\nMaybe I can clarify my use-case with a bit of context. Say I have the IMDb dataset. I create an ES index on it. Now I can save and reload the dataset from disk normally. Once I reload the dataset, it is easy to retrieve the ES index on my machine. I was wondering: is there a way I can share the (now) indexed version of the IMDb dataset with my colleagues without requiring them to re-index it?\n\nThanks a lot in advance for your consideration.\n\nBest,\n\nPietro'
 ""Thanks for the clarification.\r\n\r\nI am not familiar with ElasticSearch, but if I understand well you're trying to migrate your data along with the ES index.\r\nMy advice would be to check out ES documentation, for instance, this might help you: https://www.elastic.co/guide/en/cloud/current/ec-migrate-data.html\r\n\r\nLet me know if it helps""]","Hi there,

First of all, thank you very much for this amazing library. Datasets have become my preferred data structure for basically everything I am currently doing.

**Question:** I'm working with a dataset and I have an elasticsearch container running at localhost:9200. I added an elasticsearch index and I was wondering

- how can I know where it has been saved?

- how can I share the indexed dataset with others?

I tried to dig into the docs, but could not find anything about that.

Thank you very much for your help.

Best,
Pietro

Edit: apologies for the wrong label"
https://github.com/huggingface/datasets/issues/1299,"can't load ""german_legal_entity_recognition"" dataset","[""Please if you could tell me more about the error? \r\n\r\n1. Please check the directory you've been working on\r\n2. Check for any typos""
 '> Please if you could tell me more about the error?\r\n> \r\n> 1. Please check the directory you\'ve been working on\r\n> 2. Check for any typos\r\n\r\nError happens during the execution of this line:\r\ndataset = load_dataset(""german_legal_entity_recognition"")\r\n\r\nAlso, when I try to open mentioned links via Opera I have errors ""404: Not Found"" and ""This XML file does not appear to have any style information associated with it. The document tree is shown below."" respectively.'
 'Hello @nataly-obr, the `german_legal_entity_recognition` dataset has not yet been released (it is part of the coming soon v2 release).\r\n\r\nYou can still access it now if you want, but you will need to install `datasets` via the master branch:\r\n`pip install git+https://github.com/huggingface/datasets.git@master`\r\n\r\nPlease let me know if it solves the issue :) ']","FileNotFoundError: Couldn't find file locally at german_legal_entity_recognition/german_legal_entity_recognition.py, or remotely at https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/german_legal_entity_recognition/german_legal_entity_recognition.py or https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/german_legal_entity_recognition/german_legal_entity_recognition.py
"
https://github.com/huggingface/datasets/issues/1290,imdb dataset cannot be downloaded,"['Hi @rabeehk , I am unable to reproduce your problem locally.\r\nCan you try emptying the cache (removing the content of `/idiap/temp/rkarimi/cache_home_1/datasets`) and retry ?'
 'Hi,\r\nthanks, I did remove the cache and still the same error here\r\n\r\n```\r\n>>> a = datasets.load_dataset(""imdb"", split=""train"")\r\ncahce dir  /idiap/temp/rkarimi/cache_home_1/datasets\r\ncahce dir  /idiap/temp/rkarimi/cache_home_1/datasets\r\nDownloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.06 MiB, post-processed: Unknown size, total: 207.28 MiB) to /idiap/temp/rkarimi/cache_home_1/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3...\r\ncahce dir  /idiap/temp/rkarimi/cache_home_1/datasets\r\ncahce dir  /idiap/temp/rkarimi/cache_home_1/datasets/downloads\r\nTraceback (most recent call last):        \r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/load.py"", line 611, in load_dataset\r\n    ignore_verifications=ignore_verifications,\r\n  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/builder.py"", line 476, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/builder.py"", line 558, in _download_and_prepare\r\n    verify_splits(self.info.splits, split_dict)\r\n  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/utils/info_utils.py"", line 73, in verify_splits\r\n    raise NonMatchingSplitsSizesError(str(bad_splits))\r\ndatasets.utils.info_utils.NonMatchingSplitsSizesError: [{\'expected\': SplitInfo(name=\'unsupervised\', num_bytes=67125548, num_examples=50000, dataset_name=\'imdb\'), \'recorded\': SplitInfo(name=\'unsupervised\', num_bytes=4902716, num_examples=3680, dataset_name=\'imdb\')}]\r\n```\r\n\r\ndatasets version\r\n```\r\ndatasets                  1.1.2                     <pip>\r\ntensorflow-datasets       4.1.0                     <pip>\r\n\r\n```'
 'resolved with moving to version 1.1.3']","hi
please find error below getting imdb train spli:
thanks

`
datasets.load_dataset>>> datasets.load_dataset(""imdb"", split=""train"")`


errors


```
cahce dir  /idiap/temp/rkarimi/cache_home_1/datasets
cahce dir  /idiap/temp/rkarimi/cache_home_1/datasets
Downloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.06 MiB, post-processed: Unknown size, total: 207.28 MiB) to /idiap/temp/rkarimi/cache_home_1/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3...
cahce dir  /idiap/temp/rkarimi/cache_home_1/datasets
cahce dir  /idiap/temp/rkarimi/cache_home_1/datasets/downloads
Traceback (most recent call last):        
  File ""<stdin>"", line 1, in <module>
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/load.py"", line 611, in load_dataset
    ignore_verifications=ignore_verifications,
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/builder.py"", line 476, in download_and_prepare
    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/builder.py"", line 558, in _download_and_prepare
    verify_splits(self.info.splits, split_dict)
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/utils/info_utils.py"", line 73, in verify_splits
    raise NonMatchingSplitsSizesError(str(bad_splits))
datasets.utils.info_utils.NonMatchingSplitsSizesError: [{'expected': SplitInfo(name='unsupervised', num_bytes=67125548, num_examples=50000, dataset_name='imdb'), 'recorded': SplitInfo(name='unsupervised', num_bytes=7486451, num_examples=5628, dataset_name='imdb')}]


```"
https://github.com/huggingface/datasets/issues/1287,"'iwslt2017-ro-nl', cannot be downloaded ","['the same issue with datasets.load_dataset(""iwslt2017"", \'iwslt2017-en-nl\', split=split), ..... '
 'even with setting master like the following command, still remains \r\n\r\ndatasets.load_dataset(""iwslt2017"", \'iwslt2017-en-nl\', split=""train"", script_version=""master"")\r\n'
 'Looks like the data has been moved from its original location to google drive\r\n\r\nNew url: https://drive.google.com/u/0/uc?id=12ycYSzLIG253AFN35Y6qoyf9wtkOjakp&export=download']","Hi
I am trying 
`>>> datasets.load_dataset(""iwslt2017"", 'iwslt2017-ro-nl', split=""train"")`

getting this error thank you for your help
```
cahce dir  /idiap/temp/rkarimi/cache_home_1/datasets
cahce dir  /idiap/temp/rkarimi/cache_home_1/datasets
Downloading and preparing dataset iwsl_t217/iwslt2017-ro-nl (download: 314.07 MiB, generated: 39.92 MiB, post-processed: Unknown size, total: 354.00 MiB) to /idiap/temp/rkarimi/cache_home_1/datasets/iwsl_t217/iwslt2017-ro-nl/1.0.0/cca6935a0851a8ceac1202a62c958738bdfa23c57a51bc52ac1c5ebd2aa172cd...
cahce dir  /idiap/temp/rkarimi/cache_home_1/datasets
cahce dir  /idiap/temp/rkarimi/cache_home_1/datasets/downloads
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/load.py"", line 611, in load_dataset
    ignore_verifications=ignore_verifications,
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/builder.py"", line 476, in download_and_prepare
    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/builder.py"", line 531, in _download_and_prepare
    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)
  File "" /idiap/home/rkarimi/.cache/huggingface/modules/datasets_modules/datasets/iwslt2017/cca6935a0851a8ceac1202a62c958738bdfa23c57a51bc52ac1c5ebd2aa172cd/iwslt2017.py"", line 118, in _split_generators
    dl_dir = dl_manager.download_and_extract(MULTI_URL)
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/utils/download_manager.py"", line 254, in download_and_extract
    return self.extract(self.download(url_or_urls))
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/utils/download_manager.py"", line 179, in download
    num_proc=download_config.num_proc,
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/utils/py_utils.py"", line 216, in map_nested
    return function(data_struct)
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/utils/file_utils.py"", line 308, in cached_path
    use_etag=download_config.use_etag,
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/utils/file_utils.py"", line 477, in get_from_cache
    raise ConnectionError(""Couldn't reach {}"".format(url))
ConnectionError: Couldn't reach https://wit3.fbk.eu/archive/2017-01-trnmted//texts/DeEnItNlRo/DeEnItNlRo/DeEnItNlRo-DeEnItNlRo.tgz

```"
https://github.com/huggingface/datasets/issues/1286,[libprotobuf FATAL /sentencepiece/src/../third_party/protobuf-lite/google/protobuf/repeated_field.h:1505] CHECK failed: (index) >= (0):  terminate called after throwing an instance of 'google::protobuf::FatalException'   what():  CHECK failed: (index) >= (0):  Aborted,"['I remember also getting the same issue for several other translation datasets like all the iwslt2017 group, this is blokcing me and I really need to fix it and I was wondering if you have an idea on this. @lhoestq  thanks,. '
 'maybe there is an empty line or something inside these datasets? could you tell me why this is happening? thanks '
 'I just checked and the wmt16 en-ro doesn\'t have empty lines\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nd = load_dataset(""wmt16"", ""ro-en"", split=""train"")\r\nlen(d)  # 610320\r\nlen(d.filter(lambda x: len(x[""translation""][""en""].strip()) > 0))  # 610320\r\nlen(d.filter(lambda x: len(x[""translation""][""ro""].strip()) > 0))  # 610320\r\n# also tested for split=""validation"" and ""test""\r\n```\r\n\r\nCan you open an issue on the `transformers` repo ? also cc @sgugger '
 ""Hi @lhoestq \r\nI am not really sure which part is causing this, to me this is more related to dataset library as this is happening for some of the datassets below please find the information to reprodcue the bug, this is really blocking me and I appreciate your help\r\n\r\n\r\n## Environment info\r\n- `transformers` version: 3.5.1\r\n- Platform: GPU\r\n- Python version: 3.7 \r\n- PyTorch version (GPU?): 1.0.4\r\n- Tensorflow version (GPU?): - \r\n- Using GPU in script?: - \r\n- Using distributed or parallel set-up in script?: - \r\n\r\n### Who can help\r\n tokenizers: @mfuntowicz\r\n Trainer: @sgugger\r\n TextGeneration: @TevenLeScao \r\n nlp datasets: [different repo](https://github.com/huggingface/nlp)\r\n rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\r\n examples/seq2seq: @patil-suraj\r\n\r\n## Information\r\nHi\r\nI am testing seq2seq model with T5 on different datasets and this is always getting the following bug, this is really blocking me as this fails for many datasets. could you have a look please? thanks  \r\n\r\n```\r\n[libprotobuf FATAL /sentencepiece/src/../third_party/protobuf-lite/google/protobuf/repeated_field.h:1505] CHECK failed: (index) >= (0): \r\nterminate called after throwing an instance of 'google::protobuf::FatalException'\r\n  what():  CHECK failed: (index) >= (0): \r\nAborted\r\n\r\n```\r\n\r\nTo reproduce the error please run on 1 GPU:\r\n```\r\ngit clone git@github.com:rabeehk/debug-seq2seq.git\r\npython setup.py develop \r\ncd seq2seq \r\npython finetune_t5_trainer.py temp.json\r\n\r\n```\r\n\r\nFull output of the program:\r\n\r\n```\r\n(internship) rkarimi@vgnh008:/idiap/user/rkarimi/dev/debug-seq2seq/seq2seq$ python finetune_t5_trainer.py temp.json \r\n2020-12-12 15:38:16.234542: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n2020-12-12 15:38:16.234598: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n12/12/2020 15:38:32 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\r\n12/12/2020 15:38:32 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='outputs/test', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=64, per_device_eval_batch_size=64, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=0.01, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=2, max_steps=-1, warmup_steps=500, logging_dir='runs/Dec12_15-38-32_vgnh008', logging_first_step=True, logging_steps=200, save_steps=200, save_total_limit=1, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=200, dataloader_num_workers=0, past_index=-1, run_name='outputs/test', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, label_smoothing=0.1, sortish_sampler=False, predict_with_generate=True, adafactor=False, encoder_layerdrop=None, decoder_layerdrop=None, dropout=None, attention_dropout=None, lr_scheduler='linear', fixed_length_emb=None, encoder_projection=None, encoder_pooling=None, projection_length=None, only_projection_bottleneck=False, concat_projection_token=False, gcs_bucket='ruse-xcloud-bucket', temperature=10, train_adapters=True, do_finetune=True, parametric_task_embedding=False, eval_output_dir='outputs/finetune-adapter/test-n-1-lr-1e-02-e-20')\r\nSome weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-small and are newly initialized: ['encoder.block.0.layer.0.adapter_controller.meta_up_sampler.weight_generator.0.weight', 'encoder.block.0.layer.0.adapter_controller.meta_up_sampler.weight_generator.0.bias', 'encoder.block.0.layer.0.adapter_controller.meta_up_sampler.weight_generator.1.weight', 'encoder.block.0.layer.0.adapter_controller.meta_up_sampler.weight_generator.1.bias', 'encoder.block.0.layer.0.adapter_controller.meta_up_sampler.bias_generator.0.weight', 'encoder.block.0.layer.0.adapter_controller.meta_up_sampler.bias_generator.0.bias', 'encoder.block.0.layer.0.adapter_controller.meta_up_sampler.bias_generator.1.weight', 'encoder.block.0.layer.0.adapter_controller.meta_up_sampler.bias_generator.1.bias', 'encoder.block.0.layer.0.adapter_controller.meta_down_sampler.weight_generator.0.weight', 'encoder.block.0.layer.0.adapter_controller.meta_down_sampler.weight_generator.0.bias', 'encoder.block.0.layer.0.adapter_controller.meta_down_sampler.weight_generator.1.weight', 'encoder.block.0.layer.0.adapter_controller.meta_down_sampler.weight_generator.1.bias', 'encoder.block.0.layer.0.adapter_controller.meta_down_sampler.bias_generator.0.weight', 'encoder.block.0.layer.0.adapter_controller.meta_down_sampler.bias_generator.0.bias', 'encoder.block.0.layer.0.adapter_controller.meta_down_sampler.bias_generator.1.weight', 'encoder.block.0.layer.0.adapter_controller.meta_down_sampler.bias_generator.1.bias', 'encoder.block.0.layer.0.adapter_controller.post_layer_norm.weight', 'encoder.block.0.layer.0.adapter_controller.post_layer_norm.bias', 'encoder.block.0.layer.1.adapter_controller.meta_up_sampler.weight_generator.0.weight', 'encoder.block.0.layer.1.adapter_controller.meta_up_sampler.weight_generator.0.bias', 'encoder.block.0.layer.1.adapter_controller.meta_up_sampler.weight_generator.1.weight', 'encoder.block.0.layer.1.adapter_controller.meta_up_sampler.weight_generator.1.bias', 'encoder.block.0.layer.1.adapter_controller.meta_up_sampler.bias_generator.0.weight', 'encoder.block.0.layer.1.adapter_controller.meta_up_sampler.bias_generator.0.bias', 'encoder.block.0.layer.1.adapter_controller.meta_up_sampler.bias_generator.1.weight', 'encoder.block.0.layer.1.adapter_controller.meta_up_sampler.bias_generator.1.bias', 'encoder.block.0.layer.1.adapter_controller.meta_down_sampler.weight_generator.0.weight', 'encoder.block.0.layer.1.adapter_controller.meta_down_sampler.weight_generator.0.bias', 'encoder.block.0.layer.1.adapter_controller.meta_down_sampler.weight_generator.1.weight', 'encoder.block.0.layer.1.adapter_controller.meta_down_sampler.weight_generator.1.bias', 'encoder.block.0.layer.1.adapter_controller.meta_down_sampler.bias_generator.0.weight', 'encoder.block.0.layer.1.adapter_controller.meta_down_sampler.bias_generator.0.bias', 'encoder.block.0.layer.1.adapter_controller.meta_down_sampler.bias_generator.1.weight', 'encoder.block.0.layer.1.adapter_controller.meta_down_sampler.bias_generator.1.bias', 'encoder.block.0.layer.1.adapter_controller.post_layer_norm.weight', 'encoder.block.0.layer.1.adapter_controller.post_layer_norm.bias', 'encoder.block.1.layer.0.adapter_controller.meta_up_sampler.weight_generator.0.weight', 'encoder.block.1.layer.0.adapter_controller.meta_up_sampler.weight_generator.0.bias', 'encoder.block.1.layer.0.adapter_controller.meta_up_sampler.weight_generator.1.weight', 'encoder.block.1.layer.0.adapter_controller.meta_up_sampler.weight_generator.1.bias', 'encoder.block.1.layer.0.adapter_controller.meta_up_sampler.bias_generator.0.weight', 'encoder.block.1.layer.0.adapter_controller.meta_up_sampler.bias_generator.0.bias', 'encoder.block.1.layer.0.adapter_controller.meta_up_sampler.bias_generator.1.weight', 'encoder.block.1.layer.0.adapter_controller.meta_up_sampler.bias_generator.1.bias', 'encoder.block.1.layer.0.adapter_controller.meta_down_sampler.weight_generator.0.weight', 'encoder.block.1.layer.0.adapter_controller.meta_down_sampler.weight_generator.0.bias', 'encoder.block.1.layer.0.adapter_controller.meta_down_sampler.weight_generator.1.weight', 'encoder.block.1.layer.0.adapter_controller.meta_down_sampler.weight_generator.1.bias', 'encoder.block.1.layer.0.adapter_controller.meta_down_sampler.bias_generator.0.weight', 'encoder.block.1.layer.0.adapter_controller.meta_down_sampler.bias_generator.0.bias', 'encoder.block.1.layer.0.adapter_controller.meta_down_sampler.bias_generator.1.weight', 'encoder.block.1.layer.0.adapter_controller.meta_down_sampler.bias_generator.1.bias', 'encoder.block.1.layer.0.adapter_controller.post_layer_norm.weight', 'encoder.block.1.layer.0.adapter_controller.post_layer_norm.bias', 'encoder.block.1.layer.1.adapter_controller.meta_up_sampler.weight_generator.0.weight', 'encoder.block.1.layer.1.adapter_controller.meta_up_sampler.weight_generator.0.bias', 'encoder.block.1.layer.1.adapter_controller.meta_up_sampler.weight_generator.1.weight', 'encoder.block.1.layer.1.adapter_controller.meta_up_sampler.weight_generator.1.bias', 'encoder.block.1.layer.1.adapter_controller.meta_up_sampler.bias_generator.0.weight', 'encoder.block.1.layer.1.adapter_controller.meta_up_sampler.bias_generator.0.bias', 'encoder.block.1.layer.1.adapter_controller.meta_up_sampler.bias_generator.1.weight', 'encoder.block.1.layer.1.adapter_controller.meta_up_sampler.bias_generator.1.bias', 'encoder.block.1.layer.1.adapter_controller.meta_down_sampler.weight_generator.0.weight', 'encoder.block.1.layer.1.adapter_controller.meta_down_sampler.weight_generator.0.bias', 'encoder.block.1.layer.1.adapter_controller.meta_down_sampler.weight_generator.1.weight', 'encoder.block.1.layer.1.adapter_controller.meta_down_sampler.weight_generator.1.bias', 'encoder.block.1.layer.1.adapter_controller.meta_down_sampler.bias_generator.0.weight', 'encoder.block.1.layer.1.adapter_controller.meta_down_sampler.bias_generator.0.bias', 'encoder.block.1.layer.1.adapter_controller.meta_down_sampler.bias_generator.1.weight', 'encoder.block.1.layer.1.adapter_controller.meta_down_sampler.bias_generator.1.bias', 'encoder.block.1.layer.1.adapter_controller.post_layer_norm.weight', 'encoder.block.1.layer.1.adapter_controller.post_layer_norm.bias', 'encoder.block.2.layer.0.adapter_controller.meta_up_sampler.weight_generator.0.weight', 'encoder.block.2.layer.0.adapter_controller.meta_up_sampler.weight_generator.0.bias', 'encoder.block.2.layer.0.adapter_controller.meta_up_sampler.weight_generator.1.weight', 'encoder.block.2.layer.0.adapter_controller.meta_up_sampler.weight_generator.1.bias', 'encoder.block.2.layer.0.adapter_controller.meta_up_sampler.bias_generator.0.weight', 'encoder.block.2.layer.0.adapter_controller.meta_up_sampler.bias_generator.0.bias', 'encoder.block.2.layer.0.adapter_controller.meta_up_sampler.bias_generator.1.weight', 'encoder.block.2.layer.0.adapter_controller.meta_up_sampler.bias_generator.1.bias', 'encoder.block.2.layer.0.adapter_controller.meta_down_sampler.weight_generator.0.weight', 'encoder.block.2.layer.0.adapter_controller.meta_down_sampler.weight_generator.0.bias', 'encoder.block.2.layer.0.adapter_controller.meta_down_sampler.weight_generator.1.weight', 'encoder.block.2.layer.0.adapter_controller.meta_down_sampler.weight_generator.1.bias', 'encoder.block.2.layer.0.adapter_controller.meta_down_sampler.bias_generator.0.weight', 'encoder.block.2.layer.0.adapter_controller.meta_down_sampler.bias_generator.0.bias', 'encoder.block.2.layer.0.adapter_controller.meta_down_sampler.bias_generator.1.weight', 'encoder.block.2.layer.0.adapter_controller.meta_down_sampler.bias_generator.1.bias', 'encoder.block.2.layer.0.adapter_controller.post_layer_norm.weight', 'encoder.block.2.layer.0.adapter_controller.post_layer_norm.bias', 'encoder.block.2.layer.1.adapter_controller.meta_up_sampler.weight_generator.0.weight', 'encoder.block.2.layer.1.adapter_controller.meta_up_sampler.weight_generator.0.bias', 'encoder.block.2.layer.1.adapter_controller.meta_up_sampler.weight_generator.1.weight', 'encoder.block.2.layer.1.adapter_controller.meta_up_sampler.weight_generator.1.bias', 'encoder.block.2.layer.1.adapter_controller.meta_up_sampler.bias_generator.0.weight', 'encoder.block.2.layer.1.adapter_controller.meta_up_sampler.bias_generator.0.bias', 'encoder.block.2.layer.1.adapter_controller.meta_up_sampler.bias_generator.1.weight', 'encoder.block.2.layer.1.adapter_controller.meta_up_sampler.bias_generator.1.bias', 'encoder.block.2.layer.1.adapter_controller.meta_down_sampler.weight_generator.0.weight', 'encoder.block.2.layer.1.adapter_controller.meta_down_sampler.weight_generator.0.bias', 'encoder.block.2.layer.1.adapter_controller.meta_down_sampler.weight_generator.1.weight', 'encoder.block.2.layer.1.adapter_controller.meta_down_sampler.weight_generator.1.bias', 'encoder.block.2.layer.1.adapter_controller.meta_down_sampler.bias_generator.0.weight', 'encoder.block.2.layer.1.adapter_controller.meta_down_sampler.bias_generator.0.bias', 'encoder.block.2.layer.1.adapter_controller.meta_down_sampler.bias_generator.1.weight', 'encoder.block.2.layer.1.adapter_controller.meta_down_sampler.bias_generator.1.bias', 'encoder.block.2.layer.1.adapter_controller.post_layer_norm.weight', 'encoder.block.2.layer.1.adapter_controller.post_layer_norm.bias', 'encoder.block.3.layer.0.adapter_controller.meta_up_sampler.weight_generator.0.weight', 'encoder.block.3.layer.0.adapter_controller.meta_up_sampler.weight_generator.0.bias', 'encoder.block.3.layer.0.adapter_controller.meta_up_sampler.weight_generator.1.weight', 'encoder.block.3.layer.0.adapter_controller.meta_up_sampler.weight_generator.1.bias', 'encoder.block.3.layer.0.adapter_controller.meta_up_sampler.bias_generator.0.weight', 'encoder.block.3.layer.0.adapter_controller.meta_up_sampler.bias_generator.0.bias', 'encoder.block.3.layer.0.adapter_controller.meta_up_sampler.bias_generator.1.weight', 'encoder.block.3.layer.0.adapter_controller.meta_up_sampler.bias_generator.1.bias', 'encoder.block.3.layer.0.adapter_controller.meta_down_sampler.weight_generator.0.weight', 'encoder.block.3.layer.0.adapter_controller.meta_down_sampler.weight_generator.0.bias', 'encoder.block.3.layer.0.adapter_controller.meta_down_sampler.weight_generator.1.weight', 'encoder.block.3.layer.0.adapter_controller.meta_down_sampler.weight_generator.1.bias', 'encoder.block.3.layer.0.adapter_controller.meta_down_sampler.bias_generator.0.weight', 'encoder.block.3.layer.0.adapter_controller.meta_down_sampler.bias_generator.0.bias', 'encoder.block.3.layer.0.adapter_controller.meta_down_sampler.bias_generator.1.weight', 'encoder.block.3.layer.0.adapter_controller.meta_down_sampler.bias_generator.1.bias', 'encoder.block.3.layer.0.adapter_controller.post_layer_norm.weight', 'encoder.block.3.layer.0.adapter_controller.post_layer_norm.bias', 'encoder.block.3.layer.1.adapter_controller.meta_up_sampler.weight_generator.0.weight', 'encoder.block.3.layer.1.adapter_controller.meta_up_sampler.weight_generator.0.bias', 'encoder.block.3.layer.1.adapter_controller.meta_up_sampler.weight_generator.1.weight', 'encoder.block.3.layer.1.adapter_controller.meta_up_sampler.weight_generator.1.bias', 'encoder.block.3.layer.1.adapter_controller.meta_up_sampler.bias_generator.0.weight', 'encoder.block.3.layer.1.adapter_controller.meta_up_sampler.bias_generator.0.bias', 'encoder.block.3.layer.1.adapter_controller.meta_up_sampler.bias_generator.1.weight', 'encoder.block.3.layer.1.adapter_controller.meta_up_sampler.bias_generator.1.bias', 'encoder.block.3.layer.1.adapter_controller.meta_down_sampler.weight_generator.0.weight', 'encoder.block.3.layer.1.adapter_controller.meta_down_sampler.weight_generator.0.bias', 'encoder.block.3.layer.1.adapter_controller.meta_down_sampler.weight_generator.1.weight', 'encoder.block.3.layer.1.adapter_controller.meta_down_sampler.weight_generator.1.bias', 'encoder.block.3.layer.1.adapter_controller.meta_down_sampler.bias_generator.0.weight', 'encoder.block.3.layer.1.adapter_controller.meta_down_sampler.bias_generator.0.bias', 'encoder.block.3.layer.1.adapter_controller.meta_down_sampler.bias_generator.1.weight', 'encoder.block.3.layer.1.adapter_controller.meta_down_sampler.bias_generator.1.bias', 'encoder.block.3.layer.1.adapter_controller.post_layer_norm.weight', 'encoder.block.3.layer.1.adapter_controller.post_layer_norm.bias', 'encoder.block.4.layer.0.adapter_controller.meta_up_sampler.weight_generator.0.weight', 'encoder.block.4.layer.0.adapter_controller.meta_up_sampler.weight_generator.0.bias', 'encoder.block.4.layer.0.adapter_controller.meta_up_sampler.weight_generator.1.weight', 'encoder.block.4.layer.0.adapter_controller.meta_up_sampler.weight_generator.1.bias', 'encoder.block.4.layer.0.adapter_controller.meta_up_sampler.bias_generator.0.weight', 'encoder.block.4.layer.0.adapter_controller.meta_up_sampler.bias_generator.0.bias', 'encoder.block.4.layer.0.adapter_controller.meta_up_sampler.bias_generator.1.weight', 'encoder.block.4.layer.0.adapter_controller.meta_up_sampler.bias_generator.1.bias', 'encoder.block.4.layer.0.adapter_controller.meta_down_sampler.weight_generator.0.weight', 'encoder.block.4.layer.0.adapter_controller.meta_down_sampler.weight_generator.0.bias', 'encoder.block.4.layer.0.adapter_controller.meta_down_sampler.weight_generator.1.weight', 'encoder.block.4.layer.0.adapter_controller.meta_down_sampler.weight_generator.1.bias', 'encoder.block.4.layer.0.adapter_controller.meta_down_sampler.bias_generator.0.weight', 'encoder.block.4.layer.0.adapter_controller.meta_down_sampler.bias_generator.0.bias', 'encoder.block.4.layer.0.adapter_controller.meta_down_sampler.bias_generator.1.weight', 'encoder.block.4.layer.0.adapter_controller.meta_down_sampler.bias_generator.1.bias', 'encoder.block.4.layer.0.adapter_controller.post_layer_norm.weight', 'encoder.block.4.layer.0.adapter_controller.post_layer_norm.bias', 'encoder.block.4.layer.1.adapter_controller.meta_up_sampler.weight_generator.0.weight', 'encoder.block.4.layer.1.adapter_controller.meta_up_sampler.weight_generator.0.bias', 'encoder.block.4.layer.1.adapter_controller.meta_up_sampler.weight_generator.1.weight', 'encoder.block.4.layer.1.adapter_controller.meta_up_sampler.weight_generator.1.bias', 'encoder.block.4.layer.1.adapter_controller.meta_up_sampler.bias_generator.0.weight', 'encoder.block.4.layer.1.adapter_controller.meta_up_sampler.bias_generator.0.bias', 'encoder.block.4.layer.1.adapter_controller.meta_up_sampler.bias_generator.1.weight', 'encoder.block.4.layer.1.adapter_controller.meta_up_sampler.bias_generator.1.bias', 'encoder.block.4.layer.1.adapter_controller.meta_down_sampler.weight_generator.0.weight', 'encoder.block.4.layer.1.adapter_controller.meta_down_sampler.weight_generator.0.bias', 'encoder.block.4.layer.1.adapter_controller.meta_down_sampler.weight_generator.1.weight', 'encoder.block.4.layer.1.adapter_controller.meta_down_sampler.weight_generator.1.bias', 'encoder.block.4.layer.1.adapter_controller.meta_down_sampler.bias_generator.0.weight', 'encoder.block.4.layer.1.adapter_controller.meta_down_sampler.bias_generator.0.bias', 'encoder.block.4.layer.1.adapter_controller.meta_down_sampler.bias_generator.1.weight', 'encoder.block.4.layer.1.adapter_controller.meta_down_sampler.bias_generator.1.bias', 'encoder.block.4.layer.1.adapter_controller.post_layer_norm.weight', 'encoder.block.4.layer.1.adapter_controller.post_layer_norm.bias', 'encoder.block.5.layer.0.adapter_controller.meta_up_sampler.weight_generator.0.weight', 'encoder.block.5.layer.0.adapter_controller.meta_up_sampler.weight_generator.0.bias', 'encoder.block.5.layer.0.adapter_controller.meta_up_sampler.weight_generator.1.weight', 'encoder.block.5.layer.0.adapter_controller.meta_up_sampler.weight_generator.1.bias', 'encoder.block.5.layer.0.adapter_controller.meta_up_sampler.bias_generator.0.weight', 'encoder.block.5.layer.0.adapter_controller.meta_up_sampler.bias_generator.0.bias', 'encoder.block.5.layer.0.adapter_controller.meta_up_sampler.bias_generator.1.weight', 'encoder.block.5.layer.0.adapter_controller.meta_up_sampler.bias_generator.1.bias', 'encoder.block.5.layer.0.adapter_controller.meta_down_sampler.weight_generator.0.weight', 'encoder.block.5.layer.0.adapter_controller.meta_down_sampler.weight_generator.0.bias', 'encoder.block.5.layer.0.adapter_controller.meta_down_sampler.weight_generator.1.weight', 'encoder.block.5.layer.0.adapter_controller.meta_down_sampler.weight_generator.1.bias', 'encoder.block.5.layer.0.adapter_controller.meta_down_sampler.bias_generator.0.weight', 'encoder.block.5.layer.0.adapter_controller.meta_down_sampler.bias_generator.0.bias', 'encoder.block.5.layer.0.adapter_controller.meta_down_sampler.bias_generator.1.weight', 'encoder.block.5.layer.0.adapter_controller.meta_down_sampler.bias_generator.1.bias', 'encoder.block.5.layer.0.adapter_controller.post_layer_norm.weight', 'encoder.block.5.layer.0.adapter_controller.post_layer_norm.bias', 'encoder.block.5.layer.1.adapter_controller.meta_up_sampler.weight_generator.0.weight', 'encoder.block.5.layer.1.adapter_controller.meta_up_sampler.weight_generator.0.bias', 'encoder.block.5.layer.1.adapter_controller.meta_up_sampler.weight_generator.1.weight', 'encoder.block.5.layer.1.adapter_controller.meta_up_sampler.weight_generator.1.bias', 'encoder.block.5.layer.1.adapter_controller.meta_up_sampler.bias_generator.0.weight', 'encoder.block.5.layer.1.adapter_controller.meta_up_sampler.bias_generator.0.bias', 'encoder.block.5.layer.1.adapter_controller.meta_up_sampler.bias_generator.1.weight', 'encoder.block.5.layer.1.adapter_controller.meta_up_sampler.bias_generator.1.bias', 'encoder.block.5.layer.1.adapter_controller.meta_down_sampler.weight_generator.0.weight', 'encoder.block.5.layer.1.adapter_controller.meta_down_sampler.weight_generator.0.bias', 'encoder.block.5.layer.1.adapter_controller.meta_down_sampler.weight_generator.1.weight', 'encoder.block.5.layer.1.adapter_controller.meta_down_sampler.weight_generator.1.bias', 'encoder.block.5.layer.1.adapter_controller.meta_down_sampler.bias_generator.0.weight', 'encoder.block.5.layer.1.adapter_controller.meta_down_sampler.bias_generator.0.bias', 'encoder.block.5.layer.1.adapter_controller.meta_down_sampler.bias_generator.1.weight', 'encoder.block.5.layer.1.adapter_controller.meta_down_sampler.bias_generator.1.bias', 'encoder.block.5.layer.1.adapter_controller.post_layer_norm.weight', 'encoder.block.5.layer.1.adapter_controller.post_layer_norm.bias', 'decoder.block.0.layer.0.adapter_controller.meta_up_sampler.weight_generator.0.weight', 'decoder.block.0.layer.0.adapter_controller.meta_up_sampler.weight_generator.0.bias', 'decoder.block.0.layer.0.adapter_controller.meta_up_sampler.weight_generator.1.weight', 'decoder.block.0.layer.0.adapter_controller.meta_up_sampler.weight_generator.1.bias', 'decoder.block.0.layer.0.adapter_controller.meta_up_sampler.bias_generator.0.weight', 'decoder.block.0.layer.0.adapter_controller.meta_up_sampler.bias_generator.0.bias', 'decoder.block.0.layer.0.adapter_controller.meta_up_sampler.bias_generator.1.weight', 'decoder.block.0.layer.0.adapter_controller.meta_up_sampler.bias_generator.1.bias', 'decoder.block.0.layer.0.adapter_controller.meta_down_sampler.weight_generator.0.weight', 'decoder.block.0.layer.0.adapter_controller.meta_down_sampler.weight_generator.0.bias', 'decoder.block.0.layer.0.adapter_controller.meta_down_sampler.weight_generator.1.weight', 'decoder.block.0.layer.0.adapter_controller.meta_down_sampler.weight_generator.1.bias', 'decoder.block.0.layer.0.adapter_controller.meta_down_sampler.bias_generator.0.weight', 'decoder.block.0.layer.0.adapter_controller.meta_down_sampler.bias_generator.0.bias', 'decoder.block.0.layer.0.adapter_controller.meta_down_sampler.bias_generator.1.weight', 'decoder.block.0.layer.0.adapter_controller.meta_down_sampler.bias_generator.1.bias', 'decoder.block.0.layer.0.adapter_controller.post_layer_norm.weight', 'decoder.block.0.layer.0.adapter_controller.post_layer_norm.bias', 'decoder.block.0.layer.2.adapter_controller.meta_up_sampler.weight_generator.0.weight', 'decoder.block.0.layer.2.adapter_controller.meta_up_sampler.weight_generator.0.bias', 'decoder.block.0.layer.2.adapter_controller.meta_up_sampler.weight_generator.1.weight', 'decoder.block.0.layer.2.adapter_controller.meta_up_sampler.weight_generator.1.bias', 'decoder.block.0.layer.2.adapter_controller.meta_up_sampler.bias_generator.0.weight', 'decoder.block.0.layer.2.adapter_controller.meta_up_sampler.bias_generator.0.bias', 'decoder.block.0.layer.2.adapter_controller.meta_up_sampler.bias_generator.1.weight', 'decoder.block.0.layer.2.adapter_controller.meta_up_sampler.bias_generator.1.bias', 'decoder.block.0.layer.2.adapter_controller.meta_down_sampler.weight_generator.0.weight', 'decoder.block.0.layer.2.adapter_controller.meta_down_sampler.weight_generator.0.bias', 'decoder.block.0.layer.2.adapter_controller.meta_down_sampler.weight_generator.1.weight', 'decoder.block.0.layer.2.adapter_controller.meta_down_sampler.weight_generator.1.bias', 'decoder.block.0.layer.2.adapter_controller.meta_down_sampler.bias_generator.0.weight', 'decoder.block.0.layer.2.adapter_controller.meta_down_sampler.bias_generator.0.bias', 'decoder.block.0.layer.2.adapter_controller.meta_down_sampler.bias_generator.1.weight', 'decoder.block.0.layer.2.adapter_controller.meta_down_sampler.bias_generator.1.bias', 'decoder.block.0.layer.2.adapter_controller.post_layer_norm.weight', 'decoder.block.0.layer.2.adapter_controller.post_layer_norm.bias', 'decoder.block.1.layer.0.adapter_controller.meta_up_sampler.weight_generator.0.weight', 'decoder.block.1.layer.0.adapter_controller.meta_up_sampler.weight_generator.0.bias', 'decoder.block.1.layer.0.adapter_controller.meta_up_sampler.weight_generator.1.weight', 'decoder.block.1.layer.0.adapter_controller.meta_up_sampler.weight_generator.1.bias', 'decoder.block.1.layer.0.adapter_controller.meta_up_sampler.bias_generator.0.weight', 'decoder.block.1.layer.0.adapter_controller.meta_up_sampler.bias_generator.0.bias', 'decoder.block.1.layer.0.adapter_controller.meta_up_sampler.bias_generator.1.weight', 'decoder.block.1.layer.0.adapter_controller.meta_up_sampler.bias_generator.1.bias', 'decoder.block.1.layer.0.adapter_controller.meta_down_sampler.weight_generator.0.weight', 'decoder.block.1.layer.0.adapter_controller.meta_down_sampler.weight_generator.0.bias', 'decoder.block.1.layer.0.adapter_controller.meta_down_sampler.weight_generator.1.weight', 'decoder.block.1.layer.0.adapter_controller.meta_down_sampler.weight_generator.1.bias', 'decoder.block.1.layer.0.adapter_controller.meta_down_sampler.bias_generator.0.weight', 'decoder.block.1.layer.0.adapter_controller.meta_down_sampler.bias_generator.0.bias', 'decoder.block.1.layer.0.adapter_controller.meta_down_sampler.bias_generator.1.weight', 'decoder.block.1.layer.0.adapter_controller.meta_down_sampler.bias_generator.1.bias', 'decoder.block.1.layer.0.adapter_controller.post_layer_norm.weight', 'decoder.block.1.layer.0.adapter_controller.post_layer_norm.bias', 'decoder.block.1.layer.2.adapter_controller.meta_up_sampler.weight_generator.0.weight', 'decoder.block.1.layer.2.adapter_controller.meta_up_sampler.weight_generator.0.bias', 'decoder.block.1.layer.2.adapter_controller.meta_up_sampler.weight_generator.1.weight', 'decoder.block.1.layer.2.adapter_controller.meta_up_sampler.weight_generator.1.bias', 'decoder.block.1.layer.2.adapter_controller.meta_up_sampler.bias_generator.0.weight', 'decoder.block.1.layer.2.adapter_controller.meta_up_sampler.bias_generator.0.bias', 'decoder.block.1.layer.2.adapter_controller.meta_up_sampler.bias_generator.1.weight', 'decoder.block.1.layer.2.adapter_controller.meta_up_sampler.bias_generator.1.bias', 'decoder.block.1.layer.2.adapter_controller.meta_down_sampler.weight_generator.0.weight', 'decoder.block.1.layer.2.adapter_controller.meta_down_sampler.weight_generator.0.bias', 'decoder.block.1.layer.2.adapter_controller.meta_down_sampler.weight_generator.1.weight', 'decoder.block.1.layer.2.adapter_controller.meta_down_sampler.weight_generator.1.bias', 'decoder.block.1.layer.2.adapter_controller.meta_down_sampler.bias_generator.0.weight', 'decoder.block.1.layer.2.adapter_controller.meta_down_sampler.bias_generator.0.bias', 'decoder.block.1.layer.2.adapter_controller.meta_down_sampler.bias_generator.1.weight', 'decoder.block.1.layer.2.adapter_controller.meta_down_sampler.bias_generator.1.bias', 'decoder.block.1.layer.2.adapter_controller.post_layer_norm.weight', 'decoder.block.1.layer.2.adapter_controller.post_layer_norm.bias', 'decoder.block.2.layer.0.adapter_controller.meta_up_sampler.weight_generator.0.weight', 'decoder.block.2.layer.0.adapter_controller.meta_up_sampler.weight_generator.0.bias', 'decoder.block.2.layer.0.adapter_controller.meta_up_sampler.weight_generator.1.weight', 'decoder.block.2.layer.0.adapter_controller.meta_up_sampler.weight_generator.1.bias', 'decoder.block.2.layer.0.adapter_controller.meta_up_sampler.bias_generator.0.weight', 'decoder.block.2.layer.0.adapter_controller.meta_up_sampler.bias_generator.0.bias', 'decoder.block.2.layer.0.adapter_controller.meta_up_sampler.bias_generator.1.weight', 'decoder.block.2.layer.0.adapter_controller.meta_up_sampler.bias_generator.1.bias', 'decoder.block.2.layer.0.adapter_controller.meta_down_sampler.weight_generator.0.weight', 'decoder.block.2.layer.0.adapter_controller.meta_down_sampler.weight_generator.0.bias', 'decoder.block.2.layer.0.adapter_controller.meta_down_sampler.weight_generator.1.weight', 'decoder.block.2.layer.0.adapter_controller.meta_down_sampler.weight_generator.1.bias', 'decoder.block.2.layer.0.adapter_controller.meta_down_sampler.bias_generator.0.weight', 'decoder.block.2.layer.0.adapter_controller.meta_down_sampler.bias_generator.0.bias', 'decoder.block.2.layer.0.adapter_controller.meta_down_sampler.bias_generator.1.weight', 'decoder.block.2.layer.0.adapter_controller.meta_down_sampler.bias_generator.1.bias', 'decoder.block.2.layer.0.adapter_controller.post_layer_norm.weight', 'decoder.block.2.layer.0.adapter_controller.post_layer_norm.bias', 'decoder.block.2.layer.2.adapter_controller.meta_up_sampler.weight_generator.0.weight', 'decoder.block.2.layer.2.adapter_controller.meta_up_sampler.weight_generator.0.bias', 'decoder.block.2.layer.2.adapter_controller.meta_up_sampler.weight_generator.1.weight', 'decoder.block.2.layer.2.adapter_controller.meta_up_sampler.weight_generator.1.bias', 'decoder.block.2.layer.2.adapter_controller.meta_up_sampler.bias_generator.0.weight', 'decoder.block.2.layer.2.adapter_controller.meta_up_sampler.bias_generator.0.bias', 'decoder.block.2.layer.2.adapter_controller.meta_up_sampler.bias_generator.1.weight', 'decoder.block.2.layer.2.adapter_controller.meta_up_sampler.bias_generator.1.bias', 'decoder.block.2.layer.2.adapter_controller.meta_down_sampler.weight_generator.0.weight', 'decoder.block.2.layer.2.adapter_controller.meta_down_sampler.weight_generator.0.bias', 'decoder.block.2.layer.2.adapter_controller.meta_down_sampler.weight_generator.1.weight', 'decoder.block.2.layer.2.adapter_controller.meta_down_sampler.weight_generator.1.bias', 'decoder.block.2.layer.2.adapter_controller.meta_down_sampler.bias_generator.0.weight', 'decoder.block.2.layer.2.adapter_controller.meta_down_sampler.bias_generator.0.bias', 'decoder.block.2.layer.2.adapter_controller.meta_down_sampler.bias_generator.1.weight', 'decoder.block.2.layer.2.adapter_controller.meta_down_sampler.bias_generator.1.bias', 'decoder.block.2.layer.2.adapter_controller.post_layer_norm.weight', 'decoder.block.2.layer.2.adapter_controller.post_layer_norm.bias', 'decoder.block.3.layer.0.adapter_controller.meta_up_sampler.weight_generator.0.weight', 'decoder.block.3.layer.0.adapter_controller.meta_up_sampler.weight_generator.0.bias', 'decoder.block.3.layer.0.adapter_controller.meta_up_sampler.weight_generator.1.weight', 'decoder.block.3.layer.0.adapter_controller.meta_up_sampler.weight_generator.1.bias', 'decoder.block.3.layer.0.adapter_controller.meta_up_sampler.bias_generator.0.weight', 'decoder.block.3.layer.0.adapter_controller.meta_up_sampler.bias_generator.0.bias', 'decoder.block.3.layer.0.adapter_controller.meta_up_sampler.bias_generator.1.weight', 'decoder.block.3.layer.0.adapter_controller.meta_up_sampler.bias_generator.1.bias', 'decoder.block.3.layer.0.adapter_controller.meta_down_sampler.weight_generator.0.weight', 'decoder.block.3.layer.0.adapter_controller.meta_down_sampler.weight_generator.0.bias', 'decoder.block.3.layer.0.adapter_controller.meta_down_sampler.weight_generator.1.weight', 'decoder.block.3.layer.0.adapter_controller.meta_down_sampler.weight_generator.1.bias', 'decoder.block.3.layer.0.adapter_controller.meta_down_sampler.bias_generator.0.weight', 'decoder.block.3.layer.0.adapter_controller.meta_down_sampler.bias_generator.0.bias', 'decoder.block.3.layer.0.adapter_controller.meta_down_sampler.bias_generator.1.weight', 'decoder.block.3.layer.0.adapter_controller.meta_down_sampler.bias_generator.1.bias', 'decoder.block.3.layer.0.adapter_controller.post_layer_norm.weight', 'decoder.block.3.layer.0.adapter_controller.post_layer_norm.bias', 'decoder.block.3.layer.2.adapter_controller.meta_up_sampler.weight_generator.0.weight', 'decoder.block.3.layer.2.adapter_controller.meta_up_sampler.weight_generator.0.bias', 'decoder.block.3.layer.2.adapter_controller.meta_up_sampler.weight_generator.1.weight', 'decoder.block.3.layer.2.adapter_controller.meta_up_sampler.weight_generator.1.bias', 'decoder.block.3.layer.2.adapter_controller.meta_up_sampler.bias_generator.0.weight', 'decoder.block.3.layer.2.adapter_controller.meta_up_sampler.bias_generator.0.bias', 'decoder.block.3.layer.2.adapter_controller.meta_up_sampler.bias_generator.1.weight', 'decoder.block.3.layer.2.adapter_controller.meta_up_sampler.bias_generator.1.bias', 'decoder.block.3.layer.2.adapter_controller.meta_down_sampler.weight_generator.0.weight', 'decoder.block.3.layer.2.adapter_controller.meta_down_sampler.weight_generator.0.bias', 'decoder.block.3.layer.2.adapter_controller.meta_down_sampler.weight_generator.1.weight', 'decoder.block.3.layer.2.adapter_controller.meta_down_sampler.weight_generator.1.bias', 'decoder.block.3.layer.2.adapter_controller.meta_down_sampler.bias_generator.0.weight', 'decoder.block.3.layer.2.adapter_controller.meta_down_sampler.bias_generator.0.bias', 'decoder.block.3.layer.2.adapter_controller.meta_down_sampler.bias_generator.1.weight', 'decoder.block.3.layer.2.adapter_controller.meta_down_sampler.bias_generator.1.bias', 'decoder.block.3.layer.2.adapter_controller.post_layer_norm.weight', 'decoder.block.3.layer.2.adapter_controller.post_layer_norm.bias', 'decoder.block.4.layer.0.adapter_controller.meta_up_sampler.weight_generator.0.weight', 'decoder.block.4.layer.0.adapter_controller.meta_up_sampler.weight_generator.0.bias', 'decoder.block.4.layer.0.adapter_controller.meta_up_sampler.weight_generator.1.weight', 'decoder.block.4.layer.0.adapter_controller.meta_up_sampler.weight_generator.1.bias', 'decoder.block.4.layer.0.adapter_controller.meta_up_sampler.bias_generator.0.weight', 'decoder.block.4.layer.0.adapter_controller.meta_up_sampler.bias_generator.0.bias', 'decoder.block.4.layer.0.adapter_controller.meta_up_sampler.bias_generator.1.weight', 'decoder.block.4.layer.0.adapter_controller.meta_up_sampler.bias_generator.1.bias', 'decoder.block.4.layer.0.adapter_controller.meta_down_sampler.weight_generator.0.weight', 'decoder.block.4.layer.0.adapter_controller.meta_down_sampler.weight_generator.0.bias', 'decoder.block.4.layer.0.adapter_controller.meta_down_sampler.weight_generator.1.weight', 'decoder.block.4.layer.0.adapter_controller.meta_down_sampler.weight_generator.1.bias', 'decoder.block.4.layer.0.adapter_controller.meta_down_sampler.bias_generator.0.weight', 'decoder.block.4.layer.0.adapter_controller.meta_down_sampler.bias_generator.0.bias', 'decoder.block.4.layer.0.adapter_controller.meta_down_sampler.bias_generator.1.weight', 'decoder.block.4.layer.0.adapter_controller.meta_down_sampler.bias_generator.1.bias', 'decoder.block.4.layer.0.adapter_controller.post_layer_norm.weight', 'decoder.block.4.layer.0.adapter_controller.post_layer_norm.bias', 'decoder.block.4.layer.2.adapter_controller.meta_up_sampler.weight_generator.0.weight', 'decoder.block.4.layer.2.adapter_controller.meta_up_sampler.weight_generator.0.bias', 'decoder.block.4.layer.2.adapter_controller.meta_up_sampler.weight_generator.1.weight', 'decoder.block.4.layer.2.adapter_controller.meta_up_sampler.weight_generator.1.bias', 'decoder.block.4.layer.2.adapter_controller.meta_up_sampler.bias_generator.0.weight', 'decoder.block.4.layer.2.adapter_controller.meta_up_sampler.bias_generator.0.bias', 'decoder.block.4.layer.2.adapter_controller.meta_up_sampler.bias_generator.1.weight', 'decoder.block.4.layer.2.adapter_controller.meta_up_sampler.bias_generator.1.bias', 'decoder.block.4.layer.2.adapter_controller.meta_down_sampler.weight_generator.0.weight', 'decoder.block.4.layer.2.adapter_controller.meta_down_sampler.weight_generator.0.bias', 'decoder.block.4.layer.2.adapter_controller.meta_down_sampler.weight_generator.1.weight', 'decoder.block.4.layer.2.adapter_controller.meta_down_sampler.weight_generator.1.bias', 'decoder.block.4.layer.2.adapter_controller.meta_down_sampler.bias_generator.0.weight', 'decoder.block.4.layer.2.adapter_controller.meta_down_sampler.bias_generator.0.bias', 'decoder.block.4.layer.2.adapter_controller.meta_down_sampler.bias_generator.1.weight', 'decoder.block.4.layer.2.adapter_controller.meta_down_sampler.bias_generator.1.bias', 'decoder.block.4.layer.2.adapter_controller.post_layer_norm.weight', 'decoder.block.4.layer.2.adapter_controller.post_layer_norm.bias', 'decoder.block.5.layer.0.adapter_controller.meta_up_sampler.weight_generator.0.weight', 'decoder.block.5.layer.0.adapter_controller.meta_up_sampler.weight_generator.0.bias', 'decoder.block.5.layer.0.adapter_controller.meta_up_sampler.weight_generator.1.weight', 'decoder.block.5.layer.0.adapter_controller.meta_up_sampler.weight_generator.1.bias', 'decoder.block.5.layer.0.adapter_controller.meta_up_sampler.bias_generator.0.weight', 'decoder.block.5.layer.0.adapter_controller.meta_up_sampler.bias_generator.0.bias', 'decoder.block.5.layer.0.adapter_controller.meta_up_sampler.bias_generator.1.weight', 'decoder.block.5.layer.0.adapter_controller.meta_up_sampler.bias_generator.1.bias', 'decoder.block.5.layer.0.adapter_controller.meta_down_sampler.weight_generator.0.weight', 'decoder.block.5.layer.0.adapter_controller.meta_down_sampler.weight_generator.0.bias', 'decoder.block.5.layer.0.adapter_controller.meta_down_sampler.weight_generator.1.weight', 'decoder.block.5.layer.0.adapter_controller.meta_down_sampler.weight_generator.1.bias', 'decoder.block.5.layer.0.adapter_controller.meta_down_sampler.bias_generator.0.weight', 'decoder.block.5.layer.0.adapter_controller.meta_down_sampler.bias_generator.0.bias', 'decoder.block.5.layer.0.adapter_controller.meta_down_sampler.bias_generator.1.weight', 'decoder.block.5.layer.0.adapter_controller.meta_down_sampler.bias_generator.1.bias', 'decoder.block.5.layer.0.adapter_controller.post_layer_norm.weight', 'decoder.block.5.layer.0.adapter_controller.post_layer_norm.bias', 'decoder.block.5.layer.2.adapter_controller.meta_up_sampler.weight_generator.0.weight', 'decoder.block.5.layer.2.adapter_controller.meta_up_sampler.weight_generator.0.bias', 'decoder.block.5.layer.2.adapter_controller.meta_up_sampler.weight_generator.1.weight', 'decoder.block.5.layer.2.adapter_controller.meta_up_sampler.weight_generator.1.bias', 'decoder.block.5.layer.2.adapter_controller.meta_up_sampler.bias_generator.0.weight', 'decoder.block.5.layer.2.adapter_controller.meta_up_sampler.bias_generator.0.bias', 'decoder.block.5.layer.2.adapter_controller.meta_up_sampler.bias_generator.1.weight', 'decoder.block.5.layer.2.adapter_controller.meta_up_sampler.bias_generator.1.bias', 'decoder.block.5.layer.2.adapter_controller.meta_down_sampler.weight_generator.0.weight', 'decoder.block.5.layer.2.adapter_controller.meta_down_sampler.weight_generator.0.bias', 'decoder.block.5.layer.2.adapter_controller.meta_down_sampler.weight_generator.1.weight', 'decoder.block.5.layer.2.adapter_controller.meta_down_sampler.weight_generator.1.bias', 'decoder.block.5.layer.2.adapter_controller.meta_down_sampler.bias_generator.0.weight', 'decoder.block.5.layer.2.adapter_controller.meta_down_sampler.bias_generator.0.bias', 'decoder.block.5.layer.2.adapter_controller.meta_down_sampler.bias_generator.1.weight', 'decoder.block.5.layer.2.adapter_controller.meta_down_sampler.bias_generator.1.bias', 'decoder.block.5.layer.2.adapter_controller.post_layer_norm.weight', 'decoder.block.5.layer.2.adapter_controller.post_layer_norm.bias']\r\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\ncahce dir  /idiap/temp/rkarimi/cache_home_1/datasets\r\ncahce dir  /idiap/temp/rkarimi/cache_home_1/datasets\r\n12/12/2020 15:38:44 - INFO - filelock -   Lock 140079090376272 acquired on /idiap/home/rkarimi/.cache/huggingface/datasets/4c7b1146606607c193d1ef601d8d0c134521b2ac59f61ee98c09119be925ee16.7ad892de9d7f1b4f9dfc598ef31e4a398a7224176bc9a3110e0e2075ff943e8f.py.lock\r\n12/12/2020 15:38:44 - INFO - filelock -   Lock 140079090376272 released on /idiap/home/rkarimi/.cache/huggingface/datasets/4c7b1146606607c193d1ef601d8d0c134521b2ac59f61ee98c09119be925ee16.7ad892de9d7f1b4f9dfc598ef31e4a398a7224176bc9a3110e0e2075ff943e8f.py.lock\r\nUsing custom data configuration default\r\n12/12/2020 15:38:44 - INFO - filelock -   Lock 140082549312272 acquired on /idiap/temp/rkarimi/cache_home_1/datasets/_idiap_temp_rkarimi_cache_home_1_datasets_boolq_default_0.1.0_1fcfdc6f36dc89a2245ffbbd5248ab33890594b50396731ebc78411bdd2ca534.lock\r\n12/12/2020 15:38:44 - INFO - filelock -   Lock 140082549312272 released on /idiap/temp/rkarimi/cache_home_1/datasets/_idiap_temp_rkarimi_cache_home_1_datasets_boolq_default_0.1.0_1fcfdc6f36dc89a2245ffbbd5248ab33890594b50396731ebc78411bdd2ca534.lock\r\n12/12/2020 15:38:44 - INFO - filelock -   Lock 140082549365648 acquired on /idiap/temp/rkarimi/cache_home_1/datasets/_idiap_temp_rkarimi_cache_home_1_datasets_boolq_default_0.1.0_1fcfdc6f36dc89a2245ffbbd5248ab33890594b50396731ebc78411bdd2ca534.lock\r\nReusing dataset boolq (/idiap/temp/rkarimi/cache_home_1/datasets/boolq/default/0.1.0/1fcfdc6f36dc89a2245ffbbd5248ab33890594b50396731ebc78411bdd2ca534)\r\n12/12/2020 15:38:44 - INFO - filelock -   Lock 140082549365648 released on /idiap/temp/rkarimi/cache_home_1/datasets/_idiap_temp_rkarimi_cache_home_1_datasets_boolq_default_0.1.0_1fcfdc6f36dc89a2245ffbbd5248ab33890594b50396731ebc78411bdd2ca534.lock\r\nLoading cached processed dataset at /idiap/temp/rkarimi/cache_home_1/datasets/boolq/default/0.1.0/1fcfdc6f36dc89a2245ffbbd5248ab33890594b50396731ebc78411bdd2ca534/cache-6810ece2a440c3be.arrow\r\ncahce dir  /idiap/temp/rkarimi/cache_home_1/datasets\r\ncahce dir  /idiap/temp/rkarimi/cache_home_1/datasets\r\n12/12/2020 15:38:45 - INFO - filelock -   Lock 140082549560848 acquired on /idiap/home/rkarimi/.cache/huggingface/datasets/4c7b1146606607c193d1ef601d8d0c134521b2ac59f61ee98c09119be925ee16.7ad892de9d7f1b4f9dfc598ef31e4a398a7224176bc9a3110e0e2075ff943e8f.py.lock\r\n12/12/2020 15:38:45 - INFO - filelock -   Lock 140082549560848 released on /idiap/home/rkarimi/.cache/huggingface/datasets/4c7b1146606607c193d1ef601d8d0c134521b2ac59f61ee98c09119be925ee16.7ad892de9d7f1b4f9dfc598ef31e4a398a7224176bc9a3110e0e2075ff943e8f.py.lock\r\nUsing custom data configuration default\r\n12/12/2020 15:38:45 - INFO - filelock -   Lock 140082549560848 acquired on /idiap/temp/rkarimi/cache_home_1/datasets/_idiap_temp_rkarimi_cache_home_1_datasets_boolq_default_0.1.0_1fcfdc6f36dc89a2245ffbbd5248ab33890594b50396731ebc78411bdd2ca534.lock\r\n12/12/2020 15:38:45 - INFO - filelock -   Lock 140082549560848 released on /idiap/temp/rkarimi/cache_home_1/datasets/_idiap_temp_rkarimi_cache_home_1_datasets_boolq_default_0.1.0_1fcfdc6f36dc89a2245ffbbd5248ab33890594b50396731ebc78411bdd2ca534.lock\r\n12/12/2020 15:38:45 - INFO - filelock -   Lock 140082549365200 acquired on /idiap/temp/rkarimi/cache_home_1/datasets/_idiap_temp_rkarimi_cache_home_1_datasets_boolq_default_0.1.0_1fcfdc6f36dc89a2245ffbbd5248ab33890594b50396731ebc78411bdd2ca534.lock\r\nReusing dataset boolq (/idiap/temp/rkarimi/cache_home_1/datasets/boolq/default/0.1.0/1fcfdc6f36dc89a2245ffbbd5248ab33890594b50396731ebc78411bdd2ca534)\r\n12/12/2020 15:38:45 - INFO - filelock -   Lock 140082549365200 released on /idiap/temp/rkarimi/cache_home_1/datasets/_idiap_temp_rkarimi_cache_home_1_datasets_boolq_default_0.1.0_1fcfdc6f36dc89a2245ffbbd5248ab33890594b50396731ebc78411bdd2ca534.lock\r\nLoading cached processed dataset at /idiap/temp/rkarimi/cache_home_1/datasets/boolq/default/0.1.0/1fcfdc6f36dc89a2245ffbbd5248ab33890594b50396731ebc78411bdd2ca534/cache-9a2822394a3a4e34.arrow\r\n12/12/2020 15:38:45 - INFO - seq2seq.metrics.metrics -   selected metric <function build_compute_metrics_fn.<locals>.classification_metrics at 0x7f66b464cc20> for task boolq\r\n12/12/2020 15:38:45 - INFO - seq2seq.trainers.trainer -   ***** Running training *****\r\n12/12/2020 15:38:45 - INFO - seq2seq.trainers.trainer -     Num examples = 10\r\n12/12/2020 15:38:45 - INFO - seq2seq.trainers.trainer -     Num Epochs = 2\r\n12/12/2020 15:38:45 - INFO - seq2seq.trainers.trainer -     Instantaneous batch size per device = 64\r\n12/12/2020 15:38:45 - INFO - seq2seq.trainers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 64\r\n12/12/2020 15:38:45 - INFO - seq2seq.trainers.trainer -     Gradient Accumulation steps = 1\r\n12/12/2020 15:38:45 - INFO - seq2seq.trainers.trainer -     Total optimization steps = 2\r\n{'loss': 529.79443359375, 'learning_rate': 2e-05, 'epoch': 1.0}                                                                                                                                           \r\n100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.37it/s]12/12/2020 15:38:46 - INFO - seq2seq.trainers.trainer -   \r\n\r\nTraining completed. Do not forget to share your model on huggingface.co/models =)\r\n\r\n\r\n{'epoch': 2.0}                                                                                                                                                                                            \r\n100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.43it/s]\r\n12/12/2020 15:38:46 - INFO - seq2seq.trainers.trainer -   Saving model checkpoint to outputs/test\r\ncahce dir  /idiap/temp/rkarimi/cache_home_1/datasets\r\ncahce dir  /idiap/temp/rkarimi/cache_home_1/datasets\r\n12/12/2020 15:38:59 - INFO - filelock -   Lock 140079084929680 acquired on /idiap/home/rkarimi/.cache/huggingface/datasets/4c7b1146606607c193d1ef601d8d0c134521b2ac59f61ee98c09119be925ee16.7ad892de9d7f1b4f9dfc598ef31e4a398a7224176bc9a3110e0e2075ff943e8f.py.lock\r\n12/12/2020 15:38:59 - INFO - filelock -   Lock 140079084929680 released on /idiap/home/rkarimi/.cache/huggingface/datasets/4c7b1146606607c193d1ef601d8d0c134521b2ac59f61ee98c09119be925ee16.7ad892de9d7f1b4f9dfc598ef31e4a398a7224176bc9a3110e0e2075ff943e8f.py.lock\r\nUsing custom data configuration default\r\n12/12/2020 15:38:59 - INFO - filelock -   Lock 140079084929360 acquired on /idiap/temp/rkarimi/cache_home_1/datasets/_idiap_temp_rkarimi_cache_home_1_datasets_boolq_default_0.1.0_1fcfdc6f36dc89a2245ffbbd5248ab33890594b50396731ebc78411bdd2ca534.lock\r\n12/12/2020 15:38:59 - INFO - filelock -   Lock 140079084929360 released on /idiap/temp/rkarimi/cache_home_1/datasets/_idiap_temp_rkarimi_cache_home_1_datasets_boolq_default_0.1.0_1fcfdc6f36dc89a2245ffbbd5248ab33890594b50396731ebc78411bdd2ca534.lock\r\n12/12/2020 15:38:59 - INFO - filelock -   Lock 140079085355216 acquired on /idiap/temp/rkarimi/cache_home_1/datasets/_idiap_temp_rkarimi_cache_home_1_datasets_boolq_default_0.1.0_1fcfdc6f36dc89a2245ffbbd5248ab33890594b50396731ebc78411bdd2ca534.lock\r\nReusing dataset boolq (/idiap/temp/rkarimi/cache_home_1/datasets/boolq/default/0.1.0/1fcfdc6f36dc89a2245ffbbd5248ab33890594b50396731ebc78411bdd2ca534)\r\n12/12/2020 15:38:59 - INFO - filelock -   Lock 140079085355216 released on /idiap/temp/rkarimi/cache_home_1/datasets/_idiap_temp_rkarimi_cache_home_1_datasets_boolq_default_0.1.0_1fcfdc6f36dc89a2245ffbbd5248ab33890594b50396731ebc78411bdd2ca534.lock\r\nLoading cached processed dataset at /idiap/temp/rkarimi/cache_home_1/datasets/boolq/default/0.1.0/1fcfdc6f36dc89a2245ffbbd5248ab33890594b50396731ebc78411bdd2ca534/cache-164dd1d57e9fa69a.arrow\r\n12/12/2020 15:38:59 - INFO - seq2seq.metrics.metrics -   selected metric <function build_compute_metrics_fn.<locals>.classification_metrics at 0x7f66b40c67a0> for task boolq\r\n12/12/2020 15:38:59 - INFO - seq2seq.trainers.trainer -   ***** Running training *****\r\n12/12/2020 15:38:59 - INFO - seq2seq.trainers.trainer -     Num examples = 1\r\n12/12/2020 15:38:59 - INFO - seq2seq.trainers.trainer -     Num Epochs = 2\r\n12/12/2020 15:38:59 - INFO - seq2seq.trainers.trainer -     Instantaneous batch size per device = 64\r\n12/12/2020 15:38:59 - INFO - seq2seq.trainers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 64\r\n12/12/2020 15:38:59 - INFO - seq2seq.trainers.trainer -     Gradient Accumulation steps = 1\r\n12/12/2020 15:38:59 - INFO - seq2seq.trainers.trainer -     Total optimization steps = 2\r\n12/12/2020 15:38:59 - INFO - seq2seq.trainers.trainer -     Continuing training from checkpoint, will skip to saved global_step\r\n12/12/2020 15:38:59 - INFO - seq2seq.trainers.trainer -     Continuing training from epoch 2\r\n12/12/2020 15:38:59 - INFO - seq2seq.trainers.trainer -     Continuing training from global step 2\r\n12/12/2020 15:38:59 - INFO - seq2seq.trainers.trainer -     Will skip the first 0 steps in the first epoch\r\n  0%|                                                                                                                                                                               | 0/2 [00:00<?, ?it/s]12/12/2020 15:38:59 - INFO - seq2seq.trainers.trainer -   \r\n\r\nTraining completed. Do not forget to share your model on huggingface.co/models =)\r\n\r\n\r\n{'epoch': 2.0}                                                                                                                                                                                            \r\n  0%|                                                                                                                                                                               | 0/2 [00:00<?, ?it/s]\r\n12/12/2020 15:38:59 - INFO - seq2seq.trainers.trainer -   Saving model checkpoint to outputs/finetune-adapter/test-n-1-lr-1e-02-e-20/boolq\r\n12/12/2020 15:39:07 - INFO - seq2seq.utils.utils -   using task specific params for boolq: {'max_length': 3}\r\n12/12/2020 15:39:07 - INFO - seq2seq.trainers.trainer -   ***** Running Evaluation *****\r\n12/12/2020 15:39:07 - INFO - seq2seq.trainers.trainer -     Num examples = 3269\r\n12/12/2020 15:39:07 - INFO - seq2seq.trainers.trainer -     Batch size = 64\r\n100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 52/52 [00:12<00:00,  4.86it/s][libprotobuf FATAL /sentencepiece/src/../third_party/protobuf-lite/google/protobuf/repeated_field.h:1505] CHECK failed: (index) >= (0): \r\nterminate called after throwing an instance of 'google::protobuf::FatalException'\r\n  what():  CHECK failed: (index) >= (0): \r\nAborted\r\n```\r\n\r\n\r\n\r\n""
 'solved see https://github.com/huggingface/transformers/issues/9079?_pjax=%23js-repo-pjax-container '
 'Hii please follow me']","Hi
I am getting this error when evaluating on wmt16-ro-en using finetune_trainer.py of huggingface repo. thank for your help

{'epoch': 20.0}                                                                                                                                             
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:16<00:00,  1.22it/s]
12/08/2020 10:41:19 - INFO - seq2seq.trainers.trainer -   Saving model checkpoint to outputs/experiment/joint/finetune/lr-2e-5
12/08/2020 10:41:24 - INFO - __main__ -   {'wmt16-en-ro': Dataset(features: {'src_texts': Value(dtype='string', id=None), 'task': Value(dtype='string', id=None), 'tgt_texts': Value(dtype='string', id=None)}, num_rows: 1998), 'qnli': Dataset(features: {'src_texts': Value(dtype='string', id=None), 'task': Value(dtype='string', id=None), 'tgt_texts': Value(dtype='string', id=None)}, num_rows: 5462), 'scitail': Dataset(features: {'src_texts': Value(dtype='string', id=None), 'task': Value(dtype='string', id=None), 'tgt_texts': Value(dtype='string', id=None)}, num_rows: 1303)}
12/08/2020 10:41:24 - INFO - __main__ -   *** Evaluate ***
12/08/2020 10:41:24 - INFO - seq2seq.utils.utils -   using task specific params for wmt16-en-ro: {'max_length': 300, 'num_beams': 4}
12/08/2020 10:41:24 - INFO - seq2seq.trainers.trainer -   ***** Running Evaluation *****
12/08/2020 10:41:24 - INFO - seq2seq.trainers.trainer -     Num examples = 1998
12/08/2020 10:41:24 - INFO - seq2seq.trainers.trainer -     Batch size = 64
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:37<00:00,  1.19s/it][libprotobuf FATAL /sentencepiece/src/../third_party/protobuf-lite/google/protobuf/repeated_field.h:1505] CHECK failed: (index) >= (0): 
terminate called after throwing an instance of 'google::protobuf::FatalException'
  what():  CHECK failed: (index) >= (0): 
Aborted
"
https://github.com/huggingface/datasets/issues/1285,boolq does not work ,"['here is the minimal code to reproduce\r\n\r\n`datasets>>> datasets.load_dataset(""boolq"", ""train"")\r\n\r\nthe errors\r\n\r\n```\r\n`cahce dir  /idiap/temp/rkarimi/cache_home_1/datasets\r\ncahce dir  /idiap/temp/rkarimi/cache_home_1/datasets\r\nUsing custom data configuration train\r\nDownloading and preparing dataset boolq/train (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /idiap/temp/rkarimi/cache_home_1/datasets/boolq/train/0.1.0/2987db1f15deaa19500ae24de560eabeaf1f8ef51df88c0470beeec72943bf11...\r\ncahce dir  /idiap/temp/rkarimi/cache_home_1/datasets\r\ncahce dir  /idiap/temp/rkarimi/cache_home_1/datasets/downloads\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/load.py"", line 611, in load_dataset\r\n    ignore_verifications=ignore_verifications,\r\n  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/builder.py"", line 476, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/builder.py"", line 531, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File "" /idiap/home/rkarimi/.cache/huggingface/modules/datasets_modules/datasets/boolq/2987db1f15deaa19500ae24de560eabeaf1f8ef51df88c0470beeec72943bf11/boolq.py"", line 74, in _split_generators\r\n    downloaded_files = dl_manager.download_custom(urls_to_download, tf.io.gfile.copy)\r\n  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/utils/download_manager.py"", line 149, in download_custom\r\n    custom_download(url, path)\r\n  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py"", line 516, in copy_v2\r\n    compat.path_to_bytes(src), compat.path_to_bytes(dst), overwrite)\r\n\r\n\r\n\r\n```'
 'This has been fixed by #881 \r\nthis fix will be available in the next release soon.\r\n\r\nIf you don\'t want to wait for the release you can actually load the latest version of boolq by specifying `script_version=""master""` in `load_dataset`'
 'thank you this solved this issue, for now seems to work, thanks ']","Hi
I am getting this error when trying to load boolq, thanks for your help

ts_boolq_default_0.1.0_2987db1f15deaa19500ae24de560eabeaf1f8ef51df88c0470beeec72943bf11.lock
Traceback (most recent call last):
  File ""finetune_t5_trainer.py"", line 274, in <module>
    main()
  File ""finetune_t5_trainer.py"", line 147, in main
    for task in data_args.tasks]
  File ""finetune_t5_trainer.py"", line 147, in <listcomp>
    for task in data_args.tasks]
  File ""/remote/idiap.svm/user.active/rkarimi/dev/ruse/seq2seq/tasks/tasks.py"", line 58, in get_dataset
    dataset = self.load_dataset(split=split)
  File ""/remote/idiap.svm/user.active/rkarimi/dev/ruse/seq2seq/tasks/tasks.py"", line 54, in load_dataset
    return datasets.load_dataset(self.task.name, split=split)
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/load.py"", line 611, in load_dataset
    ignore_verifications=ignore_verifications,
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/builder.py"", line 476, in download_and_prepare
    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/builder.py"", line 531, in _download_and_prepare
    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)
  File "" /idiap/home/rkarimi/.cache/huggingface/modules/datasets_modules/datasets/boolq/2987db1f15deaa19500ae24de560eabeaf1f8ef51df88c0470beeec72943bf11/boolq.py"", line 74, in _split_generators
    downloaded_files = dl_manager.download_custom(urls_to_download, tf.io.gfile.copy)
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/utils/download_manager.py"", line 149, in download_custom
    custom_download(url, path)
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py"", line 516, in copy_v2
    compat.path_to_bytes(src), compat.path_to_bytes(dst), overwrite)
tensorflow.python.framework.errors_impl.AlreadyExistsError: file already exists

"
https://github.com/huggingface/datasets/issues/1167,"❓ On-the-fly tokenization with datasets, tokenizers, and torch Datasets and Dataloaders","['We\'re working on adding on-the-fly transforms in datasets.\r\nCurrently the only on-the-fly functions that can be applied are in `set_format` in which we transform the data in either numpy/torch/tf tensors or pandas.\r\nFor example\r\n```python\r\ndataset.set_format(""torch"")\r\n```\r\napplies `torch.Tensor` to the dataset entries on-the-fly.\r\n\r\nWe plan to extend this to user-defined formatting transforms.\r\nFor example\r\n```python\r\ndataset.set_format(transform=tokenize)\r\n```\r\n\r\nWhat do you think ?']","Hi there,

I have a question regarding ""on-the-fly"" tokenization. This question was elicited by reading the ""How to train a new language model from scratch using Transformers and Tokenizers"" [here](https://huggingface.co/blog/how-to-train). Towards the end there is this sentence: ""If your dataset is very large, you can opt to load and tokenize examples on the fly, rather than as a preprocessing step"". I've tried coming up with a solution that would combine both `datasets` and `tokenizers`, but did not manage to find a good pattern.

I guess the solution would entail wrapping a dataset into a Pytorch dataset.

As a concrete example from the [docs](https://huggingface.co/transformers/custom_datasets.html)

```python
import torch

class SquadDataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        # instead of doing this beforehand, I'd like to do tokenization on the fly
        self.encodings = encodings 

    def __getitem__(self, idx):
        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}

    def __len__(self):
        return len(self.encodings.input_ids)

train_dataset = SquadDataset(train_encodings)
```

How would one implement this with ""on-the-fly"" tokenization exploiting the vectorized capabilities of tokenizers?


----

Edit: I have come up with this solution. It does what I want, but I feel it's not very elegant

```python
class CustomPytorchDataset(Dataset):
    def __init__(self):
        self.dataset = some_hf_dataset(...)
        self.tokenizer = BertTokenizerFast.from_pretrained(""bert-base-uncased"")

    def __getitem__(self, batch_idx):
        instance = self.dataset[text_col][batch_idx]
        tokenized_text = self.tokenizer(instance, truncation=True, padding=True)
        return tokenized_text

    def __len__(self):
        return len(self.dataset)

    @staticmethod
    def collate_fn(batch):
        # batch is a list, however it will always contain 1 item because we should not use the
        # batch_size argument as batch_size is controlled by the sampler
        return {k: torch.tensor(v) for k, v in batch[0].items()}

torch_ds = CustomPytorchDataset()

# NOTE: batch_sampler returns list of integers and since here we have SequentialSampler
# it returns: [1, 2, 3], [4, 5, 6], etc. - check calling `list(batch_sampler)`
batch_sampler = BatchSampler(SequentialSampler(torch_ds), batch_size=3, drop_last=True)

# NOTE: no `batch_size` as now the it is controlled by the sampler!
dl = DataLoader(dataset=torch_ds, sampler=batch_sampler, collate_fn=torch_ds.collate_fn)
```"
https://github.com/huggingface/datasets/issues/1115,Incorrect URL for MRQA SQuAD train subset,['good catch !'],"https://github.com/huggingface/datasets/blob/4ef4c8f8b7a60e35c6fa21115fca9faae91c9f74/datasets/mrqa/mrqa.py#L53

The URL for `train+SQuAD` subset of MRQA points to the dev set instead of train set. It should be `https://s3.us-east-2.amazonaws.com/mrqa/release/v2/train/SQuAD.jsonl.gz`."
https://github.com/huggingface/datasets/issues/1110,"Using a feature named ""_type"" fails with certain operations","['Thanks for reporting !\r\n\r\nIndeed this is a keyword in the library that is used to encode/decode features to a python dictionary that we can save/load to json.\r\nWe can probably change `_type` to something that is less likely to collide with user feature names.\r\nIn this case we would want something backward compatible though.\r\n\r\nFeel free to try a fix and open a PR, and to ping me if I can help :) ']","A column named `_type` leads to a `TypeError: unhashable type: 'dict'` for certain operations:
```python
from datasets import Dataset, concatenate_datasets

ds = Dataset.from_dict({""_type"": [""whatever""]}).map()
concatenate_datasets([ds])
# or simply
Dataset(ds._data)
```
Context: We are using datasets to persist data coming from elasticsearch to feed to our pipeline, and elasticsearch has a `_type` field, hence the strange name of the column.

Not sure if you wish to support this specific column name, but if you do i would be happy to try a fix and provide a PR. I already had a look into it and i think the culprit is the `datasets.features.generate_from_dict` function. It uses the hard coded `_type` string to figure out if it reached the end of the nested feature object from a serialized dict.

Best wishes and keep up the awesome work!"
https://github.com/huggingface/datasets/issues/1103,Add support to download kaggle datasets,"['Hey, I think this is great idea. Any plan to integrate kaggle private datasets loading to `datasets`?']",We can use API key
https://github.com/huggingface/datasets/issues/1064,Not support links with 302 redirect ,"['Hi !\r\nThis kind of links is now supported by the library since #1316'
 '> Hi !\r\n> This kind of links is now supported by the library since #1316\r\n\r\nI updated links in TLC datasets to be the github links in this pull request \r\n https://github.com/huggingface/datasets/pull/1737\r\n\r\nEverything works now. Thank you.']","I have an issue adding this download link https://github.com/jitkapat/thailitcorpus/releases/download/v.2.0/tlc_v.2.0.tar.gz

it might be because it is not a direct link (it returns 302 and redirects to aws that returns 403 for head requests). 

```
r.head(""https://github.com/jitkapat/thailitcorpus/releases/download/v.2.0/tlc_v.2.0.tar.gz"", allow_redirects=True)                                      
# <Response [403]>
```"
https://github.com/huggingface/datasets/issues/1046,Dataset.map() turns tensors into lists?,"[""A solution is to have the tokenizer return a list instead of a tensor, and then use `dataset_tok.set_format(type = 'torch')` to convert that list into a tensor. Still not sure if bug.""
 'It is expected behavior, you should set the format to `""torch""` as you mentioned to get pytorch tensors back.\r\nBy default datasets returns pure python objects.']","I apply `Dataset.map()` to a function that returns a dict of torch tensors (like a tokenizer from the repo transformers). However, in the mapped dataset, these tensors have turned to lists!

```import datasets
import torch  
from datasets import load_dataset                                                                                                                 
print(""version datasets"", datasets.__version__)

dataset = load_dataset(""snli"", split='train[0:50]')  

def tokenizer_fn(example):
    # actually uses a tokenizer which does something like:
    return {'input_ids': torch.tensor([[0, 1, 2]])}

print(""First item in dataset:\n"", dataset[0])
tokenized = tokenizer_fn(dataset[0])
print(""Tokenized hyp:\n"", tokenized)
dataset_tok = dataset.map(tokenizer_fn, batched=False,
        remove_columns=['label', 'premise', 'hypothesis'])
print(""Tokenized using map:\n"", dataset_tok[0])
print(type(tokenized['input_ids']), type(dataset_tok[0]['input_ids']))
dataset_tok = dataset.map(tokenizer_fn, batched=False,
                          remove_columns=['label', 'premise', 'hypothesis'])
print(""Tokenized using map:\n"", dataset_tok[0])
print(type(tokenized['input_ids']), type(dataset_tok[0]['input_ids']))
```

The output is:

```
version datasets 1.1.3
Reusing dataset snli (/home/tom/.cache/huggingface/datasets/snli/plain_text/1.0.0/bb1102591c6230bd78813e229d5dd4c7fbf4fc478cec28f298761eb69e5b537c)
First item in dataset:
 {'premise': 'A person on a horse jumps over a broken down airplane.', 'hypothesis': 'A person is training his horse for a competition.', 'label': 1}
Tokenized hyp:
 {'input_ids': tensor([[0, 1, 2]])}
Loading cached processed dataset at /home/tom/.cache/huggingface/datasets/snli/plain_text/1.0.0/bb1102591c6230bd78813e229d5dd4c7fbf4fc478cec28f298761eb69e5b537c/cache-fe38f449fe9ac46f.arrow
Tokenized using map:
 {'input_ids': [[0, 1, 2]]}
<class 'torch.Tensor'> <class 'list'>
```

Or am I doing something wrong?
"
https://github.com/huggingface/datasets/issues/1004,how large datasets are handled under the hood ,"['This library uses Apache Arrow under the hood to store datasets on disk.\r\nThe advantage of Apache Arrow is that it allows to memory map the dataset. This allows to load datasets bigger than memory and with almost no RAM usage. It also offers excellent I/O speed.\r\n\r\nFor example when you access one element or one batch\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nsquad = load_dataset(""squad"", split=""train"")\r\nfirst_element = squad[0]\r\none_batch = squad[:8]\r\n```\r\n\r\nthen only this element/batch is loaded in memory, while the rest of the dataset is memory mapped.'
 ""How can we change how much data is loaded to memory with Arrow? I think that I am having some performance issue with it. When Arrow loads the data from disk it does it in multiprocess? It's almost twice slower training with arrow than in memory.\r\n\r\nEDIT:\r\nMy fault! I had not seen the `dataloader_num_workers` in `TrainingArguments` ! Now I can parallelize and go fast! Sorry, and thanks.""
 ""> How can we change how much data is loaded to memory with Arrow? I think that I am having some performance issue with it. When Arrow loads the data from disk it does it in multiprocess? It's almost twice slower training with arrow than in memory.\r\n\r\nLoading arrow data from disk is done with memory-mapping. This allows to load huge datasets without filling your RAM.\r\nMemory mapping is almost instantaneous and is done within one process.\r\n\r\nThen, the speed of querying examples from the dataset is I/O bounded depending on your disk. If it's an SSD then fetching examples from the dataset will be very fast.\r\nBut since the I/O speed of an SSD is lower than the one of RAM it's expected to be slower to fetch data from disk than from memory.\r\nStill, if you load the dataset in different processes then it can be faster but there will still be the I/O bottleneck of the disk.\r\n\r\n> EDIT:\r\n> My fault! I had not seen the `dataloader_num_workers` in `TrainingArguments` ! Now I can parallelize and go fast! Sorry, and thanks.\r\n\r\nOk let me know if that helps !\r\n""]","Hi
I want to use multiple large datasets with a mapping style dataloader, where they cannot fit into memory, could you tell me how you handled the datasets under the hood? is this you bring all in memory in case of mapping style ones? or is this some sharding under the hood and you bring in memory when necessary, thanks "
https://github.com/huggingface/datasets/issues/996,NotADirectoryError while loading the CNN/Dailymail dataset,"[""Looks like the google drive download failed.\r\nI'm getting a `Google Drive - Quota exceeded` error while looking at the downloaded file.\r\n\r\nWe should consider finding a better host than google drive for this dataset imo\r\nrelated : #873 #864 ""
 'It is working now, thank you. \r\n\r\nShould I leave this issue open to address the Quota-exceeded error?'
 ""Yes please. It's been happening several times, we definitely need to address it""
 ""Any updates on this one? I'm facing a similar issue trying to add CelebA.""
 ""I've looked into it and couldn't find a solution. This looks like a Google Drive limitation..\r\nPlease try to use other hosts when possible""
 'The original links are google drive links.  Would it be feasible for HF to maintain their own servers for this? Also, I think the same issue must also exist with TFDS.'
 ""It's possible to host data on our side but we should ask the authors. TFDS has the same issue and doesn't have a solution either afaik.\r\nOtherwise you can use the google drive link, but it it's not that convenient because of this quota issue.""
 'Okay. I imagine asking every author who shares their dataset on Google Drive will also be cumbersome.'
 'I am getting this error as well. Is there a fix?'
 ""Not as long as the data is stored on GG drive unfortunately.\r\nMaybe we can ask if there's a mirror ?\r\n\r\nHi @JafferWilson is there a download link to get cnn dailymail from another host than GG drive ?\r\n\r\nTo give you some context, this library provides tools to download and process datasets. For CNN DailyMail the data are downloaded from the link you provide on your github repository. Unfortunately because of GG drive quotas, many users are not able to load this dataset.""]","
Downloading and preparing dataset cnn_dailymail/3.0.0 (download: 558.32 MiB, generated: 1.28 GiB, post-processed: Unknown size, total: 1.82 GiB) to /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/0128610a44e10f25b4af6689441c72af86205282d26399642f7db38fa7535602...

---------------------------------------------------------------------------

NotADirectoryError                        Traceback (most recent call last)

<ipython-input-9-cd4bf8bea840> in <module>()
     22 
     23 
---> 24 train = load_dataset('cnn_dailymail', '3.0.0', split='train')
     25 validation = load_dataset('cnn_dailymail', '3.0.0', split='validation')
     26 test = load_dataset('cnn_dailymail', '3.0.0', split='test')

5 frames

/root/.cache/huggingface/modules/datasets_modules/datasets/cnn_dailymail/0128610a44e10f25b4af6689441c72af86205282d26399642f7db38fa7535602/cnn_dailymail.py in _find_files(dl_paths, publisher, url_dict)
    132     else:
    133         logging.fatal(""Unsupported publisher: %s"", publisher)
--> 134     files = sorted(os.listdir(top_dir))
    135 
    136     ret_files = []

NotADirectoryError: [Errno 20] Not a directory: '/root/.cache/huggingface/datasets/downloads/1bc05d24fa6dda2468e83a73cf6dc207226e01e3c48a507ea716dc0421da583b/cnn/stories'"
https://github.com/huggingface/datasets/issues/993,Problem downloading amazon_reviews_multi,"['Hi @hfawaz ! This is working fine for me. Is it a repeated occurence? Have you tried from the latest verion?'
 ""Hi, it seems a connection problem. \r\nNow it says: \r\n`ConnectionError: Couldn't reach https://amazon-reviews-ml.s3-us-west-2.amazonaws.com/json/train/dataset_ja_train.json`""]","Thanks for adding the dataset. 
After trying to load the dataset, I am getting the following error: 
`ConnectionError: Couldn't reach https://amazon-reviews-ml.s3-us-west-2.amazonaws.com/json/train/dataset_fr_train.json
`
I used the following code to load the dataset: 
`load_dataset(
            dataset_name,
            ""all_languages"",
            cache_dir="".data""
        )`

I am using version 1.1.3 of `datasets`

Note that I can perform a successfull `wget https://amazon-reviews-ml.s3-us-west-2.amazonaws.com/json/train/dataset_fr_train.json`"
https://github.com/huggingface/datasets/issues/988,making sure datasets are not loaded in memory and distributed training of them,['my implementation of sharding per TPU core: https://github.com/google-research/ruse/blob/d4dd58a2d8efe0ffb1a9e9e77e3228d6824d3c3c/seq2seq/trainers/t5_trainer.py#L316 \r\nmy implementation of dataloader for this case https://github.com/google-research/ruse/blob/d4dd58a2d8efe0ffb1a9e9e77e3228d6824d3c3c/seq2seq/tasks/tasks.py#L496 '],"Hi
I am dealing with large-scale datasets which I need to train distributedly, I used the shard function to divide the dataset across the cores, without any sampler, this does not work for distributed training and does not become any faster than 1 TPU core. 1) how I can make sure data is not loaded in memory 2) in case of distributed training with iterative datasets which measures needs to be taken? Is this all sharding the data only. I was wondering if there can be possibility for me to discuss this with someone with distributed training with iterative datasets using dataset library. thanks "
https://github.com/huggingface/datasets/issues/961,sample multiple datasets ,"['here I share my dataloader currently for multiple tasks: https://gist.github.com/rabeehkarimimahabadi/39f9444a4fb6f53dcc4fca5d73bf8195 \r\n\r\nI need to train my model distributedly with this dataloader, ""MultiTasksataloader"", currently this does not work in distributed fasion,\r\nto save on memory I tried to use iterative datasets, could you have a look in this dataloader and tell me if this is indeed the case? not sure how to make datasets being iterative to not load them in memory, then I remove the sampler for dataloader, and shard the data per core, could you tell me please how I should implement this case in datasets library? and how do you find my implementation in terms of correctness? thanks \r\n']","Hi
I am dealing with multiple datasets, I need to have a dataloader over them with a condition that in each batch data samples are coming from one of the datasets. My main question is: 
-  I need to have a way to sample the datasets first with some weights, lets say 2x dataset1 1x dataset2, could you point me how I can do it

sub-questions:
- I want to concat sampled datasets and define one dataloader on it, then I need a way to make sure batches come from 1 dataset in each iteration, could you assist me how I can do?
- I use iterative-type of datasets, but I need a method of shuffling still since it brings accuracy performance issues if not doing it, thanks for the help. "
https://github.com/huggingface/datasets/issues/937,Local machine/cluster Beam Datasets example/tutorial,"[""I tried to make it run once on the SparkRunner but it seems that this runner has some issues when it is run locally.\r\nFrom my experience the DirectRunner is fine though, even if it's clearly not memory efficient.\r\n\r\nIt would be awesome though to make it work locally on a SparkRunner !\r\nDid you manage to make your processing work ?""]","Hi,

I'm wondering if https://huggingface.co/docs/datasets/beam_dataset.html has an non-GCP or non-Dataflow version example/tutorial? I tried to migrate it to run on DirectRunner and SparkRunner, however, there were way too many runtime errors that I had to fix during the process, and even so I wasn't able to get either runner correctly producing the desired output.

Thanks!
Shang"
https://github.com/huggingface/datasets/issues/919,wrong length with datasets ,"['Also, I cannot first convert it to torch format, since huggingface seq2seq_trainer codes process the datasets afterwards during datacollector function to make it optimize for TPUs. '
 'sorry I misunderstood length of dataset with dataloader, closed. thanks ']","Hi
I have a MRPC dataset which I convert it to seq2seq format, then this is of this format:

`Dataset(features: {'src_texts': Value(dtype='string', id=None), 'tgt_texts': Value(dtype='string', id=None)}, num_rows: 10)
`

I feed it to a dataloader:
```
dataloader = DataLoader(
            train_dataset,
            batch_size=self.args.train_batch_size,
            sampler=train_sampler,
            collate_fn=self.data_collator,
            drop_last=self.args.dataloader_drop_last,
            num_workers=self.args.dataloader_num_workers,
        )
```

now if I type len(dataloader) this is 1, which is wrong, and this needs to be 10. could you assist me please? thanks 
"
https://github.com/huggingface/datasets/issues/915,Shall we change the hashing to encoding to reduce potential replicated cache files?,"['This is an interesting idea !\r\nDo you have ideas about how to approach the decoding and the normalization ?'
 '@lhoestq\r\nI think we first need to save the transformation chain to a list in `self._fingerprint`. Then we can\r\n- decode all the current saved datasets to see if there is already one that is equivalent to the transformation we need now.\r\n- or, calculate all the possible hash value of the current chain for comparison so that we could continue to use hashing.\r\nIf we find one, we can adjust the list in `self._fingerprint` to it.\r\n\r\nAs for the transformation reordering rules, we can just start with some manual rules, like two sort on the same column should merge to one, filter and select can change orders.\r\n\r\nAnd for encoding and decoding, we can just manually specify `sort` is 0, `shuffling` is 2 and create a base-n number or use some general algorithm like `base64.urlsafe_b64encode`.\r\n\r\nBecause we are not doing lazy evaluation now, we may not be able to normalize the transformation to its minimal form. If we want to support that, we can provde a `Sequential` api and let user input a list or transformation, so that user would not use the intermediate datasets. This would look like tf.data.Dataset.']","Hi there. For now, we are using `xxhash` to hash the transformations to fingerprint and we will save a copy of the processed dataset to disk if there is a new hash value. However, there are some transformations that are idempotent or commutative to each other. I think that encoding the transformation chain as the fingerprint may help in those cases, for example, use `base64.urlsafe_b64encode`. In this way, before we want to save a new copy, we can decode the transformation chain and normalize it to prevent omit potential reuse. As the main targets of this project are the really large datasets that cannot be loaded entirely in memory, I believe it would save a lot of time if we can avoid some write.

If you have interest in this, I'd love to help :)."
https://github.com/huggingface/datasets/issues/911,datasets module not found,"[""nvm, I'd made an assumption that the library gets installed with transformers. ""]","Currently, running `from datasets import load_dataset` will throw a `ModuleNotFoundError: No module named 'datasets'` error.
"
https://github.com/huggingface/datasets/issues/900,datasets.load_dataset() custom chaching directory bug,"[""Thanks for reporting ! I'm looking into it.""]","Hello,
I'm having issue with loading a dataset with a custom `cache_dir`. Despite specifying the output dir, it is still downloaded to 
 `~/.cache`.

## Environment info
- `datasets` version: 1.1.3
- Platform: Linux-4.19.129-aufs-1-x86_64-with-debian-10.1
- Python version: 3.7.3

## The code I'm running:
```python
import datasets
from pathlib import Path

validation_dataset = datasets.load_dataset(""natural_questions"", split=""validation[:5%]"", cache_dir=Path(""./data""))  
```

## The output:

* The dataset is downloaded to my home directory's `.cache` 
* A new empty directory named ""`natural_questions` is created in the specified directory `.data`
* `tree data` in the shell outputs:
```
data
└── natural_questions
    └── default
        └── 0.0.2
3 directories, 0 files
```

The output:
```
Downloading: 8.61kB [00:00, 5.11MB/s]                                                                                                                                                                              
Downloading: 13.6kB [00:00, 7.89MB/s]                                                                                                                                                                              
Using custom data configuration default                                                                                                                                                                            
Downloading and preparing dataset natural_questions/default (download: 41.97 GiB, generated: 92.95 GiB, post-processed: Unknown size, total: 134.92 GiB) to ./data/natural_questions/default/0.0.2/867dbbaf9137c1b8
3ecb19f5eb80559e1002ea26e702c6b919cfa81a17a8c531...                                                                                                                                                                
Downloading: 100%|██████████████████████████████████████████████████| 13.6k/13.6k [00:00<00:00, 1.51MB/s]                                                                                                          
Downloading:   7%|███▎                                            | 6.70G/97.4G [03:46<1:37:05, 15.6MB/s]
```

## Expected behaviour:
The dataset ""Natural Questions"" should be downloaded to the directory ""./data""
"
https://github.com/huggingface/datasets/issues/897,Dataset viewer issues,"['Thanks for reporting !\r\ncc @srush for the empty feature list issue and the encoding issue\r\ncc @julien-c maybe we can update the url and just have a redirection from the old url to the new one ?'
 ""Ok, I redirected on our side to a new url. ⚠️ @srush: if you update the Streamlit config too to `/datasets/viewer`, let me know because I'll need to change our nginx config at the same time""
 '9' '\u200f⠀\u200f\u200f\u200f⠀\u200f\u200f\u200f⠀ \u200f⠀ '
 '\u200f⠀\u200f\u200f\u200f⠀\u200f\u200f\u200f⠀ \u200f⠀ ']","I was looking through the dataset viewer and I like it a lot. Version numbers, citation information, everything's there! I've spotted a few issues/bugs though:

- the URL is still under `nlp`, perhaps an alias for `datasets` can be made
- when I remove a **feature** (and the feature list is empty), I get an error. This is probably expected, but perhaps a better error message can be shown to the user

```bash
IndexError: list index out of range
Traceback:
File ""/home/sasha/streamlit/lib/streamlit/ScriptRunner.py"", line 322, in _run_script
    exec(code, module.__dict__)
File ""/home/sasha/nlp-viewer/run.py"", line 316, in <module>
    st.table(style)
File ""/home/sasha/streamlit/lib/streamlit/DeltaGenerator.py"", line 122, in wrapped_method
    return dg._enqueue_new_element_delta(marshall_element, delta_type, last_index)
File ""/home/sasha/streamlit/lib/streamlit/DeltaGenerator.py"", line 367, in _enqueue_new_element_delta
    rv = marshall_element(msg.delta.new_element)
File ""/home/sasha/streamlit/lib/streamlit/DeltaGenerator.py"", line 120, in marshall_element
    return method(dg, element, *args, **kwargs)
File ""/home/sasha/streamlit/lib/streamlit/DeltaGenerator.py"", line 2944, in table
    data_frame_proto.marshall_data_frame(data, element.table)
File ""/home/sasha/streamlit/lib/streamlit/elements/data_frame_proto.py"", line 54, in marshall_data_frame
    _marshall_styles(proto_df.style, df, styler)
File ""/home/sasha/streamlit/lib/streamlit/elements/data_frame_proto.py"", line 73, in _marshall_styles
    translated_style = styler._translate()
File ""/home/sasha/.local/share/virtualenvs/lib-ogGKnCK_/lib/python3.7/site-packages/pandas/io/formats/style.py"", line 351, in _translate
    * (len(clabels[0]) - len(hidden_columns))
```

- there seems to be **an encoding issue** in the default view, the dataset examples are shown as raw monospace text, without a decent encoding. That makes it hard to read for languages that use a lot of special characters. Take for instance the [cs-en WMT19 set](https://huggingface.co/nlp/viewer/?dataset=wmt19&config=cs-en). This problem goes away when you enable ""List view"", because then some syntax highlighteris used, and the special characters are coded correctly.
"
https://github.com/huggingface/datasets/issues/888,Nested lists are zipped unexpectedly,"['Yes following the Tensorflow Datasets convention, objects with type `Sequence of a Dict` are actually stored as a `dictionary of lists`.\r\nSee the [documentation](https://huggingface.co/docs/datasets/features.html?highlight=features) for more details'
 ""Thanks.\r\nThis is a bit (very) confusing, but I guess if its intended, I'll just work with it as if its how my data was originally structured :) \r\n""]","I might misunderstand something, but I expect that if I define:
```python
""top"": datasets.features.Sequence({
  ""middle"": datasets.features.Sequence({
    ""bottom"": datasets.Value(""int32"")
  })
})
```

And I then create an example:
```python
yield 1, {
  ""top"": [{
    ""middle"": [
      {""bottom"": 1},
      {""bottom"": 2}
    ]
  }]
}
```

I then load my dataset:
```python
train = load_dataset(""my dataset"")[""train""]
```

and expect to be able to access `data[0][""top""][0][""middle""][0]`.

That is not the case. Here is `data[0]` as JSON:

```json
{""top"": {""middle"": [{""bottom"": [1, 2]}]}}
```

Clearly different than the thing I inputted.
```json
{""top"": [{""middle"": [{""bottom"": 1},{""bottom"": 2}]}]}
```"
https://github.com/huggingface/datasets/issues/887,pyarrow.lib.ArrowNotImplementedError: MakeBuilder: cannot construct builder for type extension<arrow.py_extension_type>,"['Yes right now `ArrayXD` can only be used as a column feature type, not a subtype.\r\nWith the current Arrow limitations I don\'t think we\'ll be able to make it work as a subtype, however it should be possible to allow dimensions of dynamic sizes (`Array3D(shape=(None, 137, 2), dtype=""float32"")` for example since the [underlying arrow  type](https://github.com/huggingface/datasets/blob/master/src/datasets/features.py#L236) allows dynamic sizes.\r\n\r\nFor now I\'d suggest the use of nested `Sequence` types. Once we have the dynamic sizes you can update the dataset.\r\nWhat do you think ?'
 ""> Yes right now ArrayXD can only be used as a column feature type, not a subtype. \r\n\r\nMeaning it can't be nested under `Sequence`?\r\nIf so, for now I'll just make it a python list and make it with the nested `Sequence` type you suggested.""
 ""Yea unfortunately..\r\nThat's a current limitation with Arrow ExtensionTypes that can't be used in the default Arrow Array objects.\r\nWe already have an ExtensionArray that allows us to use them as column types but not for subtypes.\r\nMaybe we can extend it, I haven't experimented with that yet""
 'Cool\r\nSo please consider this issue as a feature request for:\r\n```\r\nArray3D(shape=(None, 137, 2), dtype=""float32"")\r\n```\r\n\r\nits a way to represent videos, poses, and other cool sequences'
 ""@lhoestq well, so sequence of sequences doesn't work either...\r\n\r\n```\r\npyarrow.lib.ArrowCapacityError: List array cannot contain more than 2147483646 child elements, have 2147483648\r\n```\r\n\r\n\r\n""
 ""Working with Arrow can be quite fun sometimes.\r\nYou can fix this issue by trying to reduce the writer batch size (same trick than the one used to reduce the RAM usage in https://github.com/huggingface/datasets/issues/741).\r\n\r\nLet me know if it works.\r\nI haven't investigated yet on https://github.com/huggingface/datasets/issues/741 since I was preparing this week's sprint to add datasets but this is in my priority list for early next week.""
 ""The batch size fix doesn't work... not for #741 and not for this dataset I'm trying (DGS corpus)\r\nLoading the DGS corpus takes 400GB of RAM, which is fine with me as my machine is large enough\r\n""
 ""Sorry it doesn't work. Will let you know once I fixed it""
 'Hi @lhoestq , any update on dynamic sized arrays?\r\n(`Array3D(shape=(None, 137, 2), dtype=""float32"")`)'
 ""Not yet, I've been pretty busy with the dataset sprint lately but this is something that's been asked several times already. So I'll definitely work on this as soon as I'm done with the sprint and with the RAM issue you reported.""
 'Hi @lhoestq,\r\nAny chance you have some updates on the supporting `ArrayXD` as a subtype or support of dynamic sized arrays?\r\n\r\ne.g.:\r\n`datasets.features.Sequence(datasets.features.Array2D(shape=(137, 2), dtype=""float32""))`\r\n`Array3D(shape=(None, 137, 2), dtype=""float32"")`'
 ""Hi ! We haven't worked in this lately and it's not in our very short-term roadmap since it requires a bit a work to make it work with arrow. Though this will definitely be added at one point.""
 '@lhoestq, thanks for the update.\r\n\r\nI actually tried to modify some piece of code to make it work. Can you please tell if I missing anything here?\r\nI think that for vast majority of cases it\'s enough to make first dimension of the array dynamic i.e. `shape=(None, 100, 100)`. For that, it\'s enough to modify class [ArrayExtensionArray](https://github.com/huggingface/datasets/blob/9ca24250ea44e7611c4dabd01ecf9415a7f0be6c/src/datasets/features.py#L397) to output list of arrays of different sizes instead of list of arrays of same sizes (current version)\r\nBelow are my modifications of this class.\r\n\r\n```\r\nclass ArrayExtensionArray(pa.ExtensionArray):\r\n    def __array__(self):\r\n        zero_copy_only = _is_zero_copy_only(self.storage.type)\r\n        return self.to_numpy(zero_copy_only=zero_copy_only)\r\n\r\n    def __getitem__(self, i):\r\n        return self.storage[i]\r\n\r\n    def to_numpy(self, zero_copy_only=True):\r\n        storage: pa.ListArray = self.storage\r\n        size = 1\r\n        for i in range(self.type.ndims):\r\n            size *= self.type.shape[i]\r\n            storage = storage.flatten()\r\n        numpy_arr = storage.to_numpy(zero_copy_only=zero_copy_only)\r\n        numpy_arr = numpy_arr.reshape(len(self), *self.type.shape)\r\n        return numpy_arr\r\n\r\n    def to_list_of_numpy(self, zero_copy_only=True):\r\n        storage: pa.ListArray = self.storage\r\n        shape = self.type.shape\r\n        arrays = []\r\n        for dim in range(1, self.type.ndims):\r\n            assert shape[dim] is not None, f""Support only dynamic size on first dimension. Got: {shape}""\r\n\r\n        first_dim_offsets = np.array([off.as_py() for off in storage.offsets])\r\n        for i in range(len(storage)):\r\n            storage_el = storage[i:i+1]\r\n            first_dim = first_dim_offsets[i+1] - first_dim_offsets[i]\r\n            # flatten storage\r\n            for dim in range(self.type.ndims):\r\n                storage_el = storage_el.flatten()\r\n\r\n            numpy_arr = storage_el.to_numpy(zero_copy_only=zero_copy_only)\r\n            arrays.append(numpy_arr.reshape(first_dim, *shape[1:]))\r\n\r\n        return arrays\r\n\r\n    def to_pylist(self):\r\n        zero_copy_only = _is_zero_copy_only(self.storage.type)\r\n        if self.type.shape[0] is None:\r\n            return self.to_list_of_numpy(zero_copy_only=zero_copy_only)\r\n        else:\r\n            return self.to_numpy(zero_copy_only=zero_copy_only).tolist()\r\n```\r\n\r\nI ran few tests and it works as expected. Let me know what you think.'
 'Thanks for diving into this !\r\n\r\nIndeed focusing on making the first dimensions dynamic make total sense (and users could still re-order their dimensions to match this constraint).\r\nYour code looks great :) I think it can even be extended to support several dynamic dimensions if we want to.\r\n\r\nFeel free to open a PR to include these changes, then we can update our test suite to make sure it works in all use cases.\r\nIn particular I think we might need a few tweaks to allow it to be converted to pandas (though I haven\'t tested yet):\r\n\r\n```python\r\nfrom datasets import Dataset, Features, Array3D\r\n\r\n# this works\r\nmatrix = [[1, 0], [0, 1]]\r\nfeatures = Features({""a"": Array3D(dtype=""int32"", shape=(1, 2, 2))})\r\nd = Dataset.from_dict({""a"": [[matrix], [matrix]]})\r\nprint(d.to_pandas())\r\n\r\n# this should work as well\r\nmatrix = [[1, 0], [0, 1]]\r\nfeatures = Features({""a"": Array3D(dtype=""int32"", shape=(None, 2, 2))})\r\nd = Dataset.from_dict({""a"": [[matrix], [matrix] * 2]})\r\nprint(d.to_pandas())\r\n```\r\n\r\nI\'ll be happy to help you on this :)']","I set up a new dataset, with a sequence of arrays (really, I want to have an array of (None, 137, 2), and the first dimension is dynamic) 

```python
    def _info(self):
        return datasets.DatasetInfo(
            description=_DESCRIPTION,
            # This defines the different columns of the dataset and their types
            features=datasets.Features(
                {
                    ""pose"": datasets.features.Sequence(datasets.features.Array2D(shape=(137, 2), dtype=""float32""))
                }
            ),
            homepage=_HOMEPAGE,
            citation=_CITATION,
        )
    def _generate_examples(self):
        """""" Yields examples. """"""

        yield 1, {
            ""pose"": [np.zeros(shape=(137, 2), dtype=np.float32)]
        }
```

But this doesn't work -
> pyarrow.lib.ArrowNotImplementedError: MakeBuilder: cannot construct builder for type extension<arrow.py_extension_type>"
https://github.com/huggingface/datasets/issues/885,Very slow cold-start,"['Good point!'
 'Yes indeed. We can probably improve that by using lazy imports'
 '#1690 added fast start-up of the library ']","Hi,
I expect when importing `datasets` that nothing major happens in the background, and so the import should be insignificant.
When I load a metric, or a dataset, its fine that it takes time.

The following ranges from 3 to 9 seconds:
```
python -m timeit -n 1 -r 1 'from datasets import load_dataset'
```

edit:
sorry for the mis-tag, not sure how I added it."
https://github.com/huggingface/datasets/issues/883,Downloading/caching only a part of a datasets' dataset.,"['Not at the moment but we could likely support this feature.' '?'
 'I think it would be a very helpful feature, because sometimes one only wants to evaluate models on the dev set, and the whole training data may be many times bigger.\r\nThis makes the task impossible with limited memory resources.']","Hi,
I want to use the validation data *only* (of natural question).
I don't want to have the whole dataset cached in my machine, just the dev set.
Is this possible? I can't find a way to do it in the docs.

Thank you,
Sapir"
https://github.com/huggingface/datasets/issues/880,Add SQA,"['I’ll take this one to test the workflow for the sprint next week cc @yjernite @lhoestq '
 '@thomwolf here\'s a slightly adapted version of the code from the [official Tapas repository](https://github.com/google-research/tapas/blob/master/tapas/utils/interaction_utils.py) that is used to turn the `answer_coordinates` and `answer_texts` columns into true Python lists of tuples/strings:\r\n\r\n```\r\nimport pandas as pd\r\nimport ast\r\n\r\ndata = pd.read_csv(""/content/sqa_data/random-split-1-dev.tsv"", sep=\'\\t\')\r\n\r\ndef _parse_answer_coordinates(answer_coordinate_str):\r\n  """"""Parses the answer_coordinates of a question.\r\n  Args:\r\n    answer_coordinate_str: A string representation of a Python list of tuple\r\n      strings.\r\n      For example: ""[\'(1, 4)\',\'(1, 3)\', ...]""\r\n  """"""\r\n\r\n  try:\r\n    answer_coordinates = []\r\n    # make a list of strings\r\n    coords = ast.literal_eval(answer_coordinate_str)\r\n    # parse each string as a tuple\r\n    for row_index, column_index in sorted(\r\n        ast.literal_eval(coord) for coord in coords):\r\n      answer_coordinates.append((row_index, column_index))\r\n  except SyntaxError:\r\n    raise ValueError(\'Unable to evaluate %s\' % answer_coordinate_str)\r\n  \r\n  return answer_coordinates\r\n\r\n\r\ndef _parse_answer_text(answer_text):\r\n  """"""Populates the answer_texts field of `answer` by parsing `answer_text`.\r\n  Args:\r\n    answer_text: A string representation of a Python list of strings.\r\n      For example: ""[u\'test\', u\'hello\', ...]""\r\n  """"""\r\n  try:\r\n    answer = []\r\n    for value in ast.literal_eval(answer_text):\r\n      answer.append(value)\r\n  except SyntaxError:\r\n    raise ValueError(\'Unable to evaluate %s\' % answer_text)\r\n\r\n  return answer\r\n\r\ndata[\'answer_coordinates\'] = data[\'answer_coordinates\'].apply(lambda coords_str: _parse_answer_coordinates(coords_str))\r\ndata[\'answer_text\'] = data[\'answer_text\'].apply(lambda txt: _parse_answer_text(txt))\r\n```\r\n\r\nHere I\'m using Pandas to read in one of the TSV files (the dev set). \r\n\r\n'
 'Closing since SQA was added in #1566 ']","## Adding a Dataset
- **Name:** SQA (Sequential Question Answering) by Microsoft. 
- **Description:** The SQA dataset was created to explore the task of answering sequences of inter-related questions on HTML tables. It has 6,066 sequences with 17,553 questions in total.
- **Paper:** https://www.microsoft.com/en-us/research/publication/search-based-neural-structured-learning-sequential-question-answering/
- **Data:** https://www.microsoft.com/en-us/download/details.aspx?id=54253
- **Motivation:** currently, the [Tapas](https://ai.googleblog.com/2020/04/using-neural-networks-to-find-answers.html) algorithm by Google AI is being added to the Transformers library (see https://github.com/huggingface/transformers/pull/8113). It would be great to use that model in combination with this dataset, on which it achieves SOTA results (average question accuracy of 0.71).

Note 1: this dataset actually consists of 2 types of files: 
1) TSV files, containing the questions, answer coordinates and answer texts (for training, dev and test)
2) a folder of csv files, which contain the actual tabular data

Note 2: if you download the dataset straight from the download link above, then you will see that the `answer_coordinates` and `answer_text` columns are string lists of string tuples and strings respectively, which is not ideal. It would be better to make them true Python lists of tuples and strings respectively (using `ast.literal_eval`), before uploading them to the HuggingFace hub.

Adding this would be great! Then we could possibly also add [WTQ (WikiTable Questions)](https://github.com/ppasupat/WikiTableQuestions) and [TabFact (Tabular Fact Checking)](https://github.com/wenhuchen/Table-Fact-Checking) on which TAPAS also achieves state-of-the-art results. Note that the TAPAS algorithm requires these datasets to first be converted into the SQA format.

Instructions to add a new dataset can be found [here](https://huggingface.co/docs/datasets/share_dataset.html).
"
https://github.com/huggingface/datasets/issues/879,boolq does not load ,"['Hi ! It runs on my side without issues. I tried\r\n```python\r\nfrom datasets import load_dataset\r\nload_dataset(""boolq"")\r\n```\r\n\r\nWhat version of datasets and tensorflow are your runnning ?\r\nAlso if you manage to get a minimal reproducible script (on google colab for example) that would be useful.'
 'hey\ni do the exact same commands. for me it fails i guess might be issues with\ncaching maybe?\nthanks\nbest\nrabeeh\n\nOn Tue, Nov 24, 2020, 10:24 AM Quentin Lhoest <notifications@github.com>\nwrote:\n\n> Hi ! It runs on my side without issues. I tried\n>\n> from datasets import load_datasetload_dataset(""boolq"")\n>\n> What version of datasets and tensorflow are your runnning ?\n> Also if you manage to get a minimal reproducible script (on google colab\n> for example) that would be useful.\n>\n> —\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/huggingface/datasets/issues/879#issuecomment-732769114>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ABP4ZCGGDR2FUMRKZTIY5CTSRN3VXANCNFSM4T7R3U6A>\n> .\n>\n'
 'Could you check if it works on the master branch ?\r\nYou can use `load_dataset(""boolq"", script_version=""master"")` to do so.\r\nWe did some changes recently in boolq to remove the TF dependency and we changed the way the data files are downloaded in https://github.com/huggingface/datasets/pull/881']","Hi
I am getting these errors trying to load boolq thanks 

Traceback (most recent call last):
  File ""test.py"", line 5, in <module>
    data = AutoTask().get(""boolq"").get_dataset(""train"", n_obs=10)
  File ""/remote/idiap.svm/user.active/rkarimi/dev/internship/seq2seq/tasks/tasks.py"", line 42, in get_dataset
    dataset = self.load_dataset(split=split)
  File ""/remote/idiap.svm/user.active/rkarimi/dev/internship/seq2seq/tasks/tasks.py"", line 38, in load_dataset
    return datasets.load_dataset(self.task.name, split=split)
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/load.py"", line 611, in load_dataset
    ignore_verifications=ignore_verifications,
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/builder.py"", line 476, in download_and_prepare
    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/builder.py"", line 531, in _download_and_prepare
    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)
  File "" /idiap/home/rkarimi/.cache/huggingface/modules/datasets_modules/datasets/boolq/2987db1f15deaa19500ae24de560eabeaf1f8ef51df88c0470beeec72943bf11/boolq.py"", line 74, in _split_generators
    downloaded_files = dl_manager.download_custom(urls_to_download, tf.io.gfile.copy)
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/utils/download_manager.py"", line 150, in download_custom
    get_from_cache(url, cache_dir=cache_dir, local_files_only=True, use_etag=False)
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/utils/file_utils.py"", line 472, in get_from_cache
    f""Cannot find the requested files in the cached path at {cache_path} and outgoing traffic has been""
FileNotFoundError: Cannot find the requested files in the cached path at /idiap/home/rkarimi/.cache/huggingface/datasets/eaee069e38f6ceaa84de02ad088c34e63ec97671f2cd1910ddb16b10dc60808c and outgoing traffic has been disabled. To enable file online look-ups, set 'local_files_only' to False.
"
https://github.com/huggingface/datasets/issues/878,Loading Data From S3 Path in Sagemaker,"['This would be a neat feature'
 '> neat feature\r\n\r\nI dint get these clearly, can you please elaborate like how to work on these '
 'It could maybe work almost out of the box just by using `cached_path` in the text/csv/json scripts, no?'
 ""Thanks thomwolf and julien-c\r\n\r\nI'm still confusion on what you guys said, \r\n\r\nI have solved the problem as follows:\r\n\r\n1. read the csv file using pandas from s3 \r\n2. Convert to dictionary key as column name and values as list column data\r\n3. convert it to Dataset using \r\n`from datasets import Dataset`\r\n`train_dataset = Dataset.from_dict(train_dict)`""
 ""We were brainstorming around your use-case.\r\n\r\nLet's keep the issue open for now, I think this is an interesting question to think about.""
 ""> We were brainstorming around your use-case.\r\n> \r\n> Let's keep the issue open for now, I think this is an interesting question to think about.\r\n\r\nSure thomwolf, Thanks for your concern ""
 ""I agree it would be cool to have that feature. Also that's good to know that pandas supports this.\r\nFor the moment I'd suggest to first download the files locally as thom suggested and then load the dataset by providing paths to the local files""
 ""Don't get\n""
 'Any updates on this issue?\r\nI face a similar issue. I have many parquet files in S3 and I would like to train on them. \r\nTo be honest I even face issues with only getting the last layer embedding out of them.'
 'Hi dorlavie, \r\nYou can find one solution that i have mentioned above, that can help you. \r\nAnd there is one more solution also which is downloading files locally\r\n'
 '> Hi dorlavie,\r\n> You can find one solution that i have mentioned above, that can help you.\r\n> And there is one more solution also which is downloading files locally\r\n\r\nmahesh1amour, thanks for the fast reply\r\n\r\nUnfortunately, in my case I can not read with pandas. The dataset is too big (50GB). \r\nIn addition, due to security concerns I am not allowed to save the data locally'
 ""@dorlavie could use `boto3` to download the data to your local machine and then load it with `dataset`\r\n\r\nboto3 example [documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-example-download-file.html)\r\n```python\r\nimport boto3\r\n\r\ns3 = boto3.client('s3')\r\ns3.download_file('BUCKET_NAME', 'OBJECT_NAME', 'FILE_NAME')\r\n```\r\n\r\ndatasets example [documentation](https://huggingface.co/docs/datasets/loading_datasets.html)\r\n\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('csv', data_files=['my_file_1.csv', 'my_file_2.csv', 'my_file_3.csv'])\r\n```\r\n""
 'Thanks @philschmid for the suggestion.\r\nAs I mentioned in the previous comment, due to security issues I can not save the data locally.\r\nI need to read it from S3 and process it directly.\r\n\r\nI guess that many other people try to train / fit those models on huge datasets (e.g entire Wiki), what is the best practice in those cases?'
 ""If I understand correctly you're not allowed to write data on disk that you downloaded from S3 for example ?\r\nOr is it the use of the `boto3` library that is not allowed in your case ?""
 '@lhoestq yes you are correct.\r\nI am not allowed to save the ""raw text"" locally - The ""raw text"" must be saved only on S3.\r\nI am allowed to save the output of any model locally. \r\nIt doesn\'t matter how I do it boto3/pandas/pyarrow, it is forbidden'
 ""@dorlavie are you using sagemaker for training too? Then you could use S3 URI, for example `s3://my-bucket/my-training-data` and pass it within the `.fit()` function when you start the sagemaker training job. Sagemaker would then download the data from s3 into the training runtime and you could load it from disk\r\n\r\n**sagemaker start training job**\r\n```python\r\npytorch_estimator.fit({'train':'s3://my-bucket/my-training-data','eval':'s3://my-bucket/my-evaluation-data'})\r\n```\r\n\r\n**in the train.py script**\r\n```python\r\nfrom datasets import load_from_disk\r\n\r\ntrain_dataset = load_from_disk(os.environ['SM_CHANNEL_TRAIN'])\r\n```\r\n\r\nI have created an example of how to use transformers and datasets with sagemaker. \r\nhttps://github.com/philschmid/huggingface-sagemaker-example/tree/main/03_huggingface_sagemaker_trainer_with_data_from_s3\r\n\r\nThe example contains a jupyter notebook `sagemaker-example.ipynb` and an `src/` folder. The sagemaker-example is a jupyter notebook that is used to create the training job on AWS Sagemaker. The `src/` folder contains the `train.py`, our training script, and `requirements.txt` for additional dependencies.\r\n\r\n""]","In Sagemaker Im tring to load the data set from S3 path as follows

`train_path = 's3://xxxxxxxxxx/xxxxxxxxxx/train.csv'
    valid_path = 's3://xxxxxxxxxx/xxxxxxxxxx/validation.csv'
    test_path = 's3://xxxxxxxxxx/xxxxxxxxxx/test.csv'
    
    data_files = {}
    data_files[""train""] = train_path
    data_files[""validation""] = valid_path
    data_files[""test""] = test_path
    extension = train_path.split(""."")[-1]
    datasets = load_dataset(extension, data_files=data_files, s3_enabled=True)
    print(datasets)`


I getting an error of

`algo-1-7plil_1  |   File ""main.py"", line 21, in <module>
algo-1-7plil_1  |     datasets = load_dataset(extension, data_files=data_files)
algo-1-7plil_1  |   File ""/opt/conda/lib/python3.6/site-packages/datasets/load.py"", line 603, in load_dataset
algo-1-7plil_1  |     **config_kwargs,
algo-1-7plil_1  |   File ""/opt/conda/lib/python3.6/site-packages/datasets/builder.py"", line 155, in __init__
algo-1-7plil_1  |     **config_kwargs,
algo-1-7plil_1  |   File ""/opt/conda/lib/python3.6/site-packages/datasets/builder.py"", line 305, in _create_builder_config
algo-1-7plil_1  |     m.update(str(os.path.getmtime(data_file)))
algo-1-7plil_1  |   File ""/opt/conda/lib/python3.6/genericpath.py"", line 55, in getmtime
algo-1-7plil_1  |     return os.stat(filename).st_mtime
algo-1-7plil_1  | FileNotFoundError: [Errno 2] No such file or directory: 's3://lsmv-sagemaker/pubmedbert/test.csv`

But when im trying with pandas , it is able to load from S3

Does the datasets library support S3 path to load"
https://github.com/huggingface/datasets/issues/877,DataLoader(datasets) become more and more slowly within iterations,"['Hi ! Thanks for reporting.\r\nDo you have the same slowdown when you iterate through the raw dataset object as well ? (no dataloader)\r\nIt would be nice to know whether it comes from the dataloader or not'
 '> Hi ! Thanks for reporting.\r\n> Do you have the same slowdown when you iterate through the raw dataset object as well ? (no dataloader)\r\n> It would be nice to know whether it comes from the dataloader or not\r\n\r\nI did not iter data from raw dataset, maybe I will test later. Now I iter all files directly from `open(file)`,  around 20000it/s.']","Hello, when I for loop my dataloader, the loading speed is becoming more and more slowly!
```
dataset = load_from_disk(dataset_path)  # around 21,000,000 lines

lineloader = tqdm(DataLoader(dataset, batch_size=1))
for idx, line in enumerate(lineloader):
     # do some thing for each line
```
In the begining, the loading speed is around 2000it/s, but after 1 minutes later, the speed is much slower, just around 800it/s.

And when I set `num_workers=4` in DataLoader, the loading speed is much lower, just 130it/s.

Could you please help me with this problem?
Thanks a lot!"
https://github.com/huggingface/datasets/issues/876,imdb dataset cannot be loaded ,"['It looks like there was an issue while building the imdb dataset.\r\nCould you provide more information about your OS and the version of python and `datasets` ?\r\n\r\nAlso could you try again with \r\n```python\r\ndataset = datasets.load_dataset(""imdb"", split=""train"", download_mode=""force_redownload"")\r\n```\r\nto make sure it\'s not a corrupted file issue ?'
 'I was using version 1.1.2 and this resolved with version 1.1.3, thanks. ']","Hi
I am trying to load the imdb train dataset

`dataset = datasets.load_dataset(""imdb"", split=""train"")`

getting following errors, thanks for your help 
```
Traceback (most recent call last):        
  File ""<stdin>"", line 1, in <module>
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/load.py"", line 611, in load_dataset
    ignore_verifications=ignore_verifications,
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/builder.py"", line 476, in download_and_prepare
    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/builder.py"", line 558, in _download_and_prepare
    verify_splits(self.info.splits, split_dict)
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/utils/info_utils.py"", line 73, in verify_splits
    raise NonMatchingSplitsSizesError(str(bad_splits))
datasets.utils.info_utils.NonMatchingSplitsSizesError: [{'expected': SplitInfo(name='test', num_bytes=32660064, num_examples=25000, dataset_name='imdb'), 'recorded': SplitInfo(name='test', num_bytes=26476338, num_examples=20316, dataset_name='imdb')}, {'expected': SplitInfo(name='train', num_bytes=33442202, num_examples=25000, dataset_name='imdb'), 'recorded': SplitInfo(name='train', num_bytes=0, num_examples=0, dataset_name='imdb')}, {'expected': SplitInfo(name='unsupervised', num_bytes=67125548, num_examples=50000, dataset_name='imdb'), 'recorded': SplitInfo(name='unsupervised', num_bytes=0, num_examples=0, dataset_name='imdb')}]
>>> dataset = datasets.load_dataset(""imdb"", split=""train"")

```
"
https://github.com/huggingface/datasets/issues/875,bug in boolq dataset loading,['I just opened a PR to fix this.\r\nThanks for reporting !'],"Hi
I am trying to load boolq dataset:

```
import datasets
datasets.load_dataset(""boolq"")
```

I am getting the following errors, thanks for your help 

```
>>> import datasets
2020-11-22 09:16:30.070332: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2020-11-22 09:16:30.070389: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
>>> datasets.load_dataset(""boolq"")
cahce dir  /idiap/temp/rkarimi/cache_home/datasets
cahce dir  /idiap/temp/rkarimi/cache_home/datasets
Using custom data configuration default
Downloading and preparing dataset boolq/default (download: 8.36 MiB, generated: 7.47 MiB, post-processed: Unknown size, total: 15.83 MiB) to /idiap/temp/rkarimi/cache_home/datasets/boolq/default/0.1.0/2987db1f15deaa19500ae24de560eabeaf1f8ef51df88c0470beeec72943bf11...
cahce dir  /idiap/temp/rkarimi/cache_home/datasets
cahce dir  /idiap/temp/rkarimi/cache_home/datasets/downloads
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/load.py"", line 611, in load_dataset
    ignore_verifications=ignore_verifications,
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/builder.py"", line 476, in download_and_prepare
    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/builder.py"", line 531, in _download_and_prepare
    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)
  File "" /idiap/home/rkarimi/.cache/huggingface/modules/datasets_modules/datasets/boolq/2987db1f15deaa19500ae24de560eabeaf1f8ef51df88c0470beeec72943bf11/boolq.py"", line 74, in _split_generators
    downloaded_files = dl_manager.download_custom(urls_to_download, tf.io.gfile.copy)
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/utils/download_manager.py"", line 149, in download_custom
    custom_download(url, path)
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py"", line 516, in copy_v2
    compat.path_to_bytes(src), compat.path_to_bytes(dst), overwrite)
tensorflow.python.framework.errors_impl.AlreadyExistsError: file already exists


```"
https://github.com/huggingface/datasets/issues/874,trec dataset unavailable ,"['This was fixed in #740 \r\nCould you try to update `datasets` and try again ?'
 'This has been fixed in datasets 1.1.3']","Hi
when I try to load the trec dataset I am getting these errors, thanks for your help

`datasets.load_dataset(""trec"",  split=""train"")
`
```
  File ""<stdin>"", line 1, in <module>
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/load.py"", line 611, in load_dataset
    ignore_verifications=ignore_verifications,
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/builder.py"", line 476, in download_and_prepare
    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/builder.py"", line 531, in _download_and_prepare
    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)
  File "" /idiap/home/rkarimi/.cache/huggingface/modules/datasets_modules/datasets/trec/ca4248481ad244f235f4cf277186cad2ee8769f975119a2bbfc41b8932b88bd7/trec.py"", line 140, in _split_generators
    dl_files = dl_manager.download_and_extract(_URLs)
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/utils/download_manager.py"", line 254, in download_and_extract
    return self.extract(self.download(url_or_urls))
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/utils/download_manager.py"", line 179, in download
    num_proc=download_config.num_proc,
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/utils/py_utils.py"", line 225, in map_nested
    _single_map_nested((function, obj, types, None, True)) for obj in tqdm(iterable, disable=disable_tqdm)
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/utils/py_utils.py"", line 225, in <listcomp>
    _single_map_nested((function, obj, types, None, True)) for obj in tqdm(iterable, disable=disable_tqdm)
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/utils/py_utils.py"", line 163, in _single_map_nested
    return function(data_struct)
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/utils/file_utils.py"", line 308, in cached_path
    use_etag=download_config.use_etag,
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/utils/file_utils.py"", line 477, in get_from_cache
    raise ConnectionError(""Couldn't reach {}"".format(url))
ConnectionError: Couldn't reach http://cogcomp.org/Data/QA/QC/train_5500.label
```"
https://github.com/huggingface/datasets/issues/873,"load_dataset('cnn_dalymail', '3.0.0') gives a 'Not a directory' error","['I get the same error. It was fixed some days ago, but again it appears'
 ""Hi @mrm8488 it's working again today without any fix so I am closing this issue.""
 'I see the issue happening again today - \r\n\r\n[nltk_data] Downloading package stopwords to /root/nltk_data...\r\n[nltk_data]   Package stopwords is already up-to-date!\r\nDownloading and preparing dataset cnn_dailymail/3.0.0 (download: 558.32 MiB, generated: 1.28 GiB, post-processed: Unknown size, total: 1.82 GiB) to /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/0128610a44e10f25b4af6689441c72af86205282d26399642f7db38fa7535602...\r\n\r\n---------------------------------------------------------------------------\r\n\r\nNotADirectoryError                        Traceback (most recent call last)\r\n\r\n<ipython-input-9-cd4bf8bea840> in <module>()\r\n     22 \r\n     23 \r\n---> 24 train = load_dataset(\'cnn_dailymail\', \'3.0.0\', split=\'train\')\r\n     25 validation = load_dataset(\'cnn_dailymail\', \'3.0.0\', split=\'validation\')\r\n     26 test = load_dataset(\'cnn_dailymail\', \'3.0.0\', split=\'test\')\r\n\r\n5 frames\r\n\r\n/root/.cache/huggingface/modules/datasets_modules/datasets/cnn_dailymail/0128610a44e10f25b4af6689441c72af86205282d26399642f7db38fa7535602/cnn_dailymail.py in _find_files(dl_paths, publisher, url_dict)\r\n    132     else:\r\n    133         logging.fatal(""Unsupported publisher: %s"", publisher)\r\n--> 134     files = sorted(os.listdir(top_dir))\r\n    135 \r\n    136     ret_files = []\r\n\r\nNotADirectoryError: [Errno 20] Not a directory: \'/root/.cache/huggingface/datasets/downloads/1bc05d24fa6dda2468e83a73cf6dc207226e01e3c48a507ea716dc0421da583b/cnn/stories\'\r\n\r\nCan someone please take a look ?'
 'Sometimes happens. Try in a while' 'It is working now, thank you. ']","```
from datasets import load_dataset
dataset = load_dataset('cnn_dailymail', '3.0.0')
```
Stack trace:
```
---------------------------------------------------------------------------

NotADirectoryError                        Traceback (most recent call last)

<ipython-input-6-2e06a8332652> in <module>()
      1 from datasets import load_dataset
----> 2 dataset = load_dataset('cnn_dailymail', '3.0.0')

5 frames

/usr/local/lib/python3.6/dist-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, save_infos, script_version, **config_kwargs)
    608         download_config=download_config,
    609         download_mode=download_mode,
--> 610         ignore_verifications=ignore_verifications,
    611     )
    612 

/usr/local/lib/python3.6/dist-packages/datasets/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, **download_and_prepare_kwargs)
    513                     if not downloaded_from_gcs:
    514                         self._download_and_prepare(
--> 515                             dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
    516                         )
    517                     # Sync info

/usr/local/lib/python3.6/dist-packages/datasets/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)
    568         split_dict = SplitDict(dataset_name=self.name)
    569         split_generators_kwargs = self._make_split_generators_kwargs(prepare_split_kwargs)
--> 570         split_generators = self._split_generators(dl_manager, **split_generators_kwargs)
    571 
    572         # Checksums verification

/root/.cache/huggingface/modules/datasets_modules/datasets/cnn_dailymail/0128610a44e10f25b4af6689441c72af86205282d26399642f7db38fa7535602/cnn_dailymail.py in _split_generators(self, dl_manager)
    252     def _split_generators(self, dl_manager):
    253         dl_paths = dl_manager.download_and_extract(_DL_URLS)
--> 254         train_files = _subset_filenames(dl_paths, datasets.Split.TRAIN)
    255         # Generate shared vocabulary
    256 

/root/.cache/huggingface/modules/datasets_modules/datasets/cnn_dailymail/0128610a44e10f25b4af6689441c72af86205282d26399642f7db38fa7535602/cnn_dailymail.py in _subset_filenames(dl_paths, split)
    153     else:
    154         logging.fatal(""Unsupported split: %s"", split)
--> 155     cnn = _find_files(dl_paths, ""cnn"", urls)
    156     dm = _find_files(dl_paths, ""dm"", urls)
    157     return cnn + dm

/root/.cache/huggingface/modules/datasets_modules/datasets/cnn_dailymail/0128610a44e10f25b4af6689441c72af86205282d26399642f7db38fa7535602/cnn_dailymail.py in _find_files(dl_paths, publisher, url_dict)
    132     else:
    133         logging.fatal(""Unsupported publisher: %s"", publisher)
--> 134     files = sorted(os.listdir(top_dir))
    135 
    136     ret_files = []

NotADirectoryError: [Errno 20] Not a directory: '/root/.cache/huggingface/datasets/downloads/1bc05d24fa6dda2468e83a73cf6dc207226e01e3c48a507ea716dc0421da583b/cnn/stories'
```
I have ran the code on Google Colab"
https://github.com/huggingface/datasets/issues/871,terminate called after throwing an instance of 'google::protobuf::FatalException',"['Loading the iwslt2017-en-nl config of iwslt2017 works fine on my side. \r\nMaybe you can open an issue on transformers as well ? And also add more details about your environment (OS, python version, version of transformers and datasets etc.)'
 'closing now, figured out this is because the max length of decoder was set smaller than the input_dimensions. thanks ']","Hi
I am using the dataset ""iwslt2017-en-nl"", and after downloading it I am getting this error when trying to evaluate it on T5-base with seq2seq_trainer.py in the huggingface repo could you assist me please? thanks 


100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [02:47<00:00,  2.18s/it][libprotobuf FATAL /sentencepiece/src/../third_party/protobuf-lite/google/protobuf/repeated_field.h:1505] CHECK failed: (index) >= (0): 
terminate called after throwing an instance of 'google::protobuf::FatalException'
  what():  CHECK failed: (index) >= (0): 
run_t5_base_eval.sh: line 19:  5795 Aborted "
https://github.com/huggingface/datasets/issues/870,[Feature Request] Add optional parameter in text loading script to preserve linebreaks,"[""Hi ! Thanks for your message.\r\nIndeed it's a free feature we can add and that can be useful.\r\nIf you want to contribute, feel free to open a PR to add it to the text dataset script :)""]","I'm working on a project about rhyming verse using phonetic poetry and song lyrics, and line breaks are a vital part of the data. 

I recently switched over to use the datasets library when my various corpora grew larger than my computer's memory. And so far, it is SO great. 

But the first time I processed all of my data into a dataset, I hadn't realized the text loader script was processing the source files line-by-line and stripping off the newlines. 

Once I caught the issue, I made my own data loader by modifying one line in the default text loader (changing `batch = batch.splitlines()` to `batch = batch.splitlines(True)` inside `_generate_tables`). And so I'm all set as far as my project is concerned.

But if my use case is more general, it seems like it'd be pretty trivial to add a kwarg to the default text loader called keeplinebreaks or something, which would default to False and get passed to `splitlines()`. "
https://github.com/huggingface/datasets/issues/866,OSCAR from Inria group,"['PR is already open here : #348 \r\nThe only thing remaining is to compute the metadata of each subdataset (one per language + shuffled/unshuffled).\r\nAs soon as #863 is merged we can start computing them. This will take a bit of time though'
 'Grand, thanks for this!']","## Adding a Dataset
- **Name:** *OSCAR* (Open Super-large Crawled ALMAnaCH coRpus), multilingual parsing of Common Crawl (separate crawls for many different languages), [here](https://oscar-corpus.com/).
- **Description:** *OSCAR or Open Super-large Crawled ALMAnaCH coRpus is a huge multilingual corpus obtained by language classification and filtering of the Common Crawl corpus using the goclassy architecture.*
- **Paper:** *[here](https://hal.inria.fr/hal-02148693)*
- **Data:** *[here](https://oscar-corpus.com/)*
- **Motivation:** *useful for unsupervised tasks in separate languages. In an ideal world, your team would be able to obtain the unshuffled version, that could be used to train GPT-2-like models (the shuffled version, I suppose, could be used for translation).*

I am aware that you do offer the ""colossal"" Common Crawl dataset already, but this has the advantage to be available in many subcorpora for different languages.
"
https://github.com/huggingface/datasets/issues/865,Have Trouble importing `datasets`,"[""I'm sorry, this was a problem with my environment.\r\nNow that I have identified the cause of environmental dependency, I would like to fix it and try it.\r\nExcuse me for making a noise.""]","I'm failing to import transformers (v4.0.0-dev), and tracing the cause seems to be failing to import datasets.

I cloned the newest version of datasets (master branch), and do `pip install -e .`.

Then, `import datasets` causes the error below.

```
~/workspace/Clone/datasets/src/datasets/utils/file_utils.py in <module>
    116 sys.path.append(str(HF_MODULES_CACHE))
    117 
--> 118 os.makedirs(HF_MODULES_CACHE, exist_ok=True)
    119 if not os.path.exists(os.path.join(HF_MODULES_CACHE, ""__init__.py"")):
    120     with open(os.path.join(HF_MODULES_CACHE, ""__init__.py""), ""w""):

~/.pyenv/versions/anaconda3-2020.07/lib/python3.8/os.py in makedirs(name, mode, exist_ok)
    221             return
    222     try:
--> 223         mkdir(name, mode)
    224     except OSError:
    225         # Cannot rely on checking for EEXIST, since the operating system 

FileNotFoundError: [Errno 2] No such file or directory: '<MY_HOME_DIRECTORY>/.cache/huggingface/modules'
```

The error occurs in `os.makedirs` in `file_utils.py`, even though `exist_ok = True` option is set.
(I use Python 3.8, so `exist_ok` is expected to work.)

I've checked some environment variables, and they are set as below.

```
*** NameError: name 'HF_MODULES_CACHE' is not defined
*** NameError: name 'hf_cache_home' is not defined
*** NameError: name 'XDG_CACHE_HOME' is not defined
```

Should I set some environment variables before using this library?
And, do you have any idea why ""No such file or directory"" occurs even though the `exist_ok = True` option is set?

Thank you in advance."
https://github.com/huggingface/datasets/issues/864,Unable to download cnn_dailymail dataset,"['Same error here!\r\n'
 ""Same here! My kaggle notebook stopped working like yesterday. It's strange because I have fixed version of datasets==1.1.2""
 ""I'm looking at it right now""
 'I couldn\'t reproduce unfortunately. I tried\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nload_dataset(""cnn_dailymail"", ""3.0.0"", download_mode=""force_redownload"")\r\n```\r\nand it worked fine on both my env (python 3.7.2) and colab (python 3.6.9)\r\n\r\nMaybe there was an issue with the google drive download link of the dataset ?\r\nAre you still having the issue ? If so could your give me more info about your python and requests version ?'
 ""No, It's working fine now. Very strange. Here are my python and request versions\r\n\r\nrequests         2.24.0\r\nPython 3.8.2""
 ""It's working as expected.  Closing the issue \r\n\r\nThanks everybody.""]","### Script to reproduce the error
```
from datasets import load_dataset

train_dataset = load_dataset(""cnn_dailymail"", ""3.0.0"", split= 'train[:10%')
valid_dataset = load_dataset(""cnn_dailymail"",""3.0.0"", split=""validation[:5%]"")
```


### Error
```
---------------------------------------------------------------------------
NotADirectoryError                        Traceback (most recent call last)
<ipython-input-8-47c39c228935> in <module>()
      1 from datasets import load_dataset
      2 
----> 3 train_dataset = load_dataset(""cnn_dailymail"", ""3.0.0"", split= 'train[:10%')
      4 valid_dataset = load_dataset(""cnn_dailymail"",""3.0.0"", split=""validation[:5%]"")

5 frames
/usr/local/lib/python3.6/dist-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, save_infos, script_version, **config_kwargs)
    609         download_config=download_config,
    610         download_mode=download_mode,
--> 611         ignore_verifications=ignore_verifications,
    612     )
    613 

/usr/local/lib/python3.6/dist-packages/datasets/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, **download_and_prepare_kwargs)
    469                     if not downloaded_from_gcs:
    470                         self._download_and_prepare(
--> 471                             dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
    472                         )
    473                     # Sync info

/usr/local/lib/python3.6/dist-packages/datasets/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)
    524         split_dict = SplitDict(dataset_name=self.name)
    525         split_generators_kwargs = self._make_split_generators_kwargs(prepare_split_kwargs)
--> 526         split_generators = self._split_generators(dl_manager, **split_generators_kwargs)
    527 
    528         # Checksums verification

/root/.cache/huggingface/modules/datasets_modules/datasets/cnn_dailymail/0128610a44e10f25b4af6689441c72af86205282d26399642f7db38fa7535602/cnn_dailymail.py in _split_generators(self, dl_manager)
    252     def _split_generators(self, dl_manager):
    253         dl_paths = dl_manager.download_and_extract(_DL_URLS)
--> 254         train_files = _subset_filenames(dl_paths, datasets.Split.TRAIN)
    255         # Generate shared vocabulary
    256 

/root/.cache/huggingface/modules/datasets_modules/datasets/cnn_dailymail/0128610a44e10f25b4af6689441c72af86205282d26399642f7db38fa7535602/cnn_dailymail.py in _subset_filenames(dl_paths, split)
    153     else:
    154         logging.fatal(""Unsupported split: %s"", split)
--> 155     cnn = _find_files(dl_paths, ""cnn"", urls)
    156     dm = _find_files(dl_paths, ""dm"", urls)
    157     return cnn + dm

/root/.cache/huggingface/modules/datasets_modules/datasets/cnn_dailymail/0128610a44e10f25b4af6689441c72af86205282d26399642f7db38fa7535602/cnn_dailymail.py in _find_files(dl_paths, publisher, url_dict)
    132     else:
    133         logging.fatal(""Unsupported publisher: %s"", publisher)
--> 134     files = sorted(os.listdir(top_dir))
    135 
    136     ret_files = []

NotADirectoryError: [Errno 20] Not a directory: '/root/.cache/huggingface/datasets/downloads/1bc05d24fa6dda2468e83a73cf6dc207226e01e3c48a507ea716dc0421da583b/cnn/stories'

```

Thanks for any suggestions."
https://github.com/huggingface/datasets/issues/861,Possible Bug: Small training/dataset file creates gigantic output,"[""The preprocessing tokenizes the input text. Tokenization outputs `input_ids`, `attention_mask`, `token_type_ids` and `special_tokens_mask`. All those are of length`max_seq_length` because of padding. Therefore for each sample it generate 4 *`max_seq_length` integers. Currently they're all saved as int64. This is why the tokenization takes so much space.\r\n\r\nI'm sure we can optimize that though\r\nWhat do you think @sgugger ?""
 ""First I think we should disable padding in the dataset processing and let the data collator do it.\r\n\r\nThen I'm wondering if you need attention_mask and token_type_ids at this point ?\r\n\r\nFinally we can also specify the output feature types at this line https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py#L280 to use more optimized integer precisions for the output. Maybe something like:\r\n- input_ids: uint16 or uint32\r\n- token_type_ids: uint8 or bool\r\n- attention_mask: bool\r\n- special_tokens_mask: bool\r\n\r\nAlso IMO these changes are all on the `transformers` side. Maybe we should discuss on the `transformers` repo""
 ""> First I think we should disable padding in the dataset processing and let the data collator do it.\r\n\r\nNo, you can't do that on TPUs as dynamic shapes will result in a very slow training. The script can however be tweaked to use the `PaddingDataCollator` with a fixed max length instead of dynamic batching.\r\n\r\nFor the other optimizations, they can be done by changing the script directly for each user's use case. Not sure we can find something that is general enough to be in transformers or the examples script.""
 'Oh yes right..\r\nDo you think that a lazy map feature on the `datasets` side could help to avoid storing padded tokenized texts then ?'
 ""I think I can do the tweak mentioned above with the data collator as short fix (but fully focused on v4 right now so that will be for later this week, beginning of next week :-) ).\r\nIf it doesn't hurt performance to tokenize on the fly, that would clearly be the long-term solution however!""
 ""> Hey guys,\r\n> \r\n> I was trying to create a new bert model from scratch via _huggingface transformers + tokenizers + dataets_ (actually using this example script by your team: https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py). It was supposed to be a first test with a small 5 GB raw text file but I can't even end the preprocessing handled by datasets because this tiny 5 GB text file becomes more than 1 TB when processing. My system was running out of space and crashed prematurely.\r\n> \r\n> I've done training from scratch via Google's bert repo in the past and I can remember that the resulting pretraining data can become quite big. But 5 GB becoming 1 TB was never the case. Is this considered normal or is it a bug?\r\n> \r\n> I've used the following CMD:\r\n> `python xla_spawn.py --num_cores=8 run_mlm.py --model_type bert --config_name config.json --tokenizer_name tokenizer.json --train_file dataset_full.txt --do_train --output_dir out --max_steps 500000 --save_steps 2500 --save_total_limit 2 --prediction_loss_only --line_by_line --max_seq_length 128 --pad_to_max_length --preprocessing_num_workers 16 --per_device_train_batch_size 128 --overwrite_output_dir --debug`\r\n\r\nIt's actually because of the parameter 'preprocessing_num_worker' when using TPU. \r\nI am also planning to have my model trained on the google TPU with a 11gb text corpus. With x8 cores enabled, each TPU core has its own dataset.  When not using distributed training, the preprocessed file is about 77gb. On the opposite, if enable xla, the file produced will easily consume all my free space(more than 220gb, I think it will be, in the end, around 600gb ). \r\nSo I think that's maybe where the problem came from. \r\n\r\nIs there any possibility that all of the cores share the same preprocess dataset?\r\n\r\n@sgugger @RammMaschine ""
 ""Hi @NebelAI, we have optimized Datasets' disk usage in the latest release v1.5.\r\n\r\nFeel free to update your Datasets version\r\n```shell\r\npip install -U datasets\r\n```\r\nand see if it better suits your needs.""]","Hey guys,

I was trying to create a new bert model from scratch via _huggingface transformers + tokenizers + dataets_ (actually using this example script by your team: https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py). It was supposed to be a first test with a small 5 GB raw text file but I can't even end the preprocessing handled by datasets because this tiny 5 GB text file becomes more than 1 TB when processing. My system was running out of space and crashed prematurely.

I've done training from scratch via Google's bert repo in the past and I can remember that the resulting pretraining data can become quite big. But 5 GB becoming 1 TB was never the case. Is this considered normal or is it a bug?

I've used the following CMD:
`python xla_spawn.py --num_cores=8 run_mlm.py --model_type bert --config_name config.json --tokenizer_name tokenizer.json --train_file dataset_full.txt --do_train --output_dir out --max_steps 500000 --save_steps 2500 --save_total_limit 2 --prediction_loss_only --line_by_line --max_seq_length 128 --pad_to_max_length --preprocessing_num_workers 16 --per_device_train_batch_size 128 --overwrite_output_dir --debug`

"
https://github.com/huggingface/datasets/issues/854,wmt16 does not download ,"['Hi,I also posted it to the forum, but this is a bug, perhaps it needs to be reported here? thanks '
 ""It looks like the official OPUS server for WMT16 doesn't provide the data files anymore (503 error).\r\nI searched a bit and couldn't find a mirror except maybe http://nlp.ffzg.hr/resources/corpora/setimes/ (the data are a cleaned version of the original ones though)\r\nShould we consider replacing the old urls with these ones even though it's not the exact same data ?""
 'The data storage is down at the moment. Sorry. Hopefully, it will come back soon. Apologies for the inconvenience ...'
 'Dear great huggingface team, this is not working yet, I really appreciate some temporary fix on this, I need this for my project and this is time sensitive and I will be grateful for your help on this. '
 'We have reached out to the OPUS team which is currently working on making the data available again. Cc @jorgtied '
 'thank you @thomwolf  and HuggingFace team for the help. '
 'OPUS is still down - hopefully back tomorrow.'
 'Hi, this is still down, I would be really grateful if you could ping them one more time. thank you so much. '
 'Hi\r\nI am trying with multiple setting of wmt datasets and all failed so far, I need to have at least one dataset working for testing somecodes, and this is really time sensitive, I greatly appreciate letting me know of one translation datasets currently working. thanks '
 ""It is still down, unfortunately. I'm sorry for that. It should come up again later today or tomorrow at the latest if no additional complications will happen.""
 'Hi all, \r\nI pulled a request that fix this issue by replacing urls. \r\n\r\nhttps://github.com/huggingface/datasets/pull/1901\r\n\r\nThanks!\r\n'
 ""It's still down for the wmt.""]","Hi, I appreciate your help with the following error, thanks 

>>> from datasets import load_dataset
>>> dataset = load_dataset(""wmt16"", ""ro-en"", split=""train"")
Downloading and preparing dataset wmt16/ro-en (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/7b2c4443a7d34c2e13df267eaa8cab4c62dd82f6b62b0d9ecc2e3a673ce17308...
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/datasets/load.py"", line 611, in load_dataset
    ignore_verifications=ignore_verifications,
  File ""/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/datasets/builder.py"", line 476, in download_and_prepare
    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
  File ""/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/datasets/builder.py"", line 531, in _download_and_prepare
    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)
  File ""/root/.cache/huggingface/modules/datasets_modules/datasets/wmt16/7b2c4443a7d34c2e13df267eaa8cab4c62dd82f6b62b0d9ecc2e3a673ce17308/wmt_utils.py"", line 755, in _split_generators
    downloaded_files = dl_manager.download_and_extract(urls_to_download)
  File ""/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/datasets/utils/download_manager.py"", line 254, in download_and_extract
    return self.extract(self.download(url_or_urls))
  File ""/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/datasets/utils/download_manager.py"", line 179, in download
    num_proc=download_config.num_proc,
  File ""/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/datasets/utils/py_utils.py"", line 225, in map_nested
    _single_map_nested((function, obj, types, None, True)) for obj in tqdm(iterable, disable=disable_tqdm)
  File ""/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/datasets/utils/py_utils.py"", line 225, in <listcomp>
    _single_map_nested((function, obj, types, None, True)) for obj in tqdm(iterable, disable=disable_tqdm)
  File ""/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/datasets/utils/py_utils.py"", line 181, in _single_map_nested
    mapped = [_single_map_nested((function, v, types, None, True)) for v in pbar]
  File ""/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/datasets/utils/py_utils.py"", line 181, in <listcomp>
    mapped = [_single_map_nested((function, v, types, None, True)) for v in pbar]
  File ""/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/datasets/utils/py_utils.py"", line 163, in _single_map_nested
    return function(data_struct)
  File ""/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/datasets/utils/file_utils.py"", line 308, in cached_path
    use_etag=download_config.use_etag,
  File ""/root/anaconda3/envs/pytorch/lib/python3.6/site-packages/datasets/utils/file_utils.py"", line 475, in get_from_cache
    raise ConnectionError(""Couldn't reach {}"".format(url))
ConnectionError: Couldn't reach http://opus.nlpl.eu/download.php?f=SETIMES/v2/tmx/en-ro.tmx.gz"
https://github.com/huggingface/datasets/issues/853,concatenate_datasets support axis=0 or 1 ？,"['Unfortunately `concatenate_datasets` only supports concatenating the rows, while what you want to achieve is concatenate the columns.\r\nCurrently to add more columns to a dataset, one must use `map`.\r\nWhat you can do is somehting like this:\r\n```python\r\n# suppose you have datasets d1, d2, d3\r\ndef add_columns(example, index):\r\n    example.update(d2[index])\r\n    example.update(d3[index])\r\n    return example\r\n\r\nfull_dataset = d1.map(add_columns, with_indices=True)\r\n```'
 'Closing this one, feel free to re-open if you have other questions about this issue'
 ""That's not really difficult to add, though, no?\r\nI think it can be done without copy.\r\nMaybe let's add it to the roadmap?""
 ""Actually it's doable but requires to update the `Dataset._data_files` schema to support this.\r\nI'm re-opening this since we may want to add this in the future""
 'Hi @lhoestq, I would love to help and add this feature if still needed. My plan is to add an axis variable in the `concatenate_datasets` function in `arrow_dataset.py` and when that is set to 1 concatenate columns instead of rows. '
 ""Hi ! I would love to see this feature implemented as well :) Thank you for proposing your help !\r\n\r\nHere is a few things about the current implementation:\r\n- A dataset object is a wrapper of one `pyarrow.Table` that contains the data\r\n- Pyarrow offers an API that allows to transform Table objects. For example there are functions like `concat_tables`, `Table.rename_columns`, `Table.add_column` etc.\r\n\r\nTherefore adding columns from another dataset is possible thanks to the pyarrow API and in particular `Table.add_column` :) \r\n\r\nHowever this breaks some features we have regarding pickle. A dataset object can be pickled and unpickled without loading all the data in memory. It is useful for multiprocessing for example. Pickling a dataset object is possible thanks to the `Dataset._data_files` which defines the list of arrow files that will be used to form the final Table (basically all the data from each files are concatenated on axis 0).\r\n\r\nTherefore to be able to add columns to a Dataset and still be able to work with it in a multiprocessing setup, we need to extend this last aspect to be able to reconstruct a Table object from multiple arrow files that are combined in both axis 0 and 1. Currently this reconstruction mechanism only supports axis 0.\r\n\r\nI'm sure we can figure something out that enables users to add columns from another dataset while keeping the multiprocessing support.""
 '@lhoestq, we have two Pull Requests to implement:\r\n- Dataset.add_item: #1870\r\n- Dataset.add_column: #2145\r\nwhich add a single row or column, repectively.\r\n\r\nThe request here is to implement the concatenation of *multiple* rows/columns. Am I right?\r\n\r\nWe should agree on the API:\r\n- `concatenate_datasets` with `axis`?\r\n- other Dataset method name?'
 ""For the API, I like `concatenate_datasets` with `axis` personally :)\r\nFrom a list of `Dataset` objects, it would concatenate them to a new `Dataset` object backed by a `ConcatenationTable`, that is the concatenation of the tables of each input dataset. The concatenation is either on axis=0 (append rows) or on axis=1 (append columns).\r\n\r\nRegarding what we need to implement:\r\nThe axis=0 is already supported and is the current behavior of `concatenate_datasets`.\r\nAlso `add_item` is not needed to implement axis=1 (though it's an awesome addition to this library).\r\n\r\nTo implement axis=1, we either need `add_column` or a `ConcatenationTable` constructor to concatenate tables horizontally.\r\nI have a preference for using a `ConcatenationTable` constructor because this way we can end up with a `ConcatenationTable` with only 1 additional block per table, while `add_column` would add 1 block per new column.\r\n\r\nMaybe we can simply have an equivalent of `ConcatenationTable.from_tables` but for axis=1 ?\r\n`axis` could also be an argument of `ConcatenationTable.from_tables`""
 '@lhoestq I think I guessed your suggestions in advance... 😉 #2151'
 ""Cool ! Sorry I missed this one ^^\r\nI'm taking a look ;)""]","I want to achieve the following result
![image](https://user-images.githubusercontent.com/12437751/99207426-f0c8db80-27f8-11eb-820a-4d9f7287b742.png)
"
https://github.com/huggingface/datasets/issues/851,Add support for other languages for rouge,['@alexyalunin \r\n\r\nI did something similar for others languages.\r\n\r\n[Repo: rouge-metric](https://github.com/m3hrdadfi/rouge-metric)'],"I calculate rouge with
```
from datasets import load_metric
rouge = load_metric(""rouge"")
rouge_output = rouge.compute(predictions=['тест тест привет'], references=['тест тест пока'], rouge_types=[
    ""rouge2""])[""rouge2""].mid
print(rouge_output)
```
the result is
`Score(precision=0.0, recall=0.0, fmeasure=0.0)`
It seems like the `rouge_score` library that this metric uses filters all non-alphanueric latin characters 
in `rouge_scorer/tokenize.py` with `text = re.sub(r""[^a-z0-9]+"", "" "", six.ensure_str(text))`.
Please add support for other languages. "
https://github.com/huggingface/datasets/issues/849,Load amazon dataset,"['Thanks for reporting !\r\nWe plan to show information about the different configs of the datasets on the website, with the corresponding `load_dataset` calls.\r\n\r\nAlso I think the bullet points formatting has been fixed']","Hi,
I was going through amazon_us_reviews dataset and found that example API usage given on website is different from the API usage while loading dataset. 

Eg. what API usage is on the [website](https://huggingface.co/datasets/amazon_us_reviews) 
```
from datasets import load_dataset
dataset = load_dataset(""amazon_us_reviews"")
```
How it is when I tried (the error generated does point me to the right direction though)
```
from datasets import load_dataset
dataset = load_dataset(""amazon_us_reviews"", 'Books_v1_00')
``` 
Also, there is some issue with formatting as it's not showing bullet list in description with new line. Can I work on it?"
https://github.com/huggingface/datasets/issues/848,Error when concatenate_datasets,"[""As you can see in the error the test checks if `indices_mappings_in_memory` is True or not, which is different from the test you do in your script. In a dataset, both the data and the indices mapping can be either on disk or in memory.\r\n\r\nThe indices mapping correspond to a mapping on top of the data table that is used to re-order/select a sample of the original data table. For example if you do `dataset.train_test_split`, then the resulting train and test datasets will have both an indices mapping to tell which examples are in train and which ones in test.\r\n\r\nBefore saving your datasets on disk, you should call `dataset.flatten_indices()` to remove the indices mapping. It should fix your issue. Under the hood it will create a new data table using the indices mapping. The new data table is going to be a subset of the old one (for example taking only the test set examples), and since the indices mapping will be gone you'll be able to concatenate your datasets.\r\n""
 ""> As you can see in the error the test checks if `indices_mappings_in_memory` is True or not, which is different from the test you do in your script. In a dataset, both the data and the indices mapping can be either on disk or in memory.\r\n> \r\n> The indices mapping correspond to a mapping on top of the data table that is used to re-order/select a sample of the original data table. For example if you do `dataset.train_test_split`, then the resulting train and test datasets will have both an indices mapping to tell which examples are in train and which ones in test.\r\n> \r\n> Before saving your datasets on disk, you should call `dataset.flatten_indices()` to remove the indices mapping. It should fix your issue. Under the hood it will create a new data table using the indices mapping. The new data table is going to be a subset of the old one (for example taking only the test set examples), and since the indices mapping will be gone you'll be able to concatenate your datasets.\r\n\r\n`dataset.flatten_indices()` solved my problem, thanks so much!""
 '@lhoestq we can add a mention of `dataset.flatten_indices()` in the error message (no rush, just put it on your TODO list or I can do it when I come at it)'
 'Yup I agree ! And in the docs as well']","Hello, when I concatenate two dataset loading  from disk, I encountered a problem:
```
test_dataset = load_from_disk('data/test_dataset')
trn_dataset = load_from_disk('data/train_dataset')

train_dataset = concatenate_datasets([trn_dataset, test_dataset])
```
And it reported ValueError blow:
```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-38-74fa525512ca> in <module>
----> 1 train_dataset = concatenate_datasets([trn_dataset, test_dataset])

/opt/miniconda3/lib/python3.7/site-packages/datasets/arrow_dataset.py in concatenate_datasets(dsets, info, split)
   2547                 ""However datasets' indices {} come from memory and datasets' indices {} come from disk."".format(
   2548                     [i for i in range(len(dsets)) if indices_mappings_in_memory[i]],
-> 2549                     [i for i in range(len(dsets)) if not indices_mappings_in_memory[i]],
   2550                 )
   2551             )

ValueError: Datasets' indices should ALL come from memory, or should ALL come from disk.
However datasets' indices [1] come from memory and datasets' indices [0] come from disk.
```

But it's curious both of my datasets loading from disk, so I check the source code in `arrow_dataset.py` about the Error:
```
trn_dataset._data_files
# output
[{'filename': 'data/train_dataset/csv-train.arrow', 'skip': 0, 'take': 593264}]

test_dataset._data_files
# output
[{'filename': 'data/test_dataset/csv-test.arrow', 'skip': 0, 'take': 424383}]

print([not dset._data_files for dset in [trn_dataset, test_dataset]])
# [False, False]

# And I tested the code the same as arrow_dataset, but nothing happened
dsets = [trn_dataset, test_dataset]
dsets_in_memory = [not dset._data_files for dset in dsets]
if any(dset_in_memory != dsets_in_memory[0] for dset_in_memory in dsets_in_memory):
    raise ValueError(
        ""Datasets should ALL come from memory, or should ALL come from disk.\n""
        ""However datasets {} come from memory and datasets {} come from disk."".format(
            [i for i in range(len(dsets)) if dsets_in_memory[i]],
            [i for i in range(len(dsets)) if not dsets_in_memory[i]],
        )
    )
```

Any suggestions would be greatly appreciated! 
Thanks!"
https://github.com/huggingface/datasets/issues/847,"multiprocessing in dataset map ""can only test a child process""","[""It looks like an issue with wandb/tqdm here.\r\nWe're using the `multiprocess` library instead of the `multiprocessing` builtin python package to support various types of mapping functions. Maybe there's some sort of incompatibility.\r\n\r\nCould you make a minimal script to reproduce or a google colab ?""
 'hi facing the same issue here - \r\n\r\n`AssertionError: Caught AssertionError in DataLoader worker process 0.\r\nOriginal Traceback (most recent call last):\r\n  File ""/usr/lib/python3.6/logging/__init__.py"", line 996, in emit\r\n    stream.write(msg)\r\n  File ""/usr/local/lib/python3.6/dist-packages/wandb/sdk/lib/redirect.py"", line 100, in new_write\r\n    cb(name, data)\r\n  File ""/usr/local/lib/python3.6/dist-packages/wandb/sdk/wandb_run.py"", line 723, in _console_callback\r\n    self._backend.interface.publish_output(name, data)\r\n  File ""/usr/local/lib/python3.6/dist-packages/wandb/sdk/interface/interface.py"", line 153, in publish_output\r\n    self._publish_output(o)\r\n  File ""/usr/local/lib/python3.6/dist-packages/wandb/sdk/interface/interface.py"", line 158, in _publish_output\r\n    self._publish(rec)\r\n  File ""/usr/local/lib/python3.6/dist-packages/wandb/sdk/interface/interface.py"", line 456, in _publish\r\n    if self._process and not self._process.is_alive():\r\n  File ""/usr/lib/python3.6/multiprocessing/process.py"", line 134, in is_alive\r\n    assert self._parent_pid == os.getpid(), \'can only test a child process\'\r\nAssertionError: can only test a child process\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/worker.py"", line 198, in _worker_loop\r\n    data = fetcher.fetch(index)\r\n  File ""/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py"", line 44, in fetch\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n  File ""/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py"", line 44, in <listcomp>\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n  File ""<ipython-input-8-a4d9a08d114e>"", line 20, in __getitem__\r\n    return_token_type_ids=True\r\n  File ""/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py"", line 2405, in encode_plus\r\n    **kwargs,\r\n  File ""/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py"", line 2125, in _get_padding_truncation_strategies\r\n    ""Truncation was not explicitly activated but `max_length` is provided a specific value, ""\r\n  File ""/usr/lib/python3.6/logging/__init__.py"", line 1320, in warning\r\n    self._log(WARNING, msg, args, **kwargs)\r\n  File ""/usr/lib/python3.6/logging/__init__.py"", line 1444, in _log\r\n    self.handle(record)\r\n  File ""/usr/lib/python3.6/logging/__init__.py"", line 1454, in handle\r\n    self.callHandlers(record)\r\n  File ""/usr/lib/python3.6/logging/__init__.py"", line 1516, in callHandlers\r\n    hdlr.handle(record)\r\n  File ""/usr/lib/python3.6/logging/__init__.py"", line 865, in handle\r\n    self.emit(record)\r\n  File ""/usr/lib/python3.6/logging/__init__.py"", line 1000, in emit\r\n    self.handleError(record)\r\n  File ""/usr/lib/python3.6/logging/__init__.py"", line 917, in handleError\r\n    sys.stderr.write(\'--- Logging error ---\\n\')\r\n  File ""/usr/local/lib/python3.6/dist-packages/wandb/sdk/lib/redirect.py"", line 100, in new_write\r\n    cb(name, data)\r\n  File ""/usr/local/lib/python3.6/dist-packages/wandb/sdk/wandb_run.py"", line 723, in _console_callback\r\n    self._backend.interface.publish_output(name, data)\r\n  File ""/usr/local/lib/python3.6/dist-packages/wandb/sdk/interface/interface.py"", line 153, in publish_output\r\n    self._publish_output(o)\r\n  File ""/usr/local/lib/python3.6/dist-packages/wandb/sdk/interface/interface.py"", line 158, in _publish_output\r\n    self._publish(rec)\r\n  File ""/usr/local/lib/python3.6/dist-packages/wandb/sdk/interface/interface.py"", line 456, in _publish\r\n    if self._process and not self._process.is_alive():\r\n  File ""/usr/lib/python3.6/multiprocessing/process.py"", line 134, in is_alive\r\n    assert self._parent_pid == os.getpid(), \'can only test a child process\'\r\nAssertionError: can only test a child process`\r\n'
 'It looks like this warning : \r\n""Truncation was not explicitly activated but max_length is provided a specific value, ""\r\nis not handled well by wandb.\r\n\r\nThe error occurs when calling the tokenizer.\r\nMaybe you can try to specify `truncation=True` when calling the tokenizer to remove the warning ?\r\nOtherwise I don\'t know why wandb would fail on a warning. Maybe one of its logging handlers have some issues with the logging of tokenizers. Maybe @n1t0 knows more about this ?'
 'I\'m having a similar issue but when I try to do multiprocessing with the `DataLoader`\r\n\r\nCode to reproduce:\r\n\r\n```\r\nfrom datasets import load_dataset\r\n\r\nbook_corpus = load_dataset(\'bookcorpus\', \'plain_text\', cache_dir=\'/home/ad/Desktop/bookcorpus\', split=\'train[:1%]\')\r\nbook_corpus = book_corpus.map(encode, batched=True, num_proc=20, load_from_cache_file=True, batch_size=5000)\r\nbook_corpus.set_format(type=\'torch\', columns=[\'text\', ""input_ids"", ""attention_mask"", ""token_type_ids""])\r\n\r\nfrom transformers import DataCollatorForWholeWordMask\r\nfrom transformers import Trainer, TrainingArguments\r\n\r\ndata_collator = DataCollatorForWholeWordMask(\r\n    tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\r\n\r\ntraining_args = TrainingArguments(\r\n    output_dir=""./mobile_linear_att_8L_128_128_03layerdrop_shared"",\r\n    overwrite_output_dir=True,\r\n    num_train_epochs=1,\r\n    per_device_train_batch_size=64,\r\n    save_steps=50,\r\n    save_total_limit=2,\r\n    logging_first_step=True,\r\n    warmup_steps=100,\r\n    logging_steps=50,\r\n    gradient_accumulation_steps=1,\r\n    fp16=True,\r\n    **dataloader_num_workers=10**,\r\n)\r\n\r\ntrainer = Trainer(\r\n    model=model,\r\n    args=training_args,\r\n    data_collator=data_collator,\r\n    train_dataset=book_corpus,\r\n    tokenizer=tokenizer)\r\n\r\ntrainer.train()\r\n```\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n<timed eval> in <module>\r\n\r\n~/anaconda3/envs/tfm/lib/python3.6/site-packages/transformers/trainer.py in train(self, model_path, trial)\r\n    869             self.control = self.callback_handler.on_epoch_begin(self.args, self.state, self.control)\r\n    870 \r\n--> 871             for step, inputs in enumerate(epoch_iterator):\r\n    872 \r\n    873                 # Skip past any already trained steps if resuming training\r\n\r\n~/anaconda3/envs/tfm/lib/python3.6/site-packages/torch/utils/data/dataloader.py in __next__(self)\r\n    433         if self._sampler_iter is None:\r\n    434             self._reset()\r\n--> 435         data = self._next_data()\r\n    436         self._num_yielded += 1\r\n    437         if self._dataset_kind == _DatasetKind.Iterable and \\\r\n\r\n~/anaconda3/envs/tfm/lib/python3.6/site-packages/torch/utils/data/dataloader.py in _next_data(self)\r\n   1083             else:\r\n   1084                 del self._task_info[idx]\r\n-> 1085                 return self._process_data(data)\r\n   1086 \r\n   1087     def _try_put_index(self):\r\n\r\n~/anaconda3/envs/tfm/lib/python3.6/site-packages/torch/utils/data/dataloader.py in _process_data(self, data)\r\n   1109         self._try_put_index()\r\n   1110         if isinstance(data, ExceptionWrapper):\r\n-> 1111             data.reraise()\r\n   1112         return data\r\n   1113 \r\n\r\n~/anaconda3/envs/tfm/lib/python3.6/site-packages/torch/_utils.py in reraise(self)\r\n    426             # have message field\r\n    427             raise self.exc_type(message=msg)\r\n--> 428         raise self.exc_type(msg)\r\n    429 \r\n    430 \r\n\r\nAssertionError: Caught AssertionError in DataLoader worker process 0.\r\nOriginal Traceback (most recent call last):\r\n  File ""/home/ad/anaconda3/envs/tfm/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py"", line 198, in _worker_loop\r\n    data = fetcher.fetch(index)\r\n  File ""/home/ad/anaconda3/envs/tfm/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py"", line 44, in fetch\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n  File ""/home/ad/anaconda3/envs/tfm/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py"", line 44, in <listcomp>\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n  File ""/home/ad/anaconda3/envs/tfm/lib/python3.6/site-packages/datasets/arrow_dataset.py"", line 1087, in __getitem__\r\n    format_kwargs=self._format_kwargs,\r\n  File ""/home/ad/anaconda3/envs/tfm/lib/python3.6/site-packages/datasets/arrow_dataset.py"", line 1074, in _getitem\r\n    format_kwargs=format_kwargs,\r\n  File ""/home/ad/anaconda3/envs/tfm/lib/python3.6/site-packages/datasets/arrow_dataset.py"", line 890, in _convert_outputs\r\n    v = map_nested(command, v, **map_nested_kwargs)\r\n  File ""/home/ad/anaconda3/envs/tfm/lib/python3.6/site-packages/datasets/utils/py_utils.py"", line 225, in map_nested\r\n    return function(data_struct)\r\n  File ""/home/ad/anaconda3/envs/tfm/lib/python3.6/site-packages/datasets/arrow_dataset.py"", line 851, in command\r\n    return torch.tensor(x, **format_kwargs)\r\n  File ""/home/ad/anaconda3/envs/tfm/lib/python3.6/warnings.py"", line 101, in _showwarnmsg\r\n    _showwarnmsg_impl(msg)\r\n  File ""/home/ad/anaconda3/envs/tfm/lib/python3.6/warnings.py"", line 30, in _showwarnmsg_impl\r\n    file.write(text)\r\n  File ""/home/ad/anaconda3/envs/tfm/lib/python3.6/site-packages/wandb/sdk/lib/redirect.py"", line 100, in new_write\r\n    cb(name, data)\r\n  File ""/home/ad/anaconda3/envs/tfm/lib/python3.6/site-packages/wandb/sdk/wandb_run.py"", line 723, in _console_callback\r\n    self._backend.interface.publish_output(name, data)\r\n  File ""/home/ad/anaconda3/envs/tfm/lib/python3.6/site-packages/wandb/sdk/interface/interface.py"", line 153, in publish_output\r\n    self._publish_output(o)\r\n  File ""/home/ad/anaconda3/envs/tfm/lib/python3.6/site-packages/wandb/sdk/interface/interface.py"", line 158, in _publish_output\r\n    self._publish(rec)\r\n  File ""/home/ad/anaconda3/envs/tfm/lib/python3.6/site-packages/wandb/sdk/interface/interface.py"", line 456, in _publish\r\n    if self._process and not self._process.is_alive():\r\n  File ""/home/ad/anaconda3/envs/tfm/lib/python3.6/multiprocessing/process.py"", line 134, in is_alive\r\n    assert self._parent_pid == os.getpid(), \'can only test a child process\'\r\nAssertionError: can only test a child process\r\n```\r\n\r\nAs a workaround I have commented line 456 and 457 in `/home/ad/anaconda3/envs/tfm/lib/python3.6/site-packages/wandb/sdk/interface/interface.py`'
 ""Isn't it more the pytorch warning on the use of non-writable memory for tensor that trigger this here @lhoestq? (since it seems to be a warning triggered in `torch.tensor()`""
 'Yep this time this is a warning from pytorch that causes wandb to not work properly.\r\nCould this by a wandb issue ?'
 ""Hi @timothyjlaurent @gaceladri \r\nIf you're running `transformers` from `master` you can try setting the env var `WAND_DISABLE=true` (from https://github.com/huggingface/transformers/pull/9896) and try again ?\r\nThis issue might be related to https://github.com/huggingface/transformers/issues/9623 ""
 ""I have commented the lines that cause my code break. I'm now seeing my reports on Wandb and my code does not break. I am training now, so I will check probably in 6 hours. I suppose that setting wandb disable will work as well.""]","Using a dataset with a single 'text' field and a fast tokenizer in a jupyter notebook.

``` 
def tokenizer_fn(example):
    return tokenizer.batch_encode_plus(example['text'])

ds_tokenized = text_dataset.map(tokenizer_fn, batched=True, num_proc=6, remove_columns=['text'])
```


```
---------------------------------------------------------------------------
RemoteTraceback                           Traceback (most recent call last)
RemoteTraceback: 
""""""
Traceback (most recent call last):
  File ""/home/jovyan/share/users/tlaurent/invitae-bert/ve/lib/python3.6/site-packages/multiprocess/pool.py"", line 119, in worker
    result = (True, func(*args, **kwds))
  File ""/home/jovyan/share/users/tlaurent/invitae-bert/ve/lib/python3.6/site-packages/datasets/arrow_dataset.py"", line 156, in wrapper
    out: Union[""Dataset"", ""DatasetDict""] = func(self, *args, **kwargs)
  File ""/home/jovyan/share/users/tlaurent/invitae-bert/ve/lib/python3.6/site-packages/datasets/fingerprint.py"", line 163, in wrapper
    out = func(self, *args, **kwargs)
  File ""/home/jovyan/share/users/tlaurent/invitae-bert/ve/lib/python3.6/site-packages/datasets/arrow_dataset.py"", line 1510, in _map_single
    for i in pbar:
  File ""/home/jovyan/share/users/tlaurent/invitae-bert/ve/lib/python3.6/site-packages/tqdm/notebook.py"", line 228, in __iter__
    for obj in super(tqdm_notebook, self).__iter__(*args, **kwargs):
  File ""/home/jovyan/share/users/tlaurent/invitae-bert/ve/lib/python3.6/site-packages/tqdm/std.py"", line 1186, in __iter__
    self.close()
  File ""/home/jovyan/share/users/tlaurent/invitae-bert/ve/lib/python3.6/site-packages/tqdm/notebook.py"", line 251, in close
    super(tqdm_notebook, self).close(*args, **kwargs)
  File ""/home/jovyan/share/users/tlaurent/invitae-bert/ve/lib/python3.6/site-packages/tqdm/std.py"", line 1291, in close
    fp_write('')
  File ""/home/jovyan/share/users/tlaurent/invitae-bert/ve/lib/python3.6/site-packages/tqdm/std.py"", line 1288, in fp_write
    self.fp.write(_unicode(s))
  File ""/home/jovyan/share/users/tlaurent/invitae-bert/ve/lib/python3.6/site-packages/wandb/sdk/lib/redirect.py"", line 91, in new_write
    cb(name, data)
  File ""/home/jovyan/share/users/tlaurent/invitae-bert/ve/lib/python3.6/site-packages/wandb/sdk/wandb_run.py"", line 598, in _console_callback
    self._backend.interface.publish_output(name, data)
  File ""/home/jovyan/share/users/tlaurent/invitae-bert/ve/lib/python3.6/site-packages/wandb/sdk/interface/interface.py"", line 146, in publish_output
    self._publish_output(o)
  File ""/home/jovyan/share/users/tlaurent/invitae-bert/ve/lib/python3.6/site-packages/wandb/sdk/interface/interface.py"", line 151, in _publish_output
    self._publish(rec)
  File ""/home/jovyan/share/users/tlaurent/invitae-bert/ve/lib/python3.6/site-packages/wandb/sdk/interface/interface.py"", line 431, in _publish
    if self._process and not self._process.is_alive():
  File ""/usr/lib/python3.6/multiprocessing/process.py"", line 134, in is_alive
    assert self._parent_pid == os.getpid(), 'can only test a child process'
AssertionError: can only test a child process
""""""
```"
https://github.com/huggingface/datasets/issues/846,Add HoVer multi-hop fact verification dataset,"[""Hi @yjernite  I'm new but wanted to contribute. Has anyone already taken this problem and do you think it is suitable for newbies?""
 ""Hi @tenjjin! This dataset is still up for grabs! Here's the link with the guide to add it. You should play around with the library first (download and look at a few datasets), then follow the steps here:\r\n\r\nhttps://github.com/huggingface/datasets/blob/master/ADD_NEW_DATASET.md""
 'Closed by #1399 ']","## Adding a Dataset
- **Name:** HoVer
- **Description:** https://twitter.com/YichenJiang9/status/1326954363806429186 contains 20K claim verification examples
- **Paper:** https://arxiv.org/abs/2011.03088
- **Data:** https://hover-nlp.github.io/
- **Motivation:** There are still few multi-hop information extraction benchmarks (HotpotQA, which dataset wase based off, notwithstanding)

Instructions to add a new dataset can be found [here](https://huggingface.co/docs/datasets/share_dataset.html).
"
https://github.com/huggingface/datasets/issues/843,use_custom_baseline still produces errors for bertscore,"[""Thanks for reporting ! That's a bug indeed\r\nIf you want to contribute, feel free to fix this issue and open a PR :)""
 'This error is because of a mismatch between `datasets` and `bert_score`. With `datasets=1.1.2` and `bert_score>=0.3.6` it works ok. So `pip install -U bert_score` should fix the problem. '
 'Thanks for the heads up @pvl and for the PR as well :)'
 ""Hello everyone,\r\n\r\nI think the  problem is not solved: \r\n\r\n```\r\nfrom datasets import load_metric\r\nmetric=load_metric('bertscore')\r\nmetric.compute(\r\n    predictions=predictions,\r\n    references=references,\r\n    lang='fr',\r\n    rescale_with_baseline=True\r\n)\r\nTypeError: get_hash() missing 2 required positional arguments: 'use_custom_baseline' and 'use_fast_tokenizer'\r\n```\r\nThis code is produced using `Python 3.6.9 datasets==1.1.2 and bert_score==0.3.10`""
 ""Hi ! This has been fixed by https://github.com/huggingface/datasets/pull/2770, we'll do a new release soon to make the fix available :)\r\n\r\nIn the meantime please use an older version of `bert_score`""]","`metric = load_metric('bertscore')`
`a1 = ""random sentences""`
`b1 = ""random sentences""`
`metric.compute(predictions = [a1], references = [b1], lang = 'en')`

`Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/stephen_chan/.local/lib/python3.6/site-packages/datasets/metric.py"", line 393, in compute
    output = self._compute(predictions=predictions, references=references, **kwargs)
  File ""/home/stephen_chan/.cache/huggingface/modules/datasets_modules/metrics/bertscore/361e597a01a41d6cf95d94bbfb01dea16261687abc0c6c74cc9930f80488f363/bertscore.py"", line 108, in _compute
    hashcode = bert_score.utils.get_hash(model_type, num_layers, idf, rescale_with_baseline)
TypeError: get_hash() missing 1 required positional argument: 'use_custom_baseline'`

Adding 'use_custom_baseline = False' as an argument produces this error

`Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/stephen_chan/.local/lib/python3.6/site-packages/datasets/metric.py"", line 393, in compute
    output = self._compute(predictions=predictions, references=references, **kwargs)
TypeError: _compute() got an unexpected keyword argument 'use_custom_baseline'`

This is on Ubuntu 18.04, Python 3.6.9, datasets version 1.1.2"
https://github.com/huggingface/datasets/issues/842,How to enable `.map()` pre-processing pipelines to support multi-node parallelism?,"[""Right now multiprocessing only runs on single node.\r\n\r\nHowever it's probably possible to extend it to support multi nodes. Indeed we're using the `multiprocess` library from the `pathos` project to do multiprocessing in `datasets`, and `pathos` is made to support parallelism on several nodes. More info about pathos [on the pathos repo](https://github.com/uqfoundation/pathos).\r\n\r\nIf you're familiar with pathos or if you want to give it a try, it could be a nice addition to the library :)""]","Hi,

Currently, multiprocessing can be enabled for the `.map()` stages on a single node. However, in the case of multi-node training, (since more than one node would be available) I'm wondering if it's possible to extend the parallel processing among nodes, instead of only 1 node running the `.map()` while the other node is waiting for it to finish?

Thanks!"
https://github.com/huggingface/datasets/issues/841,Can not reuse datasets already downloaded,"[""It seems the process needs '/datasets.huggingface.co/datasets/datasets/wikipedia/wikipedia.py'\r\nWhere and how to assign this ```wikipedia.py``` after I manually download it ?""
 ""\r\ndownload the ```wikipedia.py``` at the working directory and go with ```dataset = load_dataset('wikipedia.py', '20200501.en')``` works.""]","Hello,
I need to connect to a frontal node (with http proxy, no gpu) before connecting to a gpu node (but no http proxy, so can not use wget so on).
I successfully downloaded and reuse the wikipedia datasets in a frontal node. 
When I connect to the gpu node, I supposed to use the downloaded datasets from cache, but failed and end with time out error.

On frontal node:
```
>>> from datasets import load_dataset
>>> dataset = load_dataset('wikipedia', '20200501.en')
Reusing dataset wikipedia (/linkhome/rech/genini01/uua34ms/.cache/huggingface/datasets/wikipedia/20200501.en/1.0.0/f92599dfccab29832c442b82870fa8f6983e5b4ebbf5e6e2dcbe894e325339cd)
/linkhome/rech/genini01/uua34ms/work/anaconda3/envs/pytorch_pip170_cuda102/lib/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() > 0
```

On gpu node:
```
>>> from datasets import load_dataset
>>> dataset = load_dataset('wikipedia', '20200501.en')
Traceback (most recent call last):
  File ""/linkhome/rech/genini01/uua34ms/work/anaconda3/envs/pytorch_pip170_cuda102/lib/python3.6/site-packages/urllib3/connection.py"", line 160, in _new_conn
    (self._dns_host, self.port), self.timeout, **extra_kw
  File ""/linkhome/rech/genini01/uua34ms/work/anaconda3/envs/pytorch_pip170_cuda102/lib/python3.6/site-packages/urllib3/util/connection.py"", line 84, in create_connection
    raise err
  File ""/linkhome/rech/genini01/uua34ms/work/anaconda3/envs/pytorch_pip170_cuda102/lib/python3.6/site-packages/urllib3/util/connection.py"", line 74, in create_connection
    sock.connect(sa)
TimeoutError: [Errno 110] Connection timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/linkhome/rech/genini01/uua34ms/work/anaconda3/envs/pytorch_pip170_cuda102/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 677, in urlopen
    chunked=chunked,
  File ""/linkhome/rech/genini01/uua34ms/work/anaconda3/envs/pytorch_pip170_cuda102/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 381, in _make_request
    self._validate_conn(conn)
  File ""/linkhome/rech/genini01/uua34ms/work/anaconda3/envs/pytorch_pip170_cuda102/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 978, in _validate_conn
    conn.connect()
  File ""/linkhome/rech/genini01/uua34ms/work/anaconda3/envs/pytorch_pip170_cuda102/lib/python3.6/site-packages/urllib3/connection.py"", line 309, in connect
    conn = self._new_conn()
  File ""/linkhome/rech/genini01/uua34ms/work/anaconda3/envs/pytorch_pip170_cuda102/lib/python3.6/site-packages/urllib3/connection.py"", line 172, in _new_conn
    self, ""Failed to establish a new connection: %s"" % e
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x14b7b73e4908>: Failed to establish a new connection: [Errno 110] Connection timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/linkhome/rech/genini01/uua34ms/work/anaconda3/envs/pytorch_pip170_cuda102/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send
    timeout=timeout
  File ""/linkhome/rech/genini01/uua34ms/work/anaconda3/envs/pytorch_pip170_cuda102/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 727, in urlopen
    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]
  File ""/linkhome/rech/genini01/uua34ms/work/anaconda3/envs/pytorch_pip170_cuda102/lib/python3.6/site-packages/urllib3/util/retry.py"", line 446, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='s3.amazonaws.com', port=443): Max retries exceeded with url: /datasets.huggingface.co/datasets/datasets/wikipedia/wikipedia.py (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x14b7b73e4908>: Failed to establish a new connection: [Errno 110] Connection timed out',))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/linkhome/rech/genini01/uua34ms/work/anaconda3/envs/pytorch_pip170_cuda102/lib/python3.6/site-packages/datasets/load.py"", line 590, in load_dataset
    path, script_version=script_version, download_config=download_config, download_mode=download_mode, dataset=True
  File ""/linkhome/rech/genini01/uua34ms/work/anaconda3/envs/pytorch_pip170_cuda102/lib/python3.6/site-packages/datasets/load.py"", line 264, in prepare_module
    head_hf_s3(path, filename=name, dataset=dataset)
  File ""/linkhome/rech/genini01/uua34ms/work/anaconda3/envs/pytorch_pip170_cuda102/lib/python3.6/site-packages/datasets/utils/file_utils.py"", line 200, in head_hf_s3
    return requests.head(hf_bucket_url(identifier=identifier, filename=filename, use_cdn=use_cdn, dataset=dataset))
  File ""/linkhome/rech/genini01/uua34ms/work/anaconda3/envs/pytorch_pip170_cuda102/lib/python3.6/site-packages/requests/api.py"", line 104, in head
    return request('head', url, **kwargs)
  File ""/linkhome/rech/genini01/uua34ms/work/anaconda3/envs/pytorch_pip170_cuda102/lib/python3.6/site-packages/requests/api.py"", line 61, in request
    return session.request(method=method, url=url, **kwargs)
  File ""/linkhome/rech/genini01/uua34ms/work/anaconda3/envs/pytorch_pip170_cuda102/lib/python3.6/site-packages/requests/sessions.py"", line 530, in request
    resp = self.send(prep, **send_kwargs)
  File ""/linkhome/rech/genini01/uua34ms/work/anaconda3/envs/pytorch_pip170_cuda102/lib/python3.6/site-packages/requests/sessions.py"", line 643, in send
    r = adapter.send(request, **kwargs)
  File ""/linkhome/rech/genini01/uua34ms/work/anaconda3/envs/pytorch_pip170_cuda102/lib/python3.6/site-packages/requests/adapters.py"", line 516, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='s3.amazonaws.com', port=443): Max retries exceeded with url: /datasets.huggingface.co/datasets/datasets/wikipedia/wikipedia.py (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x14b7b73e4908>: Failed to establish a new connection: [Errno 110] Connection timed out',))

```

Any advice?Thanks!
"
https://github.com/huggingface/datasets/issues/836,load_dataset with 'csv' is not working. while the same file is loading with 'text' mode or with pandas,"['Which version of pyarrow do you have ? Could you try to update pyarrow and try again ?'
 ""Thanks for the fast response. I have the latest version '2.0.0' (I tried to update)\r\nI am working with Python 3.8.5""
 'I think that the issue is similar to this one:https://issues.apache.org/jira/browse/ARROW-9612\r\nThe problem is in arrow when the column data contains long strings.\r\nAny ideas on how to bypass this?'
 ""We should expose the [`block_size` argument](https://arrow.apache.org/docs/python/generated/pyarrow.csv.ReadOptions.html#pyarrow.csv.ReadOptions) of Apache Arrow csv `ReadOptions` in the [script](https://github.com/huggingface/datasets/blob/master/datasets/csv/csv.py).\r\n\r\n\r\nIn the meantime you can specify yourself the `ReadOptions` config like this:\r\n```python\r\nimport pyarrow.csv as pac   # PyArrow is installed with `datasets`\r\n\r\nread_options = pac.ReadOptions(block_size=1e9)  # try to find the right value for your use-case\r\ndataset = load_dataset('csv', data_files=files, read_options=read_options)\r\n```\r\n""
 'This did help to load the data. But the problem now is that I get:\r\nArrowInvalid: CSV parse error: Expected 5 columns, got 187\r\n\r\nIt seems that this change the parsing so I changed the table to tab-separated and tried to load it directly from pyarrow\r\nBut I got a similar error, again it loaded fine in pandas so I am not sure what to do.\r\n\r\n\r\n\r\n'
 'Got almost the same error loading a ~5GB TSV file, first got the same error as OP, then tried giving it my own ReadOptions and also got the same CSV parse error.']","Hi All
I am trying to load a custom dataset  and I am trying to load a single file to make sure the file is loading correctly:
dataset = load_dataset('csv', data_files=files)
When I run it I get:

Downloading and preparing dataset csv/default-35575a1051604c88 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) tocache/huggingface/datasets/csv/default-35575a1051604c88/0.0.0/49187751790fa4d820300fd4d0707896e5b941f1a9c644652645b866716a4ac4...

I am getting this error:
6a4ac4/csv.py in _generate_tables(self, files)
     78     def _generate_tables(self, files):
     79         for i, file in enumerate(files):
---> 80             pa_table = pac.read_csv(
     81                 file,
     82                 read_options=self.config.pa_read_options,

~/anaconda2/envs/nlp/lib/python3.8/site-packages/pyarrow/_csv.pyx in pyarrow._csv.read_csv()

~/anaconda2/envs/nlp/lib/python3.8/site-packages/pyarrow/error.pxi in pyarrow.lib.pyarrow_internal_check_status()

~/anaconda2/envs/nlp/lib/python3.8/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()

**ArrowInvalid: straddling object straddles two block boundaries (try to increase block size?)**



The size of the file is 3.5 GB. When I try smaller files I do not have an issue. When I load it with 'text' parser I can see all data but it is not what I need.
There is no issue reading the file with pandas. any idea what could be the issue?
When I am running a different CSV I do not get  this line:
 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size)

Any ideas?
"
https://github.com/huggingface/datasets/issues/835,Wikipedia postprocessing,"['Hi @bminixhofer ! Parsing WikiMedia is notoriously difficult: this processing used [mwparserfromhell](https://github.com/earwig/mwparserfromhell) which is pretty good but not perfect.\r\n\r\nAs an alternative, you can also use the Wiki40b dataset which was pre-processed using an un-released Google internal tool'
 ""Ok, thanks! I'll try the Wiki40b dataset.""
 'If anyone else is concerned about this, `wiki40b` does indeed seem very well cleaned.']","Hi, thanks for this library!

Running this code:

```py
import datasets
wikipedia = datasets.load_dataset(""wikipedia"", ""20200501.de"")
print(wikipedia['train']['text'][0])
```

I get:

```
mini|Ricardo Flores Magón
mini|Mexikanische Revolutionäre, Magón in der Mitte anführend, gegen die Diktatur von Porfirio Diaz, Ausschnitt des Gemälde „Tierra y Libertad“ von Idelfonso Carrara (?) von 1930.

Ricardo Flores Magón (* 16. September 1874 in San Antonio Eloxochitlán im mexikanischen Bundesstaat Oaxaca; † 22. November 1922 im Bundesgefängnis Leavenworth im US-amerikanischen Bundesstaat Kansas) war als Journalist, Gewerkschafter und Literat ein führender anarchistischer Theoretiker und Aktivist, der die revolutionäre mexikanische Bewegung radikal beeinflusste. Magón war Gründer der Partido Liberal Mexicano und Mitglied der Industrial Workers of the World.

Politische Biografie 
Journalistisch und politisch kämpfte er und sein Bruder sehr kompromisslos gegen die Diktatur Porfirio Diaz. Philosophisch und politisch orientiert an radikal anarchistischen Idealen und den Erfahrungen seiner indigenen Vorfahren bei der gemeinschaftlichen Bewirtschaftung des Gemeindelandes, machte er die Forderung „Land und Freiheit“ (Tierra y Libertad) populär. Besonders Francisco Villa und Emiliano Zapata griffen die Forderung Land und Freiheit auf. Seine Philosophie hatte großen Einfluss auf die Landarbeiter. 1904 floh er in die USA und gründete 1906 die Partido Liberal Mexicano. Im Exil lernte er u. a. Emma Goldman kennen. Er verbrachte die meiste Zeit seines Lebens in Gefängnissen und im Exil und wurde 1918 in den USA wegen „Behinderung der Kriegsanstrengungen“ zu zwanzig Jahren Gefängnis verurteilt. Zu seinem Tod gibt es drei verschiedene Theorien. Offiziell starb er an Herzversagen. Librado Rivera, der die Leiche mit eigenen Augen gesehen hat, geht davon aus, dass Magón von einem Mitgefangenen erdrosselt wurde. Die staatstreue Gewerkschaftszeitung CROM veröffentlichte 1923 einen Beitrag, nachdem Magón von einem Gefängniswärter erschlagen wurde.
mini|Die Brüder Ricardo (links) und Enrique Flores Magón (rechts) vor dem Los Angeles County Jail, 1917

[...]
```

so some Markup like `mini|` is still left. Should I run another parser on this text before feeding it to an ML model or is this a known imperfection of parsing Wiki markup?

Apologies if this has been asked before."
https://github.com/huggingface/datasets/issues/834,[GEM] add WikiLingua cross-lingual abstractive summarization dataset,"['Hey @yjernite. This is a very interesting dataset. Would love to work on adding it but I see that the link to the data is to a gdrive folder. Can I just confirm wether dlmanager can handle gdrive urls or would this have to be a manual dl?'
 'Hi @KMFODA ! A version of WikiLingua is actually already accessible in the [GEM dataset](https://huggingface.co/datasets/gem)\r\n\r\nYou can use it for example to load the French to English translation with:\r\n```python\r\nfrom datasets import load_dataset\r\nwikilingua = load_dataset(""gem"", ""wiki_lingua_french_fr"")\r\n```\r\n\r\nClosed by https://github.com/huggingface/datasets/pull/1807']","## Adding a Dataset
- **Name:** WikiLingua
- **Description:** The dataset includes ~770k article and summary pairs in 18 languages from WikiHow. The gold-standard article-summary alignments across languages were extracted by aligning the images that are used to describe each how-to step in an article.
- **Paper:** https://arxiv.org/pdf/2010.03093.pdf
- **Data:** https://github.com/esdurmus/Wikilingua
- **Motivation:** Included in the GEM shared task. Multilingual.

Instructions to add a new dataset can be found [here](https://huggingface.co/docs/datasets/share_dataset.html).
"
https://github.com/huggingface/datasets/issues/830,[GEM] add ToTTo Table-to-text dataset,['closed via #1098 '],"## Adding a Dataset
- **Name:** ToTTo
- **Description:** ToTTo is an open-domain English table-to-text dataset with over 120,000 training examples that proposes a controlled generation task: given a Wikipedia table and a set of highlighted table cells, produce a one-sentence description.
- **Paper:** https://arxiv.org/abs/2004.14373
- **Data:** https://github.com/google-research-datasets/totto
- **Motivation:** Included in the GEM shared task

Instructions to add a new dataset can be found [here](https://huggingface.co/docs/datasets/share_dataset.html).
"
https://github.com/huggingface/datasets/issues/827,[GEM] MultiWOZ dialogue dataset,['Hi @yjernite can I help in adding this dataset? \r\n\r\nI am excited about this because this will be my first contribution to the datasets library as well as to hugginface.'],"## Adding a Dataset
- **Name:** MultiWOZ (Multi-Domain Wizard-of-Oz)
- **Description:** 10k annotated human-human dialogues. Each dialogue consists of a goal, multiple user and system utterances as well as a belief state. Only system utterances are annotated with dialogue acts – there are no annotations from the user side.
- **Paper:** https://arxiv.org/pdf/2007.12720.pdf
- **Data:** https://github.com/budzianowski/multiwoz
- **Motivation:** Will likely be part of the GEM shared task

Instructions to add a new dataset can be found [here](https://huggingface.co/docs/datasets/share_dataset.html).
"
https://github.com/huggingface/datasets/issues/824,Discussion using datasets in offline mode,"['No comments ?'
 ""I think it would be very cool. I'm currently working on a cluster from Compute Canada, and I have internet access only when I'm not in the nodes where I run the scripts. So I was expecting to be able to use the wmt14 dataset until I realized I needed internet connection even if I downloaded the data already. I'm going to try option 2 you mention for now though! Thanks ;)""
 ""Requiring online connection is a deal breaker in some cases unfortunately so it'd be great if offline mode is added similar to how `transformers` loads models offline fine.\r\n\r\n@mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?""
 'here is my way to load a dataset offline, but it **requires** an online machine\r\n1. (online machine)\r\n```\r\nimport datasets\r\ndata = datasets.load_dataset(...)\r\ndata.save_to_disk(/YOUR/DATASET/DIR)\r\n```\r\n2. copy the dir from online to the offline machine\r\n3. (offline machine)\r\n```\r\nimport datasets\r\ndata = datasets.load_from_disk(/SAVED/DATA/DIR)\r\n```\r\n\r\nHTH.'
 '> here is my way to load a dataset offline, but it **requires** an online machine\n> \n> 1. (online machine)\n> \n> ```\n> \n> import datasets\n> \n> data = datasets.load_dataset(...)\n> \n> data.save_to_disk(/YOUR/DATASET/DIR)\n> \n> ```\n> \n> 2. copy the dir from online to the offline machine\n> \n> 3. (offline machine)\n> \n> ```\n> \n> import datasets\n> \n> data = datasets.load_from_disk(/SAVED/DATA/DIR)\n> \n> ```\n> \n> \n> \n> HTH.\n\n'
 'I opened a PR that allows to reload modules that have already been loaded once even if there\'s no internet.\r\n\r\nLet me know if you know other ways that can make the offline mode experience better. I\'d be happy to add them :) \r\n\r\nI already note the ""freeze"" modules option, to prevent local modules updates. It would be a cool feature.\r\n\r\n----------\r\n\r\n> @mandubian\'s second bullet point suggests that there\'s a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?\r\n\r\nIndeed `load_dataset` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.\r\nFor example if you have a dataset script at `./my_dataset/my_dataset.py` then you can do\r\n```python\r\nload_dataset(""./my_dataset"")\r\n```\r\nand the dataset script will generate your dataset once and for all.\r\n\r\n----------\r\n\r\nAbout I\'m looking into having `csv`, `json`, `text`, `pandas` dataset builders already included in the `datasets` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.\r\ncf #1724 '
 ""The local dataset builders (csv, text , json and pandas) are now part of the `datasets` package since #1726 :)\r\nYou can now use them offline\r\n```python\r\ndatasets = load_dataset('text', data_files=data_files)\r\n```\r\n\r\nWe'll do a new release soon""]","`datasets.load_dataset(""csv"", ...)` breaks if you have no connection (There is already this issue https://github.com/huggingface/datasets/issues/761 about it). It seems to be the same for metrics too.

I create this ticket to discuss a bit and gather what you have in mind or other propositions.

Here are some points to open discussion:
- if you want to prepare your code/datasets on your machine (having internet connexion) but run it on another offline machine (not having internet connexion), it won't work as is, even if you have all files locally on this machine.
- AFAIK, you can make it work if you manually put the python files (csv.py for example) on this offline machine and change your code to `datasets.load_dataset(""MY_PATH/csv.py"", ...)`. But it would be much better if you could run ths same code without modification if files are available locally.
- I've also been considering the requirement of downloading Python code and execute on your machine to use datasets. This can be an issue in a professional context. Downloading a CSV/H5 file is acceptable, downloading an executable script can open many security issues. We certainly need a mechanism to at least ""freeze"" the dataset code you retrieved once so that you can review it if you want and then be sure you use this one everywhere and not a version dowloaded from internet.
 
WDYT? (thks)

"
https://github.com/huggingface/datasets/issues/823,how processing in batch works in datasets ,"['Hi I don’t think this is a request for a dataset like you labeled it.\r\n\r\nI also think this would be better suited for the forum at https://discuss.huggingface.co. we try to keep the issue for the repo for bug reports and new features/dataset requests and have usage questions discussed on the forum. Thanks.'
 'Hi Thomas,\nwhat I do not get from documentation is that why when you set batched=True,\nthis is processed in batch, while data is not divided to batched\nbeforehand, basically this is a question on the documentation and I do not\nget the batched=True, but sure, if you think this is more appropriate in\nforum I will post it there.\nthanks\nBest\nRabeeh\n\nOn Tue, Nov 10, 2020 at 12:21 PM Thomas Wolf <notifications@github.com>\nwrote:\n\n> Hi I don’t think this is a request for a dataset like you labeled it.\n>\n> I also think this would be better suited for the forum at\n> https://discuss.huggingface.co. we try to keep the issue for the repo for\n> bug reports and new features/dataset requests and have usage questions\n> discussed on the forum. Thanks.\n>\n> —\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/huggingface/datasets/issues/823#issuecomment-724639476>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ARPXHH4FIPFHVVUHANAE4F3SPEO2JANCNFSM4TQQVEXQ>\n> .\n>\n'
 'Yes the forum is perfect for that. You can post in the `datasets` section.\r\nThanks a lot!']","Hi,
I need to process my datasets before it is passed to dataloader in batch, 
here is my codes 

```
class AbstractTask(ABC):
    task_name: str = NotImplemented
    preprocessor: Callable = NotImplemented
    split_to_data_split: Mapping[str, str] = NotImplemented
    tokenizer: Callable = NotImplemented
    max_source_length: str = NotImplemented
    max_target_length: str = NotImplemented
    # TODO: should not be a task item, but cannot see other ways.
    tpu_num_cores: int = None

    # The arguments set are for all tasks and needs to be kept common.
    def __init__(self, config):
        self.max_source_length = config['max_source_length']
        self.max_target_length = config['max_target_length']
        self.tokenizer = config['tokenizer']
        self.tpu_num_cores = config['tpu_num_cores']

    def _encode(self, batch) -> Dict[str, torch.Tensor]:
        batch_encoding = self.tokenizer.prepare_seq2seq_batch(
            [x[""src_texts""] for x in batch],
            tgt_texts=[x[""tgt_texts""] for x in batch],
            max_length=self.max_source_length,
            max_target_length=self.max_target_length,
            padding=""max_length"" if self.tpu_num_cores is not None else ""longest"",  # TPU hack
            return_tensors=""pt""
        )
        return batch_encoding.data


    def data_split(self, split):
        return self.split_to_data_split[split]

    def get_dataset(self, split, n_obs=None):
        split = self.data_split(split)
        if n_obs is not None:
            split = split+""[:{}]"".format(n_obs)
        dataset = load_dataset(self.task_name, split=split)
        dataset = dataset.map(self.preprocessor, remove_columns=dataset.column_names)
        dataset = dataset.map(lambda batch: self._encode(batch), batched=True)
        dataset.set_format(type=""torch"", columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])
        return dataset

```

I call it like 

`AutoTask.get(task, train_dataset_config).get_dataset(split=""train"", n_obs=data_args.n_train) 
`

This gives the following error, to me because the data inside the   dataset = dataset.map(lambda batch: self._encode(batch), batched=True) is not processed in batch, could you tell me how I can process dataset in batch inside my function? thanks 

  File ""finetune_multitask_trainer.py"", line 192, in main
    if training_args.do_train else None
  File ""finetune_multitask_trainer.py"", line 191, in <dictcomp>
    split=""train"", n_obs=data_args.n_train) for task in data_args.task}
  File ""/remote/idiap.svm/user.active/rkarimi/dev/internship/seq2seq/tasks.py"", line 56, in get_dataset
    dataset = dataset.map(lambda batch: self._encode(batch), batched=True)
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/arrow_dataset.py"", line 1236, in map
    update_data = does_function_return_dict(test_inputs, test_indices)
  File ""/idiap/user/rkarimi/libs/anaconda3/envs/internship/lib/python3.7/site-packages/datasets/arrow_dataset.py"", line 1207, in does_function_return_dict
    function(*fn_args, indices, **fn_kwargs) if with_indices else function(*fn_args, **fn_kwargs)
  File ""/remote/idiap.svm/user.active/rkarimi/dev/internship/seq2seq/tasks.py"", line 56, in <lambda>
    dataset = dataset.map(lambda batch: self._encode(batch), batched=True)
  File ""/remote/idiap.svm/user.active/rkarimi/dev/internship/seq2seq/tasks.py"", line 37, in _encode
    [x[""src_texts""] for x in batch],
  File ""/remote/idiap.svm/user.active/rkarimi/dev/internship/seq2seq/tasks.py"", line 37, in <listcomp>
    [x[""src_texts""] for x in batch],
TypeError: string indices must be integers

"
https://github.com/huggingface/datasets/issues/822,datasets freezes ,"['Pytorch is unable to convert strings to tensors unfortunately.\r\nYou can use `set_format(type=""torch"")` on columns that can be converted to tensors, such as token ids.\r\n\r\nThis makes me think that we should probably raise an error or at least a warning when one tries to create pytorch tensors out of text columns']","Hi, I want to load these two datasets and convert them to Dataset format in torch and the code freezes for me, could you have a look please? thanks 

dataset1 = load_dataset(""squad"", split=""train[:10]"")
dataset1 = dataset1.set_format(type='torch', columns=['context', 'answers', 'question'])

dataset2 = load_dataset(""imdb"", split=""train[:10]"")
dataset2 = dataset2.set_format(type=""torch"", columns=[""text"", ""label""])
print(len(dataset1))
"
https://github.com/huggingface/datasets/issues/817,Add MRQA dataset,['Done! cf #1117 and #1022'],"## Adding a Dataset
- **Name:** MRQA
- **Description:** Collection of different (subsets of) QA datasets all converted to the same format to evaluate out-of-domain generalization (the datasets come from different domains, distributions, etc.). Some datasets are used for training and others are used for evaluation. This dataset was collected as part of MRQA 2019's shared task 
- **Paper:** https://arxiv.org/abs/1910.09753
- **Data:** https://github.com/mrqa/MRQA-Shared-Task-2019
- **Motivation:** Out-of-domain generalization is becoming (has become) a de-factor evaluation for NLU systems

Instructions to add a new dataset can be found [here](https://huggingface.co/docs/datasets/share_dataset.html)."
https://github.com/huggingface/datasets/issues/816,[Caching] Dill globalvars() output order is not deterministic and can cause cache issues.,"['To show the issue:\r\n```\r\npython -c ""from datasets.fingerprint import Hasher; a=[]; func = lambda : len(a); print(Hasher.hash(func))""\r\n```\r\ndoesn\'t always return the same ouput since `globs` is a dictionary with ""a"" and ""len"" as keys but sometimes not in the same order']","Dill uses `dill.detect.globalvars` to get the globals used by a function in a recursive dump. `globalvars` returns a dictionary of all the globals that a dumped function needs. However the order of the keys in this dict is not deterministic and can cause caching issues.

To fix that one could register an implementation of dill's `save_function` in the `datasets` pickler that sorts the globals keys before dumping a function."
https://github.com/huggingface/datasets/issues/815,Is dataset iterative or not?,"['Hello !\r\nCould you give more details ?\r\n\r\nIf you mean iter through one dataset then yes, `Dataset` object does implement the `__iter__` method so you can use \r\n```python\r\nfor example in dataset:\r\n    # do something\r\n```\r\n\r\nIf you want to iter through several datasets you can first concatenate them\r\n```python\r\nfrom datasets import concatenate_datasets\r\n\r\nnew_dataset = concatenate_datasets([dataset1, dataset2])\r\n```\r\nLet me know if this helps !'
 'Hi Huggingface/Datasets team,\nI want to use the datasets inside Seq2SeqDataset here\nhttps://github.com/huggingface/transformers/blob/master/examples/seq2seq/utils.py\nand there I need to return back each line from the datasets and I am not\nsure how to access each line and implement this?\nIt seems it also has get_item attribute? so I was not sure if this is\niterative dataset? or if this is non-iterable datasets?\nthanks.\n\n\n\nOn Mon, Nov 9, 2020 at 10:18 AM Quentin Lhoest <notifications@github.com>\nwrote:\n\n> Hello !\n> Could you give more details ?\n>\n> If you mean iter through one dataset then yes, Dataset object does\n> implement the __iter__ method so you can use\n>\n> for example in dataset:\n>     # do something\n>\n> If you want to iter through several datasets you can first concatenate them\n>\n> from datasets import concatenate_datasets\n> new_dataset = concatenate_datasets([dataset1, dataset2])\n>\n> Let me know if this helps !\n>\n> —\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/huggingface/datasets/issues/815#issuecomment-723881199>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ARPXHHYRLSSYW6NZN2HYDBTSO6XV5ANCNFSM4TPB7OWA>\n> .\n>\n'
 'could you tell me please if datasets also has __getitem__ any idea on how\nto integrate it with Seq2SeqDataset is appreciated thanks\n\nOn Mon, Nov 9, 2020 at 10:22 AM Rabeeh Karimi Mahabadi <rabeeh@google.com>\nwrote:\n\n> Hi Huggingface/Datasets team,\n> I want to use the datasets inside Seq2SeqDataset here\n> https://github.com/huggingface/transformers/blob/master/examples/seq2seq/utils.py\n> and there I need to return back each line from the datasets and I am not\n> sure how to access each line and implement this?\n> It seems it also has get_item attribute? so I was not sure if this is\n> iterative dataset? or if this is non-iterable datasets?\n> thanks.\n>\n>\n>\n> On Mon, Nov 9, 2020 at 10:18 AM Quentin Lhoest <notifications@github.com>\n> wrote:\n>\n>> Hello !\n>> Could you give more details ?\n>>\n>> If you mean iter through one dataset then yes, Dataset object does\n>> implement the __iter__ method so you can use\n>>\n>> for example in dataset:\n>>     # do something\n>>\n>> If you want to iter through several datasets you can first concatenate\n>> them\n>>\n>> from datasets import concatenate_datasets\n>> new_dataset = concatenate_datasets([dataset1, dataset2])\n>>\n>> Let me know if this helps !\n>>\n>> —\n>> You are receiving this because you authored the thread.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/huggingface/datasets/issues/815#issuecomment-723881199>,\n>> or unsubscribe\n>> <https://github.com/notifications/unsubscribe-auth/ARPXHHYRLSSYW6NZN2HYDBTSO6XV5ANCNFSM4TPB7OWA>\n>> .\n>>\n>\n'
 '`datasets.Dataset` objects implement indeed `__getitem__`. It returns a dictionary with one field per column.\r\n\r\nWe\'ve not added the integration of the datasets library for the seq2seq utilities yet. The current seq2seq utilities are based on text files.\r\n\r\nHowever as soon as you have a `datasets.Dataset` with columns ""tgt_texts"" (str), ""src_texts"" (str), and ""id"" (int) you should be able to implement your own Seq2SeqDataset class that wraps your dataset object. Does that make sense to you ?'
 'Hi\nI am sorry for asking it multiple times but I am not getting the dataloader\ntype, could you confirm if the dataset library returns back an iterable\ntype dataloader or a mapping type one where one has access to __getitem__,\nin the former case, one can iterate with __iter__, and how I can configure\nit to return the data back as the iterative type? I am dealing with\nlarge-scale datasets and  I do not want to bring all in memory\nthanks for your help\nBest regards\nRabeeh\n\nOn Mon, Nov 9, 2020 at 11:17 AM Quentin Lhoest <notifications@github.com>\nwrote:\n\n> datasets.Dataset objects implement indeed __getitem__. It returns a\n> dictionary with one field per column.\n>\n> We\'ve not added the integration of the datasets library for the seq2seq\n> utilities yet. The current seq2seq utilities are based on text files.\n>\n> However as soon as you have a datasets.Dataset with columns ""tgt_texts""\n> (str), ""src_texts"" (str), and ""id"" (int) you should be able to implement\n> your own Seq2SeqDataset class that wraps your dataset object. Does that\n> make sense ?\n>\n> —\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/huggingface/datasets/issues/815#issuecomment-723915556>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ARPXHHYOC22EM7F666BZSOTSO66R3ANCNFSM4TPB7OWA>\n> .\n>\n'
 '`datasets.Dataset` objects are both iterative and mapping types: it has both `__iter__` and `__getitem__`\r\nFor example you can do\r\n```python\r\nfor example in dataset:\r\n    # do something\r\n```\r\nor\r\n```python\r\nfor i in range(len(dataset)):\r\n    example = dataset[i]\r\n    # do something\r\n```\r\nWhen you do that, one and only one example is loaded into memory at a time.'
 'Hi there, \r\nHere is what I am trying, this is not working for me in map-style datasets, could you please tell me how to use datasets with being able to access ___getitem__ ? could you assist me please correcting this example? I need map-style datasets which is formed from concatenation of two datasets from your library. thanks \r\n\r\n\r\n```\r\nimport datasets\r\ndataset1 = load_dataset(""squad"", split=""train[:10]"")\r\ndataset1 = dataset1.map(lambda example: {""src_texts"": ""question: {0} context: {1} "".format(\r\n    example[""question""], example[""context""]),\r\n    ""tgt_texts"": example[""answers""][""text""][0]}, remove_columns=dataset1.column_names)\r\ndataset2 = load_dataset(""imdb"", split=""train[:10]"")\r\ndataset2 = dataset2.map(lambda example: {""src_texts"": ""imdb: "" + example[""text""],\r\n            ""tgt_texts"": str(example[""label""])}, remove_columns=dataset2.column_names)\r\ntrain_dataset = datasets.concatenate_datasets([dataset1, dataset2])\r\ntrain_dataset.set_format(type=\'torch\', columns=[\'src_texts\', \'tgt_texts\'])\r\ndataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32)\r\nfor id, batch in enumerate(dataloader):\r\n    print(batch)\r\n\r\n```'
 'closed since I found this response on the issue https://github.com/huggingface/datasets/issues/469']","Hi
I want to use your library for large-scale training, I am not sure if this is implemented as iterative datasets or not?
could you provide me with example how I can use datasets as iterative datasets?
thanks"
https://github.com/huggingface/datasets/issues/814,Joining multiple datasets ,"['found a solution here  https://discuss.pytorch.org/t/train-simultaneously-on-two-datasets/649/35, closed for now, thanks ']","Hi
I have multiple iterative datasets from your library with different size and I want to join them in a way that each datasets is sampled equally, so smaller datasets more, larger one less, could you tell me how to implement this in pytorch? thanks "
https://github.com/huggingface/datasets/issues/813,How to implement DistributedSampler with datasets ,"['Hi Apparently I need to shard the data and give one host a chunk, could you provide me please with examples on how to do it? I want to use it jointly with finetune_trainer.py in huggingface repo seq2seq examples. thanks. '
 ""Hey @rabeehkarimimahabadi I'm actually looking for the same feature. Did you manage to get somewhere?""]","Hi,
I am using your datasets to define my dataloaders, and I am training finetune_trainer.py in huggingface repo on them.
I need a distributedSampler to be able to train the models on TPUs being able to distribute the load across the TPU cores. Could you tell me how I can implement the distribued sampler when using datasets in which datasets are iterative? To give you more context, I have multiple of datasets and I need to write sampler for this case. thanks. "
https://github.com/huggingface/datasets/issues/812,Too much logging ,"[""Hi ! Thanks for reporting :) \r\nI agree these one should be hidden when the logging level is warning, we'll fix that""
 '+1, the amount of logging is excessive.\r\n\r\nMost of it indeed comes from `filelock.py`, though there are occasionally messages from other sources too. Below is an example (all of these messages were logged after I already called `datasets.logging.set_verbosity_error()`)\r\n\r\n```\r\nI1109 21:26:01.742688 139785006901056 filelock.py:318] Lock 139778216292192 released on /home/kitaev/.cache/huggingface/datasets/9ed4f2e133395826175a892c70611f68522c7bc61a35476e8b51a31afb76e4bf.e6f3e3f3e3875a07469d1cfd32e16e1d06b149616b11eef2d081c43d515b492d.py.lock\r\nI1109 21:26:01.747898 139785006901056 filelock.py:274] Lock 139778216290176 acquired on /home/kitaev/.cache/huggingface/datasets/_home_kitaev_.cache_huggingface_datasets_glue_mnli_1.0.0_7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4.lock\r\nI1109 21:26:01.748258 139785006901056 filelock.py:318] Lock 139778216290176 released on /home/kitaev/.cache/huggingface/datasets/_home_kitaev_.cache_huggingface_datasets_glue_mnli_1.0.0_7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4.lock\r\nI1109 21:26:01.748412 139785006901056 filelock.py:274] Lock 139778215853024 acquired on /home/kitaev/.cache/huggingface/datasets/_home_kitaev_.cache_huggingface_datasets_glue_mnli_1.0.0_7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4.lock\r\nI1109 21:26:01.748497 139785006901056 filelock.py:318] Lock 139778215853024 released on /home/kitaev/.cache/huggingface/datasets/_home_kitaev_.cache_huggingface_datasets_glue_mnli_1.0.0_7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4.lock\r\nI1109 21:07:17.029001 140301730502464 filelock.py:274] Lock 140289479304360 acquired on /home/kitaev/.cache/huggingface/datasets/b16d3a04bf2cad1346896852bf120ba846ea1bebb1cd60255bb3a1a2bbcc3a67.ec871b06a00118091ec63eff0a641fddcb8d3c7cd52e855bbb2be28944df4b82.py.lock\r\nI1109 21:07:17.029341 140301730502464 filelock.py:318] Lock 140289479304360 released on /home/kitaev/.cache/huggingface/datasets/b16d3a04bf2cad1346896852bf120ba846ea1bebb1cd60255bb3a1a2bbcc3a67.ec871b06a00118091ec63eff0a641fddcb8d3c7cd52e855bbb2be28944df4b82.py.lock\r\nI1109 21:07:17.058964 140301730502464 filelock.py:274] Lock 140251889388120 acquired on /home/kitaev/.cache/huggingface/metrics/glue/mnli/default_experiment-1-0.arrow.lock\r\nI1109 21:07:17.060933 140301730502464 filelock.py:318] Lock 140251889388120 released on /home/kitaev/.cache/huggingface/metrics/glue/mnli/default_experiment-1-0.arrow.lock\r\nI1109 21:07:17.061067 140301730502464 filelock.py:274] Lock 140296072521488 acquired on /home/kitaev/.cache/huggingface/metrics/glue/mnli/default_experiment-1-0.arrow.lock\r\nI1109 21:07:17.069736 140301730502464 metric.py:400] Removing /home/kitaev/.cache/huggingface/metrics/glue/mnli/default_experiment-1-0.arrow\r\nI1109 21:07:17.069949 140301730502464 filelock.py:318] Lock 140296072521488 released on /home/kitaev/.cache/huggingface/metrics/glue/mnli/default_experiment-1-0.arrow.lock\r\n```'
 'So how to solve this problem?'
 ""In the latest version of the lib the logs about locks are at the DEBUG level so you won't see them by default.\r\nAlso `set_verbosity_warning` does take into account these logs now.\r\nCan you try to update the lib ?\r\n```\r\npip install --upgrade datasets\r\n```""
 ""Thanks. For some reason I have to use the older version. Is that possible I can fix this by some surface-level trick?\r\n\r\nI'm still using 1.13 version datasets.""
 'On older versions you can use\r\n```python\r\nimport logging\r\n\r\nlogging.getLogger(""filelock"").setLevel(logging.WARNING)\r\n```'
 'Whoa Thank you! It works!']","I'm doing this in the beginning of my script:

from datasets.utils import logging as datasets_logging
datasets_logging.set_verbosity_warning()

but I'm still getting these logs:

[2020-11-07 15:45:41,908][filelock][INFO] - Lock 139958278886176 acquired on /home/username/.cache/huggingface/datasets/cfe20ffaa80ef1c145a0a210d5b9cdce2b60002831e6ed0edc7ab9275d6f0d48.1bd4ccbce9de3dad0698d84674a19d6cc66a84db736a6398110bd196795dde7e.py.lock

[2020-11-07 15:45:41,909][filelock][INFO] - Lock 139958278886176 released on /home/username/.cache/huggingface/datasets/cfe20ffaa80ef1c145a0a210d5b9cdce2b60002831e6ed0edc7ab9275d6f0d48.1bd4ccbce9de3dad0698d84674a19d6cc66a84db736a6398110bd196795dde7e.py.lock

using datasets version = 1.1.2"
https://github.com/huggingface/datasets/issues/811,nlp viewer error,"[""and also for 'blog_authorship_corpus'\r\nhttps://huggingface.co/nlp/viewer/?dataset=blog_authorship_corpus\r\n![image](https://user-images.githubusercontent.com/30210529/98557329-5c182800-22a4-11eb-9b01-5b910fb8fcd4.png)\r\n""
 'Is this the problem of my local computer or ??']","Hello, 
when I select amazon_us_reviews in nlp viewer, it shows error.
https://huggingface.co/nlp/viewer/?dataset=amazon_us_reviews
![image](https://user-images.githubusercontent.com/30210529/98447334-4aa81200-2124-11eb-9dca-82c3ab34ccc2.png)
"
https://github.com/huggingface/datasets/issues/809,Add Google Taskmaster dataset,"['Hey @yjernite. Was going to start working on this but found taskmaster 1,2 & 3 in the datasets library already so think this can be closed now?'
 'You are absolutely right :) \r\n\r\nClosed by https://github.com/huggingface/datasets/pull/1193 https://github.com/huggingface/datasets/pull/1197 https://github.com/huggingface/datasets/pull/1213']","## Adding a Dataset
- **Name:** Taskmaster
- **Description:** A large dataset of task-oriented dialogue with annotated goals (55K dialogues covering entertainment and travel reservations)
- **Paper:** https://arxiv.org/abs/1909.05358
- **Data:** https://github.com/google-research-datasets/Taskmaster
- **Motivation:** One of few annotated datasets of this size for goal-oriented dialogue

Instructions to add a new dataset can be found [here](https://huggingface.co/docs/datasets/share_dataset.html).
"
https://github.com/huggingface/datasets/issues/807,load_dataset for LOCAL CSV files report CONNECTION ERROR,"['Hi !\r\nThe url works on my side.\r\n\r\nIs the url working in your navigator ?\r\nAre you connected to internet ? Does your network block access to `raw.githubusercontent.com` ?'
 '> Hi !\r\n> The url works on my side.\r\n> \r\n> Is the url working in your navigator ?\r\n> Are you connected to internet ? Does your network block access to `raw.githubusercontent.com` ?\r\n\r\nI tried another server, it\'s working now. Thanks a lot.\r\n\r\nAnd I\'m curious about why download things from ""github"" when I load dataset from local files ?  Dose datasets work if my network crashed?'
 'It seems my network frequently crashed so most time it cannot work.'
 '\r\n\r\n\r\n> > Hi !\r\n> > The url works on my side.\r\n> > Is the url working in your navigator ?\r\n> > Are you connected to internet ? Does your network block access to `raw.githubusercontent.com` ?\r\n> \r\n> I tried another server, it\'s working now. Thanks a lot.\r\n> \r\n> And I\'m curious about why download things from ""github"" when I load dataset from local files ? Dose datasets work if my network crashed?\r\n\r\nI download the scripts `https://raw.githubusercontent.com/huggingface/datasets/1.1.2/datasets/csv/csv.py` and move it to the package dir `*/datasets/` solved the problem. Could you please put the file `datasets/datasets/csv/csv.py` to `datasets/src/datasets/`？ \r\n\r\nThanks :D'
 'hello, how did you solve this problems?\r\n\r\n> > > Hi !\r\n> > > The url works on my side.\r\n> > > Is the url working in your navigator ?\r\n> > > Are you connected to internet ? Does your network block access to `raw.githubusercontent.com` ?\r\n> > \r\n> > \r\n> > I tried another server, it\'s working now. Thanks a lot.\r\n> > And I\'m curious about why download things from ""github"" when I load dataset from local files ? Dose datasets work if my network crashed?\r\n> \r\n> I download the scripts `https://raw.githubusercontent.com/huggingface/datasets/1.1.2/datasets/csv/csv.py` and move it to the package dir `*/datasets/` solved the problem. Could you please put the file `datasets/datasets/csv/csv.py` to `datasets/src/datasets/`？\r\n> \r\n> Thanks :D\r\n\r\nhello, I tried this. but it still failed. how do you fix this error?'
 '> hello, how did you solve this problems?\r\n> \r\n> > > > Hi !\r\n> > > > The url works on my side.\r\n> > > > Is the url working in your navigator ?\r\n> > > > Are you connected to internet ? Does your network block access to `raw.githubusercontent.com` ?\r\n> > > \r\n> > > \r\n> > > I tried another server, it\'s working now. Thanks a lot.\r\n> > > And I\'m curious about why download things from ""github"" when I load dataset from local files ? Dose datasets work if my network crashed?\r\n> > \r\n> > \r\n> > I download the scripts `https://raw.githubusercontent.com/huggingface/datasets/1.1.2/datasets/csv/csv.py` and move it to the package dir `*/datasets/` solved the problem. Could you please put the file `datasets/datasets/csv/csv.py` to `datasets/src/datasets/`？\r\n> > Thanks :D\r\n> \r\n> hello, I tried this. but it still failed. how do you fix this error?\r\n\r\n你把那个脚本下载到你本地安装目录下，然后 `load_dataset(csv_script_path, data_fiels)`\r\n\r\n'
 '> > hello, how did you solve this problems?\r\n> > > > > Hi !\r\n> > > > > The url works on my side.\r\n> > > > > Is the url working in your navigator ?\r\n> > > > > Are you connected to internet ? Does your network block access to `raw.githubusercontent.com` ?\r\n> > > > \r\n> > > > \r\n> > > > I tried another server, it\'s working now. Thanks a lot.\r\n> > > > And I\'m curious about why download things from ""github"" when I load dataset from local files ? Dose datasets work if my network crashed?\r\n> > > \r\n> > > \r\n> > > I download the scripts `https://raw.githubusercontent.com/huggingface/datasets/1.1.2/datasets/csv/csv.py` and move it to the package dir `*/datasets/` solved the problem. Could you please put the file `datasets/datasets/csv/csv.py` to `datasets/src/datasets/`？\r\n> > > Thanks :D\r\n> > \r\n> > \r\n> > hello, I tried this. but it still failed. how do you fix this error?\r\n> \r\n> 你把那个脚本下载到你本地安装目录下，然后 `load_dataset(csv_script_path, data_fiels)`\r\n\r\n好的好的！解决了，感谢感谢！！！'
 '> \r\n> \r\n> > hello, how did you solve this problems?\r\n> > > > > Hi !\r\n> > > > > The url works on my side.\r\n> > > > > Is the url working in your navigator ?\r\n> > > > > Are you connected to internet ? Does your network block access to `raw.githubusercontent.com` ?\r\n> > > > \r\n> > > > \r\n> > > > I tried another server, it\'s working now. Thanks a lot.\r\n> > > > And I\'m curious about why download things from ""github"" when I load dataset from local files ? Dose datasets work if my network crashed?\r\n> > > \r\n> > > \r\n> > > I download the scripts `https://raw.githubusercontent.com/huggingface/datasets/1.1.2/datasets/csv/csv.py` and move it to the package dir `*/datasets/` solved the problem. Could you please put the file `datasets/datasets/csv/csv.py` to `datasets/src/datasets/`？\r\n> > > Thanks :D\r\n> > \r\n> > \r\n> > hello, I tried this. but it still failed. how do you fix this error?\r\n> \r\n> 你把那个脚本下载到你本地安装目录下，然后 `load_dataset(csv_script_path, data_fiels)`\r\n\r\n我照着做了，然后报错。\r\nValueError: unable to parse C:/Software/Anaconda/envs/ptk_gpu2/Lib/site-packages/datasets\\dataset_infos.json as a URL or as a local path\r\n\r\n`---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-5-fd2106a3f053> in <module>\r\n----> 1 dataset = load_dataset(\'C:/Software/Anaconda/envs/ptk_gpu2/Lib/site-packages/datasets/csv.py\', data_files=\'./test.csv\', delimiter=\',\', autogenerate_column_names=False)\r\n\r\nC:\\Software\\Anaconda\\envs\\ptk_gpu2\\lib\\site-packages\\datasets\\load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, save_infos, script_version, **config_kwargs)\r\n    588     # Download/copy dataset processing script\r\n    589     module_path, hash = prepare_module(\r\n--> 590         path, script_version=script_version, download_config=download_config, download_mode=download_mode, dataset=True\r\n    591     )\r\n    592 \r\n\r\nC:\\Software\\Anaconda\\envs\\ptk_gpu2\\lib\\site-packages\\datasets\\load.py in prepare_module(path, script_version, download_config, download_mode, dataset, force_local_path, **download_kwargs)\r\n    296         local_dataset_infos_path = cached_path(\r\n    297             dataset_infos,\r\n--> 298             download_config=download_config,\r\n    299         )\r\n    300     except (FileNotFoundError, ConnectionError):\r\n\r\nC:\\Software\\Anaconda\\envs\\ptk_gpu2\\lib\\site-packages\\datasets\\utils\\file_utils.py in cached_path(url_or_filename, download_config, **download_kwargs)\r\n    316     else:\r\n    317         # Something unknown\r\n--> 318         raise ValueError(""unable to parse {} as a URL or as a local path"".format(url_or_filename))\r\n    319 \r\n    320     if download_config.extract_compressed_file and output_path is not None:\r\n\r\nValueError: unable to parse C:/Software/Anaconda/envs/ptk_gpu2/Lib/site-packages/datasets\\dataset_infos.json as a URL or as a local path\r\n\r\n`'
 ""I also experienced this issue this morning. Looks like something specific to windows.\r\nI'm working on a fix""
 'I opened a PR @wn1652400018'
 '> \r\n> \r\n> I opened a PR @wn1652400018\r\n\r\nThanks you!, It works very well.']","## load_dataset for LOCAL CSV files report CONNECTION ERROR
- **Description:** 
A local demo csv file:
```
import pandas as pd
import numpy as np
from datasets import load_dataset
import torch
import transformers

df = pd.DataFrame(np.arange(1200).reshape(300,4))
df.to_csv('test.csv', header=False, index=False)

print('datasets version: ', datasets.__version__)
print('pytorch version: ', torch.__version__)
print('transformers version: ', transformers.__version__)

# output:
datasets version:  1.1.2
pytorch version:  1.5.0
transformers version:  3.2.0
```

when I load data through `dataset`:
```
dataset = load_dataset('csv', data_files='./test.csv', delimiter=',', autogenerate_column_names=False)
```
Error infos:
```
ConnectionError                           Traceback (most recent call last)
<ipython-input-17-bbdadb9a0c78> in <module>
----> 1 dataset = load_dataset('csv', data_files='./test.csv', delimiter=',', autogenerate_column_names=False)

~/.conda/envs/py36/lib/python3.6/site-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, save_infos, script_version, **config_kwargs)
    588     # Download/copy dataset processing script
    589     module_path, hash = prepare_module(
--> 590         path, script_version=script_version, download_config=download_config, download_mode=download_mode, dataset=True
    591     )
    592 

~/.conda/envs/py36/lib/python3.6/site-packages/datasets/load.py in prepare_module(path, script_version, download_config, download_mode, dataset, force_local_path, **download_kwargs)
    266         file_path = hf_github_url(path=path, name=name, dataset=dataset, version=script_version)
    267         try:
--> 268             local_path = cached_path(file_path, download_config=download_config)
    269         except FileNotFoundError:
    270             if script_version is not None:

~/.conda/envs/py36/lib/python3.6/site-packages/datasets/utils/file_utils.py in cached_path(url_or_filename, download_config, **download_kwargs)
    306             user_agent=download_config.user_agent,
    307             local_files_only=download_config.local_files_only,
--> 308             use_etag=download_config.use_etag,
    309         )
    310     elif os.path.exists(url_or_filename):

~/.conda/envs/py36/lib/python3.6/site-packages/datasets/utils/file_utils.py in get_from_cache(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag)
    473         elif response is not None and response.status_code == 404:
    474             raise FileNotFoundError(""Couldn't find file at {}"".format(url))
--> 475         raise ConnectionError(""Couldn't reach {}"".format(url))
    476 
    477     # Try a second time

ConnectionError: Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/1.1.2/datasets/csv/csv.py
```

And I try to connect to the site with requests:
```
import requests

requests.head(""https://raw.githubusercontent.com/huggingface/datasets/1.1.2/datasets/csv/csv.py"")
```

Similarly Error occurs:
```
---------------------------------------------------------------------------
ConnectionRefusedError                    Traceback (most recent call last)
~/.conda/envs/py36/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self)
    159             conn = connection.create_connection(
--> 160                 (self._dns_host, self.port), self.timeout, **extra_kw
    161             )

~/.conda/envs/py36/lib/python3.6/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options)
     83     if err is not None:
---> 84         raise err
     85 

~/.conda/envs/py36/lib/python3.6/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options)
     73                 sock.bind(source_address)
---> 74             sock.connect(sa)
     75             return sock

ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

NewConnectionError                        Traceback (most recent call last)
~/.conda/envs/py36/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
    676                 headers=headers,
--> 677                 chunked=chunked,
    678             )

~/.conda/envs/py36/lib/python3.6/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)
    380         try:
--> 381             self._validate_conn(conn)
    382         except (SocketTimeout, BaseSSLError) as e:

~/.conda/envs/py36/lib/python3.6/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn)
    975         if not getattr(conn, ""sock"", None):  # AppEngine might not have  `.sock`
--> 976             conn.connect()
    977 

~/.conda/envs/py36/lib/python3.6/site-packages/urllib3/connection.py in connect(self)
    307         # Add certificate verification
--> 308         conn = self._new_conn()
    309         hostname = self.host

~/.conda/envs/py36/lib/python3.6/site-packages/urllib3/connection.py in _new_conn(self)
    171             raise NewConnectionError(
--> 172                 self, ""Failed to establish a new connection: %s"" % e
    173             )

NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f3cceda5e48>: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

MaxRetryError                             Traceback (most recent call last)
~/.conda/envs/py36/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies)
    448                     retries=self.max_retries,
--> 449                     timeout=timeout
    450                 )

~/.conda/envs/py36/lib/python3.6/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
    724             retries = retries.increment(
--> 725                 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]
    726             )

~/.conda/envs/py36/lib/python3.6/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace)
    438         if new_retry.is_exhausted():
--> 439             raise MaxRetryError(_pool, url, error or ResponseError(cause))
    440 

MaxRetryError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /huggingface/datasets/1.1.2/datasets/csv/csv.py (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f3cceda5e48>: Failed to establish a new connection: [Errno 111] Connection refused',))

During handling of the above exception, another exception occurred:

ConnectionError                           Traceback (most recent call last)
<ipython-input-20-18cc3eb4a049> in <module>
      1 import requests
      2 
----> 3 requests.head(""https://raw.githubusercontent.com/huggingface/datasets/1.1.2/datasets/csv/csv.py"")

~/.conda/envs/py36/lib/python3.6/site-packages/requests/api.py in head(url, **kwargs)
    102 
    103     kwargs.setdefault('allow_redirects', False)
--> 104     return request('head', url, **kwargs)
    105 
    106 

~/.conda/envs/py36/lib/python3.6/site-packages/requests/api.py in request(method, url, **kwargs)
     59     # cases, and look like a memory leak in others.
     60     with sessions.Session() as session:
---> 61         return session.request(method=method, url=url, **kwargs)
     62 
     63 

~/.conda/envs/py36/lib/python3.6/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)
    528         }
    529         send_kwargs.update(settings)
--> 530         resp = self.send(prep, **send_kwargs)
    531 
    532         return resp

~/.conda/envs/py36/lib/python3.6/site-packages/requests/sessions.py in send(self, request, **kwargs)
    641 
    642         # Send the request
--> 643         r = adapter.send(request, **kwargs)
    644 
    645         # Total elapsed time of the request (approximately)

~/.conda/envs/py36/lib/python3.6/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies)
    514                 raise SSLError(e, request=request)
    515 
--> 516             raise ConnectionError(e, request=request)
    517 
    518         except ClosedPoolError as e:

ConnectionError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /huggingface/datasets/1.1.2/datasets/csv/csv.py (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f3cceda5e48>: Failed to establish a new connection: [Errno 111] Connection refused',))
```"
https://github.com/huggingface/datasets/issues/806,Quail dataset urls are out of date,"['Hi ! Thanks for reporting.\r\nWe should fix the urls and use quail 1.3.\r\nIf you want to contribute feel free to fix the urls and open a PR :) '
 'Done! PR [https://github.com/huggingface/datasets/pull/820](https://github.com/huggingface/datasets/pull/820)\r\n\r\nUpdated links and also regenerated the metadata and dummy data for v1.3 in order to pass verifications as described here: [https://huggingface.co/docs/datasets/share_dataset.html#adding-tests-and-metadata-to-the-dataset](https://huggingface.co/docs/datasets/share_dataset.html#adding-tests-and-metadata-to-the-dataset). '
 'Closing since #820 is merged.\r\nThanks again for fixing the urls :)']","<h3>Code</h3>

```
from datasets import load_dataset
quail = load_dataset('quail')
```

<h3>Error</h3>

```
FileNotFoundError: Couldn't find file at https://raw.githubusercontent.com/text-machine-lab/quail/master/quail_v1.2/xml/ordered/quail_1.2_train.xml
```


As per [quail v1.3 commit](https://github.com/text-machine-lab/quail/commit/506501cfa34d9ec6c042d31026ba6fea6bcec8ff) it looks like the location and suggested ordering has changed. In [https://github.com/huggingface/datasets/blob/master/datasets/quail/quail.py#L52-L58](https://github.com/huggingface/datasets/blob/master/datasets/quail/quail.py#L52-L58) the quail v1.2 datasets are being pointed to, which don't exist anymore."
https://github.com/huggingface/datasets/issues/805,"On loading a metric from datasets, I get the following error",['Hi ! We support only pyarrow > 0.17.1 so that we have access to the `PyExtensionType` object.\r\nCould you update pyarrow and try again ?\r\n```\r\npip install --upgrade pyarrow\r\n```'],"`from datasets import load_metric`

`metric = load_metric('bleurt')`

Traceback:
210 class _ArrayXDExtensionType(pa.PyExtensionType):
    211 
    212     ndims: int = None

AttributeError: module 'pyarrow' has no attribute 'PyExtensionType'

Any help will be appreciated. Thank you. "
https://github.com/huggingface/datasets/issues/804,Empty output/answer in TriviaQA test set (both in 'kilt_tasks' and 'trivia_qa'),"['cc @yjernite is this expected ?'
 'Yes: TriviaQA has a private test set for the leaderboard [here](https://competitions.codalab.org/competitions/17208)\r\n\r\nFor the KILT training and validation portions, you need to link the examples from the TriviaQA dataset as detailed here:\r\nhttps://github.com/huggingface/datasets/blob/master/datasets/kilt_tasks/README.md'
 'Oh ok, I guess I read the paper too fast 😅, thank you for your answer!']","# The issue

It's all in the title, it appears to be fine on the train and validation sets.

Is there some kind of mapping to do like for the questions (see https://github.com/huggingface/datasets/blob/master/datasets/kilt_tasks/README.md) ? 

# How to reproduce
```py
from datasets import load_dataset
kilt_tasks = load_dataset(""kilt_tasks"")
trivia_qa = load_dataset('trivia_qa', 'unfiltered.nocontext')
# both in ""kilt_tasks""
In [18]: any([output['answer'] for output in kilt_tasks['test_triviaqa']['output']])                                                                                                                        
Out[18]: False
# and ""trivia_qa""
In [13]: all([answer['value'] == '<unk>' for answer in trivia_qa['test']['answer']])                                                                                                                        
Out[13]: True
# appears to be fine on the train and validation sets.
In [14]: all([answer['value'] == '<unk>' for answer in trivia_qa['train']['answer']])                                                                                                                       
Out[14]: False

In [15]: all([answer['value'] == '<unk>' for answer in trivia_qa['validation']['answer']])                                                                                                                  
Out[15]: False

In [16]: any([output['answer'] for output in kilt_tasks['train_triviaqa']['output']])                                                                                                                       
Out[16]: True

In [17]: any([output['answer'] for output in kilt_tasks['validation_triviaqa']['output']])                                                                                                                  
Out[17]: True

```"
https://github.com/huggingface/datasets/issues/801,How to join two datasets?,"['Hi this is also my question. thanks '
 'Hi ! Currently the only way to add new fields to a dataset is by using `.map` and picking items from the other dataset\r\n'
 'Closing this one. Feel free to re-open if you have other questions about this issue.\r\n\r\nAlso linking another discussion about joining datasets: #853 ']","Hi,

I'm wondering if it's possible to join two (preprocessed) datasets with the same number of rows but different labels? 

I'm currently trying to create paired sentences for BERT from `wikipedia/'20200501.en`, and I couldn't figure out a way to create a paired sentence using `.map()` where the second sentence is **not** the next sentence (i.e., from a different article) of the first sentence.

Thanks!"
https://github.com/huggingface/datasets/issues/798,Cannot load TREC dataset: ConnectionError,"[""Hi ! Indeed there's an issue with those links.\r\nWe should probably use the target urls of the redirections instead""
 'Hi, the same issue here, could you tell me how to download it through datasets? thanks '
 'Same issue. '
 ""Actually it's already fixed on the master branch since #740 \r\nI'll do the 1.1.3 release soon""
 'Hi\nthanks, but I did tried to install from the pip install git+... and it does\nnot work for me,. thanks for the help. I have the same issue with wmt16,\n""ro-en""\nthanks.\nBest\nRabeeh\n\nOn Mon, Nov 16, 2020 at 10:29 AM Quentin Lhoest <notifications@github.com>\nwrote:\n\n> Actually it\'s already fixed on the master branch since #740\n> <https://github.com/huggingface/datasets/pull/740>\n> I\'ll do the 1.1.3 release soon\n>\n> —\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/huggingface/datasets/issues/798#issuecomment-727854736>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ABP4ZCEUBJKPOCLABXCKMPDSQDWH3ANCNFSM4TJBUKSA>\n> .\n>\n'
 'I just tested on google colab using\r\n```python\r\n!pip install git+https://github.com/huggingface/datasets.git\r\nfrom datasets import load_dataset\r\nload_dataset(""trec"")\r\n```\r\nand it works.\r\nCan you detail how you got the issue even when using the latest version on master ?\r\n\r\nAlso about wmt we\'ll look into it, thanks for reporting !']","## Problem
I cannot load ""trec"" dataset, it results with ConnectionError as shown below. I've tried on both Google Colab and locally. 
* `requests.head('http://cogcomp.org/Data/QA/QC/train_5500.label')` returns <Response [302]>. 
* `requests.head('http://cogcomp.org/Data/QA/QC/train_5500.label', allow_redirects=True)` raises `requests.exceptions.TooManyRedirects: Exceeded 30 redirects.`
* Opening `http://cogcomp.org/Data/QA/QC/train_5500.label' in a browser works, but opens a different address
* Increasing max_redirects to 100 doesn't help

Also, while debugging I've seen that requesting 'https://storage.googleapis.com/huggingface-nlp/cache/datasets/trec/default/1.1.0/dataset_info.json' returns <Response [404]> before, but it doesn't raise any errors. Not sure if that's relevant.

* datasets.__version__ == '1.1.2'
* requests.__version__ == '2.24.0'

## Error trace
```
>>> import datasets
>>> datasets.__version__
'1.1.2'
>>> dataset = load_dataset(""trec"", split=""train"")
Using custom data configuration default
Downloading and preparing dataset trec/default (download: 350.79 KiB, generated: 403.39 KiB, post-processed: Unknown size, total: 754.18 KiB) to /home/przemyslaw/.cache/huggingface/datasets/trec/default/1.1.0/ca4248481ad244f235f4cf277186cad2ee8769f975119a2bbfc41b8932b88bd7...
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/przemyslaw/.local/lib/python3.6/site-packages/datasets/load.py"", line 611, in load_dataset
    ignore_verifications=ignore_verifications,
  File ""/home/przemyslaw/.local/lib/python3.6/site-packages/datasets/builder.py"", line 476, in download_and_prepare
    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
  File ""/home/przemyslaw/.local/lib/python3.6/site-packages/datasets/builder.py"", line 531, in _download_and_prepare
    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)
  File ""/home/przemyslaw/.cache/huggingface/modules/datasets_modules/datasets/trec/ca4248481ad244f235f4cf277186cad2ee8769f975119a2bbfc41b8932b88bd7/trec.py"", line 140, in _split_generators
    dl_files = dl_manager.download_and_extract(_URLs)
  File ""/home/przemyslaw/.local/lib/python3.6/site-packages/datasets/utils/download_manager.py"", line 254, in download_and_extract
    return self.extract(self.download(url_or_urls))
  File ""/home/przemyslaw/.local/lib/python3.6/site-packages/datasets/utils/download_manager.py"", line 179, in download
    num_proc=download_config.num_proc,
  File ""/home/przemyslaw/.local/lib/python3.6/site-packages/datasets/utils/py_utils.py"", line 225, in map_nested
    _single_map_nested((function, obj, types, None, True)) for obj in tqdm(iterable, disable=disable_tqdm)
  File ""/home/przemyslaw/.local/lib/python3.6/site-packages/datasets/utils/py_utils.py"", line 225, in <listcomp>
    _single_map_nested((function, obj, types, None, True)) for obj in tqdm(iterable, disable=disable_tqdm)
  File ""/home/przemyslaw/.local/lib/python3.6/site-packages/datasets/utils/py_utils.py"", line 163, in _single_map_nested
    return function(data_struct)
  File ""/home/przemyslaw/.local/lib/python3.6/site-packages/datasets/utils/file_utils.py"", line 308, in cached_path
    use_etag=download_config.use_etag,
  File ""/home/przemyslaw/.local/lib/python3.6/site-packages/datasets/utils/file_utils.py"", line 475, in get_from_cache
    raise ConnectionError(""Couldn't reach {}"".format(url))
ConnectionError: Couldn't reach http://cogcomp.org/Data/QA/QC/train_5500.label
```

I would appreciate some suggestions here. "
https://github.com/huggingface/datasets/issues/797,Token classification labels are strings and we don't have the list of labels,"['Indeed. Pinging @stefan-it here if he want to give an expert opinion :)'
 'Related is https://github.com/huggingface/datasets/pull/636'
 'Should definitely be a ClassLabel 👍 ']","Not sure if this is an issue we want to fix or not, putting it here so it's not forgotten. Right now, in token classification datasets, the labels for NER, POS and the likes are typed as `Sequence` of `strings`, which is wrong in my opinion. These should be `Sequence` of `ClassLabel` or some types that gives easy access to the underlying labels.

The main problem for preprocessing those datasets is that the list of possible labels is not stored inside the `Dataset` object which makes converting the labels to IDs quite difficult (you either have to know the list of labels in advance or run a full pass through the dataset to get the list of labels, the `unique` method being useless with the type `Sequence[str]`)."
https://github.com/huggingface/datasets/issues/796,"Seq2Seq Metrics QOL: Bleu, Rouge","['Hi ! Thanks for letting us know your experience :) \r\nWe should at least improve the error messages indeed'
 'So what is the right way to add a batch to compute BLEU?'
 ""prediction = [['Hey', 'how', 'are', 'you', '?']]  \r\nreference=[['Hey', 'how', 'are', 'you', '?']]\r\nbleu.compute(predictions=prediction,references=reference)\r\n\r\nalso tried this kind of things lol\r\nI definitely need help too""
 'Hi !\r\n\r\nAs described in the documentation for `bleu`:\r\n```\r\nArgs:\r\n    predictions: list of translations to score.\r\n        Each translation should be tokenized into a list of tokens.\r\n    references: list of lists of references for each translation.\r\n        Each reference should be tokenized into a list of tokens.\r\n```\r\n\r\nTherefore you can use this metric this way:\r\n```python\r\nfrom datasets import load_metric\r\n\r\npredictions = [\r\n    [""hello"", ""there"", ""general"", ""kenobi""],                             # tokenized prediction of the first sample\r\n    [""foo"", ""bar"", ""foobar""]                                             # tokenized prediction of the second sample\r\n]\r\nreferences = [\r\n    [[""hello"", ""there"", ""general"", ""kenobi""], [""hello"", ""there"", ""!""]],  # tokenized references for the first sample (2 references)\r\n    [[""foo"", ""bar"", ""foobar""]]                                           # tokenized references for the second sample (1 reference)\r\n]\r\n\r\nbleu = load_metric(""bleu"")\r\nbleu.compute(predictions=predictions, references=references)\r\n# Or you can also add batches before calling compute()\r\n# bleu.add_batch(predictions=predictions, references=references)\r\n# bleu.compute()\r\n```\r\n\r\nHope this helps :)']","Putting all my QOL issues here, idt I will have time to propose fixes, but I didn't want these to be lost, in case they are useful. I tried using `rouge` and `bleu` for the first time and wrote down everything I didn't immediately understand:

+ Bleu expects tokenization, can I just kwarg it like sacrebleu?
+ different signatures, means that I would have had to add a lot of conditionals + pre and post processing: if I were going to replace the `calculate_rouge` and `calculate_bleu` functions here: https://github.com/huggingface/transformers/blob/master/examples/seq2seq/utils.py#L61


#### What I tried


Rouge experience:
```python

rouge = load_metric('rouge')
rouge.add_batch(['hi im sam'], ['im daniel']) # fails
rouge.add_batch(predictions=['hi im sam'], references=['im daniel']) # works
rouge.compute() # huge messy output, but reasonable. Not worth integrating b/c don't want to rewrite all the postprocessing.
```

BLEU experience:
```python
bleu = load_metric('bleu')
bleu.add_batch(predictions=['hi im sam'], references=['im daniel'])
bleu.add_batch(predictions=[['hi im sam']], references=[['im daniel']])

bleu.add_batch(predictions=[['hi im sam']], references=[['im daniel']])
```
All of these raise `ValueError: Got a string but expected a list instead: 'im daniel'`

#### Doc Typo
This says `dataset=load_metric(...)` which seems wrong, will cause `NameError`

![image](https://user-images.githubusercontent.com/6045025/98004483-ff0d0580-1dbd-11eb-9f35-6f35904611bb.png)

cc @lhoestq, feel free to ignore."
https://github.com/huggingface/datasets/issues/795,Descriptions of raw and processed versions of wikitext are inverted,['Yes indeed ! Thanks for reporting'],"Nothing of importance, but it looks like the descriptions of wikitext-n-v1 and wikitext-n-raw-v1 are inverted for both n=2 and n=103. I just verified by loading them and the `<unk>` tokens are present in the non-raw versions, which confirms that it's a mere inversion of the descriptions and not of the datasets themselves.

Also it would be nice if those descriptions appeared in the dataset explorer.

https://github.com/huggingface/datasets/blob/87bd0864845ea0a1dd7167918dc5f341bf807bd3/datasets/wikitext/wikitext.py#L52"
https://github.com/huggingface/datasets/issues/794,self.options cannot be converted to a Python object for pickling,"[""Hi ! Thanks for reporting that's a bug on master indeed.\r\nWe'll fix that soon""]","Hi,

Currently I am trying to load csv file with customized read_options. And the latest master seems broken if we pass the ReadOptions object.

Here is a code snippet
```python
from datasets import load_dataset
from pyarrow.csv import ReadOptions
load_dataset(""csv"", data_files=[""out.csv""], read_options=ReadOptions(block_size=16*1024*1024))
```
error is `self.options cannot be converted to a Python object for pickling`
Would you mind to take a look? Thanks!

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-28-ab83fec2ded4> in <module>
----> 1 load_dataset(""csv"", data_files=[""out.csv""], read_options=ReadOptions(block_size=16*1024*1024))

/tmp/datasets/src/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, save_infos, script_version, **config_kwargs)
    602         hash=hash,
    603         features=features,
--> 604         **config_kwargs,
    605     )
    606 

/tmp/datasets/src/datasets/builder.py in __init__(self, cache_dir, name, hash, features, **config_kwargs)
    162             name,
    163             custom_features=features,
--> 164             **config_kwargs,
    165         )
    166 

/tmp/datasets/src/datasets/builder.py in _create_builder_config(self, name, custom_features, **config_kwargs)
    281                 )
    282             else:
--> 283                 suffix = Hasher.hash(config_kwargs_to_add_to_suffix)
    284 
    285         if builder_config.data_files is not None:

/tmp/datasets/src/datasets/fingerprint.py in hash(cls, value)
     51             return cls.dispatch[type(value)](cls, value)
     52         else:
---> 53             return cls.hash_default(value)
     54 
     55     def update(self, value):

/tmp/datasets/src/datasets/fingerprint.py in hash_default(cls, value)
     44     @classmethod
     45     def hash_default(cls, value):
---> 46         return cls.hash_bytes(dumps(value))
     47 
     48     @classmethod

/tmp/datasets/src/datasets/utils/py_utils.py in dumps(obj)
    365     file = StringIO()
    366     with _no_cache_fields(obj):
--> 367         dump(obj, file)
    368     return file.getvalue()
    369 

/tmp/datasets/src/datasets/utils/py_utils.py in dump(obj, file)
    337 def dump(obj, file):
    338     """"""pickle an object to a file""""""
--> 339     Pickler(file, recurse=True).dump(obj)
    340     return
    341 

~/.local/lib/python3.6/site-packages/dill/_dill.py in dump(self, obj)
    444             raise PicklingError(msg)
    445         else:
--> 446             StockPickler.dump(self, obj)
    447         stack.clear()  # clear record of 'recursion-sensitive' pickled objects
    448         return

/usr/lib/python3.6/pickle.py in dump(self, obj)
    407         if self.proto >= 4:
    408             self.framer.start_framing()
--> 409         self.save(obj)
    410         self.write(STOP)
    411         self.framer.end_framing()

/usr/lib/python3.6/pickle.py in save(self, obj, save_persistent_id)
    474         f = self.dispatch.get(t)
    475         if f is not None:
--> 476             f(self, obj) # Call unbound method with explicit self
    477             return
    478 

~/.local/lib/python3.6/site-packages/dill/_dill.py in save_module_dict(pickler, obj)
    931             # we only care about session the first pass thru
    932             pickler._session = False
--> 933         StockPickler.save_dict(pickler, obj)
    934         log.info(""# D2"")
    935     return

/usr/lib/python3.6/pickle.py in save_dict(self, obj)
    819 
    820         self.memoize(obj)
--> 821         self._batch_setitems(obj.items())
    822 
    823     dispatch[dict] = save_dict

/usr/lib/python3.6/pickle.py in _batch_setitems(self, items)
    850                 k, v = tmp[0]
    851                 save(k)
--> 852                 save(v)
    853                 write(SETITEM)
    854             # else tmp is empty, and we're done

/usr/lib/python3.6/pickle.py in save(self, obj, save_persistent_id)
    494             reduce = getattr(obj, ""__reduce_ex__"", None)
    495             if reduce is not None:
--> 496                 rv = reduce(self.proto)
    497             else:
    498                 reduce = getattr(obj, ""__reduce__"", None)

~/.local/lib/python3.6/site-packages/pyarrow/_csv.cpython-36m-x86_64-linux-gnu.so in pyarrow._csv.ReadOptions.__reduce_cython__()

TypeError: self.options cannot be converted to a Python object for pickling
```"
https://github.com/huggingface/datasets/issues/792,KILT dataset: empty string in triviaqa input field,"['Just found out about https://github.com/huggingface/datasets/blob/master/datasets/kilt_tasks/README.md\r\n(Not very clear in https://huggingface.co/datasets/kilt_tasks links to http://github.com/huggingface/datasets/datasets/kilt_tasks/README.md which is dead, closing the issue though :))']","# What happened
Both train and test splits of the triviaqa dataset (part of the KILT benchmark) seem to have empty string in their input field (unlike the natural questions dataset, part of the same benchmark)

# Versions
KILT version is `1.0.0`
`datasets` version is `1.1.2`
[more here](https://gist.github.com/PaulLerner/3768c8d25f723edbac20d99b6a4056c1)

# How to reproduce
```py
In [1]: from datasets import load_dataset
In [4]: dataset = load_dataset(""kilt_tasks"")                                                                                                                                                                
# everything works fine, removed output for a better readibility
Dataset kilt_tasks downloaded and prepared to /people/lerner/.cache/huggingface/datasets/kilt_tasks/all_tasks/1.0.0/821c4295a2c35db2847585918d9c47d7f028f1a26b78825d8e77cd3aeb2621a1. Subsequent calls will reuse this data.

# empty string in triviaqa input field
In [36]: dataset['train_triviaqa'][0]                                                                                                                                                                       
Out[36]: 
{'id': 'dpql_5197',
 'input': '',
 'meta': {'left_context': '',
  'mention': '',
  'obj_surface': {'text': []},
  'partial_evidence': {'end_paragraph_id': [],
   'meta': [],
   'section': [],
   'start_paragraph_id': [],
   'title': [],
   'wikipedia_id': []},
  'right_context': '',
  'sub_surface': {'text': []},
  'subj_aliases': {'text': []},
  'template_questions': {'text': []}},
 'output': {'answer': ['five  £', '5 £', '£5', 'five £'],
  'meta': [],
  'provenance': [{'bleu_score': [1.0],
    'end_character': [248],
    'end_paragraph_id': [30],
    'meta': [],
    'section': ['Section::::Question of legal tender.\n'],
    'start_character': [246],
    'start_paragraph_id': [30],
    'title': ['Banknotes of the pound sterling'],
    'wikipedia_id': ['270680']}]}}
In [35]: dataset['train_triviaqa']['input'][:10]                                                                                                                                                            
Out[35]: ['', '', '', '', '', '', '', '', '', '']
# same with test set 
In [37]: dataset['test_triviaqa']['input'][:10]                                                                                                                                                             
Out[37]: ['', '', '', '', '', '', '', '', '', '']
# works fine with natural questions
In [34]: dataset['train_nq']['input'][:10]                                                                                                                                                                  
Out[34]: 
['how i.met your mother who is the mother',
 'who had the most wins in the nfl',
 'who played mantis guardians of the galaxy 2',
 'what channel is the premier league on in france',
 ""god's not dead a light in the darkness release date"",
 'who is the current president of un general assembly',
 'when do the eclipse supposed to take place',
 'what is the name of the sea surrounding dubai',
 'who holds the nba record for most points in a career',
 'when did the new maze runner movie come out']
```

Stay safe :)"
https://github.com/huggingface/datasets/issues/790,"Error running pip install -e "".[dev]"" on MacOS 10.13.6: faiss/python does not exist","['I saw that `faiss-cpu` 1.6.4.post2 was released recently to fix the installation on macos. It should work now'
 'Closing this one.\r\nFeel free to re-open if you still have issues']","I was following along with https://huggingface.co/docs/datasets/share_dataset.html#adding-tests-and-metadata-to-the-dataset when I ran into this error.

```sh
git clone https://github.com/huggingface/datasets
cd datasets
virtualenv venv -p python3 --system-site-packages
source venv/bin/activate
pip install -e "".[dev]""
```


![image](https://user-images.githubusercontent.com/59632/97868518-72871800-1cd5-11eb-9cd2-37d4e9d20b39.png)

![image](https://user-images.githubusercontent.com/59632/97868592-977b8b00-1cd5-11eb-8f3c-0c409616149c.png)

Python 3.7.7
"
https://github.com/huggingface/datasets/issues/786,feat(dataset): multiprocessing _generate_examples,"[""I agree that would be cool :)\r\nRight now the only distributed dataset builder is based on Apache Beam so you can use distributed processing frameworks like Dataflow, Spark, Flink etc. to build your dataset but it's not really well suited for single-worker parallel processing afaik""]","forking this out of #741, this issue is only regarding multiprocessing

I'd love if there was a dataset configuration parameter `workers`, where when it is `1` it behaves as it does right now, and when its `>1` maybe `_generate_examples` can also get the `pool` and return an iterable using the pool.

In my use case, I would instead of:
```python
for datum in data:
     yield self.load_datum(datum)
```
do:
```python
return pool.map(self.load_datum, data)
```

As the dataset in question, as an example, has **only** 7000 rows, and takes 10 seconds to load each row on average, it takes almost 20 hours to load the entire dataset.
If this was a larger dataset (and many such datasets exist), it would take multiple days to complete.

Using multiprocessing, for example, 40 cores, could speed it up dramatically. For this dataset, hopefully to fully load in under an hour."
https://github.com/huggingface/datasets/issues/784,Issue with downloading Wikipedia data for low resource language,"['Hello, maybe you could ty to use another date for the wikipedia dump (see the available [dates](https://dumps.wikimedia.org/jvwiki) here for `jv`) ?'
 ""@lhoestq\r\n\r\nI've tried `load_dataset('wikipedia', '20200501.zh', beam_runner='DirectRunner')` and got the same `FileNotFoundError` as @SamuelCahyawijaya.\r\n\r\nAlso, using another date (e.g. `load_dataset('wikipedia', '20201120.zh', beam_runner='DirectRunner')`) will give the following error message.\r\n\r\n```\r\nValueError: BuilderConfig 20201120.zh not found. Available: ['20200501.aa', '20200501.ab', '20200501.ace', '20200501.ady', '20200501.af', '20200501.ak', '20200501.als', '20200501.am', '20200501.an', '20200501.ang', '20200501.ar', '20200501.arc', '20200501.arz', '20200501.as', '20200501.ast', '20200501.atj', '20200501.av', '20200501.ay', '20200501.az', '20200501.azb', '20200501.ba', '20200501.bar', '20200501.bat-smg', '20200501.bcl', '20200501.be', '20200501.be-x-old', '20200501.bg', '20200501.bh', '20200501.bi', '20200501.bjn', '20200501.bm', '20200501.bn', '20200501.bo', '20200501.bpy', '20200501.br', '20200501.bs', '20200501.bug', '20200501.bxr', '20200501.ca', '20200501.cbk-zam', '20200501.cdo', '20200501.ce', '20200501.ceb', '20200501.ch', '20200501.cho', '20200501.chr', '20200501.chy', '20200501.ckb', '20200501.co', '20200501.cr', '20200501.crh', '20200501.cs', '20200501.csb', '20200501.cu', '20200501.cv', '20200501.cy', '20200501.da', '20200501.de', '20200501.din', '20200501.diq', '20200501.dsb', '20200501.dty', '20200501.dv', '20200501.dz', '20200501.ee', '20200501.el', '20200501.eml', '20200501.en', '20200501.eo', '20200501.es', '20200501.et', '20200501.eu', '20200501.ext', '20200501.fa', '20200501.ff', '20200501.fi', '20200501.fiu-vro', '20200501.fj', '20200501.fo', '20200501.fr', '20200501.frp', '20200501.frr', '20200501.fur', '20200501.fy', '20200501.ga', '20200501.gag', '20200501.gan', '20200501.gd', '20200501.gl', '20200501.glk', '20200501.gn', '20200501.gom', '20200501.gor', '20200501.got', '20200501.gu', '20200501.gv', '20200501.ha', '20200501.hak', '20200501.haw', '20200501.he', '20200501.hi', '20200501.hif', '20200501.ho', '20200501.hr', '20200501.hsb', '20200501.ht', '20200501.hu', '20200501.hy', '20200501.ia', '20200501.id', '20200501.ie', '20200501.ig', '20200501.ii', '20200501.ik', '20200501.ilo', '20200501.inh', '20200501.io', '20200501.is', '20200501.it', '20200501.iu', '20200501.ja', '20200501.jam', '20200501.jbo', '20200501.jv', '20200501.ka', '20200501.kaa', '20200501.kab', '20200501.kbd', '20200501.kbp', '20200501.kg', '20200501.ki', '20200501.kj', '20200501.kk', '20200501.kl', '20200501.km', '20200501.kn', '20200501.ko', '20200501.koi', '20200501.krc', '20200501.ks', '20200501.ksh', '20200501.ku', '20200501.kv', '20200501.kw', '20200501.ky', '20200501.la', '20200501.lad', '20200501.lb', '20200501.lbe', '20200501.lez', '20200501.lfn', '20200501.lg', '20200501.li', '20200501.lij', '20200501.lmo', '20200501.ln', '20200501.lo', '20200501.lrc', '20200501.lt', '20200501.ltg', '20200501.lv', '20200501.mai', '20200501.map-bms', '20200501.mdf', '20200501.mg', '20200501.mh', '20200501.mhr', '20200501.mi', '20200501.min', '20200501.mk', '20200501.ml', '20200501.mn', '20200501.mr', '20200501.mrj', '20200501.ms', '20200501.mt', '20200501.mus', '20200501.mwl', '20200501.my', '20200501.myv', '20200501.mzn', '20200501.na', '20200501.nah', '20200501.nap', '20200501.nds', '20200501.nds-nl', '20200501.ne', '20200501.new', '20200501.ng', '20200501.nl', '20200501.nn', '20200501.no', '20200501.nov', '20200501.nrm', '20200501.nso', '20200501.nv', '20200501.ny', '20200501.oc', '20200501.olo', '20200501.om', '20200501.or', '20200501.os', '20200501.pa', '20200501.pag', '20200501.pam', '20200501.pap', '20200501.pcd', '20200501.pdc', '20200501.pfl', '20200501.pi', '20200501.pih', '20200501.pl', '20200501.pms', '20200501.pnb', '20200501.pnt', '20200501.ps', '20200501.pt', '20200501.qu', '20200501.rm', '20200501.rmy', '20200501.rn', '20200501.ro', '20200501.roa-rup', '20200501.roa-tara', '20200501.ru', '20200501.rue', '20200501.rw', '20200501.sa', '20200501.sah', '20200501.sat', '20200501.sc', '20200501.scn', '20200501.sco', '20200501.sd', '20200501.se', '20200501.sg', '20200501.sh', '20200501.si', '20200501.simple', '20200501.sk', '20200501.sl', '20200501.sm', '20200501.sn', '20200501.so', '20200501.sq', '20200501.sr', '20200501.srn', '20200501.ss', '20200501.st', '20200501.stq', '20200501.su', '20200501.sv', '20200501.sw', '20200501.szl', '20200501.ta', '20200501.tcy', '20200501.te', '20200501.tet', '20200501.tg', '20200501.th', '20200501.ti', '20200501.tk', '20200501.tl', '20200501.tn', '20200501.to', '20200501.tpi', '20200501.tr', '20200501.ts', '20200501.tt', '20200501.tum', '20200501.tw', '20200501.ty', '20200501.tyv', '20200501.udm', '20200501.ug', '20200501.uk', '20200501.ur', '20200501.uz', '20200501.ve', '20200501.vec', '20200501.vep', '20200501.vi', '20200501.vls', '20200501.vo', '20200501.wa', '20200501.war', '20200501.wo', '20200501.wuu', '20200501.xal', '20200501.xh', '20200501.xmf', '20200501.yi', '20200501.yo', '20200501.za', '20200501.zea', '20200501.zh', '20200501.zh-classical', '20200501.zh-min-nan', '20200501.zh-yue', '20200501.zu']\r\n```\r\n\r\nI am pretty sure that `https://dumps.wikimedia.org/enwiki/20201120/dumpstatus.json` exists.""
 'Thanks for reporting I created a PR to make the custom config work (language=""zh"", date=""20201120"").'
 '@lhoestq Thanks!']","Hi, I tried to download Sundanese and Javanese wikipedia data with the following snippet
```
jv_wiki = datasets.load_dataset('wikipedia', '20200501.jv', beam_runner='DirectRunner')
su_wiki = datasets.load_dataset('wikipedia', '20200501.su', beam_runner='DirectRunner')
```
And I get the following error for these two languages:
Javanese
```
FileNotFoundError: Couldn't find file at https://dumps.wikimedia.org/jvwiki/20200501/dumpstatus.json
```

Sundanese
```
FileNotFoundError: Couldn't find file at https://dumps.wikimedia.org/suwiki/20200501/dumpstatus.json
```

I found from https://github.com/huggingface/datasets/issues/577#issuecomment-688435085 that for small languages, they are directly downloaded and parsed from the Wikipedia dump site, but both of `https://dumps.wikimedia.org/jvwiki/20200501/dumpstatus.json` and `https://dumps.wikimedia.org/suwiki/20200501/dumpstatus.json` are no longer valid.

 Any suggestions on how to handle this issue? Thanks!"
https://github.com/huggingface/datasets/issues/778,Unexpected behavior when loading cached csv file?,"['Hi ! Thanks for reporting.\r\nThe same issue was reported in #730 (but with the encodings instead of the delimiter). It was fixed by #770 .\r\nThe fix will be available in the next release :)'
 'Thanks for the prompt reply and terribly sorry for the spam! \r\nLooking forward to the new release! ']","I read a csv file from disk and forgot so specify the right delimiter. When i read the csv file again specifying the right delimiter it had no effect since it was using the cached dataset. I am not sure if this is unwanted behavior since i can always specify `download_mode=""force_redownload""`. But i think it would be nice if the information what `delimiter` or what `column_names` were used would influence the identifier of the cached dataset.

Small snippet to reproduce the behavior:
```python
import datasets

with open(""dummy_data.csv"", ""w"") as file:
    file.write(""test,this;text\n"")

print(datasets.load_dataset(""csv"", data_files=""dummy_data.csv"", split=""train"").column_names)
# [""test"", ""this;text""]

print(datasets.load_dataset(""csv"", data_files=""dummy_data.csv"", split=""train"", delimiter="";"").column_names)
# still [""test"", ""this;text""]
```

By the way, thanks a lot for this amazing library! :)"
https://github.com/huggingface/datasets/issues/773,Adding CC-100: Monolingual Datasets from Web Crawl Data,['cc @aconneau ;) '],"## Adding a Dataset
- **Name:** CC-100: Monolingual Datasets from Web Crawl Data
- **Description:** https://twitter.com/alex_conneau/status/1321507120848625665
- **Paper:** https://arxiv.org/abs/1911.02116
- **Data:** http://data.statmt.org/cc-100/
- **Motivation:** A large scale multi-lingual language modeling dataset. Text is de-duplicated and filtered by how ""Wikipedia-like"" it is, hopefully helping avoid some of the worst parts of the common crawl.

Instructions to add a new dataset can be found [here](https://huggingface.co/docs/datasets/share_dataset.html).
"
https://github.com/huggingface/datasets/issues/771,Using `Dataset.map` with `n_proc>1` print multiple progress bars,['Yes it allows to monitor the speed of each process. Currently each process takes care of one shard of the dataset.\r\n\r\nAt one point we can consider using streaming batches to a pool of processes instead of sharding the dataset in `num_proc` parts. At that point it will be easy to use only one progress bar'],"When using `Dataset.map` with `n_proc > 1`, only one of the processes should print a progress bar (to make the output readable). Right now, `n_proc` progress bars are printed."
https://github.com/huggingface/datasets/issues/769,How to choose proper download_mode in function load_dataset?,"[""`download_mode=datasets.GenerateMode.FORCE_REDOWNLOAD` should work.\r\nThis makes me think we we should rename this to DownloadMode.FORCE_REDOWNLOAD. Currently that's confusing""
 'Can we just use `features=...` in `load_dataset` for this @lhoestq?'
 ""Indeed you should use `features` in this case. \r\n```python\r\nfeatures = Features({'text': Value('string'), 'label': Value('float32')})\r\ndataset = load_dataset('csv', data_files=['sst_test.csv'], features=features)\r\n```\r\nNote that because of an issue with the caching when you change the features (see #750 ) you still need to specify the `FORCE_REDOWNLOAD ` flag. I'm working on a fix for this one""]","Hi, I am a beginner to datasets and I try to use datasets to load my csv file.
my csv file looks like this

``` 
text,label
""Effective but too-tepid biopic"",3
""If you sometimes like to go to the movies to have fun , Wasabi is a good place to start ."",4
""Emerges as something rare , an issue movie that 's so honest and keenly observed that it does n't feel like one ."",5
```

First I try to use this command to load my csv file .  

``` python
dataset=load_dataset('csv', data_files=['sst_test.csv'])
```

It seems good, but when i try to overwrite the convert_options to convert  'label' columns from int64 to float32 like this.

``` python
import pyarrow as pa
from pyarrow import csv
read_options = csv.ReadOptions(block_size=1024*1024)
parse_options = csv.ParseOptions()
convert_options = csv.ConvertOptions(column_types={'text': pa.string(), 'label': pa.float32()})
dataset = load_dataset('csv', data_files=['sst_test.csv'], read_options=read_options,
                       parse_options=parse_options, convert_options=convert_options)
```

It keeps the same:

```shell
Dataset(features: {'text': Value(dtype='string', id=None), 'label': Value(dtype='int64', id=None)}, num_rows: 2210)
```

I think this issue is caused by the parameter ""download_mode"" Default to REUSE_DATASET_IF_EXISTS because after I delete the cache_dir, it seems right.

Is it a bug? How to choose proper download_mode to avoid this issue?
"
https://github.com/huggingface/datasets/issues/768,Add a `lazy_map` method to `Dataset` and `DatasetDict`,['This is cool! I think some aspects to think about and decide in terms of API are:\r\n- do we allow several methods (chained i guess)\r\n- how do we inspect the currently set method(s)\r\n- how do we control/reset them'],"The library is great, but it would be even more awesome with a `lazy_map` method implemented on `Dataset` and `DatasetDict`. This would apply a function on a give item but when the item is requested. Two use cases:

1. load image on the fly
2. apply a random function and get different outputs at each epoch (like data augmentation or randomly masking a part of a sentence for BERT-like objectives)."
https://github.com/huggingface/datasets/issues/767,Add option for named splits when using ds.train_test_split,['Yes definitely we should give more flexibility to control the name of the splits outputted by `train_test_split`.\r\n\r\nRelated is the very interesting feedback from @bramvanroy on how we should improve this method: https://discuss.huggingface.co/t/how-to-split-main-dataset-into-train-dev-test-as-datasetdict/1090/5\r\n\r\nAnd in particular that it should advantageously be able to split in 3 splits as well instead of just 2 like we copied from sklearn.'],"### Feature Request 🚀 

Can we add a way to name your splits when using the `.train_test_split` function?

In almost every use case I've come across, I have a `train` and a `test` split in my `DatasetDict`, and I want to create a `validation` split. Therefore, its kinda useless to get a `test` split back from `train_test_split`, as it'll just overwrite my real `test` split that I intended to keep.

### Workaround

this is my hack for dealin with this, for now :slightly_smiling_face:

```python
from datasets import load_dataset
​
​
ds = load_dataset('imdb')
ds['train'], ds['validation'] = ds['train'].train_test_split(.1).values()
```
"
https://github.com/huggingface/datasets/issues/766,[GEM] add DART data-to-text generation dataset,"['Is this a duplicate of #924 ?'
 ""Yup, closing! Haven't been keeping track of the solved issues during the sprint.""]","## Adding a Dataset
- **Name:** DART
- **Description:** DART consists of 82,191 examples across different domains with each input being a semantic RDF triple set derived from data records in tables and the tree ontology of the schema, annotated with sentence descriptions that cover all facts in the triple set.
- **Paper:** https://arxiv.org/abs/2007.02871v1
- **Data:** https://github.com/Yale-LILY/dart
- **Motivation:** the dataset will likely be included in the GEM benchmark

Instructions to add a new dataset can be found [here](https://huggingface.co/docs/datasets/share_dataset.html).
"
https://github.com/huggingface/datasets/issues/761,Downloaded datasets are not usable offline,"[""Yes currently you need an internet connection because the lib tries to check for the etag of the dataset script online to see if you don't have it locally already.\r\n\r\nIf we add a way to store the etag/hash locally after the first download, it would allow users to first download the dataset with an internet connection, and still have it working without an internet connection.\r\n\r\nI'll let you know when we add this feature.""]","I've been trying to use the IMDB dataset offline, but after downloading it and turning off the internet it still raises  an error from the ```requests``` library trying to reach for the online dataset.
Is this the intended behavior ?
(Sorry, I wrote the the first version of this issue while still on nlp 0.3.0)."
https://github.com/huggingface/datasets/issues/759,(Load dataset failure) ConnectionError: Couldn’t reach https://raw.githubusercontent.com/huggingface/datasets/1.1.2/datasets/cnn_dailymail/cnn_dailymail.py,"['Are you running the script on a machine with an internet connection ?'
 'Yes ,  I can browse the url through Google Chrome.'
 'Does this HEAD request return 200 on your machine ?\r\n```python\r\nimport requests                                                                                                                                                                                                         \r\nrequests.head(""https://raw.githubusercontent.com/huggingface/datasets/1.1.2/datasets/cnn_dailymail/cnn_dailymail.py"")\r\n```\r\n\r\nIf it returns 200, could you try again to load the dataset ?'
 'Thank you very much for your response.\r\nWhen I run \r\n``` \r\nimport requests                                                                                                                                                                                                         \r\nrequests.head(""https://raw.githubusercontent.com/huggingface/datasets/1.1.2/datasets/cnn_dailymail/cnn_dailymail.py"")\r\n```\r\nIt returns 200.\r\n\r\nAnd I try again to load the dataset. I got the following errors again. \r\n\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""C:\\Users\\666666\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\datasets\\load.py"", line 608, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File ""C:\\Users\\666666\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\datasets\\builder.py"", line 475, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File ""C:\\Users\\666666\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\datasets\\builder.py"", line 531, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File ""C:\\Users\\666666\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\cnn_dailymail\\0128610a44e10f25b4af6689441c72af86205282d26399642f7db38fa7535602\\cnn_dailymail.py"", line 253, in _split_generators\r\n    dl_paths = dl_manager.download_and_extract(_DL_URLS)\r\n  File ""C:\\Users\\666666\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\datasets\\utils\\download_manager.py"", line 254, in download_and_extract\r\n    return self.extract(self.download(url_or_urls))\r\n  File ""C:\\Users\\666666\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\datasets\\utils\\download_manager.py"", line 175, in download\r\n    downloaded_path_or_paths = map_nested(\r\n  File ""C:\\Users\\666666\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\datasets\\utils\\py_utils.py"", line 224, in map_nested\r\n    mapped = [\r\n  File ""C:\\Users\\666666\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\datasets\\utils\\py_utils.py"", line 225, in <listcomp>\r\n    _single_map_nested((function, obj, types, None, True)) for obj in tqdm(iterable, disable=disable_tqdm)\r\n  File ""C:\\Users\\666666\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\datasets\\utils\\py_utils.py"", line 163, in _single_map_nested\r\n    return function(data_struct)\r\n  File ""C:\\Users\\666666\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\datasets\\utils\\file_utils.py"", line 300, in cached_path\r\n    output_path = get_from_cache(\r\n  File ""C:\\Users\\666666\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\datasets\\utils\\file_utils.py"", line 475, in get_from_cache\r\n    raise ConnectionError(""Couldn\'t reach {}"".format(url))\r\nConnectionError: Couldn\'t reach https://drive.google.com/uc?export=download&id=0BwmD_VLjROrfTHk4NFg2SndKcjQ\r\n\r\nConnection error happened but the url was different.\r\n\r\nI add the following code.\r\n```\r\nrequests.head(""https://drive.google.com/uc?export=download&id=0BwmD_VLjROrfTHk4NFg2SndKcjQ"")\r\n```\r\nThis  didn\'t  return 200\r\nIt returned like this:\r\n\r\nTraceback (most recent call last):\r\n  File ""C:\\Users\\666666\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\urllib3\\connection.py"", line 159, in _new_conn\r\n    conn = connection.create_connection(\r\n  File ""C:\\Users\\666666\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\urllib3\\util\\connection.py"", line 84, in create_connection\r\n    raise err\r\n  File ""C:\\Users\\666666\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\urllib3\\util\\connection.py"", line 74, in create_connection\r\n    sock.connect(sa)\r\nTimeoutError: [WinError 10060] \r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File ""C:\\Users\\666666\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\urllib3\\connectionpool.py"", line 670, in urlopen\r\n    httplib_response = self._make_request(\r\n  File ""C:\\Users\\666666\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\urllib3\\connectionpool.py"", line 381, in _make_request\r\n    self._validate_conn(conn)\r\n  File ""C:\\Users\\666666\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\urllib3\\connectionpool.py"", line 978, in _validate_conn\r\n    conn.connect()\r\n  File ""C:\\Users\\666666\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\urllib3\\connection.py"", line 309, in connect\r\n    conn = self._new_conn()\r\n  File ""C:\\Users\\666666\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\urllib3\\connection.py"", line 171, in _new_conn\r\n    raise NewConnectionError(\r\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x000001F6060618E0>: Failed to establish a new connection: [WinError 10060] '
 'Is google drive blocked on your network ?\r\nFor me \r\n```python\r\nrequests.head(""https://drive.google.com/uc?export=download&id=0BwmD_VLjROrfTHk4NFg2SndKcjQ"")\r\n```\r\nreturns 200'
 ""I can browse the google drive through google chrome. It's weird. I can download the dataset through google drive manually.""
 'Could you try to update `requests` maybe ?\r\nIt works with 2.23.0 on my side'
 ""My ```requests``` is 2.24.0 . It still can't return 200.""
 ""Is it possible I download the dataset manually from google drive and use it for further test ? How can I do this ? I want to reproduce the model in this link https://huggingface.co/patrickvonplaten/bert2bert-cnn_dailymail-fp16. But I can't download the dataset through load_dataset method . I have tried many times  and the connection error always happens .\r\n""
 'The head request should definitely work, not sure what\'s going on on your side.\r\nIf you find a way to make it work, please post it here since other users might encounter the same issue.\r\n\r\nIf you don\'t manage to fix it you can use `load_dataset` on google colab and then save it using `dataset.save_to_disk(""path/to/dataset"")`.\r\nThen you can download the directory on your machine and do\r\n```python\r\nfrom datasets import load_from_disk\r\ndataset = load_from_disk(""path/to/local/dataset"")\r\n```'
 'Hi\r\nI want to know if this problem has been solved because I encountered a similar issue. Thanks.\r\n`train_data = datasets.load_dataset(""xsum"", `split=""train"")`\r\n`ConnectionError:` Couldn\'t reach https://raw.githubusercontent.com/huggingface/datasets/1.1.3/datasets/xsum/xsum.py`'
 'Hi @smile0925 ! Do you have an internet connection ? Are you using some kind of proxy that may block the access to this file ?\r\n\r\nOtherwise you can try to update `datasets` since we introduced retries for http requests in the 1.2.0 version\r\n```\r\npip install --upgrade datasets\r\n```\r\nLet me know if that helps.'
 'Hi @lhoestq \r\nOh, may be you are right. I find that my server uses some kind of proxy that block the access to this file.\r\n![image](https://user-images.githubusercontent.com/46243662/106456211-2ca24180-64c8-11eb-831e-47e9b40e7da4.png)\r\n\r\n'
 '> Hi @lhoestq\r\n> Oh, may be you are right. I find that my server uses some kind of proxy that block the access to this file.\r\n> ![image](https://user-images.githubusercontent.com/46243662/106456211-2ca24180-64c8-11eb-831e-47e9b40e7da4.png)\r\n\r\nI have the same problem, have you solved it? Many thanks'
 'Hi  @ZhengxiangShi \r\nYou can first try whether your network can access these files. I need to use VPN to access these files, so I download the files that cannot be accessed to the local in advance, and then use them in the code. Like this,\r\n`train_data = datasets.load_dataset(""xsum.py"", split=""train"")`']","Hey, I want to load the cnn-dailymail dataset for fine-tune.
I write the code like this
from datasets import load_dataset

test_dataset = load_dataset(“cnn_dailymail”, “3.0.0”, split=“train”)

And I got the following errors.

Traceback (most recent call last):
File “test.py”, line 7, in
test_dataset = load_dataset(“cnn_dailymail”, “3.0.0”, split=“test”)
File “C:\Users\666666\AppData\Local\Programs\Python\Python38\lib\site-packages\datasets\load.py”, line 589, in load_dataset
module_path, hash = prepare_module(
File “C:\Users\666666\AppData\Local\Programs\Python\Python38\lib\site-packages\datasets\load.py”, line 268, in prepare_module
local_path = cached_path(file_path, download_config=download_config)
File “C:\Users\666666\AppData\Local\Programs\Python\Python38\lib\site-packages\datasets\utils\file_utils.py”, line 300, in cached_path
output_path = get_from_cache(
File “C:\Users\666666\AppData\Local\Programs\Python\Python38\lib\site-packages\datasets\utils\file_utils.py”, line 475, in get_from_cache
raise ConnectionError(“Couldn’t reach {}”.format(url))
ConnectionError: Couldn’t reach https://raw.githubusercontent.com/huggingface/datasets/1.1.2/datasets/cnn_dailymail/cnn_dailymail.py

How can I fix this ?"
https://github.com/huggingface/datasets/issues/758,Process 0 very slow when using num_procs with map to tokenizer,"['Hi ! Thanks for reporting.\r\nIs the distribution of text length of your data evenly distributed across your dataset ? I mean, could it be because the examples in the first part of your dataset are slower to process ?\r\nAlso could how many CPUs can you use for multiprocessing ?\r\n```python\r\nimport multiprocessing\r\nprint(multiprocessing.cpu_count())\r\n```\r\nWhich tokenizer are you using ?'
 ""Using pre trained HF tokenizer. The result is the same with tokenizer multiprocessing off and on.\r\nI have (absolutely) no idea about the distribution, but since this issue occurs on all of my datasets(regardless of files), I don't think distribution is the problems.\r\n\r\nI can use up to 16 cores.""
 ""Ok weird, I don't manage to reproduce this issue on my side.\r\nDoes it happen even with `num_proc=2` for example ?\r\nAlso could you provide more details about your OS and the versions of tokenizers/datasets/multiprocess that you're using ?""
 'Yes, I can confirm it also happens with ```num_proc=2```.\r\n```\r\ntokenizers               0.9.2\r\ndatasets                 1.1.2\r\nmultiprocess             0.70.10\r\n```\r\n```\r\nLinux nipa2020-0629 4.4.0-178-generic #208-Ubuntu SMP Sun Apr 5 23:45:10 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux\r\n```'
 ""I can't reproduce on my side unfortunately with the same versions.\r\n\r\nDo you have issues when doing multiprocessing with python ?\r\n```python\r\nfrom tqdm.auto import tqdm\r\nfrom multiprocess import Pool, RLock\r\n\r\ndef process_data(shard):\r\n    # implement\r\n\r\nnum_proc = 8\r\nshards = [] # implement, this must be a list of size num_proc\r\n\r\nwith Pool(num_proc, initargs=(RLock(),), initializer=tqdm.set_lock) as pool:\r\n    results = [pool.apply_async(process_data, shard=shard) for shard in shards]\r\n    transformed_shards = [r.get() for r in results]\r\n```""
 ""Nah, I'll just wait a few hours. Thank you for helping, though.""]","<img width=""721"" alt=""image"" src=""https://user-images.githubusercontent.com/17930170/97066109-776d0d00-15ed-11eb-8bba-bb4d2e0fcc33.png"">
The code I am using is
```

        dataset = load_dataset(""text"", data_files=[file_path], split='train')
        dataset = dataset.map(lambda ex: tokenizer(ex[""text""], add_special_tokens=True,
                                                truncation=True, max_length=args.block_size), num_proc=8)
        dataset.set_format(type='torch', columns=['input_ids'])
        dataset.save_to_disk(file_path+'.arrow')
```
"
https://github.com/huggingface/datasets/issues/757,CUDA out of memory,"[""Could you provide more details ? What's the code you ran ?""
 '```python\r\ntokenizer = FunnelTokenizer.from_pretrained(\'funnel-transformer/small\')\r\n\r\ndef tokenize(batch):\r\n    return tokenizer(batch[\'text\'], padding=\'max_length\', truncation=True,max_length=512)\r\n\r\ndataset = load_dataset(""bookcorpus"",split=\'train[:1000]\').shuffle()\r\ndataset = dataset.map(tokenize, batched=True, batch_size=512)\r\n\r\n# dataset = LineByLineTextDataset(\r\n#     tokenizer=tokenizer,\r\n#     file_path=""./wiki1000.txt"",\r\n#     block_size=128\r\n# )\r\n\r\ndata_collator = DataCollatorForLanguageModeling(\r\n    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\r\n)\r\n\r\nconfig=FunnelConfig(\r\n    return_dict=True\r\n)\r\n\r\nmodel= FunnelForMaskedLM(config=config)\r\n\r\ntraining_args = TrainingArguments(\r\n    output_dir=""./checkpoints"",\r\n    overwrite_output_dir=True,\r\n    do_train=True,\r\n    num_train_epochs=1,\r\n    per_device_train_batch_size=16,\r\n    per_device_eval_batch_size=16,\r\n    save_steps=10000,\r\n    logging_dir=\'./ptlogs\'\r\n)\r\n\r\ntrainer = Trainer(\r\n    model=model,\r\n    args=training_args,\r\n    data_collator=data_collator,\r\n    train_dataset=dataset,\r\n)\r\ntrainer.train()\r\n```'
 '`RuntimeError: CUDA out of memory. Tried to allocate 954.00 MiB (GPU 0; 15.90 GiB total capacity; 14.35 GiB already allocated; 753.75 MiB free; 14.39 GiB reserved in total by PyTorch)\r\nException raised from malloc at /pytorch/c10/cuda/CUDACachingAllocator.cpp:272 (most recent call first):`\r\n\r\npart of error output'
 'from funnel model to bert model : error still happened\r\n\r\nfrom your dataset to LineByLineTextDataset : error disapeared'
 'notice i just loaded 1000 rows of data'
 'the error happens when executing loss.backward()'
 ""Since you're using a data collator you don't need to tokenizer the dataset using `map`. Could you try not to use `map` and only the data collator instead ? The data collator is supposed to pad to the longest sequence in each batch afaik, instead of padding to 512.\r\n\r\nAlso cc @sgugger ""
 'Closing this one.\r\nFeel free to re-open if you have other questions about this issue']","In your dataset ,cuda run out of memory as long as the trainer begins:
however, without changing any other element/parameter,just switch dataset to `LineByLineTextDataset`,everything becames OK.
"
https://github.com/huggingface/datasets/issues/752,"Clicking on a metric in the search page points to datasets page giving ""Missing dataset"" warning","['Thanks for the report, can reproduce. Will fix'
 'Fixed now @ogabrielluiz ']","Hi! Sorry if this isn't the right place to talk about the website, I just didn't exactly where to write this.

Searching a metric in https://huggingface.co/metrics gives the right results but clicking on a metric (E.g ROUGE) points to https://huggingface.co/datasets/rouge. Clicking on a metric without searching points to the right page.

Thanks for all the great work!"
https://github.com/huggingface/datasets/issues/751,Error loading ms_marco v2.1 using load_dataset(),"['There was a similar issue in #294 \r\nClearing the cache and download again the dataset did the job. Could you try to clear your cache and download the dataset again ?'
 ""I was able to load the dataset successfully, I'm pretty sure it's just a cache issue that you have.\r\nLet me know if clearing your cache fixes the problem""
 'Yes, it indeed was a cache issue!\r\nThanks for reaching out!!']","Code:
`dataset = load_dataset('ms_marco', 'v2.1')`

Error:
```
`---------------------------------------------------------------------------
JSONDecodeError                           Traceback (most recent call last)
<ipython-input-16-34378c057212> in <module>()
      9 
     10 # Downloading and loading a dataset
---> 11 dataset = load_dataset('ms_marco', 'v2.1')

10 frames
/usr/lib/python3.6/json/decoder.py in raw_decode(self, s, idx)
    353         """"""
    354         try:
--> 355             obj, end = self.scan_once(s, idx)
    356         except StopIteration as err:
    357             raise JSONDecodeError(""Expecting value"", s, err.value) from None

JSONDecodeError: Unterminated string starting at: line 1 column 388988661 (char 388988660)
`
```"
https://github.com/huggingface/datasets/issues/749,[XGLUE] Adding new dataset,"['Amazing! '
 'Small poll  @thomwolf @yjernite @lhoestq @JetRunner @qiweizhen .\r\n\r\nAs stated in the XGLUE paper: https://arxiv.org/pdf/2004.01401.pdf , for each of the 11 down-stream tasks training data is only available in English, whereas development and test data is available in multiple different language *cf.* here: \r\n\r\n![Screenshot from 2020-11-04 15-02-17](https://user-images.githubusercontent.com/23423619/98120893-d7499a80-1eae-11eb-9d0b-57dfe5d4ee68.png)\r\n\r\nSo, I\'d suggest to have exactly 11 ""language-independent"" configs: ""ner"", ""pos"", ... and give the sample in each dataset in the config a ""language"" label being one of ""ar"", ""bg"", .... => To me this makes more sense than making languaga specific config, *e.g.* ""ner-de"", ...especially because training data is only available in English. Do you guys agree? '
 'In this case we should have named splits, so config `ner` has splits `train`, `validation`, `test-en`, `test-ar`, `test-bg`, etc...\r\n\r\nThis is more in the spirit of the task afaiu, and will avoid making users do the filtering step themselves when testing different models or different configurations of the same model.'
 'I see your point! \r\n\r\nI think this would be quite feasible to do and makes sense to me as well! In the paper results are reported per language, so it seems more natural to do it this way. \r\n\r\nGood for me @yjernite ! What do the others think? @lhoestq \r\n'
 'I agree with Yacine on this!'
 'Okey actually not that easy to add things like `test-de` to `datasets` => this would be the first dataset to have this.\r\nSee: https://github.com/huggingface/datasets/pull/802'
 ""IMO we should have one config per language. That's what we're doing for xnli, xtreme etc.\r\nHaving split names that depend on the language seems wrong. We should try to avoid split names that are not train/val/test.\r\nSorry for late response on this one""
 ""@lhoestq agreed on having one config per language, but we also need to be able to have different split names and people are going to want to use hyphens, so we should at the very least warn them why it's failing :) E.g. for ANLI with different stages of data (currently using underscores) or https://www.tau-nlp.org/commonsenseqa with their train-sanity or dev-sanity splits""
 'Yes sure ! Could you open a separate issue for that ?'
 'Really cool dataset 👍 btw. does Transformers support all 11 tasks 🤔 would be awesome to have a xglue script (like the ""normal"" glue one)'
 'Just to make sure this is what we want here. If we add one config per language, \r\n\r\nthis means that this dataset ends up with well over 100 different configs most of which will have the same `train` split. The train split is always in English. Also, I\'m not sure whether it\'s better for the user to be honest. \r\n\r\nI think it could be quite confusing for the user to have\r\n\r\n```python\r\ntrain_dataset = load_dataset(""xglue"", ""ner-de"", split=""train"")\r\n```\r\n\r\nin English even though it\'s `ner-de`.\r\n\r\nTo be honest, I\'d prefer:\r\n\r\n```python\r\ntrain_dataset = load_dataset(""xglue"", ""ner"", split=""train"")\r\ntest_dataset_de = load_dataset(""xglue"", ""ner"", split=""test-de"")\r\ntest_dataset_fr = load_dataset(""xglue"", ""ner"", split=""test-fr"")\r\n```\r\n\r\nhere'
 'Oh yes right I didn\'t notice the train set was always in english sorry.\r\nMoreover it seems that the way this dataset is used is to pick a pretrained multilingual model, fine-tune it on the english train set and then evaluate on each test set (one per language).\r\nSo to better fit the usual usage of this dataset, I agree that it\'s better to have one test split per language. \r\n\r\nSomething like your latest example patrick is fine imo :\r\n```python\r\ntrain_dataset = load_dataset(""xglue"", ""ner"", split=""train"")\r\ntest_dataset_de = load_dataset(""xglue"", ""ner"", split=""test.de"")\r\n```\r\n\r\nI just replace test-de with test.de since `-` is not allowed for split names (it has to follow the `\\w+` regex), and usually we specify the language after a point. '
 'Closing since XGLUE has been added in #802 , thanks patrick :) ']","XGLUE is a multilingual GLUE like dataset propesed in this [paper](https://arxiv.org/pdf/2004.01401.pdf).

I'm planning on adding the dataset to the library myself in a couple of weeks.
Also tagging @JetRunner @qiweizhen in case I need some guidance "
https://github.com/huggingface/datasets/issues/744,Dataset Explorer Doesn't Work for squad_es and squad_it,['Oups wrong click.\r\nThis one is for you @srush'],"https://huggingface.co/nlp/viewer/?dataset=squad_es
https://huggingface.co/nlp/viewer/?dataset=squad_it

Both pages show ""OSError: [Errno 28] No space left on device""."
https://github.com/huggingface/datasets/issues/743,load_dataset for CSV files not working,"[""Thank you !\r\nCould you provide a csv file that reproduces the error ?\r\nIt doesn't have to be one of your dataset. As long as it reproduces the error\r\nThat would help a lot !""
 'I think another good example is the following:\r\n`\r\nfrom datasets import load_dataset\r\n`\r\n`\r\ndataset = load_dataset(""csv"", data_files=[""./sts-dev.csv""], delimiter=""\\t"", column_names=[""one"", ""two"", ""three"", ""four"",  ""score"", ""sentence1"", ""sentence2""], script_version=""master"")`\r\n`\r\n\r\nDisplayed error `CSV parse error: Expected 7 columns, got 6` even tough I put 7 columns. First four columns from the csv don\'t have a name, so I\'ve named them by default. The csv file is the .dev file from STSb benchmark dataset.\r\n\r\n'
 'Hi, seems I also can\'t read csv file. I was trying with a dummy csv with only three rows.\r\n\r\n```\r\ntext,label\r\nI hate google,negative\r\nI love Microsoft,positive\r\nI don\'t like you,negative\r\n```\r\nI was using the HuggingFace image in Paperspace Gradient (datasets==1.1.3). The following code doesn\'t work:\r\n\r\n```\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\'csv\', script_version=""master"", data_files=[\'test_data.csv\'], delimiter="","")\r\n```\r\nIt outputs the following:\r\n```\r\nUsing custom data configuration default\r\nDownloading and preparing dataset csv/default-3b6254ff4dd403e5 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-3b6254ff4dd403e5/0.0.0/2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2...\r\nDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-3b6254ff4dd403e5/0.0.0/2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2. Subsequent calls will reuse this data.\r\n```\r\nBut `len(dataset)` gives `1` and I can\'t access rows with indexing `dataset[0]` (it gives `KeyError: 0`).\r\n\r\nHowever, loading from pandas dataframe is working.\r\n```\r\nfrom datasets import Dataset\r\nimport pandas as pd\r\ndf = pd.read_csv(\'test_data.csv\')\r\ndataset = Dataset.from_pandas(df)\r\n```\r\n\r\n'
 'This is because load_dataset without `split=` returns a dictionary of split names (train/validation/test) to dataset.\r\nYou can do\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\'csv\', script_version=""master"", data_files=[\'test_data.csv\'], delimiter="","")\r\nprint(dataset[""train""][0])\r\n```\r\n\r\nOr if you want to directly get the train split:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\'csv\', script_version=""master"", data_files=[\'test_data.csv\'], delimiter="","", split=""train"")\r\nprint(dataset[0])\r\n```\r\n'
 ""Good point\r\n\r\nDesign question for us, though: should `load_dataset` when no split is specified and only one split is present in the dataset (common use case with CSV/text/JSON datasets) return a `Dataset` instead of a `DatsetDict`? I feel like it's often what the user is expecting. I break a bit the paradigm of a unique return type but since this library is designed for widespread DS people more than CS people usage I would tend to think that UX should take precedence over CS reasons. What do you think?""
 ""In this case the user expects to get only one dataset object instead of the dictionary of datasets since only one csv file was specified without any split specifications.\r\nI'm ok with returning the dataset object if no split specifications are given for text/json/csv/pandas.\r\n\r\nFor the other datasets ton the other hand the user doesn't know in advance the splits so I would keep the dictionary by default. What do you think ?""
 ""Thanks for your quick response! I'm fine with specifying the split as @lhoestq suggested. My only concern is when I'm loading from python dict or pandas, the library returns a dataset instead of a dictionary of datasets when no split is specified. I know that they use a different function `Dataset.from_dict` or `Dataset.from_pandas` but the text/csv files use `load_dataset()`. However, to the user, they do the same task and we probably expect them to have the same behavior.""
 '```\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\'csv\', data_files=\'./amazon_data/Video_Games_5.csv\', delimiter="","", split=[\'train\', \'test\'])\r\n```\r\nI was running the above line, but got this error.\r\n\r\n```ValueError: Unknown split ""test"". Should be one of [\'train\'].```\r\n\r\nThe data is amazon product data. I load the Video_Games_5.json.gz data into pandas and save it as csv file. and then load the csv file using the above code. I thought, ```split=[\'train\', \'test\']``` would split the data into train and test. did I misunderstood?\r\n\r\nThank you!\r\n\r\n'
 'Hi ! the `split` argument in `load_dataset` is used to select the splits you want among the available splits.\r\nHowever when loading a csv with a single file as you did, only a `train` split is available by default.\r\n\r\nIndeed since `data_files=\'./amazon_data/Video_Games_5.csv\'` is equivalent to `data_files={""train"": \'./amazon_data/Video_Games_5.csv\'}`, you can get a dataset with \r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\'csv\', data_files=\'./amazon_data/Video_Games_5.csv\', delimiter="","", split=""train"")\r\n```\r\n\r\nAnd then to get both a train and test split you can do\r\n```python\r\ndataset = dataset.train_test_split()\r\nprint(dataset.keys())\r\n# [\'train\', \'test\']\r\n```\r\n\r\n\r\nAlso note that a csv dataset may have several available splits if it is defined this way:\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\'csv\', data_files={\r\n    ""train"": \'./amazon_data/Video_Games_5_train.csv\',\r\n    ""test"": \'./amazon_data/Video_Games_5_test.csv\'\r\n})\r\nprint(dataset.keys())\r\n# [\'train\', \'test\']\r\n```\r\n'
 ""> In this case the user expects to get only one dataset object instead of the dictionary of datasets since only one csv file was specified without any split specifications.\r\n> I'm ok with returning the dataset object if no split specifications are given for text/json/csv/pandas.\r\n> \r\n> For the other datasets ton the other hand the user doesn't know in advance the splits so I would keep the dictionary by default. What do you think ?\r\n\r\nYes maybe this would be good. I think having to select 'train' from the resulting object why the user gave no split information is a confusing and not intuitive behavior.""
 '> Similar to #622, I\'ve noticed there is a problem when trying to load a CSV file with datasets.\r\n> \r\n> `from datasets import load_dataset`\r\n> `dataset = load_dataset(""csv"", data_files=[""./sample_data.csv""], delimiter=""\\t"", column_names=[""title"", ""text""], script_version=""master"")`\r\n> \r\n> Displayed error:\r\n> `... ArrowInvalid: CSV parse error: Expected 2 columns, got 1`\r\n\r\nI\'m also facing the same issue when trying to load from a csv file locally:\r\n\r\n```python\r\nfrom nlp import load_dataset\r\ndataset = load_dataset(\'csv\', data_files=\'sample_data.csv\')\r\n```\r\n\r\nError when executed from Google Colab:\r\n```python\r\nArrowInvalid                              Traceback (most recent call last)\r\n<ipython-input-34-79a8d4f65ed6> in <module>()\r\n      1 from nlp import load_dataset\r\n----> 2 dataset = load_dataset(\'csv\', data_files=\'sample_data.csv\')\r\n\r\n9 frames\r\n/usr/local/lib/python3.7/dist-packages/nlp/load.py in load_dataset(path, name, version, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, save_infos, **config_kwargs)\r\n    547     # Download and prepare data\r\n    548     builder_instance.download_and_prepare(\r\n--> 549         download_config=download_config, download_mode=download_mode, ignore_verifications=ignore_verifications,\r\n    550     )\r\n    551 \r\n\r\n/usr/local/lib/python3.7/dist-packages/nlp/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, **download_and_prepare_kwargs)\r\n    461                 if not downloaded_from_gcs:\r\n    462                     self._download_and_prepare(\r\n--> 463                         dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    464                     )\r\n    465                 # Sync info\r\n\r\n/usr/local/lib/python3.7/dist-packages/nlp/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    535             try:\r\n    536                 # Prepare split will record examples associated to the split\r\n--> 537                 self._prepare_split(split_generator, **prepare_split_kwargs)\r\n    538             except OSError:\r\n    539                 raise OSError(""Cannot find data file. "" + (self.manual_download_instructions or """"))\r\n\r\n/usr/local/lib/python3.7/dist-packages/nlp/builder.py in _prepare_split(self, split_generator)\r\n    863 \r\n    864         generator = self._generate_tables(**split_generator.gen_kwargs)\r\n--> 865         for key, table in utils.tqdm(generator, unit="" tables"", leave=False):\r\n    866             writer.write_table(table)\r\n    867         num_examples, num_bytes = writer.finalize()\r\n\r\n/usr/local/lib/python3.7/dist-packages/tqdm/notebook.py in __iter__(self, *args, **kwargs)\r\n    213     def __iter__(self, *args, **kwargs):\r\n    214         try:\r\n--> 215             for obj in super(tqdm_notebook, self).__iter__(*args, **kwargs):\r\n    216                 # return super(tqdm...) will not catch exception\r\n    217                 yield obj\r\n\r\n/usr/local/lib/python3.7/dist-packages/tqdm/std.py in __iter__(self)\r\n   1102                 fp_write=getattr(self.fp, \'write\', sys.stderr.write))\r\n   1103 \r\n-> 1104         for obj in iterable:\r\n   1105             yield obj\r\n   1106             # Update and possibly print the progressbar.\r\n\r\n/usr/local/lib/python3.7/dist-packages/nlp/datasets/csv/ede98314803c971fef04bcee45d660c62f3332e8a74491e0b876106f3d99bd9b/csv.py in _generate_tables(self, files)\r\n     78                 read_options=self.config.pa_read_options,\r\n     79                 parse_options=self.config.pa_parse_options,\r\n---> 80                 convert_options=self.config.convert_options,\r\n     81             )\r\n     82             yield i, pa_table\r\n\r\n/usr/local/lib/python3.7/dist-packages/pyarrow/_csv.pyx in pyarrow._csv.read_csv()\r\n\r\n/usr/local/lib/python3.7/dist-packages/pyarrow/error.pxi in pyarrow.lib.pyarrow_internal_check_status()\r\n\r\n/usr/local/lib/python3.7/dist-packages/pyarrow/error.pxi in pyarrow.lib.check_status()\r\n\r\nArrowInvalid: CSV parse error: Expected 1 columns, got 8\r\n```\r\n\r\nVersion:\r\n```\r\nnlp==0.4.0\r\n```'
 ""Hi @kauvinlucas\r\n\r\nYou can use the latest versions of `datasets` to do this.\r\nTo do so, just `pip install datasets` instead of `nlp` (the library was renamed) and then\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('csv', data_files='sample_data.csv')""
 ""Hi \r\nI'm having a different problem with loading local csv. \r\n```Python\r\nfrom datasets import load_dataset  \r\ndataset = load_dataset('csv', data_files='sample.csv')  \r\n```  \r\n\r\ngives `ValueError: Specified named and prefix; you can only specify one.` error  \r\n\r\nversions:  \r\n- datasets: 1.1.3   \r\n- python: 3.9.6  \r\n- pyarrow: 2.0.0 ""
 ""Oh.. I figured it out. According to issue #[42387](https://github.com/pandas-dev/pandas/issues/42387) from pandas, this new version does not accept None for both parameters (which was being done by the repo I'm testing). Dowgrading Pandas==1.0.4 and Python==3.8 worked""
 'Hi, \r\nI got an `OSError: Cannot find data file. ` when I tried to use load_dataset with tsv files. I have checked the paths, and they are correct.  \r\n\r\nversions\r\n- python: 3.7.9\r\n- datasets: 1.1.3\r\n- pyarrow: 2.0.0\r\n- transformers: 4.2.2\r\n\r\n~~~\r\ndata_files = {""train"": ""train.tsv"", ""test"",: ""test.tsv""}\r\ndatasets = load_dataset(""csv"", data_files=data_files, delimiter=""\\t"")\r\n~~~\r\n\r\nThe entire Error message is on below:\r\n\r\n```08/14/2021 16:55:44 - INFO - __main__ -   load a local file for train: /project/media-framing/transformer4/data/0/val/label1.tsv\r\n08/14/2021 16:55:44 - INFO - __main__ -   load a local file for test: /project/media-framing/transformer4/data/unlabel/test.tsv\r\nUsing custom data configuration default\r\nDownloading and preparing dataset csv/default-00a4200ae8507533 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /usr4/cs542sp/hey1/.cache/huggingface/datasets/csv/default-00a4200ae8507533/0.0.0/2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2...\r\nTraceback (most recent call last):\r\n  File ""/projectnb2/media-framing/env-trans4/lib/python3.7/site-packages/datasets/builder.py"", line 592, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File ""/projectnb2/media-framing/env-trans4/lib/python3.7/site-packages/datasets/builder.py"", line 944, in _prepare_split\r\n    num_examples, num_bytes = writer.finalize()\r\n  File ""/projectnb2/media-framing/env-trans4/lib/python3.7/site-packages/datasets/arrow_writer.py"", line 307, in finalize\r\n    self.stream.close()\r\n  File ""pyarrow/io.pxi"", line 132, in pyarrow.lib.NativeFile.close\r\n  File ""pyarrow/error.pxi"", line 99, in pyarrow.lib.check_status\r\nOSError: error closing file\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File ""run_glue.py"", line 484, in <module>\r\n    main()\r\n  File ""run_glue.py"", line 243, in main\r\n    datasets = load_dataset(""csv"", data_files=data_files, delimiter=""\\t"")\r\n  File ""/projectnb2/media-framing/env-trans4/lib/python3.7/site-packages/datasets/load.py"", line 610, in load_dataset\r\n    ignore_verifications=ignore_verifications,\r\n  File ""/projectnb2/media-framing/env-trans4/lib/python3.7/site-packages/datasets/builder.py"", line 515, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File ""/projectnb2/media-framing/env-trans4/lib/python3.7/site-packages/datasets/builder.py"", line 594, in _download_and_prepare\r\n    raise OSError(""Cannot find data file. "" + (self.manual_download_instructions or """"))\r\nOSError: Cannot find data file. ```'
 'Hi ! It looks like the error stacktrace doesn\'t match with your code snippet.\r\n\r\nWhat error do you get when running this ?\r\n```\r\ndata_files = {""train"": ""train.tsv"", ""test"",: ""test.tsv""}\r\ndatasets = load_dataset(""csv"", data_files=data_files, delimiter=""\\t"")\r\n```\r\ncan you check that both tsv files are in the same folder as the current working directory of your shell ?'
 'Hi @lhoestq, Below is the entire error message after I move both tsv files to the same directory. It\'s the same with I got before.\r\n```\r\n/projectnb2/media-framing/env-trans4/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\r\n  return torch._C._cuda_getDeviceCount() > 0\r\n08/29/2021 22:56:43 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\r\n08/29/2021 22:56:43 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/projectnb/media-framing/pred_result/label1/, overwrite_output_dir=True, do_train=True, do_eval=False, do_predict=True, evaluation_strategy=EvaluationStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=8.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_steps=0, logging_dir=runs/Aug29_22-56-43_scc1, logging_first_step=False, logging_steps=500, save_steps=3000, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/projectnb/media-framing/pred_result/label1/, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, _n_gpu=0)\r\n08/29/2021 22:56:43 - INFO - __main__ -   load a local file for train: /project/media-framing/transformer4/temp_train.tsv\r\n08/29/2021 22:56:43 - INFO - __main__ -   load a local file for test: /project/media-framing/transformer4/temp_test.tsv\r\n08/29/2021 22:56:43 - WARNING - datasets.builder -   Using custom data configuration default-df627c23ac0e98ec\r\nDownloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /usr4/cs542sp/hey1/.cache/huggingface/datasets/csv/default-df627c23ac0e98ec/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff...\r\nTraceback (most recent call last):\r\n  File ""/projectnb2/media-framing/env-trans4/lib/python3.7/site-packages/datasets/builder.py"", line 693, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File ""/projectnb2/media-framing/env-trans4/lib/python3.7/site-packages/datasets/builder.py"", line 1166, in _prepare_split\r\n    num_examples, num_bytes = writer.finalize()\r\n  File ""/projectnb2/media-framing/env-trans4/lib/python3.7/site-packages/datasets/arrow_writer.py"", line 428, in finalize\r\n    self.stream.close()\r\n  File ""pyarrow/io.pxi"", line 132, in pyarrow.lib.NativeFile.close\r\n  File ""pyarrow/error.pxi"", line 99, in pyarrow.lib.check_status\r\nOSError: error closing file\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File ""run_glue.py"", line 487, in <module>\r\n    main()\r\n  File ""run_glue.py"", line 244, in main\r\n    datasets = load_dataset(""csv"", data_files=data_files, delimiter=""\\t"")\r\n  File ""/projectnb2/media-framing/env-trans4/lib/python3.7/site-packages/datasets/load.py"", line 852, in load_dataset\r\n    use_auth_token=use_auth_token,\r\n  File ""/projectnb2/media-framing/env-trans4/lib/python3.7/site-packages/datasets/builder.py"", line 616, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File ""/projectnb2/media-framing/env-trans4/lib/python3.7/site-packages/datasets/builder.py"", line 699, in _download_and_prepare\r\n    + str(e)\r\nOSError: Cannot find data file. \r\nOriginal error:\r\nerror closing file\r\n```'
 'Hi !\r\nCan you try running this into a python shell directly ?\r\n\r\n```python\r\nimport os\r\nfrom datasets import load_dataset\r\n\r\ndata_files = {""train"": ""train.tsv"", ""test"": ""test.tsv""}\r\nassert all(os.path.isfile(data_file) for data_file in data_files.values()), ""Couln\'t find files""\r\n\r\ndatasets = load_dataset(""csv"", data_files=data_files, delimiter=""\\t"")\r\nprint(""success !"")\r\n```\r\n\r\nThis way all the code from `run_glue.py` doesn\'t interfere with our tests :)'
 'Hi @lhoestq, \r\n\r\nBelow is what I got from terminal after I copied and run your code. I think the files themselves are good since there is no assertion error. \r\n\r\n```\r\nUsing custom data configuration default-df627c23ac0e98ec\r\nDownloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /usr4/cs542sp/hey1/.cache/huggingface/datasets/csv/default-df627c23ac0e98ec/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff...\r\nTraceback (most recent call last):\r\n  File ""/projectnb2/media-framing/env-trans4/lib/python3.7/site-packages/datasets/builder.py"", line 693, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File ""/projectnb2/media-framing/env-trans4/lib/python3.7/site-packages/datasets/builder.py"", line 1166, in _prepare_split\r\n    num_examples, num_bytes = writer.finalize()\r\n  File ""/projectnb2/media-framing/env-trans4/lib/python3.7/site-packages/datasets/arrow_writer.py"", line 428, in finalize\r\n    self.stream.close()\r\n  File ""pyarrow/io.pxi"", line 132, in pyarrow.lib.NativeFile.close\r\n  File ""pyarrow/error.pxi"", line 99, in pyarrow.lib.check_status\r\nOSError: error closing file\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File ""test.py"", line 7, in <module>\r\n    datasets = load_dataset(""csv"", data_files=data_files, delimiter=""\\t"")\r\n  File ""/projectnb2/media-framing/env-trans4/lib/python3.7/site-packages/datasets/load.py"", line 852, in load_dataset\r\n    use_auth_token=use_auth_token,\r\n  File ""/projectnb2/media-framing/env-trans4/lib/python3.7/site-packages/datasets/builder.py"", line 616, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File ""/projectnb2/media-framing/env-trans4/lib/python3.7/site-packages/datasets/builder.py"", line 699, in _download_and_prepare\r\n    + str(e)\r\nOSError: Cannot find data file. \r\nOriginal error:\r\nerror closing file\r\n```'
 'Hi, could this be a permission error ? I think it fails to close the arrow file that contains the data from your CSVs in the cache.\r\n\r\nBy default datasets are cached in `~/.cache/huggingface/datasets`, could you check that you have the right permissions ?\r\nYou can also try to change the cache directory by passing `cache_dir=""path/to/my/cache/dir""` to `load_dataset`.'
 ""Thank you!! @lhoestq\r\n\r\nFor some reason, I don't have the default path for datasets to cache, maybe because I work from a remote system.  The issue solved after I pass the `cache_dir` argument to the function. Thank you very much!!""]","Similar to #622, I've noticed there is a problem when trying to load a CSV file with datasets.

`
from datasets import load_dataset
`
`
dataset = load_dataset(""csv"", data_files=[""./sample_data.csv""], delimiter=""\t"", column_names=[""title"", ""text""], script_version=""master"")
`

Displayed error:
`
...
ArrowInvalid: CSV parse error: Expected 2 columns, got 1
`

I should mention that when I've tried to read data from `https://github.com/lhoestq/transformers/tree/custom-dataset-in-rag-retriever/examples/rag/test_data/my_knowledge_dataset.csv` it worked without a problem. I've read that there might be some problems with /r character, so I've removed them from the custom dataset, but the problem still remains.

I've added a colab reproducing the bug, but unfortunately I cannot provide the dataset.
https://colab.research.google.com/drive/1Qzu7sC-frZVeniiWOwzoCe_UHZsrlxu8?usp=sharing

Are there any work around for it ?
Thank you"
https://github.com/huggingface/datasets/issues/741,Creating dataset consumes too much memory,"[""Thanks for reporting.\r\nIn theory since the dataset script is just made to yield examples to write them into an arrow file, it's not supposed to create memory issues.\r\n\r\nCould you please try to run this exact same loop in a separate script to see if it's not an issue with `PIL` ?\r\nYou can just copy paste what's inside `_generate_examples` and remove all the code for `datasets` (remove yield).\r\n\r\nIf the RAM usage stays low after 600 examples it means that it comes from some sort of memory leak in the library, or with pyarrow.""
 'Here\'s an equivalent loading code:\r\n```python\r\nimages_path = ""PHOENIX-2014-T-release-v3/PHOENIX-2014-T/features/fullFrame-210x260px/train""\r\n\r\nfor dir_path in tqdm(os.listdir(images_path)):\r\n    frames_path = os.path.join(images_path, dir_path)\r\n    np_frames = []\r\n    for frame_name in os.listdir(frames_path):\r\n        frame_path = os.path.join(frames_path, frame_name)\r\n        im = Image.open(frame_path)\r\n        np_frames.append(np.asarray(im))\r\n        im.close()\r\n```\r\n\r\nThe process takes 0.3% of memory, even after 1000 examples on the small machine with 120GB RAM.\r\n\r\nI guess something in the datasets library doesn\'t release the reference to the objects I\'m yielding, but no idea how to test for this'
 ""I've had similar issues with Arrow once. I'll investigate...\r\n\r\nFor now maybe we can simply use the images paths in the dataset you want to add. I don't expect to fix this memory issue until 1-2 weeks unfortunately. Then we can just update the dataset with the images. What do you think ?""
 ""If it's just 1-2 weeks, I think it's best if we wait. I don't think it is very urgent to add it, and it will be much more useful with the images loaded rather than not (the images are low resolution, and thus papers using this dataset actually fit the entire video into memory anyway)\r\n\r\nI'll keep working on other datasets in the meanwhile :) ""
 ""Ok found the issue. This is because the batch size used by the writer is set to 10 000 elements by default so it would load your full dataset in memory (the writer has a buffer that flushes only after each batch). Moreover to write in Apache Arrow we have to use python objects so what's stored inside the ArrowWriter's buffer is actually python integers (32 bits).\r\n\r\nLowering the batch size to 10 should do the job.\r\n\r\nI will add a flag to the DatasetBuilder class of dataset scripts, so that we can customize the batch size.""
 ""Thanks, that's awesome you managed to find the problem.\r\n\r\nAbout the 32 bits - really? there isn't a way to serialize the numpy array somehow? 32 bits would take 4 times the memory / disk space needed to store these videos.\r\n\r\nPlease let me know when the batch size is customizable and I'll try again!""
 ""The 32 bit integrers are only used in the writer's buffer because Arrow doesn't take numpy arrays correctly as input. On disk it's stored as uint8 in arrow format ;)""
 ""> I don't expect to fix this memory issue until 1-2 weeks unfortunately.\r\n\r\nHi @lhoestq \r\nnot to rush of course, but I was wondering if you have a new timeline so I know how to plan my work around this :) ""
 'Hi ! Next week for sure :) '
 'Alright it should be good now.\r\nYou just have to specify `_writer_batch_size = 10` for example as a class attribute of the dataset builder class.'
 'I added it, but still it consumes as much memory\r\n\r\nhttps://github.com/huggingface/datasets/pull/722/files#diff-2e0d865dd4a60dedd1861d6f8c5ed281ded71508467908e1e0b1dbe7d2d420b1R66\r\n\r\nDid I not do it correctly?'
 'Yes you did it right.\r\nDid you rebase to include the changes of #828 ?\r\n\r\nEDIT: looks like you merged from master in the PR. Not sure why you still have an issue then, I will investigate'
 'Hi @lhoestq, any update on this?\r\nPerhaps even a direction I could try myself?'
 ""Sorry for the delay, I was busy with the dataset sprint and the incredible amount of contributions to the library ^^'\r\n\r\nWhat you can try to do to find what's wrong is check at which frequency the arrow writer writes all the examples from its in-memory buffer on disk. This happens [here](https://github.com/huggingface/datasets/blob/master/src/datasets/arrow_writer.py#L257-L258) in the code.\r\n\r\nThe idea is that `write_on_file` writes the examples every `writer_batch_size` examples and clear the buffer `self. current_rows`. As soon as `writer_batch_size` is small enough you shouldn't have memory issues in theory.\r\n\r\nLet me know if you have questions or if I can help.\r\n\r\nSince the dataset sprint is over and I will also be done with all the PRs soon I will be able to go back at it and take a look.""
 ""Thanks. I gave it a try and no success. I'm not sure what's happening there""
 'I had the same issue. It works for me by setting `DEFAULT_WRITER_BATCH_SIZE = 10` of my dataset builder class. (And not `_writer_batch_size` as previously mentioned).  I guess this is because `_writer_batch_size` is overwritten in `__init__` (see [here](https://github.com/huggingface/datasets/blob/0e2563e5d5c2fc193ea27d7c24607bb35607f2d5/src/datasets/builder.py#L934))'
 'Yes the class attribute you can change is `DEFAULT_WRITER_BATCH_SIZE`.\r\nOtherwise in `load_dataset` you can specify `writer_batch_size=`'
 'Ok thanks for the tips. Maybe the documentation should be updated accordingly https://huggingface.co/docs/datasets/add_dataset.html.'
 'Thanks for reporting this mistake in the docs.\r\nI just fixed it at https://github.com/huggingface/datasets/commit/85cf7ff920c90ca2e12bedca12b36d2a043c3da2']","Moving this issue from https://github.com/huggingface/datasets/pull/722 here, because it seems like a general issue.

Given the following dataset example, where each example saves a sequence of 260x210x3 images (max length 400):
```python
    def _generate_examples(self, base_path, split):
        """""" Yields examples. """"""

        filepath = os.path.join(base_path, ""annotations"", ""manual"", ""PHOENIX-2014-T."" + split + "".corpus.csv"")
        images_path = os.path.join(base_path, ""features"", ""fullFrame-210x260px"", split)

        with open(filepath, ""r"", encoding=""utf-8"") as f:
            data = csv.DictReader(f, delimiter=""|"", quoting=csv.QUOTE_NONE)
            for row in data:
                frames_path = os.path.join(images_path, row[""video""])[:-7]
                np_frames = []
                for frame_name in os.listdir(frames_path):
                    frame_path = os.path.join(frames_path, frame_name)
                    im = Image.open(frame_path)
                    np_frames.append(np.asarray(im))
                    im.close()

                yield row[""name""], {""video"": np_frames}
```

The dataset creation process goes out of memory on a machine with 500GB RAM.
I was under the impression that the ""generator"" here is exactly for that, to avoid memory constraints.


However, even if you want the entire dataset in memory, it would be in the worst case
`260x210x3 x 400 max length x 7000 samples` in bytes (uint8) = 458.64 gigabytes
So I'm not sure why it's taking more than 500GB.

And the dataset creation fails after 170 examples on a machine with 120gb RAM, and after 672 examples on a machine with 500GB RAM.


---

## Info that might help:
Iterating over examples is extremely slow.
![image](https://user-images.githubusercontent.com/5757359/96359590-3c666780-111d-11eb-9347-1f833ad982a9.png)
If I perform this iteration in my own, custom loop (Without saving to file), it runs at 8-9 examples/sec

And you can see at this state it is using 94% of the memory:
![image](https://user-images.githubusercontent.com/5757359/96359606-7afc2200-111d-11eb-8c11-0afbdba1a6a3.png)

And it is only using one CPU core, which is probably why it's so slow:
![image](https://user-images.githubusercontent.com/5757359/96359630-a3841c00-111d-11eb-9ba0-7fd3cdf51d26.png)
"
https://github.com/huggingface/datasets/issues/737,Trec Dataset Connection Error,"[""Thanks for reporting.\r\nThat's because the download url has changed. The old url now redirects to the new one but we don't support redirection for downloads.\r\n\r\nI'm opening a PR to update the url""]","**Datasets Version:**
1.1.2

**Python Version:**
3.6/3.7


**Code:**
```python
from datasets import load_dataset
load_dataset(""trec"")
```

**Expected behavior:**
Download Trec dataset and load Dataset object

**Current Behavior:**
Get a connection error saying it couldn't reach http://cogcomp.org/Data/QA/QC/train_5500.label (but the link doesn't seem broken)

<details>
  <summary>Error Logs</summary>
 

Using custom data configuration default
Downloading and preparing dataset trec/default (download: 350.79 KiB, generated: 403.39 KiB, post-processed: Unknown size, total: 754.18 KiB) to /root/.cache/huggingface/datasets/trec/default/1.1.0/ca4248481ad244f235f4cf277186cad2ee8769f975119a2bbfc41b8932b88bd7...
---------------------------------------------------------------------------
ConnectionError                           Traceback (most recent call last)
<ipython-input-8-66bf1242096e> in <module>()
----> 1 load_dataset(""trec"")

10 frames
/usr/local/lib/python3.6/dist-packages/datasets/utils/file_utils.py in get_from_cache(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag)
    473         elif response is not None and response.status_code == 404:
    474             raise FileNotFoundError(""Couldn't find file at {}"".format(url))
--> 475         raise ConnectionError(""Couldn't reach {}"".format(url))
    476 
    477     # Try a second time

ConnectionError: Couldn't reach http://cogcomp.org/Data/QA/QC/train_5500.label

</details>"
https://github.com/huggingface/datasets/issues/735,Throw error when an unexpected key is used in data_files,"[""Thanks for reporting !\r\nWe'll add support for other keys""]","I have found that only ""train"", ""validation"" and ""test"" are valid keys in the `data_files` argument. When you use any other ones, those attached files are silently ignored - leading to unexpected behaviour for the users.

So the following, unintuitively, returns only one key (namely `train`).

```python
datasets = load_dataset(""text"", data_files={""train"": train_f, ""valid"": valid_f})
print(datasets.keys())
# dict_keys(['train'])
```

whereas using `validation` instead, does return the expected result:

```python
datasets = load_dataset(""text"", data_files={""train"": train_f, ""validation"": valid_f})
print(datasets.keys())
# dict_keys(['train', 'validation'])
```

I would like to see more freedom in which keys one can use, but if that is not possible at least an error should be thrown when using an unexpected key."
https://github.com/huggingface/datasets/issues/730,Possible caching bug,"[""Thanks for reporting. That's a bug indeed.\r\nApparently only the `data_files` parameter is taken into account right now in `DatasetBuilder._create_builder_config` but it should also be the case for `config_kwargs` (or at least the instantiated `builder_config`)""]","The following code with `test1.txt` containing just ""🤗🤗🤗"":
```
dataset = datasets.load_dataset('text', data_files=['test1.txt'], split=""train"", encoding=""latin_1"")
print(dataset[0])
dataset = datasets.load_dataset('text', data_files=['test1.txt'], split=""train"", encoding=""utf-8"")
print(dataset[0])
``` 
produces this output:
```
Downloading and preparing dataset text/default-15600e4d83254059 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/arne/.cache/huggingface/datasets/text/default-15600e4d83254059/0.0.0/52cefbb2b82b015d4253f1aeb1e6ee5591124a6491e834acfe1751f765925155...
Dataset text downloaded and prepared to /home/arne/.cache/huggingface/datasets/text/default-15600e4d83254059/0.0.0/52cefbb2b82b015d4253f1aeb1e6ee5591124a6491e834acfe1751f765925155. Subsequent calls will reuse this data.
{'text': 'ð\x9f¤\x97ð\x9f¤\x97ð\x9f¤\x97'}
Using custom data configuration default
Reusing dataset text (/home/arne/.cache/huggingface/datasets/text/default-15600e4d83254059/0.0.0/52cefbb2b82b015d4253f1aeb1e6ee5591124a6491e834acfe1751f765925155)
{'text': 'ð\x9f¤\x97ð\x9f¤\x97ð\x9f¤\x97'}
```
Just changing the order (and deleting the temp files):
```
dataset = datasets.load_dataset('text', data_files=['test1.txt'], split=""train"", encoding=""utf-8"")
print(dataset[0])
dataset = datasets.load_dataset('text', data_files=['test1.txt'], split=""train"", encoding=""latin_1"")
print(dataset[0])
```
produces this:
```
Using custom data configuration default
Downloading and preparing dataset text/default-15600e4d83254059 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/arne/.cache/huggingface/datasets/text/default-15600e4d83254059/0.0.0/52cefbb2b82b015d4253f1aeb1e6ee5591124a6491e834acfe1751f765925155...
Dataset text downloaded and prepared to /home/arne/.cache/huggingface/datasets/text/default-15600e4d83254059/0.0.0/52cefbb2b82b015d4253f1aeb1e6ee5591124a6491e834acfe1751f765925155. Subsequent calls will reuse this data.
{'text': '🤗🤗🤗'}
Using custom data configuration default
Reusing dataset text (/home/arne/.cache/huggingface/datasets/text/default-15600e4d83254059/0.0.0/52cefbb2b82b015d4253f1aeb1e6ee5591124a6491e834acfe1751f765925155)
{'text': '🤗🤗🤗'}
```

Is it intended that the cache path does not depend on the config entries?

tested with datasets==1.1.2 and python==3.8.5"
https://github.com/huggingface/datasets/issues/726,"""Checksums didn't match for dataset source files"" error while loading openwebtext dataset","['Hi try, to provide more information please.\r\n\r\nExample code in a colab to reproduce the error, details on what you are trying to do and what you were expected and details on your environment (OS, PyPi packages version).'
 '> Hi try, to provide more information please.\r\n> \r\n> Example code in a colab to reproduce the error, details on what you are trying to do and what you were expected and details on your environment (OS, PyPi packages version).\r\n\r\nI have update the description, sorry for the incomplete issue by mistake.'
 ""Hi, I have manually downloaded the compressed dataset `openwebtext.tar.xz' and use the following command to preprocess the examples:\r\n```\r\n>>> dataset = load_dataset('/home/admin/workspace/datasets/datasets-master/datasets-master/datasets/openwebtext', data_dir='/home/admin/workspace/datasets')\r\nUsing custom data configuration default\r\nDownloading and preparing dataset openwebtext/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/admin/.cache/huggingface/datasets/openwebtext/default/0.0.0/5c636399c7155da97c982d0d70ecdce30fbca66a4eb4fc768ad91f8331edac02...\r\nDataset openwebtext downloaded and prepared to /home/admin/.cache/huggingface/datasets/openwebtext/default/0.0.0/5c636399c7155da97c982d0d70ecdce30fbca66a4eb4fc768ad91f8331edac02. Subsequent calls will reuse this data.\r\n>>> len(dataset['train'])\r\n74571\r\n>>>\r\n```\r\nThe size of the pre-processed example file is only 354MB, however the processed bookcorpus dataset is 4.6g. Are there any problems?""
 ""NonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n\r\ni got this issue when i try to work on my own datasets kindly tell me, from where i can get checksums of train and dev file in my github repo""
 ""Hi,  I got the similar issue for xnli dataset while working on colab with python3.7.  \r\n\r\n`nlp.load_dataset(path = 'xnli')`\r\n\r\nThe above command resulted in following issue : \r\n```\r\nNonMatchingChecksumError: Checksums didn't match for dataset source files:\r\n['https://www.nyu.edu/projects/bowman/xnli/XNLI-1.0.zip']\r\n```\r\n\r\nAny idea how to fix this  ?""]","Hi,
I have encountered this problem during loading the openwebtext dataset:
```
>>> dataset = load_dataset('openwebtext')
Downloading and preparing dataset openwebtext/plain_text (download: 12.00 GiB, generated: 37.04 GiB, post-processed: Unknown size, total: 49.03 GiB) to /home/admin/.cache/huggingface/datasets/openwebtext/plain_text/1.0.0/5c636399c7155da97c982d0d70ecdce30fbca66a4eb4fc768ad91f8331edac02...
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/admin/workspace/anaconda3/envs/torch1.6-py3.7/lib/python3.7/site-packages/datasets/load.py"", line 611, in load_dataset
    ignore_verifications=ignore_verifications,
  File ""/home/admin/workspace/anaconda3/envs/torch1.6-py3.7/lib/python3.7/site-packages/datasets/builder.py"", line 476, in download_and_prepare
    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
  File ""/home/admin/workspace/anaconda3/envs/torch1.6-py3.7/lib/python3.7/site-packages/datasets/builder.py"", line 536, in _download_and_prepare
    self.info.download_checksums, dl_manager.get_recorded_sizes_checksums(), ""dataset source files""
  File ""/home/admin/workspace/anaconda3/envs/torch1.6-py3.7/lib/python3.7/site-packages/datasets/utils/info_utils.py"", line 39, in verify_checksums
    raise NonMatchingChecksumError(error_msg + str(bad_urls))
datasets.utils.info_utils.NonMatchingChecksumError: Checksums didn't match for dataset source files:
['https://zenodo.org/record/3834942/files/openwebtext.tar.xz']
```
I think this problem is caused because the released dataset has changed. Or I should download the dataset manually?

Sorry for release the unfinised issue by mistake."
https://github.com/huggingface/datasets/issues/724,need to redirect /nlp to /datasets and remove outdated info,"['Should be fixed now: \r\n\r\n![image](https://user-images.githubusercontent.com/35882/95917301-040b0600-0d78-11eb-9655-c4ac0e788089.png)\r\n\r\nNot sure I understand what you mean by the second part?\r\n'
 ""Thank you!\r\n\r\n> Not sure I understand what you mean by the second part?\r\n\r\nCompare the 2:\r\n* https://huggingface.co/datasets/wikihow\r\n* https://huggingface.co/nlp/viewer/?dataset=wikihow&config=all\r\nCan you see the difference? 2nd has formatting, 1st doesn't.\r\n""
 ""For context, those are two different pages (not an old vs new one), one is from the dataset viewer (you can browse data inside the datasets) while the other is just a basic reference page displayed some metadata about the dataset.\r\n\r\nFor the second one, we'll move to markdown parsing soon, so it'll be formatted better.""
 'I understand. I was just flagging the lack of markup issue.']","It looks like the website still has all the `nlp` data, e.g.: https://huggingface.co/nlp/viewer/?dataset=wikihow&config=all

should probably redirect to: https://huggingface.co/datasets/wikihow

also for some reason the new information is slightly borked. If you look at the old one it was nicely formatted and had the links marked up, the new one is just a jumble of text in one chunk and no markup for links (i.e. not clickable)."
https://github.com/huggingface/datasets/issues/723,Adding pseudo-labels to datasets,"[""Nice ! :)\r\nIt's indeed the first time we have such contributions so we'll have to figure out the appropriate way to integrate them.\r\nCould you add details on what they could be used for ?\r\n""
 'They can be used as training data for a smaller model.'
 'Sounds just like a regular dataset to me then, no?'
 ""A new configuration for those datasets should do the job then.\r\nNote that until now datasets like xsum only had one configuration. It means that users didn't have to specify the configuration name when loading the dataset. If we add new configs, users that update the lib will have to update their code to specify the default/standard configuration name (not the one with pseudo labels).""
 'Could also be a `user-namespace` dataset maybe?'
 ""Oh yes why not. I'm more in favor of this actually since pseudo labels are things that users (not dataset authors in general) can compute by themselves and share with the community""
 '![image](https://user-images.githubusercontent.com/6045025/96045248-b528a380-0e3f-11eb-9124-bd55afa031bb.png)\r\n\r\nI assume I should (for example) rename the xsum dir, change the URL, and put the modified dir somewhere in S3?'
 'You can use the `datasets-cli` to upload the folder with your version of xsum with the pseudo labels.\r\n\r\n```\r\ndatasets-cli upload_dataset path/to/xsum\r\n```']","I recently [uploaded pseudo-labels](https://github.com/huggingface/transformers/blob/master/examples/seq2seq/precomputed_pseudo_labels.md) for CNN/DM, XSUM and WMT16-en-ro to s3, and thom mentioned I should add them to this repo.
Since pseudo-labels are just a large model's generations on an existing dataset, what is the right way to structure this contribution.
I read https://huggingface.co/docs/datasets/add_dataset.html, but it doesn't really cover this type of contribution.

I could, for example, make a new directory, `xsum_bart_pseudolabels` for each set of pseudolabels or add some sort of parametrization to `xsum.py`: https://github.com/huggingface/datasets/blob/5f4c6e830f603830117877b8990a0e65a2386aa6/datasets/xsum/xsum.py

What do you think @lhoestq ?


"
https://github.com/huggingface/datasets/issues/721,feat(dl_manager): add support for ftp downloads,"[""We only support http by default for downloading.\r\nIf you really need to use ftp, then feel free to use a library that allows to download through ftp in your dataset script (I see that you've started working on #722 , that's awesome !). The users will get a message to install the extra library when they load the dataset.\r\n\r\nTo make the download_manager work with a custom downloader, you can call `download_manager.download_custom` instead of `download_manager.download_and_extract`. The expected arguments are the following:\r\n```\r\nurl_or_urls: url or `list`/`dict` of urls to download and extract. Each\r\n        url is a `str`.\r\ncustom_download: Callable with signature (src_url: str, dst_path: str) -> Any\r\n        as for example `tf.io.gfile.copy`, that lets you download from google storage\r\n```\r\n""
 'Also maybe it coud be interesting to have a direct support of ftp inside the `datasets` library. Do you know any good libraries that we might consider adding as a (optional ?) dependency ?'
 ""Downloading an `ftp` file is as simple as:\r\n```python\r\nimport urllib \r\nurllib.urlretrieve('ftp://server/path/to/file', 'file')\r\n```\r\n\r\nI believe this should be supported by the library, as its not using any dependency and is trivial amount of code.""
 'I know its unorthodox, but I added `ftp` download support to `file_utils` in the same PR https://github.com/huggingface/datasets/pull/722\r\nSo its possible to understand the interaction of the download component with the ftp download ability'
 ""Awesome ! I'll take a look :)""
 '@AmitMY Can you now download the Phoenix2014 Dataset?'
 ""@hoanganhpham1006 yes.\r\nSee pull request https://github.com/huggingface/datasets/pull/722 , it has a loader for this dataset, mostly ready.\r\nThere's one issue that delays it being merged - https://github.com/huggingface/datasets/issues/741 - regarding memory consumption.""
 'The problem which I have now is that this dataset seems does not allow to download? Can you share it with me pls'
 'The dataset loader is not yet ready, because of that issue.\r\nIf you want to just download the dataset the old-fashioned way, just go to: https://www-i6.informatik.rwth-aachen.de/ftp/pub/rwth-phoenix/2016/phoenix-2014-T.v3.tar.gz (the ftp link is now broken, and its available over https)'
 'Got it, thank you so much!']","I am working on a new dataset (#302) and encounter a problem downloading it.

```python
# This is the official download link from https://www-i6.informatik.rwth-aachen.de/~koller/RWTH-PHOENIX-2014-T/
_URL = ""ftp://wasserstoff.informatik.rwth-aachen.de/pub/rwth-phoenix/2016/phoenix-2014-T.v3.tar.gz""

dl_manager.download_and_extract(_URL)
```

I get an error:

> ValueError: unable to parse ftp://wasserstoff.informatik.rwth-aachen.de/pub/rwth-phoenix/2016/phoenix-2014-T.v3.tar.gz as a URL or as a local path

I checked, and indeed you don't consider `ftp` as a remote file.
https://github.com/huggingface/datasets/blob/4c2af707a6955cf4b45f83ac67990395327c5725/src/datasets/utils/file_utils.py#L188

Adding `ftp` to that list does not immediately solve the issue, so there probably needs to be some extra work.



"
https://github.com/huggingface/datasets/issues/720,OSError: Cannot find data file when not using the dummy dataset in RAG,"['Same issue here. I will be digging further, but it looks like the [script](https://github.com/huggingface/datasets/blob/master/datasets/wiki_dpr/wiki_dpr.py#L132) is attempting to open a file that is not downloaded yet. \r\n\r\n```\r\n99dcbca09109e58502e6b9271d4d3f3791b43f61f3161a76b25d2775ab1a4498.lock\r\n```\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nUnpicklingError                           Traceback (most recent call last)\r\n~/anaconda3/envs/eqa/lib/python3.7/site-packages/numpy/lib/npyio.py in load(file, mmap_mode, allow_pickle, fix_imports, encoding)\r\n    446             try:\r\n--> 447                 return pickle.load(fid, **pickle_kwargs)\r\n    448             except Exception:\r\n\r\nUnpicklingError: pickle data was truncated\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nOSError                                   Traceback (most recent call last)\r\n~/src/datasets/src/datasets/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    559 \r\n--> 560         if verify_infos:\r\n    561             verify_splits(self.info.splits, split_dict)\r\n\r\n~/src/datasets/src/datasets/builder.py in _prepare_split(self, split_generator)\r\n    847                 writer.write(example)\r\n--> 848         finally:\r\n    849             num_examples, num_bytes = writer.finalize()\r\n\r\n~/anaconda3/envs/eqa/lib/python3.7/site-packages/tqdm/notebook.py in __iter__(self, *args, **kwargs)\r\n    227         try:\r\n--> 228             for obj in super(tqdm_notebook, self).__iter__(*args, **kwargs):\r\n    229                 # return super(tqdm...) will not catch exception\r\n\r\n~/anaconda3/envs/eqa/lib/python3.7/site-packages/tqdm/std.py in __iter__(self)\r\n   1132         try:\r\n-> 1133             for obj in iterable:\r\n   1134                 yield obj\r\n\r\n/hdd/rag/cache/huggingface/modules/datasets_modules/datasets/wiki_dpr/14b973bf2a456087ff69c0fd34526684eed22e48e0dfce4338f9a22b965ce7c2/wiki_dpr.py in _generate_examples(self, data_file, vectors_files)\r\n    131                         break\r\n--> 132                     vecs = np.load(open(vectors_files.pop(0), ""rb""), allow_pickle=True)\r\n    133                     vec_idx = 0\r\n\r\n~/anaconda3/envs/eqa/lib/python3.7/site-packages/numpy/lib/npyio.py in load(file, mmap_mode, allow_pickle, fix_imports, encoding)\r\n    449                 raise IOError(\r\n--> 450                     ""Failed to interpret file %s as a pickle"" % repr(file))\r\n    451 \r\n\r\nOSError: Failed to interpret file <_io.BufferedReader name=\'/hdd/rag/downloads/99dcbca09109e58502e6b9271d4d3f3791b43f61f3161a76b25d2775ab1a4498\'> as a pickle\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nOSError                                   Traceback (most recent call last)\r\n<ipython-input-8-24351ff8ce44> in <module>\r\n      4 retriever = RagRetriever.from_pretrained(""facebook/rag-sequence-nq"", \r\n      5                                          index_name=""exact"",\r\n----> 6                                          use_dummy_dataset=False)\r\n\r\n~/src/transformers/src/transformers/retrieval_rag.py in from_pretrained(cls, retriever_name_or_path, **kwargs)\r\n    321         generator_tokenizer = rag_tokenizer.generator\r\n    322         return cls(\r\n--> 323             config, question_encoder_tokenizer=question_encoder_tokenizer, generator_tokenizer=generator_tokenizer\r\n    324         )\r\n    325 \r\n\r\n~/src/transformers/src/transformers/retrieval_rag.py in __init__(self, config, question_encoder_tokenizer, generator_tokenizer)\r\n    310         self.config = config\r\n    311         if self._init_retrieval:\r\n--> 312             self.init_retrieval()\r\n    313 \r\n    314     @classmethod\r\n\r\n~/src/transformers/src/transformers/retrieval_rag.py in init_retrieval(self)\r\n    338 \r\n    339         logger.info(""initializing retrieval"")\r\n--> 340         self.index.init_index()\r\n    341 \r\n    342     def postprocess_docs(self, docs, input_strings, prefix, n_docs, return_tensors=None):\r\n\r\n~/src/transformers/src/transformers/retrieval_rag.py in init_index(self)\r\n    248                 split=self.dataset_split,\r\n    249                 index_name=self.index_name,\r\n--> 250                 dummy=self.use_dummy_dataset,\r\n    251             )\r\n    252             self.dataset.set_format(""numpy"", columns=[""embeddings""], output_all_columns=True)\r\n\r\n~/src/datasets/src/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, save_infos, script_version, **config_kwargs)\r\n    615     builder_instance.download_and_prepare(\r\n    616         download_config=download_config,\r\n--> 617         download_mode=download_mode,\r\n    618         ignore_verifications=ignore_verifications,\r\n    619     )\r\n\r\n~/src/datasets/src/datasets/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, **download_and_prepare_kwargs)\r\n    481                     # Sync info\r\n    482                     self.info.dataset_size = sum(split.num_bytes for split in self.info.splits.values())\r\n--> 483                     self.info.download_checksums = dl_manager.get_recorded_sizes_checksums()\r\n    484                     self.info.size_in_bytes = self.info.dataset_size + self.info.download_size\r\n    485                     # Save info\r\n\r\n~/src/datasets/src/datasets/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    560         if verify_infos:\r\n    561             verify_splits(self.info.splits, split_dict)\r\n--> 562 \r\n    563         # Update the info object with the splits.\r\n    564         self.info.splits = split_dict\r\n\r\nOSError: Cannot find data file.\r\n```\r\n\r\nThank you.'
 'An update on my end. This seems like a transient issue. Reran the script from scratch overnight with no errors. '
 'Closing this one. Feel free to re-open if you have other questions about this issue']","## Environment info

    transformers version: 3.3.1
    Platform: Linux-4.19
    Python version: 3.7.7
    PyTorch version (GPU?): 1.6.0
    Tensorflow version (GPU?): No
    Using GPU in script?: Yes
    Using distributed or parallel set-up in script?: No

## To reproduce

Steps to reproduce the behaviour:
```
import os
os.environ['HF_DATASETS_CACHE'] = '/workspace/notebooks/POCs/cache'

from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration

tokenizer = RagTokenizer.from_pretrained(""facebook/rag-token-nq"")
retriever = RagRetriever.from_pretrained(""facebook/rag-token-nq"", index_name=""exact"", use_dummy_dataset=False) 
```

Plese note that I'm using the whole dataset: **use_dummy_dataset=False**
After around 4 hours (downloading and some other things) this is returned:

```
Downloading and preparing dataset wiki_dpr/psgs_w100.nq.exact (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /workspace/notebooks/POCs/cache/wiki_dpr/psgs_w100.nq.exact/0.0.0/14b973bf2a456087ff69c0fd34526684eed22e48e0dfce4338f9a22b965ce7c2...

---------------------------------------------------------------------------
UnpicklingError                           Traceback (most recent call last)
/opt/conda/lib/python3.7/site-packages/numpy/lib/npyio.py in load(file, mmap_mode, allow_pickle, fix_imports, encoding)
    459             try:
--> 460                 return pickle.load(fid, **pickle_kwargs)
    461             except Exception:

UnpicklingError: pickle data was truncated

During handling of the above exception, another exception occurred:

OSError                                   Traceback (most recent call last)
/opt/conda/lib/python3.7/site-packages/datasets/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)
    552                 # Prepare split will record examples associated to the split
--> 553                 self._prepare_split(split_generator, **prepare_split_kwargs)
    554             except OSError:

/opt/conda/lib/python3.7/site-packages/datasets/builder.py in _prepare_split(self, split_generator)
    840             for key, record in utils.tqdm(
--> 841                 generator, unit="" examples"", total=split_info.num_examples, leave=False, disable=not_verbose
    842             ):

/opt/conda/lib/python3.7/site-packages/tqdm/notebook.py in __iter__(self, *args, **kwargs)
    217         try:
--> 218             for obj in super(tqdm_notebook, self).__iter__(*args, **kwargs):
    219                 # return super(tqdm...) will not catch exception

/opt/conda/lib/python3.7/site-packages/tqdm/std.py in __iter__(self)
   1128         try:
-> 1129             for obj in iterable:
   1130                 yield obj

~/.cache/huggingface/modules/datasets_modules/datasets/wiki_dpr/14b973bf2a456087ff69c0fd34526684eed22e48e0dfce4338f9a22b965ce7c2/wiki_dpr.py in _generate_examples(self, data_file, vectors_files)
    131                         break
--> 132                     vecs = np.load(open(vectors_files.pop(0), ""rb""), allow_pickle=True)
    133                     vec_idx = 0

/opt/conda/lib/python3.7/site-packages/numpy/lib/npyio.py in load(file, mmap_mode, allow_pickle, fix_imports, encoding)
    462                 raise IOError(
--> 463                     ""Failed to interpret file %s as a pickle"" % repr(file))
    464     finally:

OSError: Failed to interpret file <_io.BufferedReader name='/workspace/notebooks/POCs/cache/downloads/f34d5f091294259b4ca90e813631e69a6ded660d71b6cbedf89ddba50df94448'> as a pickle

During handling of the above exception, another exception occurred:

OSError                                   Traceback (most recent call last)
<ipython-input-10-f28df370ac47> in <module>
      1 # ln -s /workspace/notebooks/POCs/cache /root/.cache/huggingface/datasets
----> 2 retriever = RagRetriever.from_pretrained(""facebook/rag-token-nq"", index_name=""exact"", use_dummy_dataset=False)

/opt/conda/lib/python3.7/site-packages/transformers/retrieval_rag.py in from_pretrained(cls, retriever_name_or_path, **kwargs)
    307         generator_tokenizer = rag_tokenizer.generator
    308         return cls(
--> 309             config, question_encoder_tokenizer=question_encoder_tokenizer, generator_tokenizer=generator_tokenizer
    310         )
    311 

/opt/conda/lib/python3.7/site-packages/transformers/retrieval_rag.py in __init__(self, config, question_encoder_tokenizer, generator_tokenizer)
    298         self.config = config
    299         if self._init_retrieval:
--> 300             self.init_retrieval()
    301 
    302     @classmethod

/opt/conda/lib/python3.7/site-packages/transformers/retrieval_rag.py in init_retrieval(self)
    324 
    325         logger.info(""initializing retrieval"")
--> 326         self.index.init_index()
    327 
    328     def postprocess_docs(self, docs, input_strings, prefix, n_docs, return_tensors=None):

/opt/conda/lib/python3.7/site-packages/transformers/retrieval_rag.py in init_index(self)
    238                 split=self.dataset_split,
    239                 index_name=self.index_name,
--> 240                 dummy=self.use_dummy_dataset,
    241             )
    242             self.dataset.set_format(""numpy"", columns=[""embeddings""], output_all_columns=True)

/opt/conda/lib/python3.7/site-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, save_infos, script_version, **config_kwargs)
    609         download_config=download_config,
    610         download_mode=download_mode,
--> 611         ignore_verifications=ignore_verifications,
    612     )
    613 

/opt/conda/lib/python3.7/site-packages/datasets/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, **download_and_prepare_kwargs)
    474                     if not downloaded_from_gcs:
    475                         self._download_and_prepare(
--> 476                             dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
    477                         )
    478                     # Sync info

/opt/conda/lib/python3.7/site-packages/datasets/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)
    553                 self._prepare_split(split_generator, **prepare_split_kwargs)
    554             except OSError:
--> 555                 raise OSError(""Cannot find data file. "" + (self.manual_download_instructions or """"))
    556 
    557         if verify_infos:

OSError: Cannot find data file. 

```

Thanks 
"
https://github.com/huggingface/datasets/issues/712,Error in the notebooks/Overview.ipynb notebook,"[""Do this:\r\n``` python\r\nsquad_dataset = list_datasets(with_details=True)[datasets.index('squad')]\r\npprint(squad_dataset.__dict__)  # It's a simple python dataclass\r\n```""
 'Thanks! This worked. I have created a PR to fix this in the notebook. ']","Hi,

I got the following error in **cell number 3** while exploring the **Overview.ipynb** notebook in google colab. I used the [link ](https://colab.research.google.com/github/huggingface/datasets/blob/master/notebooks/Overview.ipynb) provided in the main README file to open it in colab. 

```python
# You can access various attributes of the datasets before downloading them
squad_dataset = list_datasets()[datasets.index('squad')]

pprint(squad_dataset.__dict__)  # It's a simple python dataclass
```

Error message
```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-5-8dc805c4949c> in <module>()
      2 squad_dataset = list_datasets()[datasets.index('squad')]
      3 
 ----> 4 pprint(squad_dataset.__dict__)  # It's a simple python dataclass
    
AttributeError: 'str' object has no attribute '__dict__'
```

The object `squad_dataset` is a `str` not a `dataclass` ."
https://github.com/huggingface/datasets/issues/709,"How to use similarity settings  other then ""BM25"" in Elasticsearch index ?","['Datasets does not use elasticsearch API to define custom similarity. If you want to use a custom similarity, the best would be to run a curl request directly to your elasticsearch instance (see sample hereafter, directly from ES documentation), then you should be able to use `my_similarity` in your configuration passed to datasets\r\n\r\n```\r\ncurl -X PUT ""localhost:9200/index?pretty"" -H \'Content-Type: application/json\' -d\'\r\n{\r\n  ""settings"": {\r\n    ""index"": {\r\n      ""similarity"": {\r\n        ""my_similarity"": {\r\n          ""type"": ""DFR"",\r\n          ""basic_model"": ""g"",\r\n          ""after_effect"": ""l"",\r\n          ""normalization"": ""h2"",\r\n          ""normalization.h2.c"": ""3.0""\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\'\r\n\r\n```']","**QUESTION : How should we use other similarity algorithms supported by Elasticsearch other than ""BM25""  ?**
**ES Reference**
https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-similarity.html
**HF doc reference:**
https://huggingface.co/docs/datasets/faiss_and_ea.html

**context :**
========

I used the latest Elasticsearch server  version 7.9.2
When I set DFR  which is one of the other similarity algorithms supported by elasticsearch  in the mapping, I get an error

For example DFR that I had tried in the first instance in mappings as below.,
`""mappings"": {""properties"": {""text"": {""type"": ""text"", ""analyzer"": ""standard"", ""similarity"": ""DFR""}}},`

I get the following error 
RequestError: RequestError(400, 'mapper_parsing_exception', 'Unknown Similarity type [DFR] for field [text]')

The other thing as another option I had tried was to declare ""similarity"": ""my_similarity"" within settings and then assigning ""my_similarity"" inside the mappings as below 

`es_config = {
        ""settings"": {
            ""number_of_shards"": 1,
             **""similarity"":  ""my_similarity""**: {
          ""type"": ""DFR"",
          ""basic_model"": ""g"",
          ""after_effect"": ""l"",
          ""normalization"": ""h2"",
          ""normalization.h2.c"": ""3.0""
        } ,
            ""analysis"": {""analyzer"": {""stop_standard"": {""type"": ""standard"", "" stopwords"": ""_english_""}}},
            
        },
        ""mappings"": {""properties"": {""text"": {""type"": ""text"", ""analyzer"": ""standard"", ""similarity"": ""my_similarity""}}},
    }`

For this , I got the following error
RequestError: RequestError(400, 'illegal_argument_exception', 'unknown setting [index.similarity] please check that any required plugins are installed, or check the breaking changes documentation for removed settings')

"
https://github.com/huggingface/datasets/issues/708,Datasets performance slow? - 6.4x slower than in memory dataset,"['Facing a similar issue here. My model using SQuAD dataset takes about 1h to process with in memory data and more than 2h with datasets directly.'
 'And if you use in-memory-data with datasets with `load_dataset(..., keep_in_memory=True)`?'
 ""Thanks for the tip @thomwolf ! I did not see that flag in the docs. I'll try with that.""
 'We should add it indeed and also maybe a specific section with all the tips for maximal speed. What do you think @lhoestq @SBrandeis @yjernite ?'
 ""By default the datasets loaded with `load_dataset` live on disk.\r\nIt's possible to load them in memory by using some transforms like `.map(..., keep_in_memory=True)`.\r\n\r\nSmall correction to @thomwolf 's comment above: currently we don't have the `keep_in_memory` parameter for `load_dataset` AFAIK but it would be nice to add it indeed :)""
 'Yes indeed we should add it!'
 ""Great! Thanks a lot.\r\n\r\nI did a test using `map(..., keep_in_memory=True)` and also a test using in-memory only data.\r\n\r\n```python\r\nfeatures = dataset.map(tokenize, batched=True, remove_columns=dataset['train'].column_names)\r\nfeatures.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask'])\r\n\r\nfeatures_in_memory = dataset.map(tokenize, batched=True, keep_in_memory=True, remove_columns=dataset['train'].column_names)\r\nfeatures_in_memory.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask'])\r\n\r\nin_memory = [features['train'][i] for i in range(len(features['train']))]\r\n```\r\n\r\nFor using the features without any tweak, I got **1min17s** for copying the entire DataLoader to CUDA:\r\n\r\n```\r\n%%time\r\n\r\nfor i, batch in enumerate(DataLoader(features['train'], batch_size=16, num_workers=4)):\r\n    batch['input_ids'].to(device)\r\n```\r\n\r\nFor using the features mapped with `keep_in_memory=True`, I also got **1min17s** for copying the entire DataLoader to CUDA:\r\n\r\n```\r\n%%time\r\n\r\nfor i, batch in enumerate(DataLoader(features_in_memory['train'], batch_size=16, num_workers=4)):\r\n    batch['input_ids'].to(device)\r\n```\r\n\r\nAnd for the case using every element in memory, converted from the original dataset, I got **12.5s**:\r\n\r\n```\r\n%%time\r\n\r\nfor i, batch in enumerate(DataLoader(in_memory, batch_size=16, num_workers=4)):\r\n    batch['input_ids'].to(device)\r\n```\r\n\r\nTaking a closer look in my SQuAD code, using a profiler, I see a lot of calls to `posix read` api. It seems that it is really reliying on disk, which results in a very high train time.""
 'I am having the same issue here. When loading from memory I can get the GPU up to 70% util but when loading after mapping I can only get 40%.\r\n\r\nIn disk:\r\n```\r\nbook_corpus = load_dataset(\'bookcorpus\', \'plain_text\', cache_dir=\'/home/ad/Desktop/bookcorpus\', split=\'train[:20%]\')\r\nbook_corpus = book_corpus.map(encode, batched=True, num_proc=20, load_from_cache_file=True, batch_size=2500)\r\nbook_corpus.set_format(type=\'torch\', columns=[\'text\', ""input_ids"", ""attention_mask"", ""token_type_ids""])\r\n\r\ntraining_args = TrainingArguments(\r\n    output_dir=""./mobile_bert_big"",\r\n    overwrite_output_dir=True,\r\n    num_train_epochs=1,\r\n    per_device_train_batch_size=32,\r\n    per_device_eval_batch_size=16,\r\n    save_steps=50,\r\n    save_total_limit=2,\r\n    logging_first_step=True,\r\n    warmup_steps=100,\r\n    logging_steps=50,\r\n    eval_steps=100,\r\n    no_cuda=False,\r\n    gradient_accumulation_steps=16,\r\n    fp16=True)\r\n\r\ntrainer = Trainer(\r\n    model=model,\r\n    args=training_args,\r\n    data_collator=data_collator,\r\n    train_dataset=book_corpus,\r\n    tokenizer=tokenizer)\r\n```\r\n\r\nIn disk I can only get 0,17 it/s:\r\n`[ 13/28907 01:03 < 46:03:27, 0.17 it/s, Epoch 0.00/1] `\r\n\r\nIf I load it with torch.utils.data.Dataset()\r\n```\r\nclass BCorpusDataset(torch.utils.data.Dataset):\r\n    def __init__(self, encodings):\r\n        self.encodings = encodings\r\n\r\n    def __getitem__(self, idx):\r\n        item = [torch.tensor(val[idx]) for key, val in self.encodings.items()][0]\r\n        return item\r\n\r\n    def __len__(self):\r\n        length = [len(val) for key, val in self.encodings.items()][0]\r\n        return length\r\n\r\n**book_corpus = book_corpus.select([i for i in range(16*2000)])** # filtering to not have 20% of BC in memory...\r\nbook_corpus = book_corpus(book_corpus)\r\n```\r\nI can get:\r\n` [ 5/62 00:09 < 03:03, 0.31 it/s, Epoch 0.06/1]`\r\n\r\nBut obviously I can not get BookCorpus in memory xD\r\n\r\nEDIT: it is something weird. If i load in disk 1% of bookcorpus:\r\n```\r\nbook_corpus = load_dataset(\'bookcorpus\', \'plain_text\', cache_dir=\'/home/ad/Desktop/bookcorpus\', split=\'train[:1%]\')\r\n```\r\n\r\nI can get 0.28 it/s, (the same that in memory) but if I load 20% of bookcorpus:\r\n```\r\nbook_corpus = load_dataset(\'bookcorpus\', \'plain_text\', cache_dir=\'/home/ad/Desktop/bookcorpus\', split=\'train[:20%]\')\r\n```\r\nI get again 0.17 it/s. \r\n\r\nI am missing something? I think it is something related to size, and not disk or in-memory.'
 'There is a way to increase the batches read from memory? or multiprocessed it? I think that one of two or it is reading with just 1 core o it is reading very small chunks from disk and left my GPU at 0 between batches'
 'My fault! I had not seen the `dataloader_num_workers` in `TrainingArguments` ! Now I can parallelize and go fast! Sorry, and thanks.']","I've been very excited about this amazing datasets project. However, I've noticed that the performance can be substantially slower than using an in-memory dataset.

Now, this is expected I guess, due to memory mapping data using arrow files, and you don't get anything for free. But I was surprised at how much slower.

For example, in the `yelp_polarity` dataset (560000 datapoints, or 17500 batches of 32), it was taking me 3:31 to just get process the data and get it on the GPU (no model involved). Whereas, the equivalent in-memory dataset would finish in just 0:33.

Is this expected? Given that one of the goals of this project is also accelerate dataset processing, this seems a bit slower than I would expect. I understand the advantages of being able to work on datasets that exceed memory, and that's very exciting to me, but thought I'd open this issue to discuss.

For reference I'm running a AMD Ryzen Threadripper 1900X 8-Core Processor CPU, with 128 GB of RAM and an NVME SSD Samsung 960 EVO. I'm running with an RTX Titan 24GB GPU.

I can see with `iotop` that the dataset gets quickly loaded into the system read buffers, and thus doesn't incur any additional IO reads. Thus in theory, all the data *should* be in RAM, but in my benchmark code below it's still 6.4 times slower.

What am I doing wrong? And is there a way to force the datasets to completely load into memory instead of being memory mapped in cases where you want maximum performance?

At 3:31 for 17500 batches, that's 12ms per batch. Does this 12ms just become insignificant as a proportion of forward and backward passes in practice, and thus it's not worth worrying about this in practice?

In any case, here's my code `benchmark.py`. If you run it with an argument of `memory` it will copy the data into memory before executing the same test.

``` py
import sys
from datasets import load_dataset
from transformers import DataCollatorWithPadding, BertTokenizerFast
from torch.utils.data import DataLoader
from tqdm import tqdm

if __name__ == '__main__':
    tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')
    collate_fn = DataCollatorWithPadding(tokenizer, padding=True)

    ds = load_dataset('yelp_polarity')

    def do_tokenize(x):
        return tokenizer(x['text'], truncation=True)

    ds = ds.map(do_tokenize, batched=True)
    ds.set_format('torch', ['input_ids', 'token_type_ids', 'attention_mask'])

    if len(sys.argv) == 2 and sys.argv[1] == 'memory':
        # copy to memory - probably a faster way to do this - but demonstrates the point
        # approximately 530 batches per second - 17500 batches in 0:33
        print('using memory')
        _ds = [data for data in tqdm(ds['train'])]
    else:
        # approximately 83 batches per second - 17500 batches in 3:31
        print('using datasets')
        _ds = ds['train']

    dl = DataLoader(_ds, shuffle=True, collate_fn=collate_fn, batch_size=32, num_workers=4)

    for data in tqdm(dl):
        for k, v in data.items():
            data[k] = v.to('cuda')
```

For reference, my conda environment is [here](https://gist.github.com/05b6101518ff70ed42a858b302a0405d)

Once again, I'm very excited about this library, and how easy it is to load datasets, and to do so without worrying about system memory constraints.

Thanks for all your great work.
"
https://github.com/huggingface/datasets/issues/707,Requirements should specify pyarrow<1,"['Hello @mathcass I would want to work on this issue. May I do the same? '
 '@punitaojha, certainly. Feel free to work on this. Let me know if you need any help or clarity.'
 'Hello @mathcass \r\n1. I did fork the repository and clone the same on my local system. \r\n\r\n2. Then learnt about how we can publish our package on pypi.org. Also, found some instructions on same in setup.py documentation.\r\n\r\n3. Then I Perplexity document link that you shared above. I created a colab link from there keep both tensorflow and pytorch means a mixed option and tried to run it in colab but I encountered no errors at a point where you mentioned. Can you help me to figure out the issue. \r\n\r\n4.Here is the link of the colab file with my saved responses. \r\nhttps://colab.research.google.com/drive/1hfYz8Ira39FnREbxgwa_goZWpOojp2NH?usp=sharing'
 'Also, please share some links which made you conclude that pyarrow < 1 would help. '
 'Access granted for the colab link. '
 ""Thanks for looking at this @punitaojha and thanks for sharing the notebook. \r\n\r\nI just tried to reproduce this on my own (based on the environment where I had this issue) and I can't reproduce it somehow. If I run into this again, I'll include some steps to reproduce it. I'll close this as invalid. \r\n\r\nThanks again. ""
 'I am sorry for hijacking this closed issue, but I believe I was able to reproduce this very issue. Strangely enough, it also turned out that running `pip install ""pyarrow<1"" --upgrade` did indeed fix the issue (PyArrow was installed in version `0.14.1` in my case).\r\n\r\nPlease see the Colab below:\r\n\r\nhttps://colab.research.google.com/drive/15QQS3xWjlKW2aK0J74eEcRFuhXUddUST\r\n\r\nThanks!']","I was looking at the docs on [Perplexity](https://huggingface.co/transformers/perplexity.html) via GPT2. When you load datasets and try to load Wikitext, you get the error,

```
module 'pyarrow' has no attribute 'PyExtensionType'
```
I traced it back to datasets having installed PyArrow 1.0.1 but there's not pinning in the setup file. 

https://github.com/huggingface/datasets/blob/e86a2a8f869b91654e782c9133d810bb82783200/setup.py#L68

Downgrading by installing `pip install ""pyarrow<1""` resolved the issue."
https://github.com/huggingface/datasets/issues/705,TypeError: '<' not supported between instances of 'NamedSplit' and 'NamedSplit',"[""Hi !\r\nThanks for reporting :) \r\nIndeed this is an issue on the `datasets` side.\r\nI'm creating a PR""
 'Thanks @lhoestq !']","## Environment info
<!-- You can run the command `transformers-cli env` and copy-and-paste its output below.
     Don't forget to fill out the missing fields in that output! -->
     
- `transformers` version: 3.3.1 (installed from master)
- `datasets` version: 1.0.2 (installed as a dependency from transformers)
- Platform: Linux-4.15.0-118-generic-x86_64-with-debian-stretch-sid
- Python version: 3.7.9

I'm testing my own text classification dataset using [this example](https://github.com/huggingface/transformers/tree/master/examples/text-classification#run-generic-text-classification-script-in-tensorflow) from transformers. The dataset is split into train / dev / test, and in csv format, containing just a text and a label columns, using comma as sep. Here's a sample:
```
text,label
""Registra-se a presença do acadêmico <name> . <REL_SEP> Ao me deparar com a descrição de dois autores no polo ativo da ação junto ao PJe , margem esquerda foi informado pela procuradora do reclamante que se trata de uma reclamação trabalhista individual . <REL_SEP> Diante disso , face a ausência injustificada do autor <name> , determina-se o ARQUIVAMENTO do presente processo , com relação a este , nos termos do [[ art . 844 da CLT ]] . <REL_SEP> CUSTAS AUTOR - DISPENSADO <REL_SEP> Custas pelo autor no importe de R $326,82 , calculadas sobre R $16.341,03 , dispensadas na forma da lei , em virtude da concessão dos benefícios da Justiça Gratuita , ora deferida . <REL_SEP> Cientes os presentes . <REL_SEP> Audiência encerrada às 8h42min . <REL_SEP> <name> <REL_SEP> Juíza do Trabalho <REL_SEP> Ata redigida por << <name> >> , Secretário de Audiência ."",NO_RELATION
```

However, @Santosh-Gupta reported in #7351 that he had the exact same problem using the ChemProt dataset. His colab notebook is referenced in the following section.

## To reproduce

Steps to reproduce the behavior:

1. Created a new conda environment using conda env -n transformers python=3.7
2. Cloned transformers master, `cd` into it and installed using pip install --editable .  -r examples/requirements.txt 
3. Installed tensorflow with `pip install tensorflow`
3. Ran `run_tf_text_classification.py` with the following parameters:

```
--train_file <DATASET_PATH>/train.csv \
--dev_file <DATASET_PATH>/dev.csv \ 
--test_file <DATASET_PATH>/test.csv \
--label_column_id 1 \
--model_name_or_path neuralmind/bert-base-portuguese-cased \
--output_dir <OUTPUT_PATH> \
--num_train_epochs 4 \
--per_device_train_batch_size 4 \
--per_device_eval_batch_size 4 \
--do_train \
--do_eval \
--do_predict \
--logging_steps 1000 \
--evaluate_during_training \
--save_steps 1000 \
--overwrite_output_dir \
--overwrite_cache
```

I have also copied [@Santosh-Gupta 's colab notebook](https://colab.research.google.com/drive/11APei6GjphCZbH5wD9yVlfGvpIkh8pwr?usp=sharing) as a reference.

<!-- If you have code snippets, error messages, stack traces please provide them here as well.
     Important! Use code tags to correctly format your code. See https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting
     Do not use screenshots, as they are hard to read and (more importantly) don't allow others to copy-and-paste your code.-->

Here is the stack trace:

```
2020-10-02 07:33:41.622011: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
/media/discoD/repositorios/transformers_pedro/src/transformers/training_args.py:333: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)
  FutureWarning,
2020-10-02 07:33:43.471648: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-10-02 07:33:43.471791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-02 07:33:43.472664: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1070 computeCapability: 6.1
coreClock: 1.7085GHz coreCount: 15 deviceMemorySize: 7.92GiB deviceMemoryBandwidth: 238.66GiB/s
2020-10-02 07:33:43.472684: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-10-02 07:33:43.472765: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-10-02 07:33:43.472809: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-10-02 07:33:43.472848: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-10-02 07:33:43.474209: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-10-02 07:33:43.474276: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-10-02 07:33:43.561219: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-10-02 07:33:43.561397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-02 07:33:43.562345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-02 07:33:43.563219: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-10-02 07:33:43.563595: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-10-02 07:33:43.570091: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3591830000 Hz
2020-10-02 07:33:43.570494: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x560842432400 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-10-02 07:33:43.570511: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-10-02 07:33:43.570702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-02 07:33:43.571599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1070 computeCapability: 6.1
coreClock: 1.7085GHz coreCount: 15 deviceMemorySize: 7.92GiB deviceMemoryBandwidth: 238.66GiB/s
2020-10-02 07:33:43.571633: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-10-02 07:33:43.571645: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-10-02 07:33:43.571654: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-10-02 07:33:43.571664: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-10-02 07:33:43.571691: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-10-02 07:33:43.571704: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-10-02 07:33:43.571718: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-10-02 07:33:43.571770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-02 07:33:43.572641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-02 07:33:43.573475: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-10-02 07:33:47.139227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-10-02 07:33:47.139265: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-10-02 07:33:47.139272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2020-10-02 07:33:47.140323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-02 07:33:47.141248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-02 07:33:47.142085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-10-02 07:33:47.142854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5371 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)
2020-10-02 07:33:47.146317: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5608b95dc5c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-10-02 07:33:47.146336: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1070, Compute Capability 6.1
10/02/2020 07:33:47 - INFO - __main__ -   n_replicas: 1, distributed training: False, 16-bits training: False
10/02/2020 07:33:47 - INFO - __main__ -   Training/evaluation parameters TFTrainingArguments(output_dir='/media/discoD/models/datalawyer/pedidos/transformers_tf', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=True, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=4, per_device_eval_batch_size=4, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=4.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Oct02_07-33-43_user-XPS-8700', logging_first_step=False, logging_steps=1000, save_steps=1000, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, dataloader_num_workers=0, past_index=-1, run_name='/media/discoD/models/datalawyer/pedidos/transformers_tf', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=False, tpu_name=None, xla=False)
10/02/2020 07:33:53 - INFO - filelock -   Lock 140407857405776 acquired on /home/user/.cache/huggingface/datasets/e0f1e9ed46db1e2429189f06b479cbd4075c0976104c1aacf8f77d9a53d2ad87.03756fef6da334f50a7ff73608e21b5018229944ca250416ce7352e25d84a552.py.lock
10/02/2020 07:33:53 - INFO - filelock -   Lock 140407857405776 released on /home/user/.cache/huggingface/datasets/e0f1e9ed46db1e2429189f06b479cbd4075c0976104c1aacf8f77d9a53d2ad87.03756fef6da334f50a7ff73608e21b5018229944ca250416ce7352e25d84a552.py.lock
Using custom data configuration default
Traceback (most recent call last):
  File ""run_tf_text_classification.py"", line 283, in <module>
    main()
  File ""run_tf_text_classification.py"", line 222, in main
    max_seq_length=data_args.max_seq_length,
  File ""run_tf_text_classification.py"", line 43, in get_tfds
    ds = datasets.load_dataset(""csv"", data_files=files)
  File ""/media/discoD/anaconda3/envs/transformers/lib/python3.7/site-packages/datasets/load.py"", line 604, in load_dataset
    **config_kwargs,
  File ""/media/discoD/anaconda3/envs/transformers/lib/python3.7/site-packages/datasets/builder.py"", line 158, in __init__
    **config_kwargs,
  File ""/media/discoD/anaconda3/envs/transformers/lib/python3.7/site-packages/datasets/builder.py"", line 269, in _create_builder_config
    for key in sorted(data_files.keys()):
TypeError: '<' not supported between instances of 'NamedSplit' and 'NamedSplit'
```

## Expected behavior

Should be able to run the text-classification example as described in [https://github.com/huggingface/transformers/tree/master/examples/text-classification#run-generic-text-classification-script-in-tensorflow](https://github.com/huggingface/transformers/tree/master/examples/text-classification#run-generic-text-classification-script-in-tensorflow)

Originally opened this issue at transformers' repository: [https://github.com/huggingface/transformers/issues/7535](https://github.com/huggingface/transformers/issues/7535). @jplu instructed me to open here, since according to [this](https://github.com/huggingface/transformers/issues/7535#issuecomment-702778885) evidence, the problem is from datasets.

Thanks!"
https://github.com/huggingface/datasets/issues/699,XNLI dataset is not loading ,"['also i tried below code to solve checksum error \r\n`datasets-cli test ./datasets/xnli --save_infos --all_configs`\r\n\r\nand it shows \r\n\r\n```\r\n2020-10-02 07:06:16.588760: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\nTraceback (most recent call last):\r\n  File ""/opt/conda/lib/python3.7/site-packages/datasets/load.py"", line 268, in prepare_module\r\n    local_path = cached_path(file_path, download_config=download_config)\r\n  File ""/opt/conda/lib/python3.7/site-packages/datasets/utils/file_utils.py"", line 308, in cached_path\r\n    use_etag=download_config.use_etag,\r\n  File ""/opt/conda/lib/python3.7/site-packages/datasets/utils/file_utils.py"", line 474, in get_from_cache\r\n    raise FileNotFoundError(""Couldn\'t find file at {}"".format(url))\r\nFileNotFoundError: Couldn\'t find file at https://raw.githubusercontent.com/huggingface/datasets/1.0.2/datasets/./datasets/xnli/xnli.py\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File ""/opt/conda/lib/python3.7/site-packages/datasets/load.py"", line 279, in prepare_module\r\n    local_path = cached_path(file_path, download_config=download_config)\r\n  File ""/opt/conda/lib/python3.7/site-packages/datasets/utils/file_utils.py"", line 308, in cached_path\r\n    use_etag=download_config.use_etag,\r\n  File ""/opt/conda/lib/python3.7/site-packages/datasets/utils/file_utils.py"", line 474, in get_from_cache\r\n    raise FileNotFoundError(""Couldn\'t find file at {}"".format(url))\r\nFileNotFoundError: Couldn\'t find file at https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/./datasets/xnli/xnli.py\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File ""/opt/conda/bin/datasets-cli"", line 36, in <module>\r\n    service.run()\r\n  File ""/opt/conda/lib/python3.7/site-packages/datasets/commands/test.py"", line 76, in run\r\n    module_path, hash = prepare_module(path)\r\n  File ""/opt/conda/lib/python3.7/site-packages/datasets/load.py"", line 283, in prepare_module\r\n    combined_path, github_file_path, file_path\r\nFileNotFoundError: Couldn\'t find file locally at ./datasets/xnli/xnli.py, or remotely at https://raw.githubusercontent.com/huggingface/datasets/1.0.2/datasets/./datasets/xnli/xnli.py or https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/./datasets/xnli/xnli.py\r\n```\r\n\r\n'
 ""Hi !\r\nYes the download url changed.\r\nIt's updated on the master branch. I'm doing a release today to fix that :)""
 'the issue is fixed with latest release \r\n\r\n']","`dataset = datasets.load_dataset(path='xnli')`

showing below error 
```
/opt/conda/lib/python3.7/site-packages/nlp/utils/info_utils.py in verify_checksums(expected_checksums, recorded_checksums, verification_name)
     36     if len(bad_urls) > 0:
     37         error_msg = ""Checksums didn't match"" + for_verification_name + "":\n""
---> 38         raise NonMatchingChecksumError(error_msg + str(bad_urls))
     39     logger.info(""All the checksums matched successfully"" + for_verification_name)
     40 

NonMatchingChecksumError: Checksums didn't match for dataset source files:
['https://www.nyu.edu/projects/bowman/xnli/XNLI-1.0.zip']
```

I think URL is now changed to ""https://cims.nyu.edu/~sbowman/xnli/XNLI-MT-1.0.zip"""
https://github.com/huggingface/datasets/issues/690,XNLI dataset: NonMatchingChecksumError,"[""Thanks for reporting.\r\nThe data file must have been updated by the host.\r\nI'll update the checksum with the new one.""
 ""Well actually it looks like the link isn't working anymore :(""
 ""The new link is https://cims.nyu.edu/~sbowman/xnli/XNLI-1.0.zip\r\nI'll update the dataset script""
 'I\'ll do a release in the next few days to make the fix available for everyone.\r\nIn the meantime you can load `xnli` with\r\n```\r\nxnli = load_dataset(\'xnli\', script_version=""master"")\r\n```\r\nThis will use the latest version of the xnli script (available on master branch), instead of the old one.'
 ""That's awesome! Thanks a lot!""]","Hi,
I tried to download ""xnli"" dataset in colab using 
`xnli = load_dataset(path='xnli')`
but got 'NonMatchingChecksumError' error

`NonMatchingChecksumError                  Traceback (most recent call last)
<ipython-input-27-a87bedc82eeb> in <module>()
----> 1 xnli = load_dataset(path='xnli')

3 frames
/usr/local/lib/python3.6/dist-packages/datasets/utils/info_utils.py in verify_checksums(expected_checksums, recorded_checksums, verification_name)
     37     if len(bad_urls) > 0:
     38         error_msg = ""Checksums didn't match"" + for_verification_name + "":\n""
---> 39         raise NonMatchingChecksumError(error_msg + str(bad_urls))
     40     logger.info(""All the checksums matched successfully"" + for_verification_name)
     41 

NonMatchingChecksumError: Checksums didn't match for dataset source files:
['https://www.nyu.edu/projects/bowman/xnli/XNLI-1.0.zip']`

The same code worked well several days ago in colab but stopped working now. Thanks!"
https://github.com/huggingface/datasets/issues/687,`ArrowInvalid` occurs while running `Dataset.map()` function,"['Hi !\r\n\r\nThis is because `encode` expects one single text as input (str), or one tokenized text (List[str]).\r\nI believe that you actually wanted to use `encode_batch` which expects a batch of texts.\r\nHowever this method is only available for our ""fast"" tokenizers (ex: BertTokenizerFast).\r\nBertJapanese is not one of them unfortunately and I don\'t think it will be added for now (see https://github.com/huggingface/transformers/pull/7141)...\r\ncc @thomwolf for confirmation.\r\n\r\nTherefore what I\'d suggest for now is disable batching and process one text at a time using `encode`.\r\nNote that you can make it faster by using multiprocessing:\r\n\r\n```python\r\nnum_proc = None  # Specify here the number of processes if you want to use multiprocessing. ex: num_proc = 4\r\nencoded = train_ds.map(\r\n    lambda example: {\'tokens\': t.encode(example[\'title\'], max_length=1000)}, num_proc=num_proc\r\n)\r\n```\r\n'
 'Thank you very much for the kind and precise suggestion!\r\nI\'m looking forward to seeing BertJapaneseTokenizer built into the ""fast"" tokenizers.\r\n\r\nI tried `map` with multiprocessing as follows, and it worked!\r\n\r\n```python\r\n# There was a Pickle problem if I use `lambda` for multiprocessing\r\ndef encode(examples):\r\n    return {\'tokens\': t.encode(examples[\'title\'], max_length=1000)}\r\n\r\nnum_proc = 8\r\nencoded = train_ds.map(encode, num_proc=num_proc)\r\n```\r\n\r\nThank you again!']","It seems to fail to process the final batch. This [colab](https://colab.research.google.com/drive/1_byLZRHwGP13PHMkJWo62Wp50S_Z2HMD?usp=sharing) can reproduce the error.

Code:

```python
# train_ds = Dataset(features: {
#     'title': Value(dtype='string', id=None), 
#     'score': Value(dtype='float64', id=None)
# }, num_rows: 99999)

# suggested in #665 
class PicklableTokenizer(BertJapaneseTokenizer):
    def __getstate__(self):
        state = dict(self.__dict__)
        state['do_lower_case'] = self.word_tokenizer.do_lower_case
        state['never_split'] = self.word_tokenizer.never_split
        del state['word_tokenizer']
        return state
    
    def __setstate(self):
        do_lower_case = state.pop('do_lower_case')
        never_split = state.pop('never_split')
        self.__dict__ = state
        self.word_tokenizer = MecabTokenizer(
            do_lower_case=do_lower_case, never_split=never_split
        )

t = PicklableTokenizer.from_pretrained('bert-base-japanese-whole-word-masking')

encoded = train_ds.map(
    lambda examples: {'tokens': t.encode(examples['title'], max_length=1000)}, batched=True, batch_size=1000
)
```

Error Message:

```
 99% 99/100 [00:22<00:00, 39.07ba/s]
---------------------------------------------------------------------------
ArrowInvalid                              Traceback (most recent call last)
<timed exec> in <module>

/usr/local/lib/python3.6/site-packages/datasets/arrow_dataset.py in map(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint)
   1242                 fn_kwargs=fn_kwargs,
   1243                 new_fingerprint=new_fingerprint,
-> 1244                 update_data=update_data,
   1245             )
   1246         else:

/usr/local/lib/python3.6/site-packages/datasets/arrow_dataset.py in wrapper(*args, **kwargs)
    151             ""output_all_columns"": self._output_all_columns,
    152         }
--> 153         out: Union[""Dataset"", ""DatasetDict""] = func(self, *args, **kwargs)
    154         if new_format[""columns""] is not None:
    155             new_format[""columns""] = list(set(new_format[""columns""]) & set(out.column_names))

/usr/local/lib/python3.6/site-packages/datasets/fingerprint.py in wrapper(*args, **kwargs)
    161             # Call actual function
    162 
--> 163             out = func(self, *args, **kwargs)
    164 
    165             # Update fingerprint of in-place transforms + update in-place history of transforms

/usr/local/lib/python3.6/site-packages/datasets/arrow_dataset.py in _map_single(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, update_data)
   1496                     if update_data:
   1497                         batch = cast_to_python_objects(batch)
-> 1498                         writer.write_batch(batch)
   1499             if update_data:
   1500                 writer.finalize()  # close_stream=bool(buf_writer is None))  # We only close if we are writing in a file

/usr/local/lib/python3.6/site-packages/datasets/arrow_writer.py in write_batch(self, batch_examples, writer_batch_size)
    271             typed_sequence = TypedSequence(batch_examples[col], type=col_type, try_type=col_try_type)
    272             typed_sequence_examples[col] = typed_sequence
--> 273         pa_table = pa.Table.from_pydict(typed_sequence_examples)
    274         self.write_table(pa_table)
    275 

/usr/local/lib/python3.6/site-packages/pyarrow/table.pxi in pyarrow.lib.Table.from_pydict()

/usr/local/lib/python3.6/site-packages/pyarrow/table.pxi in pyarrow.lib.Table.from_arrays()

/usr/local/lib/python3.6/site-packages/pyarrow/table.pxi in pyarrow.lib.Table.validate()

/usr/local/lib/python3.6/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()

ArrowInvalid: Column 4 named tokens expected length 999 but got length 1000
```
"
https://github.com/huggingface/datasets/issues/686,Dataset browser url is still https://huggingface.co/nlp/viewer/,"[""Yes! might do it with @srush one of these days. Hopefully it won't break too many links (we can always redirect from old url to new)""
 'This was fixed but forgot to close the issue. cc @lhoestq @yjernite \r\n\r\nThanks @jarednielsen!']",Might be worth updating to https://huggingface.co/datasets/viewer/
https://github.com/huggingface/datasets/issues/678,The download instructions for c4 datasets are not contained in the error message,"['Good catch !\r\nIndeed the `@property` is missing.\r\n\r\nFeel free to open a PR :)'
 ""Also not that C4 is a dataset that needs an Apache Beam runtime to be generated.\r\nFor example Dataflow, Spark, Flink etc.\r\n\r\nUsually we generate the dataset on our side once and for all, but we haven't done it for C4 yet.\r\nMore info about beam datasets [here](https://huggingface.co/docs/datasets/beam_dataset.html)\r\n\r\nLet me know if you have any questions""]","The manual download instructions are not clear 
```The dataset c4 with config en requires manual data. 
 Please follow the manual download instructions: <bound method C4.manual_download_instructions of <datasets_modules.datasets.c4.830b0c218bd41fed439812c8dd19dbd4767d2a3faa385eb695cf8666c982b1b3.c4.C4 object at 0x7ff8c5969760>>. 
 Manual data can be loaded with `datasets.load_dataset(c4, data_dir='<path/to/manual/data>')
```

Either `@property`  could be added to C4.manual_download_instrcutions (or make it a real property), or the manual_download_instructions function needs to be called I think.

Let me know if you want a PR for this, but I'm not sure which possible fix is the correct one."
https://github.com/huggingface/datasets/issues/676,train_test_split returns empty dataset item,"['The problem still exists after removing the cache files.'
 'Can you reproduce this example in a Colab so we can investigate? (or give more information on your software/hardware config)'
 ""Thanks for reporting.\r\nI just found the issue, I'm creating a PR""
 ""We'll do a release pretty soon to include the fix :)\r\nIn the meantime you can install the lib from source if you want to ""]","I try to split my dataset by `train_test_split`, but after that the item in `train` and `test` `Dataset` is empty.
The codes:
```
yelp_data = datasets.load_from_disk('/home/ssd4/huanglianzhe/test_yelp')
    print(yelp_data[0])
    yelp_data = yelp_data.train_test_split(test_size=0.1)
    print(yelp_data)
    print(yelp_data['test'])
    print(yelp_data['test'][0])
```
The outputs:
```
{'stars': 2.0, 'text': 'xxxx'}
Loading cached split indices for dataset at /home/ssd4/huanglianzhe/test_yelp/cache-f9b22d8b9d5a7346.arrow and /home/ssd4/huanglianzhe/test_yelp/cache-4aa26fa4005059d1.arrow
DatasetDict({'train': Dataset(features: {'stars': Value(dtype='float64', id=None), 'text': Value(dtype='string', id=None)}, num_rows: 7219009), 'test': Dataset(features: {'stars': Value(dtype='float64', id=None), 'text': Value(dtype='string', id=None)}, num_rows: 802113)})
Dataset(features: {'stars': Value(dtype='float64', id=None), 'text': Value(dtype='string', id=None)}, num_rows: 802113)
{}    # yelp_data['test'][0] is empty
```"
https://github.com/huggingface/datasets/issues/675,Add custom dataset to NLP?,"['Yes you can have a look here: https://huggingface.co/docs/datasets/loading_datasets.html#csv-files'
 'No activity, closing']","Is it possible to add a custom dataset such as a .csv to the NLP library?

Thanks."
https://github.com/huggingface/datasets/issues/674,load_dataset() won't download in Windows,"[""I have the same issue. Tried to download a few of them and not a single one is downloaded successfully.\r\n\r\nThis is the output:\r\n```\r\n>>> dataset = load_dataset('blended_skill_talk', split='train')\r\nUsing custom data configuration default               <-- This step never ends\r\n```""
 ""This was fixed in #644 \r\nI'll do a new release soon :)\r\n\r\nIn the meantime you can run it by installing from source""
 'Closing since version 1.1.0 got released with Windows support :) \r\nLet me know if it works for you now']","I don't know if this is just me or Windows. Maybe other Windows users can chime in if they don't have this problem. I've been trying to get some of the tutorials working on Windows, but when I use the load_dataset() function, it just stalls and the script keeps running indefinitely without downloading anything. I've waited upwards of 18 hours to download the 'multi-news' dataset (which isn't very big), and still nothing. I've tried running it through different IDE's and the command line, but it had the same behavior. I've also tried it with all virus and malware protection turned off. I've made sure python and all IDE's are exceptions to the firewall and all the requisite permissions are enabled.

Additionally, I checked to see if other packages could download content such as an nltk corpus, and they could. I've also run the same script using Ubuntu and it downloaded fine (and quickly). When I copied the downloaded datasets from my Ubuntu drive to my Windows .cache folder it worked fine by reusing the already-downloaded dataset, but it's cumbersome to do that for every dataset I want to try in my Windows environment.

Could this be a bug, or is there something I'm doing wrong or not thinking of?

Thanks."
https://github.com/huggingface/datasets/issues/673,blog_authorship_corpus crashed,"[""Thanks for reporting !\r\nWe'll free some memory""]","This is just to report that When I pick blog_authorship_corpus in 
https://huggingface.co/nlp/viewer/?dataset=blog_authorship_corpus
I get this:
![image](https://user-images.githubusercontent.com/7553188/94349542-4364f300-0013-11eb-897d-b25660a449f0.png)

"
https://github.com/huggingface/datasets/issues/672,Questions about XSUM ,"[""We should try to regenerate the data using the official script.\r\nBut iirc that's what we used in the first place, so not sure why it didn't match in the first place.\r\n\r\nI'll let you know when the dataset is updated""
 'Thanks, looking forward to hearing your update on this thread. \r\n\r\nThis is a blocking issue for us; would appreciate any progress on this front. We can also help with the fix, if you deem it appropriately. '
 ""I just started the generation on my side, I'll let you know how it goes :) ""
 ""Hmm after a first run I'm still missing 136668/226711 urls.\r\nI'll relaunch it tomorrow to try to get the remaining ones.""
 ""Update: I'm missing 36/226711 urls but I haven't managed to download them yet""
 'Thanks! That sounds like a reasonable number! '
 ""So I managed to download them all but when parsing only 226,181/226,711 worked.\r\nNot sure if it's worth digging and debugging parsing at this point :/ ""
 ""Maybe @sshleifer can help, I think he's already played with xsum at one point""
 ""Thanks @lhoestq\r\nIt would be great to improve coverage, but IDs are the really crucial part for us. We'd really appreciate an update to the dataset with IDs either way!""
 'I gave up at an even earlier point. The dataset I use has 204,017 train examples.'
 '@lhoestq @sshleifer like @jbragg said earlier, the main issue for us is that the current XSUM dataset (in your package) does not have IDs suggested by the original dataset ([here is the file](https://raw.githubusercontent.com/EdinburghNLP/XSum/master/XSum-Dataset/XSum-TRAINING-DEV-TEST-SPLIT-90-5-5.json).) Would appreciate if you update the XSUM dataset to include the instance IDs. \r\n\r\nThe missing instances is also a problem, but likely not worth pursuing given its relatively small scale. '
 "">So I managed to download them all but when parsing only 226,181/226,711 worked.\r\n\r\n@lhoestq any chance we could update the HF-hosted dataset with the IDs in your new version? Happy to help if there's something I can do.""
 ""Well I couldn't parse what I downloaded.\r\nUnfortunately I think I won't be able to take a look at it this week.\r\nI can try to send you what I got if you want to give it a shot @jbragg \r\nOtherwise feel free to re-run the xsum download script, maybe you'll be luckier than me""]","Hi there ✋ 

I'm looking into your `xsum` dataset and I have several questions on that. 
So here is how I loaded the data: 
```
>>> data = datasets.load_dataset('xsum', version='1.0.1')
>>> data['train']
Dataset(features: {'document': Value(dtype='string', id=None), 'summary': Value(dtype='string', id=None)}, num_rows: 204017)
>>> data['test']
Dataset(features: {'document': Value(dtype='string', id=None), 'summary': Value(dtype='string', id=None)}, num_rows: 11333)
```

The first issue is, the instance counts don’t match what I see on [the dataset's website](https://github.com/EdinburghNLP/XSum/tree/master/XSum-Dataset#what-builds-the-xsum-dataset) (11,333 vs 11,334 for test set; 204,017 vs 204,045 for training set)
```
 … training (90%, 204,045), validation (5%, 11,332), and test (5%, 11,334) set.
```
Any thoughts why? Perhaps @mariamabarham could help here, since she recently had a PR on this dataaset https://github.com/huggingface/datasets/pull/289  (reviewed by @patrickvonplaten) 

Another issue is that the instances don't seem to have IDs. The original datasets provides IDs for the instances: https://github.com/EdinburghNLP/XSum/blob/master/XSum-Dataset/XSum-TRAINING-DEV-TEST-SPLIT-90-5-5.json but to be able to use them, the dataset sizes need to match. 

CC @jbragg 

"
https://github.com/huggingface/datasets/issues/669,How to skip a example when running dataset.map,"[""Hi @xixiaoyao,\r\nDepending on what you want to do you can:\r\n- use a first step of `filter` to filter out the invalid examples: https://huggingface.co/docs/datasets/processing.html#filtering-rows-select-and-filter\r\n- or directly detect the invalid examples inside the callable used with `map` and return them unchanged or even remove them at the same time if you are using `map` in batched mode. Here is an example where we use `map` in batched mode to add new rows on the fly but you can also use it to remove examples on the fly (that's what `filter` actually do under-the-hood): https://huggingface.co/docs/datasets/processing.html#augmenting-the-dataset""
 'Closing this one.\r\nFeel free to re-open if you have other questions']","in processing func, I process examples and detect some invalid examples, which I did not want it to be added into train dataset. However I did not find how to skip this recognized invalid example when doing dataset.map. "
https://github.com/huggingface/datasets/issues/667,Loss not decrease with Datasets and Transformers,"['And I tested it on T5ForConditionalGeneration, that works no problem.'
 'Hi did you manage to fix your issue ?\r\n\r\nIf so feel free to share your fix and close this thread']","HI,

The following script is used to fine-tune a BertForSequenceClassification model on SST2.

The script is adapted from [this colab](https://colab.research.google.com/github/huggingface/datasets/blob/master/notebooks/Overview.ipynb) that presents an example of fine-tuning BertForQuestionAnswering using squad dataset. In that colab, loss works fine. When I adapt it to SST2, the loss fails to decrease as it should. I attach the adapted script below and appreciate anyone pointing out what I miss?

```python
import torch
from datasets import load_dataset
from transformers import BertForSequenceClassification
from transformers import BertTokenizerFast
# Load our training dataset and tokenizer
dataset = load_dataset(""glue"", 'sst2')
tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')
del dataset[""test""] # let's remove it in this demo

# Tokenize our training dataset
def convert_to_features(example_batch):
    encodings = tokenizer(example_batch[""sentence""])
    encodings.update({""labels"": example_batch[""label""]})
    return encodings

encoded_dataset = dataset.map(convert_to_features, batched=True)
# Format our dataset to outputs torch.Tensor to train a pytorch model
columns = ['input_ids', 'token_type_ids', 'attention_mask', 'labels']
encoded_dataset.set_format(type='torch', columns=columns)

# Instantiate a PyTorch Dataloader around our dataset
# Let's do dynamic batching (pad on the fly with our own collate_fn)
def collate_fn(examples):
    return tokenizer.pad(examples, return_tensors='pt')

dataloader = torch.utils.data.DataLoader(encoded_dataset['train'], collate_fn=collate_fn, batch_size=8)
# Now let's train our model
device = 'cuda' if torch.cuda.is_available() else 'cpu'
# Let's load a pretrained Bert model and a simple optimizer
model = BertForSequenceClassification.from_pretrained('bert-base-cased', return_dict=True)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)
model.train().to(device)
for i, batch in enumerate(dataloader):
    batch.to(device)
    outputs = model(**batch)
    loss = outputs.loss
    loss.backward()
    optimizer.step()
    model.zero_grad()
    print(f'Step {i} - loss: {loss:.3}')


```
In case needed.

- datasets == 1.0.2
- transformers == 3.2.0"
https://github.com/huggingface/datasets/issues/666,Does both 'bookcorpus' and 'wikipedia' belong to the same datasets which Google used for pretraining BERT?,['No they are other similar copies but they are not provided by the official Bert models authors.'],
https://github.com/huggingface/datasets/issues/665,"runing dataset.map, it raises TypeError: can't pickle Tokenizer objects","['Hi !\r\nIt works on my side with both the LongFormerTokenizer and the LongFormerTokenizerFast.\r\n\r\nWhich version of transformers/datasets are you using ?'
 'transformers and datasets are both the latest'
 'Then I guess you need to give us more informations on your setup (OS, python, GPU, etc) or a Google Colab reproducing the error for us to be able to debug this error.'
 'And your version of `dill` if possible :)'
 'I have the same issue with `transformers/BertJapaneseTokenizer`.\r\n\r\n\r\n\r\n```python\r\n# train_ds = Dataset(features: {\r\n#     \'title\': Value(dtype=\'string\', id=None), \r\n#     \'score\': Value(dtype=\'float64\', id=None)\r\n# }, num_rows: 99999)\r\n\r\nt = BertJapaneseTokenizer.from_pretrained(\'bert-base-japanese-whole-word-masking\')\r\nencoded = train_ds.map(lambda examples: {\'tokens\': t.encode(examples[\'title\'])}, batched=True)\r\n```\r\n\r\n<details><summary>Error Message</summary>\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-35-2b7d66b291c1> in <module>\r\n      2 \r\n      3 encoded = train_ds.map(lambda examples:\r\n----> 4   {\'tokens\': t.encode(examples[\'title\'])}, batched=True)\r\n\r\n/usr/local/lib/python3.6/site-packages/datasets/arrow_dataset.py in map(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint)\r\n   1242                 fn_kwargs=fn_kwargs,\r\n   1243                 new_fingerprint=new_fingerprint,\r\n-> 1244                 update_data=update_data,\r\n   1245             )\r\n   1246         else:\r\n\r\n/usr/local/lib/python3.6/site-packages/datasets/arrow_dataset.py in wrapper(*args, **kwargs)\r\n    151             ""output_all_columns"": self._output_all_columns,\r\n    152         }\r\n--> 153         out: Union[""Dataset"", ""DatasetDict""] = func(self, *args, **kwargs)\r\n    154         if new_format[""columns""] is not None:\r\n    155             new_format[""columns""] = list(set(new_format[""columns""]) & set(out.column_names))\r\n\r\n/usr/local/lib/python3.6/site-packages/datasets/fingerprint.py in wrapper(*args, **kwargs)\r\n    156                         kwargs_for_fingerprint[""fingerprint_name""] = fingerprint_name\r\n    157                         kwargs[fingerprint_name] = update_fingerprint(\r\n--> 158                             self._fingerprint, transform, kwargs_for_fingerprint\r\n    159                         )\r\n    160 \r\n\r\n/usr/local/lib/python3.6/site-packages/datasets/fingerprint.py in update_fingerprint(fingerprint, transform, transform_args)\r\n    103     for key in sorted(transform_args):\r\n    104         hasher.update(key)\r\n--> 105         hasher.update(transform_args[key])\r\n    106     return hasher.hexdigest()\r\n    107 \r\n\r\n/usr/local/lib/python3.6/site-packages/datasets/fingerprint.py in update(self, value)\r\n     55     def update(self, value):\r\n     56         self.m.update(f""=={type(value)}=="".encode(""utf8""))\r\n---> 57         self.m.update(self.hash(value).encode(""utf-8""))\r\n     58 \r\n     59     def hexdigest(self):\r\n\r\n/usr/local/lib/python3.6/site-packages/datasets/fingerprint.py in hash(cls, value)\r\n     51             return cls.dispatch[type(value)](cls, value)\r\n     52         else:\r\n---> 53             return cls.hash_default(value)\r\n     54 \r\n     55     def update(self, value):\r\n\r\n/usr/local/lib/python3.6/site-packages/datasets/fingerprint.py in hash_default(cls, value)\r\n     44     @classmethod\r\n     45     def hash_default(cls, value):\r\n---> 46         return cls.hash_bytes(dumps(value))\r\n     47 \r\n     48     @classmethod\r\n\r\n/usr/local/lib/python3.6/site-packages/datasets/utils/py_utils.py in dumps(obj)\r\n    365     file = StringIO()\r\n    366     with _no_cache_fields(obj):\r\n--> 367         dump(obj, file)\r\n    368     return file.getvalue()\r\n    369 \r\n\r\n/usr/local/lib/python3.6/site-packages/datasets/utils/py_utils.py in dump(obj, file)\r\n    337 def dump(obj, file):\r\n    338     """"""pickle an object to a file""""""\r\n--> 339     Pickler(file, recurse=True).dump(obj)\r\n    340     return\r\n    341 \r\n\r\n/usr/local/lib/python3.6/site-packages/dill/_dill.py in dump(self, obj)\r\n    444             raise PicklingError(msg)\r\n    445         else:\r\n--> 446             StockPickler.dump(self, obj)\r\n    447         stack.clear()  # clear record of \'recursion-sensitive\' pickled objects\r\n    448         return\r\n\r\n/usr/local/lib/python3.6/pickle.py in dump(self, obj)\r\n    407         if self.proto >= 4:\r\n    408             self.framer.start_framing()\r\n--> 409         self.save(obj)\r\n    410         self.write(STOP)\r\n    411         self.framer.end_framing()\r\n\r\n/usr/local/lib/python3.6/pickle.py in save(self, obj, save_persistent_id)\r\n    474         f = self.dispatch.get(t)\r\n    475         if f is not None:\r\n--> 476             f(self, obj) # Call unbound method with explicit self\r\n    477             return\r\n    478 \r\n\r\n/usr/local/lib/python3.6/site-packages/dill/_dill.py in save_function(pickler, obj)\r\n   1436                                 globs, obj.__name__,\r\n   1437                                 obj.__defaults__, obj.__closure__,\r\n-> 1438                                 obj.__dict__, fkwdefaults), obj=obj)\r\n   1439         else:\r\n   1440             _super = (\'super\' in getattr(obj.func_code,\'co_names\',())) and (_byref is not None) and getattr(pickler, \'_recurse\', False)\r\n\r\n/usr/local/lib/python3.6/pickle.py in save_reduce(self, func, args, state, listitems, dictitems, obj)\r\n    608         else:\r\n    609             save(func)\r\n--> 610             save(args)\r\n    611             write(REDUCE)\r\n    612 \r\n\r\n/usr/local/lib/python3.6/pickle.py in save(self, obj, save_persistent_id)\r\n    474         f = self.dispatch.get(t)\r\n    475         if f is not None:\r\n--> 476             f(self, obj) # Call unbound method with explicit self\r\n    477             return\r\n    478 \r\n\r\n/usr/local/lib/python3.6/pickle.py in save_tuple(self, obj)\r\n    749         write(MARK)\r\n    750         for element in obj:\r\n--> 751             save(element)\r\n    752 \r\n    753         if id(obj) in memo:\r\n\r\n/usr/local/lib/python3.6/pickle.py in save(self, obj, save_persistent_id)\r\n    474         f = self.dispatch.get(t)\r\n    475         if f is not None:\r\n--> 476             f(self, obj) # Call unbound method with explicit self\r\n    477             return\r\n    478 \r\n\r\n/usr/local/lib/python3.6/site-packages/dill/_dill.py in save_module_dict(pickler, obj)\r\n    931             # we only care about session the first pass thru\r\n    932             pickler._session = False\r\n--> 933         StockPickler.save_dict(pickler, obj)\r\n    934         log.info(""# D2"")\r\n    935     return\r\n\r\n/usr/local/lib/python3.6/pickle.py in save_dict(self, obj)\r\n    819 \r\n    820         self.memoize(obj)\r\n--> 821         self._batch_setitems(obj.items())\r\n    822 \r\n    823     dispatch[dict] = save_dict\r\n\r\n/usr/local/lib/python3.6/pickle.py in _batch_setitems(self, items)\r\n    850                 k, v = tmp[0]\r\n    851                 save(k)\r\n--> 852                 save(v)\r\n    853                 write(SETITEM)\r\n    854             # else tmp is empty, and we\'re done\r\n\r\n/usr/local/lib/python3.6/pickle.py in save(self, obj, save_persistent_id)\r\n    519 \r\n    520         # Save the reduce() output and finally memoize the object\r\n--> 521         self.save_reduce(obj=obj, *rv)\r\n    522 \r\n    523     def persistent_id(self, obj):\r\n\r\n/usr/local/lib/python3.6/pickle.py in save_reduce(self, func, args, state, listitems, dictitems, obj)\r\n    632 \r\n    633         if state is not None:\r\n--> 634             save(state)\r\n    635             write(BUILD)\r\n    636 \r\n\r\n/usr/local/lib/python3.6/pickle.py in save(self, obj, save_persistent_id)\r\n    474         f = self.dispatch.get(t)\r\n    475         if f is not None:\r\n--> 476             f(self, obj) # Call unbound method with explicit self\r\n    477             return\r\n    478 \r\n\r\n/usr/local/lib/python3.6/site-packages/dill/_dill.py in save_module_dict(pickler, obj)\r\n    931             # we only care about session the first pass thru\r\n    932             pickler._session = False\r\n--> 933         StockPickler.save_dict(pickler, obj)\r\n    934         log.info(""# D2"")\r\n    935     return\r\n\r\n/usr/local/lib/python3.6/pickle.py in save_dict(self, obj)\r\n    819 \r\n    820         self.memoize(obj)\r\n--> 821         self._batch_setitems(obj.items())\r\n    822 \r\n    823     dispatch[dict] = save_dict\r\n\r\n/usr/local/lib/python3.6/pickle.py in _batch_setitems(self, items)\r\n    845                 for k, v in tmp:\r\n    846                     save(k)\r\n--> 847                     save(v)\r\n    848                 write(SETITEMS)\r\n    849             elif n:\r\n\r\n/usr/local/lib/python3.6/pickle.py in save(self, obj, save_persistent_id)\r\n    519 \r\n    520         # Save the reduce() output and finally memoize the object\r\n--> 521         self.save_reduce(obj=obj, *rv)\r\n    522 \r\n    523     def persistent_id(self, obj):\r\n\r\n/usr/local/lib/python3.6/pickle.py in save_reduce(self, func, args, state, listitems, dictitems, obj)\r\n    632 \r\n    633         if state is not None:\r\n--> 634             save(state)\r\n    635             write(BUILD)\r\n    636 \r\n\r\n/usr/local/lib/python3.6/pickle.py in save(self, obj, save_persistent_id)\r\n    474         f = self.dispatch.get(t)\r\n    475         if f is not None:\r\n--> 476             f(self, obj) # Call unbound method with explicit self\r\n    477             return\r\n    478 \r\n\r\n/usr/local/lib/python3.6/site-packages/dill/_dill.py in save_module_dict(pickler, obj)\r\n    931             # we only care about session the first pass thru\r\n    932             pickler._session = False\r\n--> 933         StockPickler.save_dict(pickler, obj)\r\n    934         log.info(""# D2"")\r\n    935     return\r\n\r\n/usr/local/lib/python3.6/pickle.py in save_dict(self, obj)\r\n    819 \r\n    820         self.memoize(obj)\r\n--> 821         self._batch_setitems(obj.items())\r\n    822 \r\n    823     dispatch[dict] = save_dict\r\n\r\n/usr/local/lib/python3.6/pickle.py in _batch_setitems(self, items)\r\n    845                 for k, v in tmp:\r\n    846                     save(k)\r\n--> 847                     save(v)\r\n    848                 write(SETITEMS)\r\n    849             elif n:\r\n\r\n/usr/local/lib/python3.6/pickle.py in save(self, obj, save_persistent_id)\r\n    494             reduce = getattr(obj, ""__reduce_ex__"", None)\r\n    495             if reduce is not None:\r\n--> 496                 rv = reduce(self.proto)\r\n    497             else:\r\n    498                 reduce = getattr(obj, ""__reduce__"", None)\r\n\r\nTypeError: can\'t pickle Tagger objects\r\n```\r\n\r\n</details>\r\n\r\ntrainsformers: 2.10.0\r\ndatasets: 1.0.2\r\ndill: 0.3.2\r\npython: 3.6.8\r\n\r\nOS: ubuntu 16.04 (Docker Image) on [Deep Learning VM](https://console.cloud.google.com/marketplace/details/click-to-deploy-images/deeplearning) (GCP)\r\nGPU: Tesla P100 (CUDA 10)\r\n'
 '> I have the same issue with `transformers/BertJapaneseTokenizer`.\r\n\r\nIt looks like it this tokenizer is not supported unfortunately.\r\nThis is because `t.word_tokenizer.mecab` is a `fugashi.fugashi.GenericTagger` which is not compatible with pickle nor dill.\r\n\r\nWe need objects passes to `map` to be picklable for our caching system to work properly.\r\nHere it crashes because the caching system is not able to pickle the GenericTagger.\r\n\r\n\\> Maybe you can create an issue on [fugashi](https://github.com/polm/fugashi/issues) \'s repo and ask to make `fugashi.fugashi.GenericTagger` compatible with pickle ?\r\n\r\nWhat you can do in the meantime is use a picklable wrapper of the tokenizer:\r\n\r\n\r\n```python\r\nfrom transformers import BertJapaneseTokenizer, MecabTokenizer\r\n\r\nclass PicklableTokenizer(BertJapaneseTokenizer):\r\n\r\n    def __getstate__(self):\r\n        state = dict(self.__dict__)\r\n        state[""do_lower_case""] = self.word_tokenizer.do_lower_case\r\n        state[""never_split""] = self.word_tokenizer.never_split \r\n        del state[""word_tokenizer""]\r\n        return state\r\n\r\n    def __setstate__(self, state):\r\n        do_lower_case = state.pop(""do_lower_case"")\r\n        never_split = state.pop(""never_split"")\r\n        self.__dict__ = state\r\n        self.word_tokenizer = MecabTokenizer(\r\n            do_lower_case=do_lower_case, never_split=never_split)\r\n        )\r\n\r\nt = PicklableTokenizer.from_pretrained(""cl-tohoku/bert-base-japanese-whole-word-masking"")\r\nencoded = train_ds.map(lambda examples: {\'tokens\': t.encode(examples[\'title\'])}, batched=True)  # it works\r\n```'
 ""We can also update the `BertJapaneseTokenizer` in `transformers` as you just shown @lhoestq to make it compatible with pickle. It will be faster than asking on fugashi 's repo and good for the other users of `transformers` as well.\r\n\r\nI'm currently working on `transformers` I'll include it in the https://github.com/huggingface/transformers/pull/7141 PR and the next release of `transformers`.""
 ""Thank you for the rapid and polite response!\r\n\r\n@lhoestq Thanks for the suggestion! I've passed the pickle phase, but another `ArrowInvalid` problem occored. I created another issue #687 .\r\n\r\n@thomwolf Wow, really fast work. I'm looking forward to the next release 🤗""]","I load squad dataset. Then want to process data use following function with `Huggingface Transformers LongformerTokenizer`.

```
def convert_to_features(example):
    # Tokenize contexts and questions (as pairs of inputs)
    input_pairs = [example['question'], example['context']]
    encodings = tokenizer.encode_plus(input_pairs, pad_to_max_length=True, max_length=512)
    context_encodings = tokenizer.encode_plus(example['context'])
    

    # Compute start and end tokens for labels using Transformers's fast tokenizers alignement methodes.
    # this will give us the position of answer span in the context text
    start_idx, end_idx = get_correct_alignement(example['context'], example['answers'])
    start_positions_context = context_encodings.char_to_token(start_idx)
    end_positions_context = context_encodings.char_to_token(end_idx-1)

    # here we will compute the start and end position of the answer in the whole example
    # as the example is encoded like this <s> question</s></s> context</s>
    # and we know the postion of the answer in the context
    # we can just find out the index of the sep token and then add that to position + 1 (+1 because there are two sep tokens)
    # this will give us the position of the answer span in whole example 
    sep_idx = encodings['input_ids'].index(tokenizer.sep_token_id)
    start_positions = start_positions_context + sep_idx + 1
    end_positions = end_positions_context + sep_idx + 1

    if end_positions > 512:
      start_positions, end_positions = 0, 0

    encodings.update({'start_positions': start_positions,
                      'end_positions': end_positions,
                      'attention_mask': encodings['attention_mask']})
    return encodings
```

Then I run `dataset.map(convert_to_features)`, it raise
```
In [59]: a.map(convert_to_features)                                                                                                                        
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-59-c453b508761d> in <module>
----> 1 a.map(convert_to_features)

/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py in map(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint)
   1242                 fn_kwargs=fn_kwargs,
   1243                 new_fingerprint=new_fingerprint,
-> 1244                 update_data=update_data,
   1245             )
   1246         else:

/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py in wrapper(*args, **kwargs)
    151             ""output_all_columns"": self._output_all_columns,
    152         }
--> 153         out: Union[""Dataset"", ""DatasetDict""] = func(self, *args, **kwargs)
    154         if new_format[""columns""] is not None:
    155             new_format[""columns""] = list(set(new_format[""columns""]) & set(out.column_names))

/opt/conda/lib/python3.7/site-packages/datasets/fingerprint.py in wrapper(*args, **kwargs)
    156                         kwargs_for_fingerprint[""fingerprint_name""] = fingerprint_name
    157                         kwargs[fingerprint_name] = update_fingerprint(
--> 158                             self._fingerprint, transform, kwargs_for_fingerprint
    159                         )
    160 

/opt/conda/lib/python3.7/site-packages/datasets/fingerprint.py in update_fingerprint(fingerprint, transform, transform_args)
    103     for key in sorted(transform_args):
    104         hasher.update(key)
--> 105         hasher.update(transform_args[key])
    106     return hasher.hexdigest()
    107 

/opt/conda/lib/python3.7/site-packages/datasets/fingerprint.py in update(self, value)
     55     def update(self, value):
     56         self.m.update(f""=={type(value)}=="".encode(""utf8""))
---> 57         self.m.update(self.hash(value).encode(""utf-8""))
     58 
     59     def hexdigest(self):

/opt/conda/lib/python3.7/site-packages/datasets/fingerprint.py in hash(cls, value)
     51             return cls.dispatch[type(value)](cls, value)
     52         else:
---> 53             return cls.hash_default(value)
     54 
     55     def update(self, value):

/opt/conda/lib/python3.7/site-packages/datasets/fingerprint.py in hash_default(cls, value)
     44     @classmethod
     45     def hash_default(cls, value):
---> 46         return cls.hash_bytes(dumps(value))
     47 
     48     @classmethod

/opt/conda/lib/python3.7/site-packages/datasets/utils/py_utils.py in dumps(obj)
    365     file = StringIO()
    366     with _no_cache_fields(obj):
--> 367         dump(obj, file)
    368     return file.getvalue()
    369 

/opt/conda/lib/python3.7/site-packages/datasets/utils/py_utils.py in dump(obj, file)
    337 def dump(obj, file):
    338     """"""pickle an object to a file""""""
--> 339     Pickler(file, recurse=True).dump(obj)
    340     return
    341 

/opt/conda/lib/python3.7/site-packages/dill/_dill.py in dump(self, obj)
    444             raise PicklingError(msg)
    445         else:
--> 446             StockPickler.dump(self, obj)
    447         stack.clear()  # clear record of 'recursion-sensitive' pickled objects
    448         return

/opt/conda/lib/python3.7/pickle.py in dump(self, obj)
    435         if self.proto >= 4:
    436             self.framer.start_framing()
--> 437         self.save(obj)
    438         self.write(STOP)
    439         self.framer.end_framing()

/opt/conda/lib/python3.7/pickle.py in save(self, obj, save_persistent_id)
    502         f = self.dispatch.get(t)
    503         if f is not None:
--> 504             f(self, obj) # Call unbound method with explicit self
    505             return
    506 

/opt/conda/lib/python3.7/site-packages/dill/_dill.py in save_function(pickler, obj)
   1436                                 globs, obj.__name__,
   1437                                 obj.__defaults__, obj.__closure__,
-> 1438                                 obj.__dict__, fkwdefaults), obj=obj)
   1439         else:
   1440             _super = ('super' in getattr(obj.func_code,'co_names',())) and (_byref is not None) and getattr(pickler, '_recurse', False)

/opt/conda/lib/python3.7/pickle.py in save_reduce(self, func, args, state, listitems, dictitems, obj)
    636         else:
    637             save(func)
--> 638             save(args)
    639             write(REDUCE)
    640 

/opt/conda/lib/python3.7/pickle.py in save(self, obj, save_persistent_id)
    502         f = self.dispatch.get(t)
    503         if f is not None:
--> 504             f(self, obj) # Call unbound method with explicit self
    505             return
    506 

/opt/conda/lib/python3.7/pickle.py in save_tuple(self, obj)
    787         write(MARK)
    788         for element in obj:
--> 789             save(element)
    790 
    791         if id(obj) in memo:

/opt/conda/lib/python3.7/pickle.py in save(self, obj, save_persistent_id)
    502         f = self.dispatch.get(t)
    503         if f is not None:
--> 504             f(self, obj) # Call unbound method with explicit self
    505             return
    506 

/opt/conda/lib/python3.7/site-packages/dill/_dill.py in save_module_dict(pickler, obj)
    931             # we only care about session the first pass thru
    932             pickler._session = False
--> 933         StockPickler.save_dict(pickler, obj)
    934         log.info(""# D2"")
    935     return

/opt/conda/lib/python3.7/pickle.py in save_dict(self, obj)
    857 
    858         self.memoize(obj)
--> 859         self._batch_setitems(obj.items())
    860 
    861     dispatch[dict] = save_dict

/opt/conda/lib/python3.7/pickle.py in _batch_setitems(self, items)
    883                 for k, v in tmp:
    884                     save(k)
--> 885                     save(v)
    886                 write(SETITEMS)
    887             elif n:

/opt/conda/lib/python3.7/pickle.py in save(self, obj, save_persistent_id)
    547 
    548         # Save the reduce() output and finally memoize the object
--> 549         self.save_reduce(obj=obj, *rv)
    550 
    551     def persistent_id(self, obj):

/opt/conda/lib/python3.7/pickle.py in save_reduce(self, func, args, state, listitems, dictitems, obj)
    660 
    661         if state is not None:
--> 662             save(state)
    663             write(BUILD)
    664 

/opt/conda/lib/python3.7/pickle.py in save(self, obj, save_persistent_id)
    502         f = self.dispatch.get(t)
    503         if f is not None:
--> 504             f(self, obj) # Call unbound method with explicit self
    505             return
    506 

/opt/conda/lib/python3.7/site-packages/dill/_dill.py in save_module_dict(pickler, obj)
    931             # we only care about session the first pass thru
    932             pickler._session = False
--> 933         StockPickler.save_dict(pickler, obj)
    934         log.info(""# D2"")
    935     return

/opt/conda/lib/python3.7/pickle.py in save_dict(self, obj)
    857 
    858         self.memoize(obj)
--> 859         self._batch_setitems(obj.items())
    860 
    861     dispatch[dict] = save_dict

/opt/conda/lib/python3.7/pickle.py in _batch_setitems(self, items)
    883                 for k, v in tmp:
    884                     save(k)
--> 885                     save(v)
    886                 write(SETITEMS)
    887             elif n:

/opt/conda/lib/python3.7/pickle.py in save(self, obj, save_persistent_id)
    547 
    548         # Save the reduce() output and finally memoize the object
--> 549         self.save_reduce(obj=obj, *rv)
    550 
    551     def persistent_id(self, obj):

/opt/conda/lib/python3.7/pickle.py in save_reduce(self, func, args, state, listitems, dictitems, obj)
    660 
    661         if state is not None:
--> 662             save(state)
    663             write(BUILD)
    664 

/opt/conda/lib/python3.7/pickle.py in save(self, obj, save_persistent_id)
    502         f = self.dispatch.get(t)
    503         if f is not None:
--> 504             f(self, obj) # Call unbound method with explicit self
    505             return
    506 

/opt/conda/lib/python3.7/site-packages/dill/_dill.py in save_module_dict(pickler, obj)
    931             # we only care about session the first pass thru
    932             pickler._session = False
--> 933         StockPickler.save_dict(pickler, obj)
    934         log.info(""# D2"")
    935     return

/opt/conda/lib/python3.7/pickle.py in save_dict(self, obj)
    857 
    858         self.memoize(obj)
--> 859         self._batch_setitems(obj.items())
    860 
    861     dispatch[dict] = save_dict

/opt/conda/lib/python3.7/pickle.py in _batch_setitems(self, items)
    883                 for k, v in tmp:
    884                     save(k)
--> 885                     save(v)
    886                 write(SETITEMS)
    887             elif n:

/opt/conda/lib/python3.7/pickle.py in save(self, obj, save_persistent_id)
    522             reduce = getattr(obj, ""__reduce_ex__"", None)
    523             if reduce is not None:
--> 524                 rv = reduce(self.proto)
    525             else:
    526                 reduce = getattr(obj, ""__reduce__"", None)

TypeError: can't pickle Tokenizer objects
```

"
https://github.com/huggingface/datasets/issues/664,"load_dataset from local squad.py, raise error: TypeError: 'NoneType' object is not callable ","['Hi !\r\nThanks for reporting.\r\nIt looks like no object inherits from `datasets.GeneratorBasedBuilder` (or more generally from `datasets.DatasetBuilder`) in your script.\r\n\r\nCould you check that there exist at least one dataset builder class ?'
 'Hi @xixiaoyao did you manage to fix your issue ?' 'No activity, closing']","
version: 1.0.2

```
train_dataset  = datasets.load_dataset('squad') 
```

The above code can works. However, when I download the squad.py from your server, and saved as `my_squad.py` to local. I run followings raise errors.
```
train_dataset  = datasets.load_dataset('./my_squad.py')                                                                                                
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-28-25a84b4d1581> in <module>
----> 1 train_dataset  = nlp.load_dataset('./my_squad.py')

/opt/conda/lib/python3.7/site-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, save_infos, script_version, **config_kwargs)
    602         hash=hash,
    603         features=features,
--> 604         **config_kwargs,
    605     )
    606 

TypeError: 'NoneType' object is not callable
"
https://github.com/huggingface/datasets/issues/657,Squad Metric Description & Feature Mismatch,"['Thanks for reporting !\r\nThere indeed a mismatch between the features and the kwargs description\r\n\r\nI believe `answer_start` was added to match the squad dataset format for consistency, even though it is not used in the metric computation. I think I\'d rather keep it this way, so that you can just give `references=squad[""answers""]` to `.compute()`.\r\nMaybe we can just fix the description then.'
 'But then providing the `answer_start` becomes mandatory since the format of the features is checked against the one provided in the squad [file](https://github.com/huggingface/datasets/pull/658/files).']",The [description](https://github.com/huggingface/datasets/blob/master/metrics/squad/squad.py#L39) doesn't mention `answer_start` in squad. However the `datasets.features` require [it](https://github.com/huggingface/datasets/blob/master/metrics/squad/squad.py#L68). It's also not used in the evaluation.
https://github.com/huggingface/datasets/issues/651,Problem with JSON dataset format,"['Currently the `json` dataset doesn\'t support this format unfortunately.\r\nHowever you could load it with\r\n```python\r\nfrom datasets import Dataset\r\nimport pandas as pd\r\n\r\ndf = pd.read_json(""path_to_local.json"", orient=""index"")\r\ndataset = Dataset.from_pandas(df)\r\n```'
 'or you can make a custom dataset script as explained in doc here: https://huggingface.co/docs/datasets/add_dataset.html']","I have a local json dataset with the following form.

{
    'id01234': {'key1': value1, 'key2': value2, 'key3': value3},
    'id01235': {'key1': value1, 'key2': value2, 'key3': value3},
    .
    .
    .
    'id09999': {'key1': value1, 'key2': value2, 'key3': value3}
}
Note that instead of a list of records it's basically a dictionary of key value pairs with the keys being the record_ids and the values being the corresponding record.

Reading this with json:

```
data = datasets.load('json', data_files='path_to_local.json')
```
Throws an error and asks me to chose a field. What's the right way to handle this?"
https://github.com/huggingface/datasets/issues/650,dummy data testing can't test datasets using `dl_manager.extract` in `_split_generators`,"['Hi :) \r\nIn your dummy data zip file you can just have `subset000.xz` as directories instead of compressed files.\r\nLet me know if it helps'
 ""Thanks for your comment @lhoestq ,\r\nJust for confirmation, changing dummy data like this won't make dummy test test the functionality to extract `subsetxxx.xz` but actually kind of circumvent it. But since we will test the real data so it is ok ?""
 ""Yes it's fine for now. We plan to add a job for slow tests.\r\nAnd at one point we'll also do another pass on the dummy data handling and consider extracting files.""
 'Thanks for the confirmation.\r\nAlso the suggestion works. Thank you.']","Hi, I recently want to add a dataset whose source data is like this
```
openwebtext.tar.xz
  |__ openwebtext
         |__subset000.xz
         |     |__ ....txt
         |     |__ ....txt
         |     ...
         |__ subset001.xz
         |
         ....
```
So I wrote `openwebtext.py` like this
```
 def _split_generators(self, dl_manager):
        dl_dir = dl_manager.download_and_extract(_URL)
        owt_dir = os.path.join(dl_dir, 'openwebtext')
        subset_xzs = [
            os.path.join(owt_dir, file_name) for file_name in os.listdir(owt_dir) if file_name.endswith('xz') # filter out ...xz.lock
        ]
        ex_dirs = dl_manager.extract(subset_xzs, num_proc=round(os.cpu_count()*0.75))
        nested_txt_files = [ 
          [ 
            os.path.join(ex_dir,txt_file_name) for txt_file_name in os.listdir(ex_dir) if txt_file_name.endswith('txt')
          ] for ex_dir in ex_dirs
        ]
        txt_files = chain(*nested_txt_files)
        return [
            datasets.SplitGenerator(
                name=datasets.Split.TRAIN, gen_kwargs={""txt_files"": txt_files}
            ),
        ]
```
All went good, I can load and use real openwebtext, except when I try to test with dummy data. The problem is  `MockDownloadManager.extract` do nothing, so `ex_dirs = dl_manager.extract(subset_xzs)` won't decompress `subset_xxx.xz`s for me.

How should I do ? Or you can modify `MockDownloadManager` to make it like a real `DownloadManager` ?"
https://github.com/huggingface/datasets/issues/649,Inconsistent behavior in map,"[""Thanks for reporting !\r\n\r\nThis issue must have appeared when we refactored type inference in `nlp`\r\nBy default the library tries to keep the same feature types when applying `map` but apparently it has troubles with nested structures. I'll try to fix that next week""]","I'm observing inconsistent behavior when applying .map(). This happens specifically when I'm incrementally adding onto a feature that is a nested dictionary. Here's a simple example that reproduces the problem.

```python
import datasets

# Dataset with a single feature called 'field' consisting of two examples
dataset = datasets.Dataset.from_dict({'field': ['a', 'b']})
print(dataset[0])
# outputs
{'field': 'a'}

# Map this dataset to create another feature called 'otherfield', which is a dictionary containing a key called 'capital'
dataset = dataset.map(lambda example: {'otherfield': {'capital': example['field'].capitalize()}})
print(dataset[0])
# output is okay
{'field': 'a', 'otherfield': {'capital': 'A'}}

# Now I want to map again to modify 'otherfield', by adding another key called 'append_x' to the dictionary under 'otherfield'
print(dataset.map(lambda example: {'otherfield': {'append_x': example['field'] + 'x'}})[0])
# printing out the first example after applying the map shows that the new key 'append_x' doesn't get added
# it also messes up the value stored at 'capital'
{'field': 'a', 'otherfield': {'capital': None}}

# Instead, I try to do the same thing by using a different mapped fn
print(dataset.map(lambda example:  {'otherfield': {'append_x': example['field'] + 'x', 'capital': example['otherfield']['capital']}})[0])
# this preserves the value under capital, but still no 'append_x'
{'field': 'a', 'otherfield': {'capital': 'A'}}

# Instead, I try to pass 'otherfield' to remove_columns
print(dataset.map(lambda example:  {'otherfield': {'append_x': example['field'] + 'x', 'capital': example['otherfield']['capital']}}, remove_columns=['otherfield'])[0])
# this still doesn't fix the problem
{'field': 'a', 'otherfield': {'capital': 'A'}}

# Alternately, here's what happens if I just directly map both 'capital' and 'append_x' on a fresh dataset.

# Recreate the dataset
dataset = datasets.Dataset.from_dict({'field': ['a', 'b']})
# Now map the entire 'otherfield' dict directly, instead of incrementally as before
print(dataset.map(lambda example:  {'otherfield': {'append_x': example['field'] + 'x', 'capital': example['field'].capitalize()}})[0])
# This looks good!
{'field': 'a', 'otherfield': {'append_x': 'ax', 'capital': 'A'}}
```

This might be a new issue, because I didn't see this behavior in the `nlp` library. 

Any help is appreciated!"
https://github.com/huggingface/datasets/issues/648,offset overflow when multiprocessing batched map on large datasets.,"['This should be fixed with #645 '
 'Feel free to re-open if it still occurs']","It only happened when ""multiprocessing"" + ""batched"" + ""large dataset"" at the same time.

```
def bprocess(examples):
  examples['len'] = []
  for text in examples['text']:
    examples['len'].append(len(text))
  return examples
wiki.map(brpocess, batched=True, num_proc=8)
```
```
---------------------------------------------------------------------------
RemoteTraceback                           Traceback (most recent call last)
RemoteTraceback: 
""""""
Traceback (most recent call last):
  File ""/home/yisiang/miniconda3/envs/ml/lib/python3.7/multiprocessing/pool.py"", line 121, in worker
    result = (True, func(*args, **kwds))
  File ""/home/yisiang/datasets/src/datasets/arrow_dataset.py"", line 153, in wrapper
    out: Union[""Dataset"", ""DatasetDict""] = func(self, *args, **kwargs)
  File ""/home/yisiang/datasets/src/datasets/fingerprint.py"", line 163, in wrapper
    out = func(self, *args, **kwargs)
  File ""/home/yisiang/datasets/src/datasets/arrow_dataset.py"", line 1486, in _map_single
    batch = self[i : i + batch_size]
  File ""/home/yisiang/datasets/src/datasets/arrow_dataset.py"", line 1071, in __getitem__
    format_kwargs=self._format_kwargs,
  File ""/home/yisiang/datasets/src/datasets/arrow_dataset.py"", line 972, in _getitem
    data_subset = self._data.take(indices_array)
  File ""pyarrow/table.pxi"", line 1145, in pyarrow.lib.Table.take
  File ""/home/yisiang/miniconda3/envs/ml/lib/python3.7/site-packages/pyarrow/compute.py"", line 268, in take
    return call_function('take', [data, indices], options)
  File ""pyarrow/_compute.pyx"", line 298, in pyarrow._compute.call_function
  File ""pyarrow/_compute.pyx"", line 192, in pyarrow._compute.Function.call
  File ""pyarrow/error.pxi"", line 122, in pyarrow.lib.pyarrow_internal_check_status
  File ""pyarrow/error.pxi"", line 84, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: offset overflow while concatenating arrays
""""""

The above exception was the direct cause of the following exception:

ArrowInvalid                              Traceback (most recent call last)
 in 
     30   owt = datasets.load_dataset('/home/yisiang/datasets/datasets/openwebtext/openwebtext.py', cache_dir='./datasets')['train']
     31   print('load/create data from OpenWebText Corpus for ELECTRA')
---> 32   e_owt = ELECTRAProcessor(owt, apply_cleaning=False).map(cache_file_name=f""electra_owt_{c.max_length}.arrow"")
     33   dsets.append(e_owt)
     34 

~/Reexamine_Attention/electra_pytorch/_utils/utils.py in map(self, **kwargs)
    126       writer_batch_size=10**4,
    127       num_proc=num_proc,
--> 128       **kwargs
    129     )
    130 

~/hugdatafast/hugdatafast/transform.py in my_map(self, *args, **kwargs)
     21     if not cache_file_name.endswith('.arrow'): cache_file_name += '.arrow'
     22     if '/' not in cache_file_name: cache_file_name = os.path.join(self.cache_directory(), cache_file_name)
---> 23   return self.map(*args, cache_file_name=cache_file_name, **kwargs)
     24 
     25 @patch

~/datasets/src/datasets/arrow_dataset.py in map(self, function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint)
   1285                 logger.info(""Spawning {} processes"".format(num_proc))
   1286                 results = [pool.apply_async(self.__class__._map_single, kwds=kwds) for kwds in kwds_per_shard]
-> 1287                 transformed_shards = [r.get() for r in results]
   1288                 logger.info(""Concatenating {} shards from multiprocessing"".format(num_proc))
   1289                 result = concatenate_datasets(transformed_shards)

~/datasets/src/datasets/arrow_dataset.py in (.0)
   1285                 logger.info(""Spawning {} processes"".format(num_proc))
   1286                 results = [pool.apply_async(self.__class__._map_single, kwds=kwds) for kwds in kwds_per_shard]
-> 1287                 transformed_shards = [r.get() for r in results]
   1288                 logger.info(""Concatenating {} shards from multiprocessing"".format(num_proc))
   1289                 result = concatenate_datasets(transformed_shards)

~/miniconda3/envs/ml/lib/python3.7/multiprocessing/pool.py in get(self, timeout)
    655             return self._value
    656         else:
--> 657             raise self._value
    658 
    659     def _set(self, i, obj):

ArrowInvalid: offset overflow while concatenating arrays
```"
https://github.com/huggingface/datasets/issues/647,Cannot download dataset_info.json,"[""Thanks for reporting !\r\nWe should add support for servers without internet connection indeed\r\nI'll do that early next week""
 'Thanks, @lhoestq !\r\nPlease let me know when it is available. '
 'Right now the recommended way is to create the dataset on a server with internet connection and then to save it and copy the serialized dataset to the server without internet connection.'
 '#652 should allow you to load text/json/csv/pandas datasets without an internet connection **IF** you\'ve the dataset script locally.\r\n\r\nExample: \r\nIf you have `datasets/text/text.py` locally, then you can do `load_dataset(""./datasets/text"", data_files=...)`']","I am running my job on a cloud server where does not provide for connections from the standard compute nodes to outside resources. Hence, when I use `dataset.load_dataset()` to load data, I got an error like this:

```
ConnectionError: Couldn't reach https://storage.googleapis.com/huggingface-nlp/cache/datasets/text/default-53ee3045f07ba8ca/0.0.0/dataset_info.json
```

I tried to open this link manually, but I cannot access this file. How can I download this file and pass it through `dataset.load_dataset()` manually?

Versions:
Python version 3.7.3
PyTorch version 1.6.0
TensorFlow version 2.3.0
datasets version: 1.0.1 
"
https://github.com/huggingface/datasets/issues/643,Caching processed dataset at wrong folder,"['Thanks for reporting !\r\nIt uses a temporary file to write the data.\r\nHowever it looks like the temporary file is not placed in the right directory during the processing'
 'Well actually I just tested and the temporary file is placed in the same directory, so it should work as expected.\r\nWhich version of `datasets` are you using ?'
 '`datasets-1.0.1`\r\nHere you can reproduce it here:\r\nhttps://colab.research.google.com/drive/1O0KcepTFsmpkBbrbLLMq42iwTKmQh8d5?usp=sharing\r\n'
 'It looks like a pyarrow issue with google colab.\r\nFor some reason this code increases the disk usage of google colab while it actually writes into google drive:\r\n\r\n```python\r\nimport pyarrow as pa\r\n\r\nstream = pa.OSFile(""/content/drive/My Drive/path/to/file.arrow"", ""wb"")\r\nwriter = pa.RecordBatchStreamWriter(stream, schema=pa.schema({""text"": pa.string()}))\r\nwriter.write_table(pa.Table.from_pydict({""text"": [""a""*511 + ""\\n""] * ((1 << 30) // 512)}))  # 1GiB\r\nwriter.close()\r\nstream.close()\r\n```\r\n\r\nMoreover if I `rm` the file on google drive, it frees disk space on google colab.'
 ""It looks like replacing `pa.OSFile` by `open` fixes it, I'm going to open a PR""
 'Ok. Thank you so much!'
 ""Actually I did more tests it doesn't >.<\r\nI'll let you know if I find a way to fix that""
 'Actually I also have the issue when writing a regular text file\r\n\r\n```python\r\nf = open(""/content/drive/My Drive/path/to/file"", ""w"")\r\nf.write((""a""*511 + ""\\n"") * ((1 << 30) // 512))  # 1GiB\r\nf.close()\r\n```\r\n\r\nIs that supposed to happen ?'
 ""The code you wrote should write a 1GB file in the Google Drive folder. Doesn't it? ""
 'Yes it does, but the disk usage of google colab also increases by 1GB'
 'I could check it and as you say as I write to te Drive disk the colab disk also increases...'
 'To reproduce it: \r\n```bash\r\n!df -h | grep sda1\r\n```\r\n```python\r\nf = open(""/content/drive/My Drive/test_to_remove.txt"", ""w"")\r\nf.write((""a""*511 + ""\\n"") * ((1 << 30) // 512))  # 1GiB\r\nf.write((""a""*511 + ""\\n"") * ((1 << 30) // 512))  # 1GiB\r\nf.close()\r\n```\r\n```bash\r\n!ls -lh /content/drive/My\\ Drive/test_to_remove.txt\r\n\r\n!df -h | grep sda1\r\n\r\n!rm -rf /content/drive/My\\ Drive/test_to_remove.txt\r\n\r\n```\r\n[Colab](https://colab.research.google.com/drive/1D0UiweCYQwwWZ65EEhuqqbaDDbhJYXfm?usp=sharing)\r\n\r\n\r\n']","Hi guys, I run this on my Colab (PRO):

```python
from datasets import load_dataset
dataset = load_dataset('text', data_files='/content/corpus.txt', cache_dir='/content/drive/My Drive', split='train')

def encode(examples):
  return tokenizer(examples['text'], truncation=True, padding='max_length')

dataset = dataset.map(encode, batched=True)
```
The file is about 4 GB, so I cannot process it on the Colab HD because there is no enough space. So I decided to mount my Google Drive fs and do it on it.
The dataset is cached in the right place but by processing it (applying `encode` function) seems to use a different folder because Colab HD starts to grow and it crashes when it should be done in the Drive fs.

What gets me crazy, it prints it is processing/encoding the dataset in the right folder:
```
Testing the mapped function outputs
Testing finished, running the mapping function on the dataset
Caching processed dataset at /content/drive/My Drive/text/default-ad3e69d6242ee916/0.0.0/7e13bc0fa76783d4ef197f079dc8acfe54c3efda980f2c9adfab046ede2f0ff7/cache-b16341780a59747d.arrow
```"
https://github.com/huggingface/datasets/issues/638,GLUE/QQP dataset: NonMatchingChecksumError,"[""Hi ! Sure I'll take a look""]","Hi @lhoestq , I know you are busy and there are also other important issues. But if this is easy to be fixed, I am shamelessly wondering if you can give me some help , so I can evaluate my models and restart with my developing cycle asap. 😚

datasets version: editable install of master at 9/17

`datasets.load_dataset('glue','qqp', cache_dir='./datasets')`

```
Downloading and preparing dataset glue/qqp (download: 57.73 MiB, generated: 107.02 MiB, post-processed: Unknown size, total: 164.75 MiB) to ./datasets/glue/qqp/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4...
---------------------------------------------------------------------------
NonMatchingChecksumError                  Traceback (most recent call last)
 in 
----> 1 datasets.load_dataset('glue','qqp', cache_dir='./datasets')

~/datasets/src/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, save_infos, script_version, **config_kwargs)
    609         download_config=download_config,
    610         download_mode=download_mode,
--> 611         ignore_verifications=ignore_verifications,
    612     )
    613 

~/datasets/src/datasets/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, **download_and_prepare_kwargs)
    467                     if not downloaded_from_gcs:
    468                         self._download_and_prepare(
--> 469                             dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
    470                         )
    471                     # Sync info

~/datasets/src/datasets/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)
    527         if verify_infos:
    528             verify_checksums(
--> 529                 self.info.download_checksums, dl_manager.get_recorded_sizes_checksums(), ""dataset source files""
    530             )
    531 

~/datasets/src/datasets/utils/info_utils.py in verify_checksums(expected_checksums, recorded_checksums, verification_name)
     37     if len(bad_urls) > 0:
     38         error_msg = ""Checksums didn't match"" + for_verification_name + "":\n""
---> 39         raise NonMatchingChecksumError(error_msg + str(bad_urls))
     40     logger.info(""All the checksums matched successfully"" + for_verification_name)
     41 

NonMatchingChecksumError: Checksums didn't match for dataset source files:
['https://dl.fbaipublicfiles.com/glue/data/QQP-clean.zip']
```"
https://github.com/huggingface/datasets/issues/633,Load large text file for LM pre-training resulting in OOM,"['Not sure what could cause that on the `datasets` side. Could this be a `Trainer` issue ? cc @julien-c @sgugger ?'
 'There was a memory leak issue fixed recently in master. You should install from source and see if it fixes your problem.'
 '@lhoestq @sgugger Thanks for your comments. I have install from source code as you told, but the problem is still there.\r\nTo reproduce the issue, just replace [these lines](https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_language_modeling.py#L241-L258) with: \r\n(load_dataset and DataCollatorForDatasetsLanguageModeling as [above mentioned](https://github.com/huggingface/datasets/issues/633#issue-702440484))\r\n```python\r\n    dataset = load_dataset(""bookcorpus"")\r\n    dataset = dataset.train_test_split(test_size=0.1)\r\n    train_dataset = dataset[\'train\']\r\n    eval_dataset = dataset[\'test\'] if training_args.do_eval else None\r\n\r\n    data_collator = DataCollatorForDatasetsLanguageModeling(\r\n        tokenizer=tokenizer,\r\n        mlm=data_args.mlm,\r\n        mlm_probability=data_args.mlm_probability,\r\n        block_size=data_args.block_size\r\n    )\r\n```\r\nand run by:\r\n```bash\r\npython run_language_modeling.py\r\n--output_dir=output \\\r\n--model_type=bert \\\r\n--model_name_or_path=bert-base-uncased \\\r\n--do_train \\\r\n--do_eval \\\r\n--mlm \r\n```'
 'Same here. Pre-training on wikitext-103 to do some test. At the end of the training it takes 32GB of RAM + ~30GB of SWAP. I installed dataset==1.1.0, not built from source. I will try uninstalling and building from source when it finish.'
 'This seems to be on the `transformers` library side.\r\n\r\nIf you have more informations (pip env) or even better, a colab reproducing the error we can investigate.'
 ""It seems like it's solved with freshed versions of transformers. I have tried to replicate the error doing a fresh pip install transformers & datasets on colab and the error doesn't continue. On colab it keeps stable on 5GB! (Y)\r\n\r\nEdit: **Thanks for your great work**. Have a good day.""
 '@gaceladri witch version transformers and datasets  are you using now? I want to try again. Thanks.'
 'transformers==3.3.1\r\ndatasets==1.1.0\r\ntokenizers==0.8.1rc2\r\n'
 'doing some modifications to mobilebert\r\nhttps://colab.research.google.com/drive/1ba09ZOpyHGAOQLcsxiQAHRXl10qnMU5o?usp=sharing '
 'It does not happen to me anymore. Can we close? @leethu2012 '
 ""It's happening to me again. After 4 hours of pre-training, my ram memory gets full and the kernel dies. I am using the last transformers version as today. 4.4.0 and the last version of datasets 1.2.1, both installed from master. The memory consumption keeps increasing.""
 'It looks like it is something from pytorch/python itself :face_with_head_bandage:  https://github.com/pytorch/pytorch/issues/13246 '
 ""Thanks for the investigation @gaceladri \r\n\r\nApparently this happens when `num_workers>0` and has to do with objects being copied-on-write.\r\nDid you try setting num_workers to 0 @gaceladri ?\r\nIf the issue doesn't happen with `num_workers=0` then this would confirm that it's indeed related to this python/pytorch issue.\r\n\r\nSince a `Dataset` object is a wrapper of a pyarrow Table, we should investigate if the data being copied comes from the Table itself or from metadata in the `Dataset` object. If it comes from the metadata in the `Dataset` object, we should be able to implement a workaround. But if it comes from the Table, we'll need to see with the pyarrow team what we can do... ""
 '@lhoestq I have tried and it keeps increasing also with `dataloader_num_workers=0`'
 ""Hmmm so this might come from another issue...\r\nSince it doesn't seem to be related to multiprocessing it should be easier to investigate though.\r\nDo you have some ideas @gaceladri ?""
 '@lhoestq I looked quickly to a previously spoted bug in my env wandb /sdk/interface/interface.py, because sometimes when I load the dataset I got a multiprocessing error at line 510 in wandb...interface.py\r\n\r\nThis bug is reported here https://github.com/huggingface/datasets/issues/847\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n<timed eval> in <module>\r\n\r\n~/anaconda3/envs/tfm/lib/python3.6/site-packages/transformers/trainer.py in train(self, model_path, trial)\r\n    877             print(len(epoch_iterator))\r\n    878 \r\n--> 879             for step, inputs in enumerate(epoch_iterator):\r\n    880 \r\n    881                 start_step = time.time()\r\n\r\n~/anaconda3/envs/tfm/lib/python3.6/site-packages/torch/utils/data/dataloader.py in __next__(self)\r\n    433         if self._sampler_iter is None:\r\n    434             self._reset()\r\n--> 435         data = self._next_data()\r\n    436         self._num_yielded += 1\r\n    437         if self._dataset_kind == _DatasetKind.Iterable and \\\r\n\r\n~/anaconda3/envs/tfm/lib/python3.6/site-packages/torch/utils/data/dataloader.py in _next_data(self)\r\n   1083             else:\r\n   1084                 del self._task_info[idx]\r\n-> 1085                 return self._process_data(data)\r\n   1086 \r\n   1087     def _try_put_index(self):\r\n\r\n~/anaconda3/envs/tfm/lib/python3.6/site-packages/torch/utils/data/dataloader.py in _process_data(self, data)\r\n   1109         self._try_put_index()\r\n   1110         if isinstance(data, ExceptionWrapper):\r\n-> 1111             data.reraise()\r\n   1112         return data\r\n   1113 \r\n\r\n~/anaconda3/envs/tfm/lib/python3.6/site-packages/torch/_utils.py in reraise(self)\r\n    426             # have message field\r\n    427             raise self.exc_type(message=msg)\r\n--> 428         raise self.exc_type(msg)\r\n    429 \r\n    430 \r\n\r\nAssertionError: Caught AssertionError in DataLoader worker process 0.\r\nOriginal Traceback (most recent call last):\r\n  File ""/home/ad/anaconda3/envs/tfm/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py"", line 198, in _worker_loop\r\n    data = fetcher.fetch(index)\r\n  File ""/home/ad/anaconda3/envs/tfm/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py"", line 44, in fetch\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n  File ""/home/ad/anaconda3/envs/tfm/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py"", line 44, in <listcomp>\r\n    data = [self.dataset[idx] for idx in possibly_batched_index]\r\n  File ""/home/ad/anaconda3/envs/tfm/lib/python3.6/site-packages/datasets/arrow_dataset.py"", line 1083, in __getitem__\r\n    format_kwargs=self._format_kwargs,\r\n  File ""/home/ad/anaconda3/envs/tfm/lib/python3.6/site-packages/datasets/arrow_dataset.py"", line 1070, in _getitem\r\n    format_kwargs=format_kwargs,\r\n  File ""/home/ad/anaconda3/envs/tfm/lib/python3.6/site-packages/datasets/arrow_dataset.py"", line 886, in _convert_outputs\r\n    v = map_nested(command, v, **map_nested_kwargs)\r\n  File ""/home/ad/anaconda3/envs/tfm/lib/python3.6/site-packages/datasets/utils/py_utils.py"", line 216, in map_nested\r\n    return function(data_struct)\r\n  File ""/home/ad/anaconda3/envs/tfm/lib/python3.6/site-packages/datasets/arrow_dataset.py"", line 847, in command\r\n    return torch.tensor(x, **format_kwargs)\r\n  File ""/home/ad/anaconda3/envs/tfm/lib/python3.6/warnings.py"", line 101, in _showwarnmsg\r\n    _showwarnmsg_impl(msg)\r\n  File ""/home/ad/anaconda3/envs/tfm/lib/python3.6/warnings.py"", line 30, in _showwarnmsg_impl\r\n    file.write(text)\r\n  File ""/home/ad/anaconda3/envs/tfm/lib/python3.6/site-packages/wandb/sdk/lib/redirect.py"", line 100, in new_write\r\n    cb(name, data)\r\n  File ""/home/ad/anaconda3/envs/tfm/lib/python3.6/site-packages/wandb/sdk/wandb_run.py"", line 729, in _console_callback\r\n    self._backend.interface.publish_output(name, data)\r\n  File ""/home/ad/anaconda3/envs/tfm/lib/python3.6/site-packages/wandb/sdk/interface/interface.py"", line 186, in publish_output\r\n    self._publish_output(o)\r\n  File ""/home/ad/anaconda3/envs/tfm/lib/python3.6/site-packages/wandb/sdk/interface/interface.py"", line 191, in _publish_output\r\n    self._publish(rec)\r\n  File ""/home/ad/anaconda3/envs/tfm/lib/python3.6/site-packages/wandb/sdk/interface/interface.py"", line 510, in _publish\r\n    if self._process and not self._process.is_alive():\r\n  File ""/home/ad/anaconda3/envs/tfm/lib/python3.6/multiprocessing/process.py"", line 134, in is_alive\r\n    assert self._parent_pid == os.getpid(), \'can only test a child process\'\r\nAssertionError: can only test a child process\r\n```\r\n\r\nMy workaround was to just comment those lines without looking to much into consecuences:\r\n\r\n```\r\ndef _publish(self, record: pb.Record, local: bool = None) -> None:\r\n        #if self._process and not self._process.is_alive():\r\n        #    raise Exception(""The wandb backend process has shutdown"")\r\n```\r\n\r\nIt worked so far... I need to try running without wandb and see if it could be causing something wrong with multiprocessing. I am going to try to launch the training setting wandb to false and I will let you know again.'
 '@lhoestq But despite this, I got lost into the [class Dataset()](https://huggingface.co/docs/datasets/_modules/datasets/arrow_dataset.html#Dataset) reading the pyarrow files.\r\n\r\nEdit: but you should be rigth, that it does not have to be related to multiprocessing since it keeps happening when `num_workers=0` '
 'Or maybe wandb uses multiprocessing ? One process for wandb logging and one for actual training ? If this is the case then even setting `num_workers=0` would cause the process to be forked for wandb and therefore cause the memory issue.'
 '@lhoestq could be, but if we set wandb to false this should not happen. I am going to try.'
 '@lhoestq It keeps happening. I have uninstalled wandb from my env, setted `%env WANDB_DISABLED=true` on my notebook, and commented this func:\r\n\r\n```\r\ndef get_available_reporting_integrations():\r\n    integrations = []\r\n    if is_azureml_available():\r\n        integrations.append(""azure_ml"")\r\n    if is_comet_available():\r\n        integrations.append(""comet_ml"")\r\n    if is_mlflow_available():\r\n        integrations.append(""mlflow"")\r\n    if is_tensorboard_available():\r\n        integrations.append(""tensorboard"")\r\n    # if is_wandb_available():\r\n    #     integrations.append(""wandb"")\r\n    return integrations\r\n```\r\nAs a fast test and it keeps increasing the ram memory. Wandb could not be the blameworthy here.'
 ""Thanks for checking @gaceladri . Let's investigate the single process setting then.\r\nIf you have some sort of colab notebook with a minimal code example that shows this behavior feel free to share it @gaceladri so that we can play around with it to find what causes this. Otherwise I'll probably try to reproduce on my side at one point""
 '@lhoestq sure. Here you have https://colab.research.google.com/drive/1ba09ZOpyHGAOQLcsxiQAHRXl10qnMU5o?usp=sharing let me know if the link works and it reproduces the issue. To me, it reproduces the issue, since if you start the training the ram memory keeps increasing.\r\n\r\nLet me know. Thanks!'
 'Could the bug be comming from tokenizers?\r\n\r\nI got this warning at the terminal from my jupyter notebook: \r\n```\r\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\r\nTo disable this warning, you can either:\r\n\t- Avoid using `tokenizers` before the fork if possible\r\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\r\n```'
 ""I've never experienced memory issues with tokenizers so I don't know\r\nCc @n1t0 are you aware of any issue that would cause memory to keep increasing when the tokenizer is used in the Data Collator for language modeling ?""
 '@lhoestq Thanks for pointing to n1t0, just to clarify. That warning was doing fine-tuning, without collator:\r\n```\r\n\r\nfrom datasets import load_dataset, load_metric\r\nimport numpy as np\r\n\r\nGLUE_TASKS = [\r\n    ""cola"",\r\n    ""mnli"",\r\n    ""mnli-mm"",\r\n    ""mrpc"",\r\n    ""qnli"",\r\n    ""qqp"",\r\n    ""rte"",\r\n    ""sst2"",\r\n    ""stsb"",\r\n    ""wnli"",\r\n]\r\ntask = ""mnli""\r\nactual_task = ""mnli"" if task == ""mnli-mm"" else task\r\ndataset = load_dataset(""glue"", actual_task)\r\nmetric = load_metric(""glue"", actual_task)\r\nbatch_size = 16\r\nattention_type = ""linear""\r\n\r\nfrom transformers.models.mobilebert_mod import (\r\n    MobileBertForSequenceClassification,\r\n    MobileBertTokenizerFast,\r\n)\r\nfrom transformers.models.mobilebert_mod.configuration_mobilebert import (\r\n    MobileBertConfigMod,\r\n)\r\nfrom transformers import TrainingArguments, Trainer\r\n\r\nnum_labels = 3 if task.startswith(""mnli"") else 1 if task == ""stsb"" else 2\r\ntokenizer = MobileBertTokenizerFast.from_pretrained(\r\n    ""/media/ad/00b5422b-9d54-4449-8b5d-08eab5cdac8c/training_trfm/big_linear_layerdrop_shared/checkpoint-23000/"",\r\n    max_len=512,\r\n)\r\nmodel = MobileBertForSequenceClassification.from_pretrained(\r\n    ""/media/ad/00b5422b-9d54-4449-8b5d-08eab5cdac8c/training_trfm/big_linear_layerdrop_shared/checkpoint-23000/"",\r\n    num_labels=num_labels,\r\n)\r\nprint(model.num_parameters())\r\n\r\ntask_to_keys = {\r\n    ""cola"": (""sentence"", None),\r\n    ""mnli"": (""premise"", ""hypothesis""),\r\n    ""mnli-mm"": (""premise"", ""hypothesis""),\r\n    ""mrpc"": (""sentence1"", ""sentence2""),\r\n    ""qnli"": (""question"", ""sentence""),\r\n    ""qqp"": (""question1"", ""question2""),\r\n    ""rte"": (""sentence1"", ""sentence2""),\r\n    ""sst2"": (""sentence"", None),\r\n    ""stsb"": (""sentence1"", ""sentence2""),\r\n    ""wnli"": (""sentence1"", ""sentence2""),\r\n}\r\n\r\nsentence1_key, sentence2_key = task_to_keys[task]\r\nif sentence2_key is None:\r\n    print(f""Sentence: {dataset[\'train\'][0][sentence1_key]}"")\r\nelse:\r\n    print(f""Sentence 1: {dataset[\'train\'][0][sentence1_key]}"")\r\n    print(f""Sentence 2: {dataset[\'train\'][0][sentence2_key]}"")\r\n\r\n\r\ndef preprocess_function(examples):\r\n    if sentence2_key is None:\r\n        return tokenizer(examples[sentence1_key], truncation=True)\r\n    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True)\r\n\r\n\r\nencoded_dataset = dataset.map(preprocess_function, batched=True)\r\nmetric_name = (\r\n    ""pearson""\r\n    if task == ""stsb""\r\n    else ""matthews_correlation""\r\n    if task == ""cola""\r\n    else ""accuracy""\r\n)\r\n\r\nargs = TrainingArguments(\r\n    f""test-glue/{task}_{attention_type}"",\r\n    evaluation_strategy=""steps"",\r\n    learning_rate=1e-5,\r\n    per_device_train_batch_size=batch_size,\r\n    per_device_eval_batch_size=batch_size,\r\n    logging_steps=200,\r\n    num_train_epochs=5,\r\n    gradient_accumulation_steps=1,\r\n    warmup_steps=10000,\r\n    fp16=True,\r\n    dataloader_num_workers=10,\r\n    weight_decay=0.1,\r\n    load_best_model_at_end=True,\r\n    metric_for_best_model=metric_name,\r\n)\r\n\r\n\r\ndef compute_metrics(eval_pred):\r\n    predictions, labels = eval_pred\r\n    if task != ""stsb"":\r\n        predictions = np.argmax(predictions, axis=1)\r\n    else:\r\n        predictions = predictions[:, 0]\r\n    return metric.compute(predictions=predictions, references=labels)\r\n\r\n\r\nvalidation_key = (\r\n    ""validation_mismatched""\r\n    if task == ""mnli-mm""\r\n    else ""validation_matched""\r\n    if task == ""mnli""\r\n    else ""validation""\r\n)\r\n\r\ntrainer = Trainer(\r\n    model,\r\n    args,\r\n    train_dataset=encoded_dataset[""train""],\r\n    eval_dataset=encoded_dataset[validation_key],\r\n    tokenizer=tokenizer,\r\n    compute_metrics=compute_metrics,\r\n)\r\n\r\ntrainer.train()\r\n```\r\n\r\nNow, I have come back to pre-training. The changes that I think I have done are: not formatting the dataset to torch: ~~`big_dataset.set_format(type=\'torch\', columns=[""text"", ""input_ids"", ""attention_mask"", ""token_type_ids""])`~~ so maybe some column is dropped and not freezed in memory and now I have not setted any validation dataset in the trainer. \r\n\r\nMy validation dataset before:\r\n```\r\nbook_corpus_eval = load_dataset(\r\n    ""bookcorpus"",\r\n    ""plain_text"",\r\n    cache_dir=""/home/ad/Desktop/bookcorpus"",\r\n    split=""train[98:99%]"",\r\n)\r\nbook_corpus_eval = book_corpus_eval.map(encode, batched=True)\r\nbook_corpus_eval.set_format(\r\n    type=""torch"", columns=[""text"", ""input_ids"", ""attention_mask"", ""token_type_ids""]\r\n)\r\n**book_corpus_eval = book_corpus_eval.select([i for i in range(1500)])**\r\n```\r\nMaybe _selecting_ or indexing the dataset before feeding it to the trainer, do something strange.\r\n\r\nMy trainer now:\r\n```\r\n\r\nbig_dataset = load_from_disk(""/home/ad/Desktop/35percent_data.arrow/"")\r\n\r\nfrom transformers import DataCollatorForWholeWordMask\r\n\r\ndata_collator = DataCollatorForWholeWordMask(\r\n    tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\r\n\r\nfrom transformers import Trainer, TrainingArguments\r\n\r\ntraining_args = TrainingArguments(\r\n    output_dir=""./big_linear_layerdrop_shared_silu_secondtry"",\r\n    overwrite_output_dir=True,\r\n    per_device_train_batch_size=60,\r\n    per_device_eval_batch_size=60,\r\n    save_steps=500,\r\n    save_total_limit=10,\r\n    logging_first_step=True,\r\n    logging_steps=100,\r\n#     evaluation_strategy=\'steps\',\r\n#     eval_steps=250,\r\n    gradient_accumulation_steps=8,\r\n    fp16=True,\r\n    dataloader_num_workers=10,\r\n    warmup_steps=15000,\r\n    learning_rate=6e-4,\r\n    adam_epsilon=1e-6,\r\n    adam_beta2=0.98,\r\n    weight_decay=0.01,\r\n    max_grad_norm=1.0,\r\n    max_steps=500000, \r\n)\r\n\r\ntrainer = Trainer(\r\n    model=model,\r\n    args=training_args,\r\n    data_collator=data_collator,\r\n    train_dataset=big_dataset,\r\n#     eval_dataset=book_corpus_eval,\r\n    tokenizer=tokenizer)\r\n\r\nimport wandb\r\nwandb.login()\r\n\r\ntrainer.train()\r\n```\r\n\r\nAnd surprisingly, the ram now keeps going up and down. The training is up now for 12h without collapse the ram. I don\'t know what could cause the leakage. :mag: \r\n\r\nEdit: I didn\'t see the swap memory, that keeps increasing. So the problem persist. '
 'Thanks for sharing your results.\r\nSo you still had the issue for fine-tuning ?\r\nAnd the issue still appears with a bare-bone dataset from an arrow file...'
 'Yes, on both cases. Fine-tuning a pre-trained model and pre-training from scratch with a local arrow file already pre-processed.']","I tried to pretrain Longformer using transformers and datasets. But I got OOM issues with loading a large text file. My script is almost like this:

```python
from datasets import load_dataset

@dataclass
class DataCollatorForDatasetsLanguageModeling(DataCollatorForLanguageModeling):
    """"""
    Data collator used for language modeling based on DataCollatorForLazyLanguageModeling
    - collates batches of tensors, honoring their tokenizer's pad_token
    - preprocesses batches for masked language modeling
    """"""

    block_size: int = 512

    def __call__(self, examples: List[dict]) -> Dict[str, torch.Tensor]:
        examples = [example['text'] for example in examples]
        batch, attention_mask = self._tensorize_batch(examples)
        if self.mlm:
            inputs, labels = self.mask_tokens(batch)
            return {""input_ids"": inputs, ""labels"": labels}
        else:
            labels = batch.clone().detach()
            if self.tokenizer.pad_token_id is not None:
                labels[labels == self.tokenizer.pad_token_id] = -100
            return {""input_ids"": batch, ""labels"": labels}

    def _tensorize_batch(self, examples: List[str]) -> Tuple[torch.Tensor, torch.Tensor]:

        if self.tokenizer._pad_token is None:
            raise ValueError(
                ""You are attempting to pad samples but the tokenizer you are using""
                f"" ({self.tokenizer.__class__.__name__}) does not have one.""
            )

        tensor_examples = self.tokenizer.batch_encode_plus(
            [ex for ex in examples if ex],
            max_length=self.block_size,
            return_tensors=""pt"",
            pad_to_max_length=True,
            return_attention_mask=True,
            truncation=True,
        )

        input_ids, attention_mask = tensor_examples[""input_ids""], tensor_examples[""attention_mask""]
        return input_ids, attention_mask

dataset = load_dataset('text', data_files='train.txt',cache_dir=""./"", , split='train')
data_collator = DataCollatorForDatasetsLanguageModeling(tokenizer=tokenizer, mlm=True, 
                      mlm_probability=0.15, block_size=tokenizer.max_len)
trainer = Trainer(model=model, args=args, data_collator=data_collator,
                      train_dataset=train_dataset, prediction_loss_only=True, )
trainer.train(model_path=model_path)
```
This train.txt is about 1.1GB and has 90k lines where each line is a sequence of 4k words. 
During training, the memory usage increased fast as the following graph and resulted in OOM before the finish of training.

![image](https://user-images.githubusercontent.com/29704017/93292112-5576b280-f817-11ea-8da2-b2db9bf35665.png)

Could you please give me any suggestions on why this happened and how to fix it?
Thanks. "
https://github.com/huggingface/datasets/issues/630,Text dataset not working with large files,"['Seems like it works when setting ```block_size=2100000000``` or something arbitrarily large though.'
 'Can you give us some stats on the data files you use as inputs?'
 'Basically ~600MB txt files(UTF-8) * 59. \r\ncontents like ```안녕하세요, 이것은 예제로 한번 말해보는 텍스트입니다. 그냥 이렇다고요.<|endoftext|>\\n```\r\n\r\nAlso, it gets stuck for a loooong time at ```Testing the mapped function outputs```, for more than 12 hours(currently ongoing)'
 'It gets stuck while doing `.map()` ? Are you using multiprocessing ?\r\nIf you could provide a code snippet it could be very useful'
 'From transformers/examples/language-modeling/run-language-modeling.py :\r\n```\r\ndef get_dataset(\r\n    args: DataTrainingArguments,\r\n    tokenizer: PreTrainedTokenizer,\r\n    evaluate: bool = False,\r\n    cache_dir: Optional[str] = None,\r\n):\r\n    file_path = args.eval_data_file if evaluate else args.train_data_file\r\n    if True:\r\n        dataset = load_dataset(""text"", data_files=glob.glob(file_path), split=\'train\', use_threads=True, \r\n        ignore_verifications=True, save_infos=True, block_size=104857600)\r\n        dataset = dataset.map(lambda ex: tokenizer(ex[""text""], add_special_tokens=True,\r\n                                                truncation=True, max_length=args.block_size), batched=True)\r\n        dataset.set_format(type=\'torch\', columns=[\'input_ids\'])\r\n        return dataset\r\n    if args.line_by_line:\r\n        return LineByLineTextDataset(tokenizer=tokenizer, file_path=file_path, block_size=args.block_size)\r\n    else:\r\n        return TextDataset(\r\n            tokenizer=tokenizer,\r\n            file_path=file_path,\r\n            block_size=args.block_size,\r\n            overwrite_cache=args.overwrite_cache,\r\n            cache_dir=cache_dir,\r\n        )\r\n```\r\n\r\nNo, I\'m not using multiprocessing.'
 ""I am not able to reproduce on my side :/\r\n\r\nCould you send the version of `datasets` and `pyarrow` you're using ?\r\nCould you try to update the lib and try again ?\r\nOr do you think you could try to reproduce it on google colab ?""
 ""Huh, weird. It's fixed on my side too.\r\nBut now ```Caching processed dataset``` is taking forever - how can I disable it? Any flags?""
 ""Right after `Caching processed dataset`, your function is applied to the dataset and there's a progress bar that shows how much time is left. How much time does it take for you ?\r\n\r\nAlso caching isn't supposed to slow down your processing. But if you still want to disable it you can do `.map(..., load_from_cache_file=False)`""
 'Ah, it’s much faster now(Takes around 15~20min). \r\nBTW, any way to set default tensor output as plain tensors with distributed training? The ragged tensors are incompatible with tpustrategy :('
 ""> Ah, it’s much faster now(Takes around 15~20min).\r\n\r\nGlad to see that it's faster now. What did you change exactly ?\r\n\r\n> BTW, any way to set default tensor output as plain tensors with distributed training? The ragged tensors are incompatible with tpustrategy :(\r\n\r\nOh I didn't know about that. Feel free to open an issue to mention that.\r\nI guess what you can do for now is set the dataset format to numpy instead of tensorflow, and use a wrapper of the dataset that converts the numpy arrays to tf tensors.\r\n\r\n""
 "">>> Glad to see that it's faster now. What did you change exactly ?\r\nI don't know, it just worked...? Sorry I couldn't be more helpful.\r\n\r\nSetting with numpy array is a great idea! Thanks.""]","```
Traceback (most recent call last):
  File ""examples/language-modeling/run_language_modeling.py"", line 333, in <module>
    main()
  File ""examples/language-modeling/run_language_modeling.py"", line 262, in main
    get_dataset(data_args, tokenizer=tokenizer, cache_dir=model_args.cache_dir) if training_args.do_train else None
  File ""examples/language-modeling/run_language_modeling.py"", line 144, in get_dataset
    dataset = load_dataset(""text"", data_files=file_path, split='train+test')
  File ""/home/ksjae/.local/lib/python3.7/site-packages/datasets/load.py"", line 611, in load_dataset
    ignore_verifications=ignore_verifications,
  File ""/home/ksjae/.local/lib/python3.7/site-packages/datasets/builder.py"", line 469, in download_and_prepare
    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
  File ""/home/ksjae/.local/lib/python3.7/site-packages/datasets/builder.py"", line 546, in _download_and_prepare
    self._prepare_split(split_generator, **prepare_split_kwargs)
  File ""/home/ksjae/.local/lib/python3.7/site-packages/datasets/builder.py"", line 888, in _prepare_split
    for key, table in utils.tqdm(generator, unit="" tables"", leave=False, disable=not_verbose):
  File ""/home/ksjae/.local/lib/python3.7/site-packages/tqdm/std.py"", line 1129, in __iter__
    for obj in iterable:
  File ""/home/ksjae/.cache/huggingface/modules/datasets_modules/datasets/text/7e13bc0fa76783d4ef197f079dc8acfe54c3efda980f2c9adfab046ede2f0ff7/text.py"", line 104, in _generate_tables
    convert_options=self.config.convert_options,
  File ""pyarrow/_csv.pyx"", line 714, in pyarrow._csv.read_csv
  File ""pyarrow/error.pxi"", line 122, in pyarrow.lib.pyarrow_internal_check_status
  File ""pyarrow/error.pxi"", line 84, in pyarrow.lib.check_status
```

**pyarrow.lib.ArrowInvalid: straddling object straddles two block boundaries (try to increase block size?)**

It gives the same message for both 200MB, 10GB .tx files but not for 700MB file.
Can't upload due to size & copyright problem. sorry."
https://github.com/huggingface/datasets/issues/629,straddling object straddles two block boundaries,"[""sorry it's an apache arrow issue.""]","I am trying to read json data (it's an array with lots of dictionaries) and getting block boundaries issue as below : 

I tried calling read_json with readOptions but no luck .

```
table = json.read_json(fn)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""pyarrow/_json.pyx"", line 246, in pyarrow._json.read_json
  File ""pyarrow/error.pxi"", line 122, in pyarrow.lib.pyarrow_internal_check_status
  File ""pyarrow/error.pxi"", line 84, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: straddling object straddles two block boundaries (try to increase block size?)
```
"
https://github.com/huggingface/datasets/issues/625,dtype of tensors should be preserved,"['Indeed we convert tensors to list to be able to write in arrow format. Because of this conversion we lose the dtype information. We should add the dtype detection when we do type inference. However it would require a bit of refactoring since currently the conversion happens before the type inference..\r\n\r\nAnd then for your information, when reading from arrow format we have to cast from arrow to numpy (which is fast since pyarrow has a numpy integration), and then to torch.\r\n\r\nHowever there\'s one thing that can help you: we make sure that the dtypes correspond to what is defined in `features`.\r\nTherefore what you can do is provide `features` in `.map(preprocess, feature=...)` to specify the output types.\r\n\r\nFor example in your case:\r\n```python\r\nfrom datasets import Features, Value, Sequence\r\n\r\nfeatures = Features({\r\n    ""input_ids"": Sequence(Value(""int32"")),\r\n    ""sembedding"": Sequence(Value(""float32""))\r\n})\r\npreprocessed_dataset = dataset.map(preprocess, features=features)\r\n\r\npreprocessed_dataset.set_format(""torch"", columns=[""input_ids"", ""sembedding""])\r\nprint(preprocessed_dataset[0][""sembedding""].dtype)\r\n# ""torch.float32""\r\n```\r\n\r\nLet me know if it helps'
 'If the arrow format is basically lists, why is the intermediate step to numpy necessary? I am a bit confused about that part.\r\n\r\nThanks for your suggestion. as I have currently implemented this, I cast to torch.Tensor in my collate_fn to save disk space (so I do not have to save padded tensors to max_len but can pad up to max batch len in collate_fn) at the cost of a bit slower processing. So for me this is not relevant anymore, but I am sure it is for others!'
 ""I'm glad you managed to figure something out :)\r\n\r\nCasting from arrow to numpy can be 100x faster than casting from arrow to list.\r\nThis is because arrow has an integration with numpy that allows it to instantiate numpy arrays with zero-copy from arrow.\r\nOn the other hand to create python lists it is slow since it has to recreate the list object by iterating through each element in python.""
 ""Ah that is interesting. I have no direct experience with arrow so I didn't know. ""
 ""I encountered a simliar issue: `datasets` converted my float numpy array to `torch.float64` tensors, while many pytorch operations require `torch.float32` inputs and it's very troublesome. \r\n\r\nI tried @lhoestq 's solution, but since it's mixed with the preprocess function, it's not very intuitive. \r\n\r\nI just want to share another possible simpler solution: directly cast the dtype of the processed dataset.\r\n\r\nNow I want to change the type of `labels` in `train_dataset` from float64 to float32, I can do this.\r\n\r\n```\r\nfrom datasets import Value, Sequence, Features\r\nfeats = train_dataset.features.copy()\r\nfeats['labels'].feature = Value(dtype='float32')\r\nfeats = Features(feats)\r\ntrain_dataset.cast_(feats)\r\n```\r\n""
 'Reopening since @bhavitvyamalik started looking into it !\r\n\r\nAlso I\'m posting here a function that could be helpful to support preserving the dtype of tensors.\r\n\r\nIt\'s used to build a pyarrow array out of a numpy array and:\r\n- it doesn\'t convert the numpy array to a python list\r\n- it keeps the precision of the numpy array for the pyarrow array\r\n- it works with multidimensional arrays (while `pa.array` can only take a 1D array as input)\r\n- it builds the pyarrow ListArray from offsets created on-the-fly and values that come from the flattened numpy array\r\n\r\n```python\r\nfrom functools import reduce\r\nfrom operator import mul\r\n\r\nimport numpy as np\r\nimport pyarrow as pa\r\n\r\ndef pa_ndarray(a):\r\n    """"""Build a PyArrow ListArray from a multidimensional NumPy array""""""\r\n    values = pa.array(a.flatten()) \r\n    for i in range(a.ndim - 1): \r\n        n_offsets = reduce(mul, a.shape[:a.ndim - i - 1], 1) \r\n        step_offsets = a.shape[a.ndim - i - 1] \r\n        offsets = pa.array(np.arange(n_offsets + 1) * step_offsets, type=pa.int32()) \r\n        values = pa.ListArray.from_arrays(offsets, values) \r\n    return values \r\n\r\nnarr = np.arange(42).reshape(7, 2, 3).astype(np.uint8)\r\nparr = pa_ndarray(narr)\r\nassert isinstance(parr, pa.Array)\r\nassert parr.type == pa.list_(pa.list_(pa.uint8()))\r\nassert narr.tolist() == parr.to_pylist()\r\n```\r\n\r\nThe only costly operation is the offsets computations. Since it doesn\'t iterate on the numpy array values this function is pretty fast.'
 '@lhoestq Have you thought about this further?\r\n\r\nWe have a use case where we\'re attempting to load data containing numpy arrays using the `datasets` library.\r\n\r\nWhen using one of the ""standard"" methods (`[Value(...)]` or `Sequence()`) we see ~200 samples processed per second during the call to `_prepare_split`. This slowdown is caused by the vast number of calls to `encode_nested_example` (each sequence is converted to a list, and each element in the sequence...). \r\n\r\nUsing the `Feature` `ArrayND` improves this somewhat to ~500/s as it now uses numpy\'s `tolist()` rather than iterating over each value in the array and converting them individually.\r\n\r\nHowever, it\'s still pretty slow and in theory it should be possible to avoid the `numpy -> python -> arrow` dance altogether. To demonstrate this, if you keep the `Feature` set to an `ArrayND` but instead return a `pa_ndarray(...)` in `_generate_examples` it skips the conversion (`return obj, False`) and hits ~11_000/s. Two orders of magnitude speed up! The problem is this then fails later on when the `ArrowWriter` tries to write the examples to disk :-( \r\n\r\nIt would be nice to have first-class support for user-defined PyArrow objects. Is this a possibility? We have _large_ datasets where even an order of magnitude difference is important so settling on the middle ~500/s is less than ideal! \r\n\r\nIs there a workaround for this or another method that should be used instead that gets  near-to or equal performance to returning PyArrow arrays?'
 'Note that manually generating the table using `pyarrow` achieves ~30_000/s'
 ""Hi !\r\n\r\nIt would be awesome to achieve this speed for numpy arrays !\r\nFor now we have to use `encode_nested_example` to convert numpy arrays to python lists since pyarrow doesn't support multidimensional numpy arrays (only 1D).\r\n\r\nMaybe let's start a new PR from your PR @bhavitvyamalik (idk why we didn't answer your PR at that time, sorry about that).\r\nBasically the idea is to allow `TypedSequence` to support numpy arrays as you did, and remove the numpy->python casting in `_cast_to_python_objects`.\r\n\r\nThis is really important since we are starting to have a focus on other modalities than text as well (audio, images).\r\n\r\nThough until then @samgd, there is another feature that may interest you and that may give you the speed you want:\r\n\r\nIn a dataset script you can subclass either a GeneratorBasedBuilder (with the `_generate_examples ` method) or an ArrowBasedBuilder if you want. the ArrowBasedBuilder allows to yield arrow data by implementing the `_generate_tables` method (it's the same as `_generate_examples` except you must yield arrow tables). Since the data are already in arrow format, it doesn't call `encode_nested_example`. Let me know if that helps.""]","After switching to `datasets` my model just broke. After a weekend of debugging, the issue was that my model could not handle the double that the Dataset provided, as it expected a float (but didn't give a warning, which seems a [PyTorch issue](https://discuss.pytorch.org/t/is-it-required-that-input-and-hidden-for-gru-have-the-same-dtype-float32/96221)). 

As a user I did not expect this bug. I have a `map` function that I call on the Dataset that looks like this:

```python
def preprocess(sentences: List[str]):
    token_ids = [[vocab.to_index(t) for t in s.split()] for s in sentences]

    sembeddings = stransformer.encode(sentences)
    print(sembeddings.dtype)
    return {""input_ids"": token_ids, ""sembedding"": sembeddings}
```

Given a list of `sentences` (`List[str]`), it converts those into token_ids on the one hand (list of lists of ints; `List[List[int]]`) and into sentence embeddings on the other (Tensor of dtype `torch.float32`). That means that I actually set the column ""sembedding"" to a tensor that I as a user expect to be a float32.

It appears though that behind the scenes, this tensor is converted into a **list**. I did not find this documented anywhere but I might have missed it. From a user's perspective this is incredibly important though, because it means you cannot do any data_type or tensor casting yourself in a mapping function! Furthermore, this can lead to issues, as was my case. 

My model expected float32 precision, which I thought `sembedding` was because that is what `stransformer.encode` outputs. But behind the scenes this tensor is first cast to a list, and when we then set its format, as below, this column is cast not to float32 but to double precision float64.

```python
dataset.set_format(type=""torch"", columns=[""input_ids"", ""sembedding""])
```

This happens because apparently there is an intermediate step of casting to a **numpy** array (?) **whose dtype creation/deduction is different from torch dtypes** (see the snippet below).  As you can see, this means that the dtype is not preserved: if I got it right, the dataset goes from torch.float32 -> list -> float64 (numpy) -> torch.float64. 

```python
import torch
import numpy as np

l = [-0.03010837361216545, -0.035979013890028, -0.016949838027358055]
torch_tensor = torch.tensor(l)
np_array = np.array(l)
np_to_torch = torch.from_numpy(np_array)

print(torch_tensor.dtype)
# torch.float32
print(np_array.dtype)
# float64
print(np_to_torch.dtype)
# torch.float64
```

This might lead to unwanted behaviour. I understand that the whole library is probably built around casting from numpy to other frameworks, so this might be difficult to solve. Perhaps `set_format` should include a `dtypes` option where for each input column the user can specify the wanted precision.

The alternative is that the user needs to cast manually after loading data from the dataset but that does not seem user-friendly, makes the dataset less portable, and might use more space in memory as well as on disk than is actually needed."
https://github.com/huggingface/datasets/issues/623,Custom feature types in `load_dataset` from CSV,"[""Currently `csv` doesn't support the `features` attribute (unlike `json`).\r\nWhat you can do for now is cast the features using the in-place transform `cast_`\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset('csv', data_files=file_dict, delimiter=';', column_names=['text', 'label'])\r\ndataset.cast_(emotion_features)\r\n```\r\n""
 'Thanks for the clarification!'
 ""Hi @lhoestq we've tried out your suggestion but are now running into the following error:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-163-81ffd5ac18c9> in <module>\r\n----> 1 dataset.cast_(emotion_features)\r\n\r\n/usr/local/lib/python3.6/dist-packages/datasets/dataset_dict.py in cast_(self, features)\r\n    125         self._check_values_type()\r\n    126         for dataset in self.values():\r\n--> 127             dataset.cast_(features=features)\r\n    128 \r\n    129     def remove_columns_(self, column_names: Union[str, List[str]]):\r\n\r\n/usr/local/lib/python3.6/dist-packages/datasets/fingerprint.py in wrapper(*args, **kwargs)\r\n    161             # Call actual function\r\n    162 \r\n--> 163             out = func(self, *args, **kwargs)\r\n    164 \r\n    165             # Update fingerprint of in-place transforms + update in-place history of transforms\r\n\r\n/usr/local/lib/python3.6/dist-packages/datasets/arrow_dataset.py in cast_(self, features)\r\n    602         self._info.features = features\r\n    603         schema = pa.schema(features.type)\r\n--> 604         self._data = self._data.cast(schema)\r\n    605 \r\n    606     @fingerprint(inplace=True)\r\n\r\n/usr/local/lib/python3.6/dist-packages/pyarrow/table.pxi in pyarrow.lib.Table.cast()\r\n\r\nValueError: Target schema's field names are not matching the table's field names: ['text', 'label'], ['label', 'text']\r\n```\r\n\r\nLooking at the types in `emotion_features` we see that `label` and `text` appear to be swapped in the Arrow table:\r\n\r\n```\r\nemotion_features.type\r\nStructType(struct<label: int64, text: string>)\r\n```\r\n\r\nDid we define the `emotion_features` incorrectly? We just followed the instructions from the [docs](https://huggingface.co/docs/datasets/features.html?highlight=features#dataset-features), but perhaps we misunderstood something 😬 \r\n\r\n""
 ""In general, I don't think there is any hard reason we don't allow to use `features` in the csv script, right @lhoestq?\r\n\r\nShould I add it?""
 ""> In general, I don't think there is any hard reason we don't allow to use `features` in the csv script, right @lhoestq?\r\n> \r\n> Should I add it?\r\n\r\nSure let's add it. Setting the convert options should do the job\r\n\r\n> Hi @lhoestq we've tried out your suggestion but are now running into the following error:\r\n> \r\n> ```\r\n> ---------------------------------------------------------------------------\r\n> ValueError                                Traceback (most recent call last)\r\n> <ipython-input-163-81ffd5ac18c9> in <module>\r\n> ----> 1 dataset.cast_(emotion_features)\r\n>\r\n>  /usr/local/lib/python3.6/dist-packages/pyarrow/table.pxi in pyarrow.lib.Table.cast()\r\n> \r\n> ValueError: Target schema's field names are not matching the table's field names: ['text', 'label'], ['label', 'text']\r\n> ```\r\n>\r\n> Did we define the `emotion_features` incorrectly? We just followed the instructions from the [docs](https://huggingface.co/docs/datasets/features.html?highlight=features#dataset-features), but perhaps we misunderstood something 😬\r\n\r\nThanks for reporting, that's a bug :) I'm fixing it right now""
 ""PR is open for the `ValueError: Target schema's field names are not matching the table's field names` error.\r\n\r\nI'm adding the features parameter to csv""
 'Thanks a lot for the PR and quick fix @lhoestq!']","I am trying to load a local file with the `load_dataset` function and I want to predefine the feature types with the `features` argument. However, the types are always the same independent of the value of `features`. 

I am working with the local files from the emotion dataset. To get the data you can use the following code:

```Python
from pathlib import Path
import wget

EMOTION_PATH = Path(""./data/emotion"")
DOWNLOAD_URLS = [
    ""https://www.dropbox.com/s/1pzkadrvffbqw6o/train.txt?dl=1"",
    ""https://www.dropbox.com/s/2mzialpsgf9k5l3/val.txt?dl=1"",
    ""https://www.dropbox.com/s/ikkqxfdbdec3fuj/test.txt?dl=1"",
]

if not Path.is_dir(EMOTION_PATH):
     Path.mkdir(EMOTION_PATH)
for url in DOWNLOAD_URLS:
     wget.download(url, str(EMOTION_PATH))
```

The first five lines of the train set are:
```
i didnt feel humiliated;sadness
i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake;sadness
im grabbing a minute to post i feel greedy wrong;anger
i am ever feeling nostalgic about the fireplace i will know that it is still on the property;love
i am feeling grouchy;anger
```

Here the code to reproduce the issue:
```Python
from datasets import Features, Value, ClassLabel, load_dataset

class_names = [""sadness"", ""joy"", ""love"", ""anger"", ""fear"", ""surprise""]
emotion_features = Features({'text': Value('string'), 'label': ClassLabel(names=class_names)})
file_dict = {'train': EMOTION_PATH/'train.txt'}

dataset = load_dataset('csv', data_files=file_dict, delimiter=';', column_names=['text', 'label'], features=emotion_features)
```

**Observed behaviour:**
```Python
dataset['train'].features
```
```Python
{'text': Value(dtype='string', id=None),
 'label': Value(dtype='string', id=None)}
```
**Expected behaviour:**
```Python
dataset['train'].features
```
```Python
{'text': Value(dtype='string', id=None),
 'label': ClassLabel(num_classes=6, names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'], names_file=None, id=None)}
```

**Things I've tried:**
- deleting the cache
- trying other types such as `int64`

Am I missing anything? Thanks for any pointer in the right direction."
https://github.com/huggingface/datasets/issues/622,load_dataset for text files not working,"['Can you give us more information on your os and pip environments (pip list)?'
 ""@thomwolf Sure. I'll try downgrading to 3.7 now even though Arrow say they support >=3.5.\r\n\r\nLinux (Ubuntu 18.04) - Python 3.8\r\n======================\r\nPackage   -   Version\r\n---------------------\r\ncertifi               2020.6.20\r\nchardet               3.0.4\r\nclick                 7.1.2\r\ndatasets              1.0.1\r\ndill                  0.3.2\r\nfasttext              0.9.2\r\nfilelock              3.0.12\r\nfuture                0.18.2\r\nidna                  2.10\r\njoblib                0.16.0\r\nnltk                  3.5\r\nnumpy                 1.19.1\r\npackaging             20.4\r\npandas                1.1.2\r\npip                   20.0.2\r\nprotobuf              3.13.0\r\npyarrow               1.0.1\r\npybind11              2.5.0\r\npyparsing             2.4.7\r\npython-dateutil       2.8.1\r\npytz                  2020.1\r\nregex                 2020.7.14\r\nrequests              2.24.0\r\nsacremoses            0.0.43\r\nscikit-learn          0.23.2\r\nscipy                 1.5.2\r\nsentence-transformers 0.3.6\r\nsentencepiece         0.1.91\r\nsetuptools            46.1.3\r\nsix                   1.15.0\r\nstanza                1.1.1\r\nthreadpoolctl         2.1.0\r\ntokenizers            0.8.1rc2\r\ntorch                 1.6.0+cu101\r\ntqdm                  4.48.2\r\ntransformers          3.1.0\r\nurllib3               1.25.10\r\nwheel                 0.34.2\r\nxxhash                2.0.0\r\n\r\nWindows 10 - Python 3.8\r\n================\r\nPackage       -        Version\r\n----------------------------\r\ncertifi               2020.6.20\r\nchardet               3.0.4\r\nclick                 7.1.2\r\ndatasets              1.0.1\r\ndill                  0.3.2\r\nfasttext              0.9.2\r\nfilelock              3.0.12\r\nfuture                0.18.2\r\nidna                  2.10\r\njoblib                0.16.0\r\nnlp                   0.4.0\r\nnltk                  3.5\r\nnumpy                 1.19.1\r\npackaging             20.4\r\npandas                1.1.1\r\npip                   20.0.2\r\nprotobuf              3.13.0\r\npyarrow               1.0.1\r\npybind11              2.5.0\r\npyparsing             2.4.7\r\npython-dateutil       2.8.1\r\npytz                  2020.1\r\nregex                 2020.7.14\r\nrequests              2.24.0\r\nsacremoses            0.0.43\r\nscikit-learn          0.23.2\r\nscipy                 1.5.2\r\nsentence-transformers 0.3.5.1\r\nsentencepiece         0.1.91\r\nsetuptools            46.1.3\r\nsix                   1.15.0\r\nstanza                1.1.1\r\nthreadpoolctl         2.1.0\r\ntokenizers            0.8.1rc1\r\ntorch                 1.6.0+cu101\r\ntqdm                  4.48.2\r\ntransformers          3.0.2\r\nurllib3               1.25.10\r\nwheel                 0.34.2\r\nxxhash                2.0.0""
 'Downgrading to 3.7 does not help. Here is a dummy text file:\r\n\r\n```text\r\nVerzekering weigert vaker te betalen\r\nBedrijven van verzekeringen erkennen steeds minder arbeidsongevallen .\r\nIn 2012 weigerden de bedrijven te betalen voor 21.055 ongevallen op het werk .\r\nDat is 11,8 % van alle ongevallen op het werk .\r\nNog nooit weigerden verzekeraars zoveel zaken .\r\nIn 2012 hadden 135.118 mensen een ongeval op het werk .\r\nDat zijn elke werkdag 530 mensen .\r\nBij die ongevallen stierven 67 mensen .\r\nBijna 12.000 hebben een handicap na het ongeval .\r\nGeen echt arbeidsongeval Bedrijven moeten een verzekering hebben voor hun werknemers .\r\n```\r\n\r\nA temporary work around for the ""text"" type, is\r\n\r\n```python\r\ndataset = Dataset.from_dict({""text"": Path(dataset_f).read_text().splitlines()})\r\n```'
 '![image](https://user-images.githubusercontent.com/6847024/92997714-d2add900-f532-11ea-83d4-e3473c2d94d7.png)\r\n![image](https://user-images.githubusercontent.com/6847024/92997724-e22d2200-f532-11ea-951d-b1d8f4582ea3.png)\r\neven i am facing the same issue.'
 '@banunitte Please do not post screenshots in the future but copy-paste your code and the errors. That allows others to copy-and-paste your code and test it. You may also want to provide the Python version that you are using.'
 'I have the exact same problem in Windows 10, Python 3.8.\r\n'
 ""I have the same  problem on Linux of the  script crashing with a CSV error.  This may be caused by 'CRLF', when changed 'CRLF' to 'LF', the problem solved.""
 'I pushed a fix for `pyarrow.lib.ArrowInvalid: CSV parse error`. Let me know if you still have this issue.\r\n\r\nNot sure about the windows one yet'
 ""To complete what @lhoestq is saying, I think that to use the new version of the `text` processing script (which is on master right now) you need to either specify the version of the script to be the `master` one or to install the lib from source (in which case it uses the `master` version of the script by default):\r\n```python\r\ndataset = load_dataset('text', script_version='master', data_files=XXX)\r\n```\r\nWe do versioning by default, i.e. your version of the dataset lib will use the script with the same version by default (i.e. only the `1.0.1` version of the script if you have the PyPI version `1.0.1` of the lib).""
 ""![image](https://user-images.githubusercontent.com/36957508/93300760-fa9a8680-f829-11ea-9105-7a6f67ad8373.png)\r\nwin10, py3.6\r\n\r\n\r\n```\r\nfrom datasets import Features, Value, ClassLabel, load_dataset\r\n\r\n\r\nfeatures = Features({'text': Value('string'), 'ctext': Value('string')})\r\nfile_dict = {'train': PATH/'summary.csv'}\r\n\r\ndataset = load_dataset('csv', data_files=file_dict, script_version='master', delimiter='\\t', column_names=['text', 'ctext'], features=features)\r\n```""
 '```python\r\nTraceback` (most recent call last):\r\n  File ""main.py"", line 281, in <module>\r\n    main()\r\n  File ""main.py"", line 190, in main\r\n    train_data, test_data = data_factory(\r\n  File ""main.py"", line 129, in data_factory\r\n    train_data = load_dataset(\'text\', \r\n  File ""/home/me/Downloads/datasets/src/datasets/load.py"", line 608, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File ""/home/me/Downloads/datasets/src/datasets/builder.py"", line 468, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File ""/home/me/Downloads/datasets/src/datasets/builder.py"", line 546, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File ""/home/me/Downloads/datasets/src/datasets/builder.py"", line 888, in _prepare_split\r\n    for key, table in utils.tqdm(generator, unit="" tables"", leave=False, disable=not_verbose):\r\n  File ""/home/me/.local/lib/python3.8/site-packages/tqdm/std.py"", line 1130, in __iter__\r\n    for obj in iterable:\r\n  File ""/home/me/.cache/huggingface/modules/datasets_modules/datasets/text/512f465342e4f4cd07a8791428a629c043bb89d55ad7817cbf7fcc649178b014/text.py"", line 103, in _generate_tables\r\n    pa_table = pac.read_csv(\r\n  File ""pyarrow/_csv.pyx"", line 617, in pyarrow._csv.read_csv\r\n  File ""pyarrow/error.pxi"", line 123, in pyarrow.lib.pyarrow_internal_check_status\r\n  File ""pyarrow/error.pxi"", line 85, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: CSV parse error: Expected 1 columns, got 2\r\n```\r\n\r\nUnfortunately i am still getting this issue on Linux. I installed datasets from source and specified script_version to master.\r\n\r\n'
 '> ![image](https://user-images.githubusercontent.com/36957508/93300760-fa9a8680-f829-11ea-9105-7a6f67ad8373.png)\r\n> win10, py3.6\r\n> \r\n> ```\r\n> from datasets import Features, Value, ClassLabel, load_dataset\r\n> \r\n> \r\n> features = Features({\'text\': Value(\'string\'), \'ctext\': Value(\'string\')})\r\n> file_dict = {\'train\': PATH/\'summary.csv\'}\r\n> \r\n> dataset = load_dataset(\'csv\', data_files=file_dict, script_version=\'master\', delimiter=\'\\t\', column_names=[\'text\', \'ctext\'], features=features)\r\n> ```\r\n\r\nSince #644 it should now work on windows @ScottishFold007 \r\n\r\n> Trying the following snippet, I get different problems on Linux and Windows.\r\n> \r\n> ```python\r\n> dataset = load_dataset(""text"", data_files=""data.txt"")\r\n> # or \r\n> dataset = load_dataset(""text"", data_files=[""data.txt""])\r\n> ```\r\n>\r\n> Windows just seems to get stuck. Even with a tiny dataset of 10 lines, it has been stuck for 15 minutes already at this message:\r\n> \r\n> ```\r\n> Checking C:\\Users\\bramv\\.cache\\huggingface\\datasets\\b1d50a0e74da9a7b9822cea8ff4e4f217dd892e09eb14f6274a2169e5436e2ea.30c25842cda32b0540d88b7195147decf9671ee442f4bc2fb6ad74016852978e.py for additional imports.\r\n> Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/text/text.py at C:\\Users\\bramv\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\text\r\n> Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/text/text.py at C:\\Users\\bramv\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\text\\7e13bc0fa76783d4ef197f079dc8acfe54c3efda980f2c9adfab046ede2f0ff7\r\n> Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/text/text.py to C:\\Users\\bramv\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\text\\7e13bc0fa76783d4ef197f079dc8acfe54c3efda980f2c9adfab046ede2f0ff7\\text.py\r\n> Couldn\'t find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/text\\dataset_infos.json\r\n> Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/text/text.py at C:\\Users\\bramv\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\text\\7e13bc0fa76783d4ef197f079dc8acfe54c3efda980f2c9adfab046ede2f0ff7\\text.json\r\n> Using custom data configuration default\r\n> ```\r\n\r\nSame for you @BramVanroy .\r\n\r\nNot sure about the one on linux though'
 '> To complete what @lhoestq is saying, I think that to use the new version of the `text` processing script (which is on master right now) you need to either specify the version of the script to be the `master` one or to install the lib from source (in which case it uses the `master` version of the script by default):\r\n> \r\n> ```python\r\n> dataset = load_dataset(\'text\', script_version=\'master\', data_files=XXX)\r\n> ```\r\n> \r\n> We do versioning by default, i.e. your version of the dataset lib will use the script with the same version by default (i.e. only the `1.0.1` version of the script if you have the PyPI version `1.0.1` of the lib).\r\n\r\nLinux here:\r\n\r\nI was using the 0.4.0 nlp library load_dataset to load a text dataset of 9-10Gb without collapsing the RAM memory. However, today I got the csv error message mentioned in this issue. After installing the new (datasets) library from source and specifying the script_verson = \'master\' I\'m still having this same error message. Furthermore, I cannot use the dictionary ""trick"" to load the dataset since the system kills the process due to a RAM out of memory problem. Is there any other solution to this error? Thank you in advance. '
 ""Hi @raruidol \r\nTo fix the RAM issue you'll need to shard your text files into smaller files (see https://github.com/huggingface/datasets/issues/610#issuecomment-691672919 for example)\r\n\r\nI'm not sure why you're having the csv error on linux.\r\nDo you think you could to to reproduce it on google colab for example ?\r\nOr send me a dummy .txt file that reproduces the issue ?""
 '@lhoestq \r\n\r\nThe crash message shows up when loading the dataset:\r\n```\r\nprint(\'Loading corpus...\') \r\nfiles = glob.glob(\'corpora/shards/*\') \r\n-> dataset = load_dataset(\'text\', script_version=\'master\', data_files=files) \r\nprint(\'Corpus loaded.\')\r\n```\r\nAnd this is the exact message:\r\n```\r\nTraceback (most recent call last):\r\n  File ""run_language_modeling.py"", line 27, in <module>\r\n    dataset = load_dataset(\'text\', script_version=\'master\', data_files=files)\r\n  File ""/home/jupyter-raruidol/DebatAnalyser/env/lib/python3.7/site-packages/datasets/load.py"", line 611, in load_dataset\r\n    ignore_verifications=ignore_verifications,\r\n  File ""/home/jupyter-raruidol/DebatAnalyser/env/lib/python3.7/site-packages/datasets/builder.py"", line 471, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File ""/home/jupyter-raruidol/DebatAnalyser/env/lib/python3.7/site-packages/datasets/builder.py"", line 548, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File ""/home/jupyter-raruidol/DebatAnalyser/env/lib/python3.7/site-packages/datasets/builder.py"", line 892, in _prepare_split\r\n    for key, table in utils.tqdm(generator, unit="" tables"", leave=False, disable=not_verbose):\r\n  File ""/home/jupyter-raruidol/DebatAnalyser/env/lib/python3.7/site-packages/tqdm/std.py"", line 1130, in __iter__\r\n    for obj in iterable:\r\n  File ""/home/jupyter-raruidol/.cache/huggingface/modules/datasets_modules/datasets/text/512f465342e4f4cd07a8791428a629c043bb89d55ad7817cbf7fcc649178b014/text.py"", line 107, in _generate_tables\r\n    convert_options=self.config.convert_options,\r\n  File ""pyarrow/_csv.pyx"", line 714, in pyarrow._csv.read_csv\r\n  File ""pyarrow/error.pxi"", line 122, in pyarrow.lib.pyarrow_internal_check_status\r\n  File ""pyarrow/error.pxi"", line 84, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: CSV parse error: Expected 1 columns, got 2\r\n```\r\n\r\nAnd these are the pip packages I have atm and their versions:\r\n\r\n```\r\nPackage         Version   Location                                                     \r\n--------------- --------- -------------------------------------------------------------\r\ncertifi         2020.6.20 \r\nchardet         3.0.4     \r\nclick           7.1.2     \r\ndatasets        1.0.2     \r\ndill            0.3.2     \r\nfilelock        3.0.12    \r\nfuture          0.18.2    \r\nidna            2.10      \r\njoblib          0.16.0    \r\nnumpy           1.19.1    \r\npackaging       20.4      \r\npandas          1.1.1     \r\npip             19.0.3    \r\npyarrow         1.0.1     \r\npyparsing       2.4.7     \r\npython-dateutil 2.8.1     \r\npytz            2020.1    \r\nregex           2020.7.14 \r\nrequests        2.24.0    \r\nsacremoses      0.0.43    \r\nsentencepiece   0.1.91    \r\nsetuptools      40.8.0    \r\nsix             1.15.0    \r\ntokenizers      0.8.1rc2  \r\ntorch           1.6.0     \r\ntqdm            4.48.2    \r\ntransformers    3.0.2     /home/jupyter-raruidol/DebatAnalyser/env/src/transformers/src\r\n```\r\n\r\n\r\n'
 'I tested on google colab which is also linux using this code:\r\n\r\n- first download an arbitrary text file\r\n```bash\r\nwget https://raw.githubusercontent.com/abisee/cnn-dailymail/master/url_lists/all_train.txt\r\n```\r\n- then run\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nd = load_dataset(""text"", data_files=""all_train.txt"", script_version=\'master\')\r\n```\r\nAnd I don\'t get this issue.\r\n\r\n\\> Could you test on your side if these lines work @raruidol ?\r\n\r\nalso cc @Skyy93 as it seems you have the same issue\r\n\r\nIf it works:\r\nIt could mean that the issue could come from unexpected patterns in the files you want to use.\r\nIn that case we should find a way to handle them.\r\n\r\nAnd if it doesn\'t work:\r\nIt could mean that it comes from the way pyarrow reads text files on linux.\r\nIn that case we should report it to pyarrow and find a workaround in the meantime\r\n\r\nEither way it should help to find where this bug comes from and fix it :)\r\n\r\nThank you in advance !'
 'Update: also tested the above code in a docker container from [jupyter/minimal-notebook](https://hub.docker.com/r/jupyter/minimal-notebook/) (based on ubuntu) and still not able to reproduce'
 ""It looks like with your text input file works without any problem. I have been doing some experiments this morning with my input files and I'm almost certain that the crash is caused by some unexpected pattern in the files. However, I've not been able to spot the main cause of it. What I find strange is that this same corpus was being loaded by the nlp 0.4.0 library without any problem... Where can I find the code where you structure the input text data in order to use it with pyarrow?""
 'Under the hood it does\r\n```python\r\nimport pyarrow as pa\r\nimport pyarrow.csv\r\n\r\n# Use csv reader from Pyarrow with one column for text files\r\n\r\n# To force the one-column setting, we set an arbitrary character\r\n# that is not in text files as delimiter, such as \\b or \\v.\r\n# The bell character, \\b, was used to make beeps back in the days\r\nparse_options = pa.csv.ParseOptions( \r\n    delimiter=""\\b"", \r\n    quote_char=False, \r\n    double_quote=False, \r\n    escape_char=False, \r\n    newlines_in_values=False, \r\n    ignore_empty_lines=False, \r\n)\r\n\r\nread_options= pa.csv.ReadOptions(use_threads=True, column_names=[""text""])\r\n\r\npa_table = pa.csv.read_csv(""all_train.txt"", read_options=read_options, parse_options=parse_options)\r\n```\r\n\r\nNote that we changed the parse options with datasets 1.0\r\nIn particular the delimiter used to be `\\r` but this delimiter doesn\'t work on windows.'
 'Could you try with `\\a` instead of `\\b` ? It looks like the bell character is \\a in python and not \\b'
 ""I was just exploring if the crash was happening in every shard or not, and which shards were generating the error message. With \\b I got the following list of shards crashing:\r\n\r\n```\r\nErrors on files:  ['corpora/shards/shard_0069', 'corpora/shards/shard_0043', 'corpora/shards/shard_0014', 'corpora/shards/shard_0032', 'corpora/shards/shard_0088', 'corpora/shards/shard_0018', 'corpora/shards/shard_0073', 'corpora/shards/shard_0079', 'corpora/shards/shard_0038', 'corpora/shards/shard_0041', 'corpora/shards/shard_0007', 'corpora/shards/shard_0004', 'corpora/shards/shard_0102', 'corpora/shards/shard_0096', 'corpora/shards/shard_0030', 'corpora/shards/shard_0076', 'corpora/shards/shard_0067', 'corpora/shards/shard_0052', 'corpora/shards/shard_0026', 'corpora/shards/shard_0024', 'corpora/shards/shard_0064', 'corpora/shards/shard_0044', 'corpora/shards/shard_0013', 'corpora/shards/shard_0062', 'corpora/shards/shard_0057', 'corpora/shards/shard_0097', 'corpora/shards/shard_0094', 'corpora/shards/shard_0078', 'corpora/shards/shard_0075', 'corpora/shards/shard_0039', 'corpora/shards/shard_0077', 'corpora/shards/shard_0021', 'corpora/shards/shard_0040', 'corpora/shards/shard_0009', 'corpora/shards/shard_0023', 'corpora/shards/shard_0095', 'corpora/shards/shard_0107', 'corpora/shards/shard_0063', 'corpora/shards/shard_0086', 'corpora/shards/shard_0047', 'corpora/shards/shard_0089', 'corpora/shards/shard_0037', 'corpora/shards/shard_0101', 'corpora/shards/shard_0093', 'corpora/shards/shard_0082', 'corpora/shards/shard_0091', 'corpora/shards/shard_0065', 'corpora/shards/shard_0020', 'corpora/shards/shard_0070', 'corpora/shards/shard_0008', 'corpora/shards/shard_0058', 'corpora/shards/shard_0060', 'corpora/shards/shard_0022', 'corpora/shards/shard_0059', 'corpora/shards/shard_0100', 'corpora/shards/shard_0027', 'corpora/shards/shard_0072', 'corpora/shards/shard_0098', 'corpora/shards/shard_0019', 'corpora/shards/shard_0066', 'corpora/shards/shard_0042', 'corpora/shards/shard_0053']\r\n```\r\n\r\nI also tried with \\a and the list decreased but there were still several crashes:\r\n\r\n```\r\nErrors on files:  ['corpora/shards/shard_0069', 'corpora/shards/shard_0055', 'corpora/shards/shard_0043', 'corpora/shards/shard_0014', 'corpora/shards/shard_0073', 'corpora/shards/shard_0025', 'corpora/shards/shard_0068', 'corpora/shards/shard_0102', 'corpora/shards/shard_0096', 'corpora/shards/shard_0076', 'corpora/shards/shard_0067', 'corpora/shards/shard_0026', 'corpora/shards/shard_0024', 'corpora/shards/shard_0044', 'corpora/shards/shard_0087', 'corpora/shards/shard_0092', 'corpora/shards/shard_0074', 'corpora/shards/shard_0094', 'corpora/shards/shard_0078', 'corpora/shards/shard_0039', 'corpora/shards/shard_0077', 'corpora/shards/shard_0040', 'corpora/shards/shard_0009', 'corpora/shards/shard_0107', 'corpora/shards/shard_0063', 'corpora/shards/shard_0103', 'corpora/shards/shard_0047', 'corpora/shards/shard_0033', 'corpora/shards/shard_0089', 'corpora/shards/shard_0037', 'corpora/shards/shard_0082', 'corpora/shards/shard_0071', 'corpora/shards/shard_0091', 'corpora/shards/shard_0065', 'corpora/shards/shard_0070', 'corpora/shards/shard_0058', 'corpora/shards/shard_0081', 'corpora/shards/shard_0060', 'corpora/shards/shard_0002', 'corpora/shards/shard_0059', 'corpora/shards/shard_0027', 'corpora/shards/shard_0072', 'corpora/shards/shard_0098', 'corpora/shards/shard_0019', 'corpora/shards/shard_0045', 'corpora/shards/shard_0036', 'corpora/shards/shard_0066', 'corpora/shards/shard_0053']\r\n```\r\n\r\nWhich means that it is quite possible that the assumption of that some unexpected pattern in the files is causing the crashes is true. If I am able to reach any conclusion I will post It here asap.""
 'Hmmm I was expecting it to work with \\a, not sure why they appear in your text files though'
 'Hi @lhoestq, is there any input length restriction which was not before the update of the nlp library?'
 ""No we never set any input length restriction on our side (maybe arrow but I don't think so)""
 '@lhoestq Can you ever be certain that a delimiter character is not present in a plain text file? In other formats (e.g. CSV) , rules are set of what is allowed and what isn\'t so that it actually constitutes a CSV file. In a text file you basically have ""anything goes"", so I don\'t think you can ever be entirely sure that the chosen delimiter does not exist in the text file, or am I wrong? \r\n\r\nIf I understand correctly you choose a delimiter that we hope does not exist in the file, so that when the CSV parser starts splitting into columns, it will only ever create one column? Why can\'t we use a newline character though?'
 ""Okay, I have splitted the crashing shards into individual sentences and some examples of the inputs that are causing the crashes are the following ones:\r\n\r\n\r\n_4.\u2003DE L’ORGANITZACIÓ ESTAMENTAL A L’ORGANITZACIÓ EN CLASSES A mesura que es desenvolupava un sistema econòmic capitalista i naixia una classe burgesa cada vegada més preparada per a substituir els dirigents de les velles monarquies absolutistes, es qüestionava l’abundància de béns amortitzats, que com s’ha dit estaven fora del mercat i no pagaven tributs, pels perjudicis que ocasionaven a les finances públiques i a l’economia en general. Aquest estat d’opinió revolucionari va desembocar en un conjunt de mesures pràctiques de caràcter liberal. D’una banda, les que intentaven desposseir les mans mortes del domini de béns acumulats, procés que acostumem a denominar desamortització, i que no és més que la nacionalització i venda d’aquests béns eclesiàstics o civils en subhasta pública al millor postor. D’altra banda, les que redimien o reduïen els censos i delmes o aixecaven les prohibicions de venda, és a dir, les vinculacions. La desamortització, que va afectar béns dels ordes religiosos, dels pobles i d’algunes corporacions civils, no va ser un camí fàcil, perquè costava i costa trobar algú que sigui indiferent a la pèrdua de béns, drets i privilegis. I té una gran transcendència, va privar els antics estaments de les Espanyes, clero i pobles —la noblesa en queda al marge—, de la força econòmica que els donaven bona part de les seves terres i, en última instància, va preparar el terreny per a la substitució de la vella societat estamental per la nova societat classista. En aquesta societat, en teoria, les agrupacions socials són obertes, no tenen cap estatut jurídic privilegiat i estan definides per la possessió o no d’uns béns econòmics que són lliurement alienables. A les Espanyes la transformació va afectar poc l’aristocràcia latifundista, allà on n’hi havia. Aquesta situació va afavorir, en part, la persistència de la vella cultura de la societat estamental en determinats ambients, i això ha influït decisivament en la manca de democràcia que caracteritza la majoria de règims polítics que s’han anat succeint. Una manera de pensar que sempre sura en un moment o altre, i que de fet no acaba de desaparèixer del tot. 5.\u2003INICI DE LA DESAMORTITZACIÓ A LES ESPANYES Durant el segle xviii, dins d’aquesta visió lliberal, va agafar força en alguns cercles de les Espanyes el corrent d’opinió contrari a les mans mortes. Durant el regnat de Carles III, s’arbitraren les primeres mesures desamortitzadores proposades per alguns ministres il·lustrats. Aquestes disposicions foren modestes i poc eficaces, no van aturar l’acumulació de terres per part dels estaments que constituïen les mans mortes i varen afectar principalment béns dels pobles. L’Església no va ser tocada, excepte en el cas de 110_\r\n\r\n_la revolució liberal, perquè, encara que havia perdut els seus drets jurisdiccionals, havia conservat la majoria de terres i fins i tot les havia incrementat amb d’altres que procedien de la desamortització. En la nova situació, les mans mortes del bosc públic eren l’Estat, que no cerca mai l’autofinançament de les despeses de gestió; els diners que manquin ja els posarà l’Estat. 9.\u2003DEFENSA I INTENTS DE RECUPERACIÓ DELS BÉNS COMUNALS DESAMORTITZATS El procés de centralització no era senzill, perquè, d’una banda, la nova organització apartava de la gestió moltes corporacions locals i molts veïns que l’havien portada des de l’edat mitjana, i, de l’altra, era difícil de coordinar la nova silvicultura amb moltes pràctiques forestals i drets tradicionals, com la pastura, fer llenya o tallar un arbre aquí i un altre allà quan tenia el gruix suficient, les pràctiques que s’havien fet sempre. Les primeres passes de la nova organització centralitzada varen tenir moltes dificultats en aquells indrets en què els terrenys municipals i comunals tenien un paper important en l’economia local. La desobediència a determinades normes imposades varen prendre formes diferents. Algunes institucions, com, per exemple, la Diputació de Lleida, varen retardar la tramitació d’alguns expedients i varen evitar la venda de béns municipals. Molts pobles permeteren deixar que els veïns continuessin amb les seves pràctiques tradicionals, d’altres varen boicotejar les subhastes d’aprofitaments. L’Estat va reaccionar encomanant a la Guàrdia Civil el compliment de les noves directrius. Imposar el nou règim va costar a l’Administració un grapat d’anys, però de mica en mica, amb molta, molta guarderia i gens de negociació, ho va aconseguir. La nova gestió estatal dels béns municipals va deixar, com hem comentat, molta gent sense uns recursos necessaris per a la supervivència, sobre tot en àrees on predominaven les grans propietats, i on els pagesos sense terra treballaven de jornalers temporers. Això va afavorir que, a bona part de les Espanyes, les primeres lluites camperoles de la segona meitat del segle xix defensessin la recuperació dels comunals desamortitzats; per a molts aquella expropiació i venda dirigida pels governs monàrquics era la causa de molta misèria. D’altres, més radicalitzats, varen entendre que l’eliminació de la propietat col·lectiva i la gestió estatal dels boscos no desamortitzats suposava una usurpació pura i dura. En les zones més afectades per la desamortització això va donar lloc a un imaginari centrat en la defensa del comunal. La Segona República va arribar en una conjuntura econòmica de crisi, generada pel crac del 1929. Al camp, aquesta situació va produir una forta caiguda dels preus dels productes agraris i un increment important de l’atur. QUADERNS AGRARIS 42\u2002(juny 2017), p. 105-126_\r\n\r\nI think that the main difference between the crashing samples and the rest is their length. Therefore, couldn't the length be causing the message errors? I hope with these samples you can identify what is causing the crashes considering that the 0.4.0 nlp library was loading them properly.""
 'So we\'re using the csv reader to read text files because arrow doesn\'t have a text reader.\r\nTo workaround the fact that text files are just csv with one column, we want to set a delimiter that doesn\'t appear in text files.\r\nUntil now I thought that it would do the job but unfortunately it looks like even characters like \\a appear in text files.\r\n\r\nSo we have to option:\r\n- find another delimiter that does the job (maybe `\\x1b` esc or `\\x18` cancel)\r\n- don\'t use the csv reader from arrow but the text reader from pandas instead (or any other reader). The only important thing is that it must be fast (arrow\'s reader has a nice and fast multithreaded for csv that we\'re using now but hopefully we can find an alternative)\r\n\r\n\r\n\r\n> @lhoestq Can you ever be certain that a delimiter character is not present in a plain text file? In other formats (e.g. CSV) , rules are set of what is allowed and what isn\'t so that it actually constitutes a CSV file. In a text file you basically have ""anything goes"", so I don\'t think you can ever be entirely sure that the chosen delimiter does not exist in the text file, or am I wrong?\r\n\r\nAs long as the text file follows some encoding it wouldn\'t make sense to have characters such as the bell character. However I agree it can happen.\r\n\r\n> If I understand correctly you choose a delimiter that we hope does not exist in the file, so that when the CSV parser starts splitting into columns, it will only ever create one column? Why can\'t we use a newline character though?\r\n\r\nExactly. Arrow doesn\'t allow the newline character unfortunately.'
 '> Okay, I have splitted the crashing shards into individual sentences and some examples of the inputs that are causing the crashes are the following ones\r\n\r\nThanks for digging into it !\r\n\r\nCharacters like \\a or \\b are not shown when printing the text, so as it is I can\'t tell if it contains unexpected characters.\r\nMaybe could could open the file in python and check if `""\\b"" in open(""path/to/file"", ""r"").read()` ?\r\n\r\n> I think that the main difference between the crashing samples and the rest is their length. Therefore, couldn\'t the length be causing the message errors? I hope with these samples you can identify what is causing the crashes considering that the 0.4.0 nlp library was loading them properly.\r\n\r\nTo check that you could try to run \r\n\r\n```python\r\nimport pyarrow as pa\r\nimport pyarrow.csv\r\n\r\nopen(""dummy.txt"", ""w"").write(((""a"" * 10_000) + ""\\n"") * 4)  # 4 lines of 10 000 \'a\'\r\n\r\nparse_options = pa.csv.ParseOptions( \r\n    delimiter=""\\b"", \r\n    quote_char=False, \r\n    double_quote=False, \r\n    escape_char=False, \r\n    newlines_in_values=False, \r\n    ignore_empty_lines=False, \r\n)\r\n\r\nread_options= pa.csv.ReadOptions(use_threads=True, column_names=[""text""])\r\n\r\npa_table = pa.csv.read_csv(""dummy.txt"", read_options=read_options, parse_options=parse_options)\r\n```\r\n\r\non my side it runs without error though'
 'That\'s true, It was my error printing the text that way. Maybe as a workaround, I can force all my input samples to have ""\\b"" at the end?'
 '> That\'s true, It was my error printing the text that way. Maybe as a workaround, I can force all my input samples to have ""\\b"" at the end?\r\n\r\nI don\'t think it would work since we only want one column, and ""\\b"" is set to be the delimiter between two columns, so it will raise the same issue again. Pyarrow would think that there is more than one column if the delimiter is found somewhere.\r\n\r\nAnyway, I I\'ll work on a new text reader if we don\'t find the right workaround about this delimiter issue.']","Trying the following snippet, I get different problems on Linux and Windows.


```python
dataset = load_dataset(""text"", data_files=""data.txt"")
# or 
dataset = load_dataset(""text"", data_files=[""data.txt""])
```

(ps [This example](https://huggingface.co/docs/datasets/loading_datasets.html#json-files) shows that you can use a string as input for data_files, but the signature is `Union[Dict, List]`.)

The problem on Linux is that the script crashes with a CSV error (even though it isn't a CSV file). On Windows the script just seems to freeze or get stuck after loading the config file.

Linux stack trace:
```
PyTorch version 1.6.0+cu101 available.
Checking /home/bram/.cache/huggingface/datasets/b1d50a0e74da9a7b9822cea8ff4e4f217dd892e09eb14f6274a2169e5436e2ea.30c25842cda32b0540d88b7195147decf9671ee442f4bc2fb6ad74016852978e.py for additional imports.
Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/text/text.py at /home/bram/.cache/huggingface/modules/datasets_modules/datasets/text
Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/text/text.py at /home/bram/.cache/huggingface/modules/datasets_modules/datasets/text/7e13bc0fa76783d4ef197f079dc8acfe54c3efda980f2c9adfab046ede2f0ff7
Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/text/text.py to /home/bram/.cache/huggingface/modules/datasets_modules/datasets/text/7e13bc0fa76783d4ef197f079dc8acfe54c3efda980f2c9adfab046ede2f0ff7/text.py
Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/text/dataset_infos.json
Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/text/text.py at /home/bram/.cache/huggingface/modules/datasets_modules/datasets/text/7e13bc0fa76783d4ef197f079dc8acfe54c3efda980f2c9adfab046ede2f0ff7/text.json
Using custom data configuration default
Generating dataset text (/home/bram/.cache/huggingface/datasets/text/default-0907112cc6cd2a38/0.0.0/7e13bc0fa76783d4ef197f079dc8acfe54c3efda980f2c9adfab046ede2f0ff7)
Downloading and preparing dataset text/default-0907112cc6cd2a38 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/bram/.cache/huggingface/datasets/text/default-0907112cc6cd2a38/0.0.0/7e13bc0fa76783d4ef197f079dc8acfe54c3efda980f2c9adfab046ede2f0ff7...
Dataset not on Hf google storage. Downloading and preparing it from source
Downloading took 0.0 min
Checksum Computation took 0.0 min
Unable to verify checksums.
Generating split train
Traceback (most recent call last):
  File ""/home/bram/Python/projects/dutch-simplification/utils.py"", line 45, in prepare_data
    dataset = load_dataset(""text"", data_files=dataset_f)
  File ""/home/bram/.local/share/virtualenvs/dutch-simplification-NcpPZtDF/lib/python3.8/site-packages/datasets/load.py"", line 608, in load_dataset
    builder_instance.download_and_prepare(
  File ""/home/bram/.local/share/virtualenvs/dutch-simplification-NcpPZtDF/lib/python3.8/site-packages/datasets/builder.py"", line 468, in download_and_prepare
    self._download_and_prepare(
  File ""/home/bram/.local/share/virtualenvs/dutch-simplification-NcpPZtDF/lib/python3.8/site-packages/datasets/builder.py"", line 546, in _download_and_prepare
    self._prepare_split(split_generator, **prepare_split_kwargs)
  File ""/home/bram/.local/share/virtualenvs/dutch-simplification-NcpPZtDF/lib/python3.8/site-packages/datasets/builder.py"", line 888, in _prepare_split
    for key, table in utils.tqdm(generator, unit="" tables"", leave=False, disable=not_verbose):
  File ""/home/bram/.local/share/virtualenvs/dutch-simplification-NcpPZtDF/lib/python3.8/site-packages/tqdm/std.py"", line 1130, in __iter__
    for obj in iterable:
  File ""/home/bram/.cache/huggingface/modules/datasets_modules/datasets/text/7e13bc0fa76783d4ef197f079dc8acfe54c3efda980f2c9adfab046ede2f0ff7/text.py"", line 100, in _generate_tables
    pa_table = pac.read_csv(
  File ""pyarrow/_csv.pyx"", line 714, in pyarrow._csv.read_csv
  File ""pyarrow/error.pxi"", line 122, in pyarrow.lib.pyarrow_internal_check_status
  File ""pyarrow/error.pxi"", line 84, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: CSV parse error: Expected 1 columns, got 2
```

Windows just seems to get stuck. Even with a tiny dataset of 10 lines, it has been stuck for 15 minutes already at this message:

```
Checking C:\Users\bramv\.cache\huggingface\datasets\b1d50a0e74da9a7b9822cea8ff4e4f217dd892e09eb14f6274a2169e5436e2ea.30c25842cda32b0540d88b7195147decf9671ee442f4bc2fb6ad74016852978e.py for additional imports.
Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/text/text.py at C:\Users\bramv\.cache\huggingface\modules\datasets_modules\datasets\text
Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/text/text.py at C:\Users\bramv\.cache\huggingface\modules\datasets_modules\datasets\text\7e13bc0fa76783d4ef197f079dc8acfe54c3efda980f2c9adfab046ede2f0ff7
Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/text/text.py to C:\Users\bramv\.cache\huggingface\modules\datasets_modules\datasets\text\7e13bc0fa76783d4ef197f079dc8acfe54c3efda980f2c9adfab046ede2f0ff7\text.py
Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/text\dataset_infos.json
Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/text/text.py at C:\Users\bramv\.cache\huggingface\modules\datasets_modules\datasets\text\7e13bc0fa76783d4ef197f079dc8acfe54c3efda980f2c9adfab046ede2f0ff7\text.json
Using custom data configuration default
```
"
https://github.com/huggingface/datasets/issues/620,map/filter multiprocessing raises errors and corrupts datasets,"[""It seems that I ran into the same problem\r\n```\r\ndef tokenize(cols, example):\r\n  for in_col, out_col in cols.items():\r\n    example[out_col] = hf_tokenizer.convert_tokens_to_ids(hf_tokenizer.tokenize(example[in_col]))\r\n  return example\r\ncola = datasets.load_dataset('glue', 'cola')\r\ntokenized_cola = cola.map(partial(tokenize, {'sentence': 'text_idxs'}),\r\n                             num_proc=2,)\r\n```\r\nand it outpus (exceprts)\r\n```\r\nConcatenating 2 shards from multiprocessing\r\nSet __getitem__(key) output type to python objects for ['idx', 'label', 'sentence', 'text_idxs'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\r\nTesting the mapped function outputs\r\nTesting finished, running the mapping function on the dataset\r\nDone writing 532 indices in 4256 bytes .\r\nDone writing 531 indices in 4248 bytes .\r\nProcess #0 will write at /home/yisiang/.cache/huggingface/datasets/glue/cola/1.0.0/930e9d141872db65102cabb9fa8ac01c11ffc8a1b72c2e364d8cdda4610df542/tokenized_test_00000_of_00002.arrow\r\nProcess #1 will write at /home/yisiang/.cache/huggingface/datasets/glue/cola/1.0.0/930e9d141872db65102cabb9fa8ac01c11ffc8a1b72c2e364d8cdda4610df542/tokenized_test_00001_of_00002.arrow\r\nSpawning 2 processes\r\n```\r\nand then the program never stop.""
 'same problem.\r\n`encoded_dataset = core_data.map(lambda examples: tokenizer(examples[""query""], examples[""document""], padding=True, truncation=\'longest_first\', return_tensors=""pt"", max_length=384), num_proc=16, keep_in_memory=True)`\r\nit outputs:\r\n```\r\nSet __getitem__(key) output type to python objects for [\'document\', \'is_random\', \'query\'] columns  (when key is int or slice) and don\'t output other (un-formatted) columns.\r\nDone writing 1787500 indices in 25568400000 bytes .\r\nSet __getitem__(key) output type to python objects for [\'document\', \'is_random\', \'query\'] columns  (when key is int or slice) and don\'t output other (un-formatted) columns.\r\nDone writing 1787500 indices in 25568400000 bytes .\r\nSet __getitem__(key) output type to python objects for [\'document\', \'is_random\', \'query\'] columns  (when key is int or slice) and don\'t output other (un-formatted) columns.\r\nDone writing 1787500 indices in 25568400000 bytes .\r\nSet __getitem__(key) output type to python objects for [\'document\', \'is_random\', \'query\'] columns  (when key is int or slice) and don\'t output other (un-formatted) columns.\r\nDone writing 1787500 indices in 25568400000 bytes .\r\nSet __getitem__(key) output type to python objects for [\'document\', \'is_random\', \'query\'] columns  (when key is int or slice) and don\'t output other (un-formatted) columns.\r\nDone writing 1787500 indices in 25568400000 bytes .\r\nSet __getitem__(key) output type to python objects for [\'document\', \'is_random\', \'query\'] columns  (when key is int or slice) and don\'t output other (un-formatted) columns.\r\nDone writing 1787500 indices in 25568400000 bytes .\r\nSet __getitem__(key) output type to python objects for [\'document\', \'is_random\', \'query\'] columns  (when key is int or slice) and don\'t output other (un-formatted) columns.\r\nDone writing 1787500 indices in 25568400000 bytes .\r\nSet __getitem__(key) output type to python objects for [\'document\', \'is_random\', \'query\'] columns  (when key is int or slice) and don\'t output other (un-formatted) columns.\r\nDone writing 1787499 indices in 25568385696 bytes .\r\nSet __getitem__(key) output type to python objects for [\'document\', \'is_random\', \'query\'] columns  (when key is int or slice) and don\'t output other (un-formatted) columns.\r\nSpawning 16 processes\r\n```'
 ""Thanks for reporting.\r\n\r\n\r\nWhich tokenizers are you using ? What platform are you on ? Can you tell me which version of datasets and pyarrow you're using ? @timothyjlaurent @richarddwang @HuangLianzhe \r\n\r\nAlso if you're able to reproduce the issue on google colab that would be very helpful.\r\n\r\nI tried to run your code @richarddwang with the bert tokenizer and I wasn't able to reproduce""
 'Hi, Sorry that I forgot to see what my version was.\r\nBut after updating datasets to master (editable install), and latest pyarrow. \r\nIt works now ~'
 'Sorry,  I just noticed this.\r\nI\'m running this on MACOS the version of datasets I\'m was 1.0.0 but I\'ve also tried it on 1.0.2. `pyarrow==1.0.1`, Python 3.6\r\n\r\nConsider this code:\r\n```python\r\n\r\n    loader_path = str(Path(__file__).parent / ""prodigy_dataset_builder.py"")\r\n    ds = load_dataset(\r\n        loader_path, name=""prodigy-ds"", data_files=list(file_paths), cache_dir=cache_dir\r\n    )[""train""]\r\n    valid_relations = set(vocabulary.relation_types.keys())\r\n\r\n    ds = ds.filter(filter_good_rows, fn_kwargs=dict(valid_rel_labels=valid_relations))\r\n\r\n    ds = ds.map(map_bpe_encodings, batched=True, fn_kwargs=dict(tokenizer=vocabulary.tokenizer), num_proc=10)\r\n\r\n    # add all feature data\r\n    ner_ds: Dataset = ds.map(\r\n        add_bio_tags,\r\n        fn_kwargs=dict(ner_label_map=vocabulary.ner_labels, tokenizer=vocabulary.tokenizer),\r\n    )\r\n    rel_ds: Dataset = ner_ds.map(\r\n        relation_ds_factory,\r\n        batched=True,\r\n        writer_batch_size=100,\r\n        fn_kwargs=dict(tokenizer=vocabulary.tokenizer, vocabulary=vocabulary),\r\n    )\r\n```\r\nThe loader is essentially a jsonloader with some extra error handling. The data is a jsonlines format with text field and a list of span objects and relation objects. \r\n\r\nIn the `ner_ds` a field, `ner_labels` is added, this is used in the downstream `relation_ds_factory`. It all runs fine in a single process but I get a KeyError error if run with num_proc set\r\n:\r\n\r\n```\r\n  File ""/Users/timothy.laurent/src/inv-text2struct/text2struct/model/dataset.py"", line 348, in relation_ds_factory\r\n    ner_labels = example[""ner_labels""]\r\nKeyError: \'ner_labels\'\r\n``` \r\n\r\nThis is just one example of what goes wrong.  I\'ve started just saving the dataset as arrow at the end because it takes a long time to map/filter/shuffle and the caching isn\'t working (tracked it down to byte differences in the pickled functions). \r\n\r\n^^ Interestingly if I heed the warning from Tokenizers and set the environment variable, `TOKENIZERS_PARALLELISM=true` the map just hangs:\r\n\r\n```\r\n[I 200921 21:43:18 filelock:318] Lock 5694118768 released on /Users/timothy.laurent/.cache/huggingface/datasets/_Users_timothy.laurent_.cache_huggingface_datasets_prodigy_dataset_builder_prodigy-ds-5f34378723c4e83f_0.0.0_e67d9b43d5cd82c50b1eae8f2097daf95b601a04dc03ddd504f2b234a5fa247a.lock\r\n100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.34ba/s]\r\n#0:   0%|                                                                                                            | 0/1 [00:00<?, ?ba/s]\r\n#1:   0%|                                                                                                            | 0/1 [00:00<?, ?ba/s]\r\n#2:   0%|                                                                                                            | 0/1 [00:00<?, ?ba/s]\r\n#3:   0%|                                                                                                            | 0/1 [00:00<?, ?ba/s]\r\n#4:   0%|                                                                                                            | 0/1 [00:00<?, ?ba/s]\r\n#5:   0%|                                                                                                            | 0/1 [00:00<?, ?ba/s]\r\n#6:   0%|                                                                                                            | 0/1 [00:00<?, ?ba/s]\r\n#7:   0%|                                                                                                            | 0/1 [00:00<?, ?ba/s]\r\n#8:   0%|                                                                                                            | 0/1 [00:00<?, ?ba/s]\r\n```'
 ""Thank you, I was able to reproduce :)\r\nI'm on it""
 '#659 should fix the `KeyError` issue. It was due to the formatting not getting updated the right way'
 'Also maybe @n1t0 knows why setting `TOKENIZERS_PARALLELISM=true` creates deadlock issues when calling `map` with multiprocessing ?'
 '@lhoestq  \r\n\r\nThanks for taking a look. I pulled the master but I still see the key error.\r\n\r\n```\r\nTo disable this warning, you can either:\r\n        - Avoid using `tokenizers` before the fork if possible\r\n        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\r\n#0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 21.56ba/s]\r\n#1: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 17.71ba/s]\r\n#2: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 20.45ba/s]\r\n#3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 26.05ba/s]\r\n#4: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 26.83ba/s]\r\n#5: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 27.00ba/s]\r\n#6: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 27.40ba/s]\r\n#7: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.91ba/s]\r\n#8: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 22.46ba/s]\r\n#9: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 20.15ba/s]\r\n#10: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 26.81ba/s]\r\n#11: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 27.45ba/s]\r\n100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 322/322 [00:00<00:00, 1462.85ex/s]\r\nTraceback (most recent call last):                                                                                      | 0/1 [00:00<?, ?ba/s]\r\n  File ""text2struct/run_model.py"", line 372, in <module>\r\n    main()\r\n  File ""text2struct/run_model.py"", line 368, in main                                                                    | 0/1 [00:00<?, ?ba/s]\r\n    run_model(auto_envvar_prefix=""GFB_CIES"")  # pragma: no cover\r\n  File ""/Users/timothy.laurent/.virtualenvs/inv-text2struct/lib/python3.6/site-packages/click/core.py"", line 829, in __call__\r\n    return self.main(*args, **kwargs)                                                                                   | 0/1 [00:00<?, ?ba/s]\r\n  File ""/Users/timothy.laurent/.virtualenvs/inv-text2struct/lib/python3.6/site-packages/click/core.py"", line 782, in main\r\n    rv = self.invoke(ctx)\r\n  File ""/Users/timothy.laurent/.virtualenvs/inv-text2struct/lib/python3.6/site-packages/click/core.py"", line 1236, in invoke\r\n    return Command.invoke(self, ctx)\r\n  File ""/Users/timothy.laurent/.virtualenvs/inv-text2struct/lib/python3.6/site-packages/click/core.py"", line 1066, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File ""/Users/timothy.laurent/.virtualenvs/inv-text2struct/lib/python3.6/site-packages/click/core.py"", line 610, in invoke\r\n    return callback(*args, **kwargs)\r\n  File ""/Users/timothy.laurent/.virtualenvs/inv-text2struct/lib/python3.6/site-packages/click/decorators.py"", line 21, in new_func\r\n    return f(get_current_context(), *args, **kwargs)\r\n  File ""text2struct/run_model.py"", line 136, in run_model\r\n    ctx.invoke(ctx.command.commands[config_dict[""mode""]])\r\n  File ""/Users/timothy.laurent/.virtualenvs/inv-text2struct/lib/python3.6/site-packages/click/core.py"", line 610, in invoke\r\n    return callback(*args, **kwargs)\r\n  File ""/Users/timothy.laurent/.virtualenvs/inv-text2struct/lib/python3.6/site-packages/click/decorators.py"", line 21, in new_func\r\n    return f(get_current_context(), *args, **kwargs)\r\n  File ""text2struct/run_model.py"", line 187, in train\r\n    run_train_model(_parse_subcommand(ctx))\r\n  File ""text2struct/run_model.py"", line 241, in run_train_model\r\n    checkpoint_steps=config.train.checkpoint_steps,\r\n  File ""/Users/timothy.laurent/src/inv-text2struct/text2struct/model/train.py"", line 153, in alternate_training\r\n    max_len=config.model.dim.max_len,\r\n  File ""/Users/timothy.laurent/src/inv-text2struct/text2struct/model/dataset.py"", line 466, in load_prodigy_tf_datasets\r\n    folder, file_patterns, vocabulary, cache_dir=cache_dir, test_pct=test_pct\r\n  File ""/Users/timothy.laurent/src/inv-text2struct/text2struct/model/dataset.py"", line 447, in load_prodigy_arrow_datasets\r\n    fn_kwargs=dict(tokenizer=vocabulary.tokenizer, vocabulary=vocabulary),\r\n  File ""/Users/timothy.laurent/.virtualenvs/inv-text2struct/lib/python3.6/site-packages/datasets/arrow_dataset.py"", line 1224, in map\r\n    update_data = does_function_return_dict(test_inputs, test_indices)\r\n  File ""/Users/timothy.laurent/.virtualenvs/inv-text2struct/lib/python3.6/site-packages/datasets/arrow_dataset.py"", line 1195, in does_function_return_dict\r\n    function(*fn_args, indices, **fn_kwargs) if with_indices else function(*fn_args, **fn_kwargs)\r\n  File ""/Users/timothy.laurent/src/inv-text2struct/text2struct/model/dataset.py"", line 348, in relation_ds_factory\r\n    ner_labels = example[""ner_labels""]\r\nKeyError: \'ner_labels\'\r\n\r\n```'
 ""The parallelism is automatically disabled on `tokenizers` when the process gets forked, while we already used the parallelism capabilities of a tokenizer. We have to do it in order to avoid having the process hang, because we cannot safely fork a multithreaded process (cf https://github.com/huggingface/tokenizers/issues/187).\r\nSo if possible, the tokenizers shouldn't be used before the fork, so that each process can then make use of the parallelism. Otherwise using `TOKENIZERS_PARALLELISM=false` is the way to go.""
 '> Thanks for taking a look. I pulled the master but I still see the key error.\r\n\r\nI am no longer able to get the error since #659 was merged. Not sure why you still have it @timothyjlaurent \r\nMaybe it is a cache issue ? Could you try to use `load_from_cache_file=False` in your `.map()` calls ?'
 ""> The parallelism is automatically disabled on `tokenizers` when the process gets forked, while we already used the parallelism capabilities of a tokenizer. We have to do it in order to avoid having the process hang, because we cannot safely fork a multithreaded process (cf [huggingface/tokenizers#187](https://github.com/huggingface/tokenizers/issues/187)).\r\n> So if possible, the tokenizers shouldn't be used before the fork, so that each process can then make use of the parallelism. Otherwise using `TOKENIZERS_PARALLELISM=false` is the way to go.\r\n\r\nOk thanks :)\r\n\r\nIs there something we should do on the `datasets` side to avoid that that the program hangs ?\r\n\r\nAlso when doing `.map` with a tokenizer, the tokenizer is called once on the first examples of the dataset to check the function output before spawning the processes. Is that compatible with how tokenizers are supposed to be used with multiprocessing ?""
 '#659 fixes the empty dict issue\r\n#688 fixes the hang issue'
 ""Hmmm I pulled the latest commit, `b93c5517f70a480533a44e0c42638392fd53d90`, and I'm still seeing both the hanging and the key error. ""
 ""Hi @timothyjlaurent \r\n\r\nThe hanging fix just got merged, that why you still had it.\r\n\r\nFor the key error it's possible that the code you ran reused cached datasets from where the KeyError bug was still there.\r\nCould you try to clear your cache or make sure that it doesn't reuse cached data with `.map(..., load_from_cache=False)` ?\r\nLet me know if it it helps""
 'Hi @lhoestq , \r\n\r\nThanks for letting me know about the update.\r\n\r\nSo I don\'t think it\'s the caching - because hashing mechanism isn\'t stable for me -- but that\'s a different issue. In any case I `rm -rf ~/.cache/huggingface` to make a clean slate.\r\n\r\nI synced with master and I see the key error has gone away, I tried with and without the `TOKENIZERS_PARALLELISM` variable set and see the log line for setting the value false before the map.\r\n\r\nNow I\'m seeing an issue with `.train_test_split()` on datasets that are the product of a multiprocess map.\r\n\r\nHere is the stack trace\r\n\r\n```\r\n  File ""/Users/timothy.laurent/src/inv-text2struct/text2struct/model/dataset.py"", line 451, in load_prodigy_arrow_datasets\r\n    ner_ds_dict = ner_ds.train_test_split(test_size=test_pct, shuffle=True, seed=seed)\r\n  File ""/Users/timothy.laurent/.virtualenvs/inv-text2struct/src/datasets/src/datasets/arrow_dataset.py"", line 168, in wrapper\r\n    dataset.set_format(**new_format)\r\n  File ""/Users/timothy.laurent/.virtualenvs/inv-text2struct/src/datasets/src/datasets/fingerprint.py"", line 163, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File ""/Users/timothy.laurent/.virtualenvs/inv-text2struct/src/datasets/src/datasets/arrow_dataset.py"", line 794, in set_format\r\n    list(filter(lambda col: col not in self._data.column_names, columns)), self._data.column_names\r\nValueError: Columns [\'train\', \'test\'] not in the dataset. Current columns in the dataset: [\'_input_hash\', \'_task_hash\', \'_view_id\', \'answer\', \'encoding__ids\', \'encoding__offsets\', \'encoding__overflowing\', \'encoding__tokens\', \'encoding__words\', \'ner_ids\', \'ner_labels\', \'relations\', \'spans\', \'text\', \'tokens\']\r\n```\r\n\r\n\r\n'
 ""Thanks for reporting.\r\nI'm going to fix that and add a test case so that it doesn't happen again :) \r\nI'll let you know when it's done\r\n\r\nIn the meantime if you could make a google colab that reproduces the issue it would be helpful ! @timothyjlaurent ""
 'Sure thing, @lhoestq.\r\n\r\nhttps://colab.research.google.com/drive/1lg4fbyrUO6m8ssQ2dNdVFaUqMUfA2zZ3?usp=sharing'
 ""Thanks @timothyjlaurent ! I just merged a fix on master. I also checked your notebook and it looks like it's working now.\r\nI added some tests to make sure it works as expected now :)""
 ""Great, @lhoestq . I'm trying to verify in the colab:\r\nchanged\r\n```\r\n!pip install datasets\r\n```\r\nto \r\n\r\n```\r\n!pip install git+https://github.com/huggingface/datasets@master\r\n```\r\n\r\nBut I'm still seeing the error - I wonder why?""
 ""It works on my side @timothyjlaurent on google colab.\r\nDid you try to uninstall datasets first, before updating it to master's version ?""
 ""I didn't -- it was a new sessions --- buuut - look like it's working today -- woot! I'll close this issue. Thanks @lhoestq ""]","After upgrading to the 1.0 started seeing errors in my data loading script after enabling multiprocessing.

```python
    ...
    ner_ds_dict = ner_ds.train_test_split(test_size=test_pct, shuffle=True, seed=seed)
    ner_ds_dict[""validation""] = ner_ds_dict[""test""]
    rel_ds_dict = rel_ds.train_test_split(test_size=test_pct, shuffle=True, seed=seed)
    rel_ds_dict[""validation""] = rel_ds_dict[""test""]
    return ner_ds_dict, rel_ds_dict
```

The first train_test_split, `ner_ds`/`ner_ds_dict`, returns a `train` and `test` split that are iterable.
The second, `rel_ds`/`rel_ds_dict` in this case, returns a Dataset dict that has rows but if selected from or sliced into into returns an empty dictionary. eg `rel_ds_dict['train'][0] == {}` and `rel_ds_dict['train'][0:100] == {}`.

Ok I think I know the problem -- the rel_ds was mapped though a mapper with `num_proc=12`. If I remove `num_proc`. The dataset loads.

I also see errors with other map and filter functions when `num_proc` is set.

```
Done writing 67 indices in 536 bytes .
Done writing 67 indices in 536 bytes .
Fatal Python error: PyCOND_WAIT(gil_cond) failed
```"
https://github.com/huggingface/datasets/issues/619,Mistakes in MLQA features names,"[""Indeed you're right ! Thanks for reporting that\r\n\r\nCould you open a PR to fix the features names ?""]","I think the following features in MLQA shouldn't be named the way they are:
1. `questions` (should be `question`)
2. `ids` (should be `id`)
3. `start` (should be `answer_start`)

The reasons I'm suggesting these features be renamed are:
* To make them consistent with other QA datasets like SQuAD, XQuAD, TyDiQA etc. and hence make it easier to concatenate multiple QA datasets.
* The features names are not the same as the ones provided in the original MLQA datasets (it uses the names I suggested).

I know these columns can be renamed using  using `Dataset.rename_column_`, `questions` and `ids` can be easily renamed but `start` on the other hand is annoying to rename since it's nested inside the feature `answers`.
"
https://github.com/huggingface/datasets/issues/617,Compare different Rouge implementations  ,"[""Updates - the differences between the following three\r\n(1) https://github.com/bheinzerling/pyrouge (previously popular. The one I trust the most)\r\n(2) https://github.com/google-research/google-research/tree/master/rouge\r\n(3) https://github.com/pltrdy/files2rouge (used in fairseq)\r\ncan be explained by two things, stemming and handling multiple sentences.\r\n\r\nStemming: \r\n(1), (2): default is no stemming. (3): default is with stemming ==> No stemming is the correct default as you did [here](https://github.com/huggingface/datasets/blob/master/metrics/rouge/rouge.py#L84)\r\n\r\nMultiple sentences:\r\n(1) `rougeL` splits text using `\\n`\r\n(2) `rougeL` ignores `\\n`. \r\n(2) `rougeLsum` splits text using `\\n`\r\n(3) `rougeL` splits text using `.`\r\n\r\nFor (2), `rougeL` and `rougeLsum` are identical if the sequence doesn't contain `\\n`. With `\\n`, it is `rougeLsum` that matches (1) not `rougeL`. \r\n\r\nOverall, and as far as I understand, for your implementation here https://github.com/huggingface/datasets/blob/master/metrics/rouge/rouge.py#L65 to match the default, you only need to change `rougeL` [here](https://github.com/huggingface/datasets/blob/master/metrics/rouge/rouge.py#L86) to `rougeLsum` to correctly compute metrics for text with newlines.\r\n\r\nTagging @sshleifer who might be interested.""
 ""Thanks for the clarification !\r\nWe're adding Rouge Lsum in #701 ""
 ""This is a real issue, sorry for missing the mention @ibeltagy\r\n\r\nWe implemented a more involved [solution](https://github.com/huggingface/transformers/blob/99cb924bfb6c4092bed9232bea3c242e27c6911f/examples/seq2seq/utils.py#L481) that enforces that sentences are split with `\\n` so that rougeLsum scores match papers even if models don't generate newlines. \r\n\r\nUnfortunately, the best/laziest way I found to do this introduced an `nltk` dependency (For sentence splitting, all sentences don't end in `.`!!!), but this might be avoidable with some effort.\r\n\r\n#### Sidebar: Wouldn't Deterministic Be Better?\r\n\r\n`rouge_scorer.scoring.BootstrapAggregator` is well named but is not deterministic which I would like to change for my mental health, unless there is some really good reason to sample 500 observations before computing f-scores.\r\n\r\nI have a fix on a branch, but I wanted to get some context before introducting a 4th way to compute rouge. Scores are generally within .03 Rouge2 of boostrap after multiplying by 100, e.g 22.05 vs 22.08 Rouge2.\r\n\r\n""
 ""> This is a real issue, sorry for missing the mention @ibeltagy\r\n> \r\n> We implemented a more involved [solution](https://github.com/huggingface/transformers/blob/99cb924bfb6c4092bed9232bea3c242e27c6911f/examples/seq2seq/utils.py#L481) that enforces that sentences are split with `\\n` so that rougeLsum scores match papers even if models don't generate newlines.\r\n> \r\n> Unfortunately, the best/laziest way I found to do this introduced an `nltk` dependency (For sentence splitting, all sentences don't end in `.`!!!), but this might be avoidable with some effort.\r\n\r\nThanks for the details, I didn't know about that. Maybe we should consider adding this processing step or at least mention it somewhere in the library or the documentation\r\n\r\n> #### Sidebar: Wouldn't Deterministic Be Better?\r\n> `rouge_scorer.scoring.BootstrapAggregator` is well named but is not deterministic which I would like to change for my mental health, unless there is some really good reason to sample 500 observations before computing f-scores.\r\n> \r\n> I have a fix on a branch, but I wanted to get some context before introducting a 4th way to compute rouge. Scores are generally within .03 Rouge2 of boostrap after multiplying by 100, e.g 22.05 vs 22.08 Rouge2.\r\n\r\nI think the default `n_samples` of the aggregator is 1000. We could increase it or at least allow users to change it if they want more precise results.""
 'Hi, thanks for the solution. \r\n\r\nI am not sure if this is a bug, but on line [510](https://github.com/huggingface/transformers/blob/99cb924bfb6c4092bed9232bea3c242e27c6911f/examples/seq2seq/utils.py#L510), are pred, tgt supposed to be swapped?'
 'This looks like a bug in an old version of the examples in `transformers`']","I used RougeL implementation provided in `datasets` [here](https://github.com/huggingface/datasets/blob/master/metrics/rouge/rouge.py) and it gives numbers that match those reported in the pegasus paper but very different from those reported in other papers, [this](https://arxiv.org/pdf/1909.03186.pdf) for example.
Can you make sure the google-research implementation you are using matches the official perl implementation? 
There are a couple of python wrappers around the perl implementation, [this](https://pypi.org/project/pyrouge/) has been commonly used, and [this](https://github.com/pltrdy/files2rouge) is used in fairseq). 
There's also a python reimplementation [here](https://github.com/pltrdy/rouge) but its RougeL numbers are way off. 
"
https://github.com/huggingface/datasets/issues/616,"UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors","['I have the same issue'
 'Same issue here when Trying to load a dataset from disk.'
 ""I am also experiencing this issue, and don't know if it's affecting my training.""
 'Same here. I hope the dataset is not being modified in-place.'
 ""I think the only way to avoid this warning would be to do a copy of the numpy array before providing it.\r\n\r\nThis would slow down a bit the iteration over the dataset but maybe it would be safer. We could disable the copy with a flag on the `set_format` command.\r\n\r\nIn most typical cases of training a NLP model, PyTorch shouldn't modify the input so it's ok to have a non-writable array but I can understand the warning is a bit scary so maybe we could choose the side of non-warning/slower by default and have an option to speedup.\r\n\r\nWhat do you think @lhoestq ? ""
 ""@thomwolf Would it be possible to have the array look writeable, but raise an error if it is actually written to?\r\n\r\nI would like to keep my code free of warning, but I also wouldn't like to slow down the program because of unnecessary copy operations.   ""
 '@AndreasMadsen probably not I would guess (no free lunch hahah)'
 '@thomwolf Why not? Writable is checked with `arr.flags.writeable`, and writing is done via magic methods.'
 ""Well because I don't know the internal of numpy as well as you I guess hahahah, do you want to try to open a PR proposing a solution?""
 ""@thomwolf @AndreasMadsen I think this is a terrible idea, n/o, and I am very much against it. Modifying internals of an array in such a hacky way is bound to run into other (user) issues down the line. To users it would not be clear at all what is going on e.g. when they check for write access (which will return True) but then they get a warning that the array is not writeable. That's extremely confusing. \r\n\r\nIf your only goal is to get rid of warnings in your code, then you can just use a [simplefilter](https://docs.python.org/3.8/library/warnings.html#temporarily-suppressing-warnings) for UserWarnings in your own code. Changing the code-base in such an intuitive way does not seem like a good way to go and sets a bad precedent, imo. \r\n\r\n(Feel free to disagree, of course.)\r\n\r\nIMO a warning can stay (as they can be filtered by users anyway), but it can be clarified why the warning takes place.""
 ""> To users it would not be clear at all what is going on e.g. when they check for write access (which will return True) but then they get a warning that the array is not writeable. That's extremely confusing.\r\n\r\nConfusion can be resolved with a helpful error message. In this case, that error message can be controlled by huggingface/datasets. The right argument here is that if code depends on `.flags.writable` being truthful (not just for warnings), then it will cause unavoidable errors. Although, I can't imagine such a use-case.\r\n\r\n> If your only goal is to get rid of warnings in your code, then you can just use a simplefilter for UserWarnings in your own code. Changing the code-base in such an intuitive way does not seem like a good way to go and sets a bad precedent, imo.\r\n\r\nI don't want to ignore all `UserWarnings`, nor all warnings regarding non-writable arrays. Ignoring warnings leads to hard to debug issues.\r\n\r\n> IMO a warning can stay (as they can be filtered by users anyway), but it can be clarified why the warning takes place.\r\n\r\nPlain use cases should really not generate warnings. It teaches developers to ignore warnings which is a terrible practice.\r\n\r\n---\r\n\r\nThe best solution would be to allow non-writable arrays in `DataLoader`, but that is a PyTorch issue.""
 '> The right argument here is that if code depends on `.flags.writable` being truthful (not just for warnings), then it will cause unavoidable errors. Although, I can\'t imagine such a use-case.\r\n\r\nThat\'s exactly the argument in my first sentence. Too often someone ""cannot think of a use-case"", but you can not foresee the use-cases of a whole research community.\r\n \r\n> I don\'t want to ignore all `UserWarnings`, nor all warnings regarding non-writable arrays. Ignoring warnings leads to hard to debug issues.\r\n\r\nThat\'s fair.\r\n\r\n> Plain use cases should really not generate warnings. It teaches developers to ignore warnings which is a terrible practice.\r\n\r\nBut this is not a plain use-case (because Pytorch does not support these read-only tensors). Manually setting the flag to writable will solve the issue on the surface but is basically just a hack to compensate for something that is not allowed in another library. \r\n\r\nWhat about an ""ignore_warnings"" flag in `set_format` that when True wraps the offending code in a block to ignore userwarnings at that specific step in [_convert_outputs](https://github.com/huggingface/datasets/blob/880c2c76a8223a00c303eab2909371e857113063/src/datasets/arrow_dataset.py#L821)? Something like:\r\n\r\n```python\r\ndef _convert_outputs(..., ignore_warnings=True):\r\n    ...\r\n    with warnings.catch_warnings():\r\n        if ignore_warnings:\r\n            warnings.simplefilter(""ignore"", UserWarning)\r\n        return torch.tensor(...)\r\n# continues without warning filter after context manager...\r\n```'
 '> But this is not a plain use-case (because Pytorch does not support these read-only tensors).\r\n\r\nBy ""plain"", I mean the recommended way to use `datasets` with PyTorch according to the `datasets` documentation.'
 'This error is what I see when I run the first lines of the Pytorch Quickstart.  It should also say that it should be ignored and/or how to fix it.   BTW, this is a Pytorch error message -- not a Huggingface error message.   My code runs anyway.']","I am trying out the library and want to load in pickled data with `from_dict`. In that dict, one column `text` should be tokenized and the other (an embedding vector) should be retained. All other columns should be removed. When I eventually try to set the format for the columns with `set_format` I am getting this strange Userwarning without a stack trace:

> Set __getitem__(key) output type to torch for ['input_ids', 'sembedding'] columns  (when key is int or slice) and don't output other (un-formatted) columns.
> C:\Users\bramv\.virtualenvs\dutch-simplification-nbNdqK9u\lib\site-packages\datasets\arrow_dataset.py:835: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\torch\csrc\utils\tensor_numpy.cpp:141.)
>   return torch.tensor(x, **format_kwargs)

The first one might not be related to the warning, but it is odd that it is shown, too. It is unclear whether that is something that I should do or something that that the program is doing at that moment.

Snippet:
```
    dataset = Dataset.from_dict(torch.load(""data/dummy.pt.pt""))
    print(dataset)
    tokenizer = AutoTokenizer.from_pretrained(""bert-base-cased"")
    keys_to_retain = {""input_ids"", ""sembedding""}
    dataset = dataset.map(lambda example: tokenizer(example[""text""], padding='max_length'), batched=True)
    dataset.remove_columns_(set(dataset.column_names) - keys_to_retain)

    dataset.set_format(type=""torch"", columns=[""input_ids"", ""sembedding""])
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=2)

    print(next(iter(dataloader)))
```

PS: the input type for `remove_columns_` should probably be an Iterable rather than just a List."
https://github.com/huggingface/datasets/issues/615,Offset overflow when slicing a big dataset with an array of indices in Pyarrow >= 1.0.0,"[""Related: https://issues.apache.org/jira/browse/ARROW-9773\r\n\r\nIt's definitely a size thing. I took a smaller dataset with 87000 rows and did:\r\n```\r\nfor i in range(10,1000,20):\r\n    table = pa.concat_tables([dset._data]*i)\r\n    table.take([0])\r\n```\r\nand it broke at around i=300.\r\n\r\nAlso when `_indices` is not None, this breaks indexing by slice. E.g. `dset.shuffle()[:1]` breaks.\r\n\r\nLuckily so far I haven't seen `_indices.column(0).take` break, which means it doesn't break `select` or anything like that which is where the speed really matters, it's just `_getitem`. So I'm currently working around it by just doing the arrow v0 method in `_getitem`:\r\n```\r\n#if PYARROW_V0:\r\ndata_subset = pa.concat_tables(\r\n    self._data.slice(indices_array[i].as_py(), 1) for i in range(len(indices_array))\r\n)\r\n#else:\r\n    #data_subset = self._data.take(indices_array)\r\n```""
 'Let me know if you meet other offset overflow issues @joeddav ']","How to reproduce:

```python
from datasets import load_dataset

wiki = load_dataset(""wikipedia"", ""20200501.en"", split=""train"")
wiki[[0]]

---------------------------------------------------------------------------
ArrowInvalid                              Traceback (most recent call last)
<ipython-input-13-381aedc9811b> in <module>
----> 1 wikipedia[[0]]

~/Desktop/hf/nlp/src/datasets/arrow_dataset.py in __getitem__(self, key)
   1069             format_columns=self._format_columns,
   1070             output_all_columns=self._output_all_columns,
-> 1071             format_kwargs=self._format_kwargs,
   1072         )
   1073 

~/Desktop/hf/nlp/src/datasets/arrow_dataset.py in _getitem(self, key, format_type, format_columns, output_all_columns, format_kwargs)
   1037                 )
   1038             else:
-> 1039                 data_subset = self._data.take(indices_array)
   1040 
   1041             if format_type is not None:

~/.virtualenvs/hf-datasets/lib/python3.7/site-packages/pyarrow/table.pxi in pyarrow.lib.Table.take()

~/.virtualenvs/hf-datasets/lib/python3.7/site-packages/pyarrow/compute.py in take(data, indices, boundscheck)
    266     """"""
    267     options = TakeOptions(boundscheck)
--> 268     return call_function('take', [data, indices], options)
    269 
    270 

~/.virtualenvs/hf-datasets/lib/python3.7/site-packages/pyarrow/_compute.pyx in pyarrow._compute.call_function()

~/.virtualenvs/hf-datasets/lib/python3.7/site-packages/pyarrow/_compute.pyx in pyarrow._compute.Function.call()

~/.virtualenvs/hf-datasets/lib/python3.7/site-packages/pyarrow/error.pxi in pyarrow.lib.pyarrow_internal_check_status()

~/.virtualenvs/hf-datasets/lib/python3.7/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()

ArrowInvalid: offset overflow while concatenating arrays
```

It seems to work fine with small datasets or with pyarrow 0.17.1"
https://github.com/huggingface/datasets/issues/611,"ArrowCapacityError: List array cannot contain more than 2147483646 child elements, have 2147483648","['Can you give us stats/information on your pandas DataFrame?'
 ""```\r\n<class 'pandas.core.frame.DataFrame'>\r\nInt64Index: 17136104 entries, 0 to 17136103\r\nData columns (total 6 columns):\r\n #   Column        Dtype  \r\n---  ------        -----  \r\n 0   item_id       int64  \r\n 1   item_titl     object \r\n 2   start_price   float64\r\n 3   shipping_fee  float64\r\n 4   picture_url   object \r\n 5   embeddings    object \r\ndtypes: float64(2), int64(1), object(3)\r\nmemory usage: 915.2+ MB\r\n```""
 'Thanks and some more on the `embeddings` and `picture_url` would be nice as well (type and max lengths of the elements)'
 ""`embedding` is `np.array` of shape `(128,)`. `picture_url` is url, such as 'https://i.ebayimg.com/00/s/MTE5OVgxNjAw/z/ZOsAAOSwAG9fHQq5/$_12.JPG?set_id=880000500F;https://i.ebayimg.com/00/s/MTE5OVgxNjAw/z/OSgAAOSwokBfHQq8/$_12.JPG?set_id=880000500F'""
 'It looks like a Pyarrow limitation.\r\nI was able to reproduce the error with \r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\nimport pyarrow as pa\r\n\r\n n = 1713614\r\ndf = pd.DataFrame.from_dict({""a"": list(np.zeros((n, 128))), ""b"": range(n)})\r\npa.Table.from_pandas(df)\r\n```\r\n\r\nI also tried with 50% of the dataframe and it actually works.\r\nI created an issue on Apache Arrow\'s JIRA [here](https://issues.apache.org/jira/browse/ARROW-9976)\r\n\r\nOne way to fix that would be to chunk the dataframe and concatenate arrow tables.'
 ""It looks like it's going to be fixed in pyarrow 2.0.0 :)\r\n\r\nIn the meantime I suggest to chunk big dataframes to create several small datasets, and then concatenate them using [concatenate_datasets](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=concatenate#datasets.concatenate_datasets)""]","Hi, I'm trying to load a dataset from Dataframe, but I get the error:
```bash
---------------------------------------------------------------------------
ArrowCapacityError                        Traceback (most recent call last)
<ipython-input-7-146b6b495963> in <module>
----> 1 dataset = Dataset.from_pandas(emb)

~/miniconda3/envs/dev/lib/python3.7/site-packages/nlp/arrow_dataset.py in from_pandas(cls, df, features, info, split)
    223         info.features = features
    224         pa_table: pa.Table = pa.Table.from_pandas(
--> 225             df=df, schema=pa.schema(features.type) if features is not None else None
    226         )
    227         return cls(pa_table, info=info, split=split)

~/miniconda3/envs/dev/lib/python3.7/site-packages/pyarrow/table.pxi in pyarrow.lib.Table.from_pandas()

~/miniconda3/envs/dev/lib/python3.7/site-packages/pyarrow/pandas_compat.py in dataframe_to_arrays(df, schema, preserve_index, nthreads, columns, safe)
    591         for i, maybe_fut in enumerate(arrays):
    592             if isinstance(maybe_fut, futures.Future):
--> 593                 arrays[i] = maybe_fut.result()
    594 
    595     types = [x.type for x in arrays]

~/miniconda3/envs/dev/lib/python3.7/concurrent/futures/_base.py in result(self, timeout)
    426                 raise CancelledError()
    427             elif self._state == FINISHED:
--> 428                 return self.__get_result()
    429 
    430             self._condition.wait(timeout)

~/miniconda3/envs/dev/lib/python3.7/concurrent/futures/_base.py in __get_result(self)
    382     def __get_result(self):
    383         if self._exception:
--> 384             raise self._exception
    385         else:
    386             return self._result

~/miniconda3/envs/dev/lib/python3.7/concurrent/futures/thread.py in run(self)
     55 
     56         try:
---> 57             result = self.fn(*self.args, **self.kwargs)
     58         except BaseException as exc:
     59             self.future.set_exception(exc)

~/miniconda3/envs/dev/lib/python3.7/site-packages/pyarrow/pandas_compat.py in convert_column(col, field)
    557 
    558         try:
--> 559             result = pa.array(col, type=type_, from_pandas=True, safe=safe)
    560         except (pa.ArrowInvalid,
    561                 pa.ArrowNotImplementedError,

~/miniconda3/envs/dev/lib/python3.7/site-packages/pyarrow/array.pxi in pyarrow.lib.array()

~/miniconda3/envs/dev/lib/python3.7/site-packages/pyarrow/array.pxi in pyarrow.lib._ndarray_to_array()

~/miniconda3/envs/dev/lib/python3.7/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()

ArrowCapacityError: List array cannot contain more than 2147483646 child elements, have 2147483648
```
My code is :
```python
from nlp import Dataset
dataset = Dataset.from_pandas(emb)
```"
https://github.com/huggingface/datasets/issues/610,Load text file for RoBERTa pre-training. ,"['Could you try\r\n```python\r\nload_dataset(\'text\', data_files=\'test.txt\',cache_dir=""./"", split=""train"")\r\n```\r\n?\r\n\r\n`load_dataset` returns a dictionary by default, like {""train"": your_dataset}'
 'Hi @lhoestq\r\nThanks for your suggestion.\r\n\r\nI tried \r\n```\r\ndataset = load_dataset(\'text\', data_files=\'test.txt\',cache_dir=""./"", split=""train"")\r\nprint(dataset)\r\ndataset.set_format(type=\'torch\',columns=[""text""])\r\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=8)\r\nnext(iter(dataloader))\r\n```\r\n\r\nBut it still doesn\'t work and got error:\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-7-388aca337e2f> in <module>\r\n----> 1 next(iter(dataloader))\r\n\r\n/Library/Python/3.7/site-packages/torch/utils/data/dataloader.py in __next__(self)\r\n    361 \r\n    362     def __next__(self):\r\n--> 363         data = self._next_data()\r\n    364         self._num_yielded += 1\r\n    365         if self._dataset_kind == _DatasetKind.Iterable and \\\r\n\r\n/Library/Python/3.7/site-packages/torch/utils/data/dataloader.py in _next_data(self)\r\n    401     def _next_data(self):\r\n    402         index = self._next_index()  # may raise StopIteration\r\n--> 403         data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\r\n    404         if self._pin_memory:\r\n    405             data = _utils.pin_memory.pin_memory(data)\r\n\r\n/Library/Python/3.7/site-packages/torch/utils/data/_utils/fetch.py in fetch(self, possibly_batched_index)\r\n     42     def fetch(self, possibly_batched_index):\r\n     43         if self.auto_collation:\r\n---> 44             data = [self.dataset[idx] for idx in possibly_batched_index]\r\n     45         else:\r\n     46             data = self.dataset[possibly_batched_index]\r\n\r\n/Library/Python/3.7/site-packages/torch/utils/data/_utils/fetch.py in <listcomp>(.0)\r\n     42     def fetch(self, possibly_batched_index):\r\n     43         if self.auto_collation:\r\n---> 44             data = [self.dataset[idx] for idx in possibly_batched_index]\r\n     45         else:\r\n     46             data = self.dataset[possibly_batched_index]\r\n\r\n/Library/Python/3.7/site-packages/datasets-0.4.0-py3.7.egg/datasets/arrow_dataset.py in __getitem__(self, key)\r\n   1069             format_columns=self._format_columns,\r\n   1070             output_all_columns=self._output_all_columns,\r\n-> 1071             format_kwargs=self._format_kwargs,\r\n   1072         )\r\n   1073 \r\n\r\n/Library/Python/3.7/site-packages/datasets-0.4.0-py3.7.egg/datasets/arrow_dataset.py in _getitem(self, key, format_type, format_columns, output_all_columns, format_kwargs)\r\n   1056                 format_columns=format_columns,\r\n   1057                 output_all_columns=output_all_columns,\r\n-> 1058                 format_kwargs=format_kwargs,\r\n   1059             )\r\n   1060         return outputs\r\n\r\n/Library/Python/3.7/site-packages/datasets-0.4.0-py3.7.egg/datasets/arrow_dataset.py in _convert_outputs(self, outputs, format_type, format_columns, output_all_columns, format_kwargs)\r\n    872                     continue\r\n    873                 if format_columns is None or k in format_columns:\r\n--> 874                     v = map_nested(command, v, **map_nested_kwargs)\r\n    875                 output_dict[k] = v\r\n    876         return output_dict\r\n\r\n/Library/Python/3.7/site-packages/datasets-0.4.0-py3.7.egg/datasets/utils/py_utils.py in map_nested(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, types)\r\n    214     # Singleton\r\n    215     if not isinstance(data_struct, dict) and not isinstance(data_struct, types):\r\n--> 216         return function(data_struct)\r\n    217 \r\n    218     disable_tqdm = bool(logger.getEffectiveLevel() > INFO)\r\n\r\n/Library/Python/3.7/site-packages/datasets-0.4.0-py3.7.egg/datasets/arrow_dataset.py in command(x)\r\n    833                     if x.dtype == np.object:  # pytorch tensors cannot be instantied from an array of objects\r\n    834                         return [map_nested(command, i, **map_nested_kwargs) for i in x]\r\n--> 835                 return torch.tensor(x, **format_kwargs)\r\n    836 \r\n    837         elif format_type == ""tensorflow"":\r\n\r\nTypeError: new(): invalid data type \'str\'\r\n```\r\n\r\nI found type can be [\'numpy\', \'torch\', \'tensorflow\', \'pandas\'] only, how can I deal with the string type?'
 'You need to tokenize the string inputs to convert them in integers before you can feed them to a pytorch dataloader.\r\n\r\nYou can read the quicktour of the datasets or the transformers libraries to know more about that:\r\n- transformers: https://huggingface.co/transformers/quicktour.html\r\n- dataset: https://huggingface.co/docs/datasets/quicktour.html'
 ""Hey @chiyuzhang94, I was also having trouble in loading a large text file (11GB).\r\nBut finally got it working. This is what I did after looking into the documentation.\r\n\r\n1. split the whole dataset file into smaller files\r\n```bash\r\nmkdir ./shards\r\nsplit -a 4 -l 256000 -d full_raw_corpus.txt ./shards/shard_\r\n````\r\n2. Pass paths of small data files to `load_dataset`\r\n```python\r\nfiles = glob.glob('shards/*')\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('text', data_files=files, split='train')\r\n```\r\n(On passing the whole dataset file (11GB) directly to `load_dataset` was resulting into RAM issue)\r\n\r\n3. Tokenization\r\n```python\r\ndef encode(examples):\r\n  return tokenizer(examples['text'], truncation=True, padding='max_length')\r\ndataset = dataset.map(encode, batched=True)\r\ndataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\r\n```\r\n Now you can pass `dataset` to `Trainer` or `pytorch DataLoader`\r\n```python\r\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=4)\r\nnext(iter(dataloader))\r\n```\r\nHope this helps\r\n""
 'Thanks, @thomwolf  and @sipah00 ,\r\n\r\nI tried to implement your suggestions in my scripts. \r\nNow, I am facing some connection time-out error. I am using my local file, I have no idea why the module request s3 database.\r\n\r\nThe log is:\r\n```\r\nTraceback (most recent call last):\r\n  File ""/home/.local/lib/python3.6/site-packages/requests/adapters.py"", line 449, in send\r\n    raise err\r\n  File ""/home/.local/lib/python3.6/site-packages/urllib3/util/connection.py"", line 74, in create_connection\r\n    timeout=timeout\r\n  File ""/home/.local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 720, in urlopen\r\n    sock.connect(sa)\r\nTimeoutError: [Errno 110] Connection timed out\r\n\r\nTraceback (most recent call last):\r\n  File ""/home/.local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 672, in urlopen\r\n    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\r\n  File ""/home/.local/lib/python3.6/site-packages/urllib3/util/retry.py"", line 436, in increment\r\n    chunked=chunked,\r\n  File ""/home/.local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 376, in _make_request\r\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\r\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host=\'s3.amazonaws.com\', port=443): Max retries exceeded with url: /datasets.huggingface.co/datasets/datasets/text/text.py (Caused by NewConnectionError(\'<urllib3.connection.VerifiedHTTPSConnection obj\r\nect at 0x7fff401e0e48>: Failed to establish a new connection: [Errno 110] Connection timed out\',))\r\n\r\nTraceback (most recent call last):\r\n  File ""/scratch/roberta_emohash/run_language_modeling.py"", line 1019, in <module>\r\n    main()\r\n  File ""/scratch/roberta_emohash/run_language_modeling.py"", line 962, in main\r\n    train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False)\r\n  File ""/scratch/roberta_emohash/run_language_modeling.py"", line 177, in load_and_cache_examples\r\n    return HG_Datasets(tokenizer, file_path, args)\r\n  File ""/scratch/roberta_emohash/run_language_modeling.py"", line 117, in HG_Datasets\r\n    dataset = load_dataset(\'text\', data_files=files, cache_dir = args.data_cache_dir, split=""train"")\r\n  File ""/arc/project/evn_py36/datasets/datasets/src/datasets/load.py"", line 590, in load_dataset\r\n    self._validate_conn(conn)\r\n  File ""/home/.local/lib/python3.6/site-packages/urllib3/connectionpool.py"", line 994, in _validate_conn\r\n    conn.connect()\r\n  File ""/home/.local/lib/python3.6/site-packages/urllib3/connection.py"", line 300, in connect\r\n    conn = self._new_conn()\r\n  File ""/home/.local/lib/python3.6/site-packages/urllib3/connection.py"", line 169, in _new_conn\r\n    self, ""Failed to establish a new connection: %s"" % e\r\nurllib3.exceptions.NewConnectionError: <urllib3.connection.VerifiedHTTPSConnection object at 0x7fff401e0da0>: Failed to establish a new connection: [Errno 110] Connection timed out\r\n\r\n``` \r\n\r\nDo you have any experience on this issue?'
 ""No, I didn't encounter this problem, it seems to me a network problem""
 'I noticed this is because I use a cloud server where does not provide for connections from our standard compute nodes to outside resources. \r\n\r\nFor the `datasets` package, it seems that if the loading script is not already cached in the library it will attempt to connect to an AWS resource to download the dataset loading script. \r\n\r\nI am wondering why the package works in this way. Do you have any suggestions to solve this issue? '
 'I solved the above issue by downloading text.py manually and passing the path to the `load_dataset` function. \r\n\r\nNow, I have a new issue with the Read-only file system.\r\n\r\nThe error is: \r\n```\r\nI0916 22:14:38.453380 140737353971520 filelock.py:274] Lock 140734268996072 acquired on /scratch/chiyuzh/roberta/text.py.lock\r\nFound main folder for dataset /scratch/chiyuzh/roberta/text.py at /home/chiyuzh/.cache/huggingface/modules/datasets_modules/datasets/text\r\nCreating specific version folder for dataset /scratch/chiyuzh/roberta/text.py at /home/chiyuzh/.cache/huggingface/modules/datasets_modules/datasets/text/512f465342e4f4cd07a8791428a629c043bb89d55ad7817cbf7fcc649178b014\r\nI0916 22:14:38.530371 140737353971520 filelock.py:318] Lock 140734268996072 released on /scratch/chiyuzh/roberta/text.py.lock\r\nTraceback (most recent call last):\r\n  File ""/scratch/chiyuzh/roberta/run_language_modeling_hg.py"", line 1019, in <module>\r\n    main()\r\n  File ""/scratch/chiyuzh/roberta/run_language_modeling_hg.py"", line 962, in main\r\n    train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False)\r\n  File ""/scratch/chiyuzh/roberta/run_language_modeling_hg.py"", line 177, in load_and_cache_examples\r\n    return HG_Datasets(tokenizer, file_path, args)\r\n  File ""/scratch/chiyuzh/roberta/run_language_modeling_hg.py"", line 117, in HG_Datasets\r\n    dataset = load_dataset(\'/scratch/chiyuzh/roberta/text.py\', data_files=files, cache_dir = args.data_cache_dir, split=""train"")\r\n  File ""/arc/project/chiyuzh/evn_py36/datasets/src/datasets/load.py"", line 590, in load_dataset\r\n    path, script_version=script_version, download_config=download_config, download_mode=download_mode, dataset=True\r\n  File ""/arc/project/chiyuzh/evn_py36/datasets/src/datasets/load.py"", line 385, in prepare_module\r\n    os.makedirs(hash_folder_path)\r\n  File ""/project/chiyuzh/evn_py36/lib/python3.6/os.py"", line 220, in makedirs\r\n    mkdir(name, mode)\r\nOSError: [Errno 30] Read-only file system: \'/home/chiyuzh/.cache/huggingface/modules/datasets_modules/datasets/text/512f465342e4f4cd07a8791428a629c043bb89d55ad7817cbf7fcc649178b014\'\r\n\r\n```\r\n\r\nI installed datasets at /project/chiyuzh/evn_py36/datasets/src where is a writable directory.\r\nI also tried change the environment variables to the writable directory:\r\n`export HF_MODULES_PATH=/project/chiyuzh/evn_py36/datasets/cache_dir/`\r\n`export HF_DATASETS_CACHE=/project/chiyuzh/evn_py36/datasets/cache_dir/`\r\n \r\nIn my scripts, I also changed to:\r\n`dataset = load_dataset(\'/scratch/chiyuzh/roberta/text.py\', data_files=files, cache_dir = args.data_cache_dir, split=""train"")`\r\n`data_cache_dir = $TMPDIR/data/` that also a writable directory.\r\n \r\nBut it still try to make directory at /home/chiyuzh/.cache/huggingface/modules/.\r\nDo you have any idea about this issue? @thomwolf \r\n'
 '> Hey @chiyuzhang94, I was also having trouble in loading a large text file (11GB).\r\n> But finally got it working. This is what I did after looking into the documentation.\r\n> \r\n> 1. split the whole dataset file into smaller files\r\n> \r\n> ```shell\r\n> mkdir ./shards\r\n> split -a 4 -l 256000 -d full_raw_corpus.txt ./shards/shard_\r\n> ```\r\n> \r\n> 1. Pass paths of small data files to `load_dataset`\r\n> \r\n> ```python\r\n> files = glob.glob(\'shards/*\')\r\n> from datasets import load_dataset\r\n> dataset = load_dataset(\'text\', data_files=files, split=\'train\')\r\n> ```\r\n> \r\n> (On passing the whole dataset file (11GB) directly to `load_dataset` was resulting into RAM issue)\r\n> \r\n> 1. Tokenization\r\n> \r\n> ```python\r\n> def encode(examples):\r\n>   return tokenizer(examples[\'text\'], truncation=True, padding=\'max_length\')\r\n> dataset = dataset.map(encode, batched=True)\r\n> dataset.set_format(type=\'torch\', columns=[\'input_ids\', \'attention_mask\'])\r\n> ```\r\n> \r\n> Now you can pass `dataset` to `Trainer` or `pytorch DataLoader`\r\n> \r\n> ```python\r\n> dataloader = torch.utils.data.DataLoader(dataset, batch_size=4)\r\n> next(iter(dataloader))\r\n> ```\r\n> \r\n> Hope this helps\r\n\r\nWhen I run \'dataset = dataset.map(encode, batched=True)\',\r\nI encountered a problem like this:\r\n\r\n> Testing the mapped function outputs\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/datasets/dataset_dict.py"", line 300, in map\r\n    for k, dataset in self.items()\r\n  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/datasets/dataset_dict.py"", line 300, in <dictcomp>\r\n    for k, dataset in self.items()\r\n  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/datasets/arrow_dataset.py"", line 1224, in map\r\n    update_data = does_function_return_dict(test_inputs, test_indices)\r\n  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/datasets/arrow_dataset.py"", line 1195, in does_function_return_dict\r\n    function(*fn_args, indices, **fn_kwargs) if with_indices else function(*fn_args, **fn_kwargs)\r\n  File ""<stdin>"", line 3, in encode\r\nTypeError: __init__() takes 1 positional argument but 2 were given'
 '> > Hey @chiyuzhang94, I was also having trouble in loading a large text file (11GB).\r\n> > But finally got it working. This is what I did after looking into the documentation.\r\n> > \r\n> > 1. split the whole dataset file into smaller files\r\n> > \r\n> > ```shell\r\n> > mkdir ./shards\r\n> > split -a 4 -l 256000 -d full_raw_corpus.txt ./shards/shard_\r\n> > ```\r\n> > \r\n> > \r\n> > \r\n> > 1. Pass paths of small data files to `load_dataset`\r\n> > \r\n> > ```python\r\n> > files = glob.glob(\'shards/*\')\r\n> > from datasets import load_dataset\r\n> > dataset = load_dataset(\'text\', data_files=files, split=\'train\')\r\n> > ```\r\n> > \r\n> > \r\n> > (On passing the whole dataset file (11GB) directly to `load_dataset` was resulting into RAM issue)\r\n> > \r\n> > 1. Tokenization\r\n> > \r\n> > ```python\r\n> > def encode(examples):\r\n> >   return tokenizer(examples[\'text\'], truncation=True, padding=\'max_length\')\r\n> > dataset = dataset.map(encode, batched=True)\r\n> > dataset.set_format(type=\'torch\', columns=[\'input_ids\', \'attention_mask\'])\r\n> > ```\r\n> > \r\n> > \r\n> > Now you can pass `dataset` to `Trainer` or `pytorch DataLoader`\r\n> > ```python\r\n> > dataloader = torch.utils.data.DataLoader(dataset, batch_size=4)\r\n> > next(iter(dataloader))\r\n> > ```\r\n> > \r\n> > \r\n> > Hope this helps\r\n> \r\n> When I run \'dataset = dataset.map(encode, batched=True)\',\r\n> I encountered a problem like this:\r\n> \r\n> > Testing the mapped function outputs\r\n> > Traceback (most recent call last):\r\n> > File """", line 1, in \r\n> > File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/datasets/dataset_dict.py"", line 300, in map\r\n> > for k, dataset in self.items()\r\n> > File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/datasets/dataset_dict.py"", line 300, in \r\n> > for k, dataset in self.items()\r\n> > File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/datasets/arrow_dataset.py"", line 1224, in map\r\n> > update_data = does_function_return_dict(test_inputs, test_indices)\r\n> > File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/datasets/arrow_dataset.py"", line 1195, in does_function_return_dict\r\n> > function(*fn_args, indices, **fn_kwargs) if with_indices else function(*fn_args, **fn_kwargs)\r\n> > File """", line 3, in encode\r\n> > TypeError: **init**() takes 1 positional argument but 2 were given\r\n\r\nWhat is your encoder function?'
 ""> ```python\r\n> def encode(examples):\r\n>   return tokenizer(examples['text'], truncation=True, padding='max_length')\r\n> ```\r\n\r\nIt is the same as suggested:\r\n\r\n> def encode(examples):\r\n  return tokenizer(examples['text'], truncation=True, padding='max_length')""
 ""> > ```python\r\n> > def encode(examples):\r\n> >   return tokenizer(examples['text'], truncation=True, padding='max_length')\r\n> > ```\r\n> \r\n> It is the same as suggested:\r\n> \r\n> > def encode(examples):\r\n> > return tokenizer(examples['text'], truncation=True, padding='max_length')\r\n\r\nDo you use this function in a `class` object?  \r\n\r\ninit() takes 1 positional argument but 2 were given. I guess the additional argument is self?""
 '> > > ```python\r\n> > > def encode(examples):\r\n> > >   return tokenizer(examples[\'text\'], truncation=True, padding=\'max_length\')\r\n> > > ```\r\n> > \r\n> > \r\n> > It is the same as suggested:\r\n> > > def encode(examples):\r\n> > > return tokenizer(examples[\'text\'], truncation=True, padding=\'max_length\')\r\n> \r\n> Do you use this function in a `class` object?\r\n> \r\n> init() takes 1 positional argument but 2 were given. I guess the additional argument is self?\r\n\r\nThanks for your reply.\r\nCould you provide some simple example here?\r\nCurrently, I do not use this function in a class object. \r\nI think you are right and I was wondering how to construct this class.\r\nI try to modify it based on transformers\' LineByLineTextDataset. Am I correct?\r\n\r\n> class LineByLineTextDataset(Dataset):\r\n    """"""\r\n    This will be superseded by a framework-agnostic approach\r\n    soon.\r\n    """"""\r\n\r\n    def __init__(self, tokenizer: PreTrainedTokenizer, file_path: str, block_size: int):\r\n        assert os.path.isfile(file_path), f""Input file path {file_path} not found""\r\n        # Here, we do not cache the features, operating under the assumption\r\n        # that we will soon use fast multithreaded tokenizers from the\r\n        # `tokenizers` repo everywhere =)\r\n        #logger.info(""Creating features from dataset file at %s"", file_path)\r\n        #with open(file_path, encoding=""utf-8"") as f:\r\n        #    lines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\r\n        #batch_encoding = tokenizer(lines, add_special_tokens=True, truncation=True, max_length=block_size)\r\n\r\n\timport glob\r\n\tfiles = glob.glob(\'/home/mtzhang111/fairseq/cs_doc/shards/shard_003*\')\r\n\tfrom datasets import load_dataset\r\n\tdataset = load_dataset(\'text\', data_files=files)\r\n        batch_encoding= dataset.map(encode, batched=True)\r\n        self.examples = batch_encoding[""input_ids""]\r\n\t\r\n\r\n    def encode(examples):\r\n        return tokenizer(examples[\'text\'], truncation=True, padding=\'max_length\')\r\n\r\n    def __len__(self):\r\n        return len(self.examples)\r\n\r\n    def __getitem__(self, i) -> torch.Tensor:\r\n        return torch.tensor(self.examples[i], dtype=torch.long)\r\n'
 '> > > > ```python\r\n> > > > def encode(examples):\r\n> > > >   return tokenizer(examples[\'text\'], truncation=True, padding=\'max_length\')\r\n> > > > ```\r\n> > > \r\n> > > \r\n> > > It is the same as suggested:\r\n> > > > def encode(examples):\r\n> > > > return tokenizer(examples[\'text\'], truncation=True, padding=\'max_length\')\r\n> > \r\n> > \r\n> > Do you use this function in a `class` object?\r\n> > init() takes 1 positional argument but 2 were given. I guess the additional argument is self?\r\n> \r\n> Thanks for your reply.\r\n> Could you provide some simple example here?\r\n> Currently, I do not use this function in a class object.\r\n> I think you are right and I was wondering how to construct this class.\r\n> I try to modify it based on transformers\' LineByLineTextDataset. Am I correct?\r\n> \r\n> > class LineByLineTextDataset(Dataset):\r\n> > """"""\r\n> > This will be superseded by a framework-agnostic approach\r\n> > soon.\r\n> > """"""\r\n> \r\n> ```\r\n> def __init__(self, tokenizer: PreTrainedTokenizer, file_path: str, block_size: int):\r\n>     assert os.path.isfile(file_path), f""Input file path {file_path} not found""\r\n>     # Here, we do not cache the features, operating under the assumption\r\n>     # that we will soon use fast multithreaded tokenizers from the\r\n>     # `tokenizers` repo everywhere =)\r\n>     #logger.info(""Creating features from dataset file at %s"", file_path)\r\n>     #with open(file_path, encoding=""utf-8"") as f:\r\n>     #    lines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\r\n>     #batch_encoding = tokenizer(lines, add_special_tokens=True, truncation=True, max_length=block_size)\r\n> \r\n> import glob\r\n> files = glob.glob(\'/home/mtzhang111/fairseq/cs_doc/shards/shard_003*\')\r\n> from datasets import load_dataset\r\n> dataset = load_dataset(\'text\', data_files=files)\r\n>     batch_encoding= dataset.map(encode, batched=True)\r\n>     self.examples = batch_encoding[""input_ids""]\r\n> \r\n> \r\n> def encode(examples):\r\n>     return tokenizer(examples[\'text\'], truncation=True, padding=\'max_length\')\r\n> \r\n> def __len__(self):\r\n>     return len(self.examples)\r\n> \r\n> def __getitem__(self, i) -> torch.Tensor:\r\n>     return torch.tensor(self.examples[i], dtype=torch.long)\r\n> ```\r\n\r\nI am also struggling with this adaptation. \r\nI am not sure whether I am right.\r\n\r\nI think you don\'t need to construct `class LazyLineByLineTextDataset(Dataset)` at all. \r\ntorch.utils.data.Dataset is a generator. \r\n\r\nNow, we use `dataset = dataset.map(encode, batched=True)` as a generator. So we just pass dataset to torch.utils.data.DataLoader. '
 '@chiyuzhang94 Thanks for your reply. After some changes, currently, I managed to make the data loading process running.\r\nI published it in case you might want to take a look. Thanks for your help!\r\nhttps://github.com/shizhediao/Transformers_TPU'
 ""Hi @shizhediao ,\r\n\r\nThanks! It looks great!\r\n\r\nBut my problem still is the cache directory is a read-only file system. \r\n[As I mentioned](https://github.com/huggingface/datasets/issues/610#issuecomment-693912285), I tried to change the cache directory but it didn't work. \r\n\r\nDo you have any suggestions?\r\n\r\n""
 '> I installed datasets at /project/chiyuzh/evn_py36/datasets/src where is a writable directory.\r\n> I also tried change the environment variables to the writable directory:\r\n> `export HF_MODULES_PATH=/project/chiyuzh/evn_py36/datasets/cache_dir/`\r\n\r\nI think it is `HF_MODULES_CACHE` and not `HF_MODULES_PATH` @chiyuzhang94 .\r\nCould you try again and let me know if it fixes your issue ?\r\n'
 'We should probably add a section in the doc on the caching system with the env variables in particular.'
 'Hi @thomwolf , @lhoestq ,\r\n\r\nThanks for your suggestions. With the latest version of this package, I can load text data without Internet. \r\n\r\nBut I found the speed of dataset loading is very slow. \r\n\r\nMy scrips like this: \r\n```\r\n    def token_encode(examples):\r\n        tokenizer_out = tokenizer(examples[\'text\'], truncation=True,  padding=""max_length"", add_special_tokens=True, max_length=args.block_size)\r\n        return tokenizer_out\r\n    \r\n    path = Path(file_path)\r\n    files = sorted(path.glob(\'*\'))\r\n    dataset = load_dataset(\'./text.py\', data_files=files, cache_dir = args.data_cache_dir, split=""train"")\r\n    dataset = dataset.map(token_encode, batched=True)\r\n\r\n    dataset.set_format(type=\'torch\', columns=[\'input_ids\', \'attention_mask\'])\r\n```\r\n\r\nI have 1,123,870,657 lines in my input directory. \r\nI can find the processing speed as following. It is very slow.  \r\n```\r\n| 13/1123871 [00:02<62:37:39,  4.98ba/s]^M  0%|   \r\n| 14/1123871 [00:03<61:27:31,  5.08ba/s]^M  0%|          \r\n| 15/1123871 [00:03<66:34:19,  4.69ba/s]^M  0%|         \r\n| 16/1123871 [00:03<68:25:01,  4.56ba/s]^M  0%|          \r\n| 17/1123871 [00:03<72:00:03,  4.34ba/s]^M  0%|       \r\n```\r\nDo you have any suggestions to accelerate this loading process?'
 'You can use multiprocessing by specifying `num_proc=` in `.map()`\r\n\r\nAlso it looks like you have `1123871` batches of 1000 elements (default batch size), i.e. 1,123,871,000 lines in total.\r\nAm I right ?'
 '> You can use multiprocessing by specifying `num_proc=` in `.map()`\r\n> \r\n> Also it looks like you have `1123871` batches of 1000 elements (default batch size), i.e. 1,123,871,000 lines in total.\r\n> Am I right ?\r\n\r\nHi @lhoestq ,\r\n\r\nThanks. I will try it.\r\n\r\nYou are right. I have 1,123,870,657 lines totally in the path. I split the large file into 440 small files. Each file has 2,560,000 lines.\r\n\r\nI have another question. Because I am using a cloud server where only allows running a job up to 7 days. Hence, I need to resume my model every week. If the script needs to load and process the dataset every time. It is very low efficient based on the current processing speed. Is it possible that I process the dataset once and use the process cache to in the future work? \r\n'
 'Hi @lhoestq ,\r\n\r\nI tried to use multi-processor, but I got errors as follow: \r\nBecause I am using python distributed training, it seems some conflicts with the distributed job. \r\n\r\nDo you have any suggestions?\r\n```\r\nI0925 10:19:35.603023 140737353971520 filelock.py:318] Lock 140737229443368 released on /tmp/pbs.1120510.pbsha.ib.sockeye/cache/_tmp_pbs.1120510.pbsha.ib.sockeye_cache_text_default-7fb934ed6fac5d01_0.0.0_512f465342e4f4cd07a8791428a629c043bb89d55ad7817cbf7\r\nfcc649178b014.lock\r\nTraceback (most recent call last):\r\n  File ""/scratch/chiyuzh/roberta/run_language_modeling.py"", line 1024, in <module>\r\n    main()\r\n  File ""/scratch/chiyuzh/roberta/run_language_modeling.py"", line 967, in main\r\n    train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False)\r\n  File ""/scratch/chiyuzh/roberta/run_language_modeling.py"", line 180, in load_and_cache_examples\r\n    return HG_Datasets(tokenizer, file_path, args)\r\n  File ""/scratch/chiyuzh/roberta/run_language_modeling.py"", line 119, in HG_Datasets\r\n    dataset = dataset.map(token_encode, batched=True, batch_size = 10000, num_proc = 16)\r\n  File ""/project/chiyuzh/evn_py36/lib/python3.6/site-packages/datasets/arrow_dataset.py"", line 1287, in map\r\n    transformed_shards = [r.get() for r in results]\r\n  File ""/project/chiyuzh/evn_py36/lib/python3.6/site-packages/datasets/arrow_dataset.py"", line 1287, in <listcomp>\r\n    transformed_shards = [r.get() for r in results]\r\n  File ""/project/chiyuzh/evn_py36/lib/python3.6/multiprocessing/pool.py"", line 644, in get\r\n    raise self._value\r\n  File ""/project/chiyuzh/evn_py36/lib/python3.6/multiprocessing/pool.py"", line 424, in _handle_tasks\r\n    put(task)\r\n  File ""/project/chiyuzh/evn_py36/lib/python3.6/multiprocessing/connection.py"", line 206, in send\r\n    self._send_bytes(_ForkingPickler.dumps(obj))\r\n  File ""/project/chiyuzh/evn_py36/lib/python3.6/multiprocessing/reduction.py"", line 51, in dumps\r\n    cls(buf, protocol).dump(obj)\r\nAttributeError: Can\'t pickle local object \'HG_Datasets.<locals>.token_encode\'\r\n```'
 ""For multiprocessing, the function given to `map` must be picklable.\r\nMaybe you could try to define `token_encode` outside `HG_Datasets` ?\r\n\r\nAlso maybe #656 could make functions defined locally picklable for multiprocessing, once it's merged.""
 '> I have another question. Because I am using a cloud server where only allows running a job up to 7 days. Hence, I need to resume my model every week. If the script needs to load and process the dataset every time. It is very low efficient based on the current processing speed. Is it possible that I process the dataset once and use the process cache to in the future work?\r\n\r\nFeel free to save your processed dataset using `dataset.save_to_disk(""path/to/save/directory"")`.\r\n\r\nThen you\'ll be able to reload it again using\r\n```python\r\nfrom datasets import load_from_disk\r\n\r\ndataset = load_from_disk(""path/to/save/directory"")\r\n```'
 'Hi @lhoestq ,\r\n\r\nThanks for your suggestion. \r\nI tried to process the dataset and save it to disk. \r\nI have 1.12B samples in the raw dataset. I used 16 processors.\r\nI run this process job for 7 days. But it didn\'t finish. I don\'t why the processing is such slow. \r\n\r\nThe log shows that some processors (\\#12, \\#14, \\#15) are very slow. The different processor has a different speed. These slow processors look like a bottleneck. \r\n\r\nCould you please give me any suggestion to improve the processing speed? \r\n\r\nThanks. \r\nChiyu\r\n\r\nHere is my code:\r\n```\r\ndef token_encode(examples):\r\n        tokenizer_out = tokenizer(examples[\'text\'], truncation=True,  padding=""max_length"", add_special_tokens=True, max_length=args.block_size)\r\n        return tokenizer_out\r\n\r\npath = Path(file_path)\r\nfiles = sorted(path.glob(\'*\'))\r\ndataset = load_dataset(\'./text.py\', data_files=files, cache_dir = args.data_cache_dir, split=""train"")\r\ndataset = dataset.map(token_encode, batched=True, batch_size = 16384, num_proc = 16)\r\ndataset.set_format(type=\'torch\', columns=[\'input_ids\', \'attention_mask\'])\r\ndataset.save_to_disk(output_dir)\r\n```\r\nHere is the log. \r\n```\r\n^M#6:   1%|▏         | 59/4288 [55:10<66:11:58, 56.35s/ba]\r\n^M#1:   8%|▊         | 356/4288 [55:39<10:40:02,  9.77s/ba]\r\n^M#2:   5%|▍         | 210/4288 [55:33<17:47:19, 15.70s/ba]\r\n^M#0:  19%|█▉        | 836/4288 [55:53<4:08:56,  4.33s/ba]\r\n^M#0:  20%|█▉        | 837/4288 [55:57<4:01:52,  4.21s/ba]\r\n^M#1:   8%|▊         | 357/4288 [55:48<10:38:09,  9.74s/ba]\r\n^M#0:  20%|█▉        | 838/4288 [56:01<4:02:56,  4.23s/ba]\r\n^M#3:   4%|▎         | 155/4288 [55:43<24:41:20, 21.51s/ba]\r\n^M#0:  20%|█▉        | 839/4288 [56:05<4:04:48,  4.26s/ba]\r\n^M#12:   1%|          | 29/4288 [54:50<133:20:53, 112.72s/ba]\r\n^M#2:   5%|▍         | 211/4288 [55:48<17:40:33, 15.61s/ba]\r\n^M#14:   0%|          | 2/4288 [04:24<157:17:50, 132.12s/ba]\r\n^M#15:   0%|          | 1/4288 [02:24<172:11:37, 144.60s/ba]\r\n```'
 ""Hi !\r\n\r\nAs far as I can tell, there could be several reasons for your processes to have different speeds:\r\n- some parts of your dataset have short passages while some have longer passages, that take more time to be processed\r\n- OR there are other processes running that prevent some of them to run at full speed\r\n- OR the value of `num_proc` is higher than the number of actual processes that you can run in parallel at full speed.\r\n\r\nSo I'd suggest you to check that you have nothing else running in parallel to your processing job, and also maybe take a look at the slow parts of the datasets.\r\nWhen doing multiprocessing, the dataset is sharded in `num_proc` contiguous parts that are processed individually in each process. If you want to take a look at the dataset processed in the 12th shard of 16 for example, you can do:\r\n\r\n```python\r\nmy_shard = dataset.shard(num_shards=16, index=12, contiguous=True)\r\n```\r\n\r\nHope this helps, let me know if you find what is causing this slow down.""
 'Do you use a fast or a slow tokenizer from the `transformers` library @chiyuzhang94?'
 '> Do you use a fast or a slow tokenizer from the `transformers` library @chiyuzhang94?\r\n\r\nHi @thomwolf ,\r\n I use this: \r\n```\r\nfrom transformers import\r\nAutoTokenizer.from_pretrained(args.model_name_or_path, cache_dir=args.cache_dir)\r\n```\r\n\r\nI guess this is a slow one, let me explore the fast tokenizer. '
 ""> Hi !\r\n> \r\n> As far as I can tell, there could be several reasons for your processes to have different speeds:\r\n> \r\n> * some parts of your dataset have short passages while some have longer passages, that take more time to be processed\r\n> * OR there are other processes running that prevent some of them to run at full speed\r\n> * OR the value of `num_proc` is higher than the number of actual processes that you can run in parallel at full speed.\r\n> \r\n> So I'd suggest you to check that you have nothing else running in parallel to your processing job, and also maybe take a look at the slow parts of the datasets.\r\n> When doing multiprocessing, the dataset is sharded in `num_proc` contiguous parts that are processed individually in each process. If you want to take a look at the dataset processed in the 12th shard of 16 for example, you can do:\r\n> \r\n> ```python\r\n> my_shard = dataset.shard(num_shards=16, index=12, contiguous=True)\r\n> ```\r\n> \r\n> Hope this helps, let me know if you find what is causing this slow down.\r\n\r\nHi @lhoestq ,\r\n\r\nThanks for your suggestions. \r\nI don't think my problem is due to any one of these seasons. \r\n\r\n1.  I have 1,123,870,657 lines totally in the path. I split the large file into 440 small files. Each file has 2,560,000 lines. The last file is smaller a little bit. But they are similar. I randomly shuffled all the 1,123,870,657 lines. Hence, the sequences should also be similar across all the files. \r\n\r\n2. I run this script on the entire node. I requested all the resources on the nodes (40 CPUs, 384GB memory). Hence, these were not any other processes. \r\n\r\n 3. As I say, the node has 40 CPUs, but I set num_proc = 16. This should not be a problem.""
 'Hi @thomwolf \r\nI am using `RobertaTokenizerFast` now. \r\n\r\nBut the speed is still imbalanced, some processors are still slow. \r\nHere is the part of the log. #0 is always much fast than lower rank processors. \r\n\r\n```\r\n#15:   3%|▎         | 115/3513 [3:18:36<98:01:33, 103.85s/ba]\r\n#2:  24%|██▍       | 847/3513 [3:20:43<11:06:49, 15.01s/ba]\r\n#1:  37%|███▋      | 1287/3513 [3:20:52<6:19:02, 10.22s/ba]\r\n#0:  72%|███████▏  | 2546/3513 [3:20:52<1:51:03,  6.89s/ba]\r\n#3:  18%|█▊        | 617/3513 [3:20:36<15:50:30, 19.69s/ba]\r\n#0:  73%|███████▎  | 2547/3513 [3:20:59<1:50:25,  6.86s/ba]\r\n#1:  37%|███▋      | 1288/3513 [3:21:02<6:21:13, 10.28s/ba]\r\n#7:   7%|▋         | 252/3513 [3:20:09<44:09:03, 48.74s/ba]\r\n#12:   4%|▍         | 144/3513 [3:19:19<78:00:54, 83.36s/ba]\r\n#4:  14%|█▍        | 494/3513 [3:20:37<20:46:06, 24.77s/ba]\r\n#0:  73%|███████▎  | 2548/3513 [3:21:06<1:49:26,  6.80s/ba]\r\n#2:  24%|██▍       | 848/3513 [3:20:58<11:06:17, 15.00s/ba]\r\n```\r\nHere is my script related to the datasets processing, \r\n\r\n```\r\ntokenizer = RobertaTokenizerFast.from_pretrained(args.model_name_or_path, cache_dir=args.cache_dir)\r\n\r\ndef token_encode(examples):\r\n    tokenizer_out = tokenizer(examples[\'text\'], truncation=True,  padding=""max_length"", add_special_tokens=True, max_length=128)\r\n    return tokenizer_out\r\n\r\ndef HG_Datasets(tokenizer, file_path, args):\r\n    \r\n    path = Path(file_path)\r\n    files = sorted(path.glob(\'*\'))\r\n    dataset = load_dataset(\'./text.py\', data_files=files, cache_dir = """"./, split=""train"")\r\n    dataset = dataset.map(token_encode, batched=True, batch_size = 20000, num_proc = 16)\r\n\r\n    dataset.set_format(type=\'torch\', columns=[\'input_ids\', \'attention_mask\'])\r\n    return dataset\r\n\r\n```\r\nI have 1,123,870,657 lines totally in the path. I split the large file into 440 small files. Each file has 2,560,000 lines.\r\n\r\nCould you please give any suggestion? Thanks very much!!']","I migrate my question from https://github.com/huggingface/transformers/pull/4009#issuecomment-690039444

I tried to train a Roberta from scratch using transformers. But I got OOM issues with loading a large text file. 
According to the suggestion from @thomwolf , I tried to implement `datasets` to load my text file. This test.txt is a simple sample where each line is a sentence.
```
from datasets import load_dataset
dataset = load_dataset('text', data_files='test.txt',cache_dir=""./"")
dataset.set_format(type='torch',columns=[""text""])
dataloader = torch.utils.data.DataLoader(dataset, batch_size=8)
next(iter(dataloader))
```

But dataload cannot yield sample and error is:
```
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-12-388aca337e2f> in <module>
----> 1 next(iter(dataloader))

/Library/Python/3.7/site-packages/torch/utils/data/dataloader.py in __next__(self)
    361 
    362     def __next__(self):
--> 363         data = self._next_data()
    364         self._num_yielded += 1
    365         if self._dataset_kind == _DatasetKind.Iterable and \

/Library/Python/3.7/site-packages/torch/utils/data/dataloader.py in _next_data(self)
    401     def _next_data(self):
    402         index = self._next_index()  # may raise StopIteration
--> 403         data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
    404         if self._pin_memory:
    405             data = _utils.pin_memory.pin_memory(data)

/Library/Python/3.7/site-packages/torch/utils/data/_utils/fetch.py in fetch(self, possibly_batched_index)
     42     def fetch(self, possibly_batched_index):
     43         if self.auto_collation:
---> 44             data = [self.dataset[idx] for idx in possibly_batched_index]
     45         else:
     46             data = self.dataset[possibly_batched_index]

/Library/Python/3.7/site-packages/torch/utils/data/_utils/fetch.py in <listcomp>(.0)
     42     def fetch(self, possibly_batched_index):
     43         if self.auto_collation:
---> 44             data = [self.dataset[idx] for idx in possibly_batched_index]
     45         else:
     46             data = self.dataset[possibly_batched_index]

KeyError: 0
```

`dataset.set_format(type='torch',columns=[""text""])` returns a log says:
```
Set __getitem__(key) output type to torch for ['text'] columns (when key is int or slice) and don't output other (un-formatted) columns.
```

I noticed the dataset is `DatasetDict({'train': Dataset(features: {'text': Value(dtype='string', id=None)}, num_rows: 44)})`.
Each sample can be accessed by `dataset[""train""][""text""]` instead of `dataset[""text""]`. 

Could you please give me any suggestions on how to modify this code to load the text file?

Versions:
Python version 3.7.3
PyTorch version 1.6.0 
TensorFlow version 2.3.0 
datasets version: 1.0.1"
https://github.com/huggingface/datasets/issues/608,Don't use the old NYU GLUE dataset URLs,['Feel free to open the PR ;)\r\nThanks for updating the dataset_info.json file !'],"NYU is switching dataset hosting from Google to FB. Initial changes to `datasets` are in https://github.com/jeswan/nlp/commit/b7d4a071d432592ded971e30ef73330529de25ce. What tests do you suggest I run before opening a PR?

See: https://github.com/jiant-dev/jiant/issues/161 and https://github.com/nyu-mll/jiant/pull/1112"
https://github.com/huggingface/datasets/issues/600,Pickling error when loading dataset,"['When I change from python3.6 to python3.8, it works! '
 'Does it work when you install `nlp` from source on python 3.6?'
 'No, still the pickling error.'
 'I wasn\'t able to reproduce on google colab (python 3.6.9 as well) with \r\n\r\npickle==4.0\r\ndill=0.3.2\r\ntransformers==3.1.0\r\ndatasets=1.0.1 (also tried nlp 0.4.0)\r\n\r\nIf I try\r\n\r\n```python\r\nfrom datasets import load_dataset  # or from nlp\r\nfrom transformers import BertTokenizer\r\n\r\ntokenizer = BertTokenizer.from_pretrained(""bert-base-uncased"")\r\ndataset = load_dataset(""text"", data_files=file_path, split=""train"")\r\ndataset = dataset.map(lambda ex: tokenizer(ex[""text""], add_special_tokens=True,\r\n                                        truncation=True, max_length=512), batched=True)\r\ndataset.set_format(type=\'torch\', columns=[\'input_ids\'])\r\n```\r\nIt runs without error'
 ""Closing since it looks like it's working on >= 3.6.9\r\nFeel free to re-open if you have other questions :)""]","Hi,

I modified line 136 in the original [run_language_modeling.py](https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_language_modeling.py) as:

```
# line 136: return LineByLineTextDataset(tokenizer=tokenizer, file_path=file_path, block_size=args.block_size)
dataset = load_dataset(""text"", data_files=file_path, split=""train"")
dataset = dataset.map(lambda ex: tokenizer(ex[""text""], add_special_tokens=True,
                                        truncation=True, max_length=args.block_size), batched=True)
dataset.set_format(type='torch', columns=['input_ids'])
return dataset
```

When I run this with transformers (3.1.0) and nlp (0.4.0), I get the following error:

```
Traceback (most recent call last):
  File ""src/run_language_modeling.py"", line 319, in <module>
    main()
  File ""src/run_language_modeling.py"", line 248, in main
    get_dataset(data_args, tokenizer=tokenizer, cache_dir=model_args.cache_dir) if training_args.do_train else None
  File ""src/run_language_modeling.py"", line 139, in get_dataset
    dataset = dataset.map(lambda ex: tokenizer(ex[""text""], add_special_tokens=True, truncation=True, max_length=args.block_size), batched=True)
  File ""/data/nlp/src/nlp/arrow_dataset.py"", line 1136, in map
    new_fingerprint=new_fingerprint,
  File ""/data/nlp/src/nlp/fingerprint.py"", line 158, in wrapper
    self._fingerprint, transform, kwargs_for_fingerprint
  File ""/data/nlp/src/nlp/fingerprint.py"", line 105, in update_fingerprint
    hasher.update(transform_args[key])
  File ""/data/nlp/src/nlp/fingerprint.py"", line 57, in update
    self.m.update(self.hash(value).encode(""utf-8""))
  File ""/data/nlp/src/nlp/fingerprint.py"", line 53, in hash
    return cls.hash_default(value)
  File ""/data/nlp/src/nlp/fingerprint.py"", line 46, in hash_default
    return cls.hash_bytes(dumps(value))
  File ""/data/nlp/src/nlp/utils/py_utils.py"", line 362, in dumps
    dump(obj, file)
  File ""/data/nlp/src/nlp/utils/py_utils.py"", line 339, in dump
    Pickler(file, recurse=True).dump(obj)
  File ""/root/miniconda3/envs/py3.6/lib/python3.6/site-packages/dill/_dill.py"", line 446, in dump
    StockPickler.dump(self, obj)
  File ""/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py"", line 409, in dump
    self.save(obj)
  File ""/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py"", line 476, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/root/miniconda3/envs/py3.6/lib/python3.6/site-packages/dill/_dill.py"", line 1438, in save_function
    obj.__dict__, fkwdefaults), obj=obj)
  File ""/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py"", line 610, in save_reduce
    save(args)
  File ""/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py"", line 476, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py"", line 751, in save_tuple
    save(element)
  File ""/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py"", line 476, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py"", line 736, in save_tuple
    save(element)
  File ""/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py"", line 476, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/root/miniconda3/envs/py3.6/lib/python3.6/site-packages/dill/_dill.py"", line 1170, in save_cell
    pickler.save_reduce(_create_cell, (f,), obj=obj)
  File ""/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py"", line 610, in save_reduce
    save(args)
  File ""/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py"", line 476, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py"", line 736, in save_tuple
    save(element)
  File ""/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py"", line 521, in save
    self.save_reduce(obj=obj, *rv)
  File ""/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py"", line 605, in save_reduce
    save(cls)
  File ""/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py"", line 476, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/root/miniconda3/envs/py3.6/lib/python3.6/site-packages/dill/_dill.py"", line 1365, in save_type
    obj.__bases__, _dict), obj=obj)
  File ""/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py"", line 610, in save_reduce
    save(args)
  File ""/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py"", line 476, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py"", line 751, in save_tuple
    save(element)
  File ""/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py"", line 476, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/root/miniconda3/envs/py3.6/lib/python3.6/site-packages/dill/_dill.py"", line 933, in save_module_dict
    StockPickler.save_dict(pickler, obj)
  File ""/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py"", line 821, in save_dict
    self._batch_setitems(obj.items())
  File ""/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py"", line 847, in _batch_setitems
    save(v)
  File ""/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py"", line 476, in save
    f(self, obj) # Call unbound method with explicit self
  File ""/root/miniconda3/envs/py3.6/lib/python3.6/site-packages/dill/_dill.py"", line 933, in save_module_dict
    StockPickler.save_dict(pickler, obj)
  File ""/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py"", line 821, in save_dict
    self._batch_setitems(obj.items())
  File ""/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py"", line 847, in _batch_setitems
    save(v)
  File ""/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py"", line 507, in save
    self.save_global(obj, rv)
  File ""/root/miniconda3/envs/py3.6/lib/python3.6/pickle.py"", line 927, in save_global
    (obj, module_name, name))
_pickle.PicklingError: Can't pickle typing.Union[str, NoneType]: it's not the same object as typing.Union
```"
https://github.com/huggingface/datasets/issues/598,The current version of the package on github has an error when loading dataset,"[""Thanks for reporting !\r\nWhich version of transformers are you using ?\r\nIt looks like it doesn't have the PreTrainedTokenizerBase class""
 ""I was using transformer 2.9. And I switch to the latest transformer package. Everything works just fine!!\r\n\r\nThanks for helping! I should look more carefully next time. Didn't realize loading the data part requires using tokenizer.\r\n""
 'Yes it shouldn’t fail with older version of transformers since this is only a special feature to make caching more efficient when using transformers for tokenization.\r\nWe’ll update this.']","Instead of downloading the package from pip, downloading the version from source will result in an error when loading dataset (the pip version is completely fine):

To recreate the error: 
First, installing nlp directly from source:
```
git clone https://github.com/huggingface/nlp.git
cd nlp
pip install -e .
```
Then run:
```
from nlp import load_dataset
dataset = load_dataset('wikitext', 'wikitext-2-v1',split = 'train') 
```
will give error:

```
>>> dataset = load_dataset('wikitext', 'wikitext-2-v1',split = 'train')
Checking /home/zeyuy/.cache/huggingface/datasets/84a754b488511b109e2904672d809c041008416ae74e38f9ee0c80a8dffa1383.2e21f48d63b5572d19c97e441fbb802257cf6a4c03fbc5ed8fae3d2c2273f59e.py for additional imports.
Found main folder for dataset https://raw.githubusercontent.com/huggingface/nlp/0.4.0/datasets/wikitext/wikitext.py at /home/zeyuy/.cache/huggingface/modules/nlp_modules/datasets/wikitext
Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/nlp/0.4.0/datasets/wikitext/wikitext.py at /home/zeyuy/.cache/huggingface/modules/nlp_modules/datasets/wikitext/5de6e79516446f747fcccc09aa2614fa159053b75909594d28d262395f72d89d
Found script file from https://raw.githubusercontent.com/huggingface/nlp/0.4.0/datasets/wikitext/wikitext.py to /home/zeyuy/.cache/huggingface/modules/nlp_modules/datasets/wikitext/5de6e79516446f747fcccc09aa2614fa159053b75909594d28d262395f72d89d/wikitext.py
Found dataset infos file from https://raw.githubusercontent.com/huggingface/nlp/0.4.0/datasets/wikitext/dataset_infos.json to /home/zeyuy/.cache/huggingface/modules/nlp_modules/datasets/wikitext/5de6e79516446f747fcccc09aa2614fa159053b75909594d28d262395f72d89d/dataset_infos.json
Found metadata file for dataset https://raw.githubusercontent.com/huggingface/nlp/0.4.0/datasets/wikitext/wikitext.py at /home/zeyuy/.cache/huggingface/modules/nlp_modules/datasets/wikitext/5de6e79516446f747fcccc09aa2614fa159053b75909594d28d262395f72d89d/wikitext.json
Loading Dataset Infos from /home/zeyuy/.cache/huggingface/modules/nlp_modules/datasets/wikitext/5de6e79516446f747fcccc09aa2614fa159053b75909594d28d262395f72d89d
Overwrite dataset info from restored data version.
Loading Dataset info from /home/zeyuy/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/5de6e79516446f747fcccc09aa2614fa159053b75909594d28d262395f72d89d
Reusing dataset wikitext (/home/zeyuy/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/5de6e79516446f747fcccc09aa2614fa159053b75909594d28d262395f72d89d)
Constructing Dataset for split train, from /home/zeyuy/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/5de6e79516446f747fcccc09aa2614fa159053b75909594d28d262395f72d89d
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/zeyuy/transformers/examples/language-modeling/nlp/src/nlp/load.py"", line 600, in load_dataset
    ds = builder_instance.as_dataset(split=split, ignore_verifications=ignore_verifications)
  File ""/home/zeyuy/transformers/examples/language-modeling/nlp/src/nlp/builder.py"", line 611, in as_dataset
    datasets = utils.map_nested(
  File ""/home/zeyuy/transformers/examples/language-modeling/nlp/src/nlp/utils/py_utils.py"", line 216, in map_nested
    return function(data_struct)
  File ""/home/zeyuy/transformers/examples/language-modeling/nlp/src/nlp/builder.py"", line 631, in _build_single_dataset
    ds = self._as_dataset(
  File ""/home/zeyuy/transformers/examples/language-modeling/nlp/src/nlp/builder.py"", line 704, in _as_dataset
    return Dataset(**dataset_kwargs)
  File ""/home/zeyuy/transformers/examples/language-modeling/nlp/src/nlp/arrow_dataset.py"", line 188, in __init__
    self._fingerprint = generate_fingerprint(self)
  File ""/home/zeyuy/transformers/examples/language-modeling/nlp/src/nlp/fingerprint.py"", line 91, in generate_fingerprint
    hasher.update(key)
  File ""/home/zeyuy/transformers/examples/language-modeling/nlp/src/nlp/fingerprint.py"", line 57, in update
    self.m.update(self.hash(value).encode(""utf-8""))
  File ""/home/zeyuy/transformers/examples/language-modeling/nlp/src/nlp/fingerprint.py"", line 53, in hash
    return cls.hash_default(value)
  File ""/home/zeyuy/transformers/examples/language-modeling/nlp/src/nlp/fingerprint.py"", line 46, in hash_default
    return cls.hash_bytes(dumps(value))
  File ""/home/zeyuy/transformers/examples/language-modeling/nlp/src/nlp/utils/py_utils.py"", line 361, in dumps
    with _no_cache_fields(obj):
  File ""/home/zeyuy/miniconda3/lib/python3.8/contextlib.py"", line 113, in __enter__
    return next(self.gen)
  File ""/home/zeyuy/transformers/examples/language-modeling/nlp/src/nlp/utils/py_utils.py"", line 348, in _no_cache_fields
    if isinstance(obj, tr.PreTrainedTokenizerBase) and hasattr(obj, ""cache"") and isinstance(obj.cache, dict):
AttributeError: module 'transformers' has no attribute 'PreTrainedTokenizerBase'

```


"
https://github.com/huggingface/datasets/issues/597,Indices incorrect with multiprocessing,"['I fixed a bug that could cause this issue earlier today. Could you pull the latest version and try again ?'
 'Still the case on master.\r\nI guess we should have an offset in the multi-procs indeed (hopefully it\'s enough).\r\n\r\nAlso, side note is that we should add some logging before the ""test"" to say we are testing the function otherwise its confusing for the user to see two outputs I think. Proposal (see the ""Testing the mapped function outputs:"" lines):\r\n```\r\n>>> d.select(range(10)).map(fn, with_indices=True, batched=True, num_proc=2)\r\nDone writing 10 indices in 80 bytes .\r\nDone writing 5 indices in 41 bytes .\r\nDone writing 5 indices in 41 bytes .\r\nSpawning 2 processes\r\nTesting the mapped function outputs:\r\ninds: [0, 1]\r\ninds: [0, 1]\r\nTesting finished, running the mapped function on the dataset:\r\n#0:   0%|                                                                                                                                                                    | 0/1 [00:00<?, ?ba/s]\r\ninds: [0, 1, 2, 3, 4]                                                                                                                                                                             inds: [0, 1, 2, 3, 4]                                                                                                                                                         | 0/1 [00:00<?, ?ba/s]\r\n#0: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1321.04ba/s]\r\n#1: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1841.22ba/s]\r\nConcatenating 2 shards from multiprocessing\r\nDataset(features: {\'text\': Value(dtype=\'string\', id=None), \'label\': ClassLabel(num_classes=2, names=[\'neg\', \'pos\'], names_file=None, id=None)}, num_rows: 10)\r\n```']","When `num_proc` > 1, the indices argument passed to the map function is incorrect:

```python
d = load_dataset('imdb', split='test[:1%]')

def fn(x, inds):
    print(inds)
    return x

d.select(range(10)).map(fn, with_indices=True, batched=True)
# [0, 1]
# [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]

d.select(range(10)).map(fn, with_indices=True, batched=True, num_proc=2)
# [0, 1]
# [0, 1]
# [0, 1, 2, 3, 4]
# [0, 1, 2, 3, 4]
```

As you can see, the subset passed to each thread is indexed from 0 to N which doesn't reflect their positions in `d`."
https://github.com/huggingface/datasets/issues/595,`Dataset`/`DatasetDict` has no attribute 'save_to_disk',"['`pip install git+https://github.com/huggingface/nlp.git` should have done the job.\r\n\r\nDid you uninstall `nlp` before installing from github ?'
 '> Did you uninstall `nlp` before installing from github ?\r\n\r\nI did not. I created a new environment and installed `nlp` directly from `github` and it worked!\r\n\r\nThanks.\r\n']","Hi,

As the title indicates, both `Dataset` and `DatasetDict` classes don't seem to have the `save_to_disk` method.  While the file [`arrow_dataset.py`](https://github.com/huggingface/nlp/blob/34bf0b03bfe03e7f77b8fec1cd48f5452c4fc7c1/src/nlp/arrow_dataset.py) in the repo here has the method, the file `arrow_dataset.py` which is saved after `pip install nlp -U` in my `conda` environment DOES NOT contain the `save_to_disk` method. I even tried `pip install git+https://github.com/huggingface/nlp.git ` and still no luck. Do I need to install the library in another way?"
https://github.com/huggingface/datasets/issues/590,The process cannot access the file because it is being used by another process (windows),"[""Hi, which version of `nlp` are you using?\r\n\r\nBy the way we'll be releasing today a significant update fixing many issues (but also comprising a few breaking changes).\r\nYou can see more informations here #545 and try it by installing from source from the master branch.""
 ""I'm using version 0.4.0.\r\n\r\n""
 ""Ok, it's probably fixed on master. Otherwise if you can give me a fully self-contained exemple to reproduce the error, I can try to investigate.""
 ""I get the same behavior, on Windows, when `map`ping a function to a loaded dataset. \r\nThe error doesn't occur if I re-run the cell a second time though! \r\nI'm on version 1.0.1.""
 'This is going to be fixed by #644 '
 ""@saareliad I got the same issue that troubled me quite a while. Unfortunately, there are no good answers to this issue online, I tried it on Linux and that's absolutely fine. After hacking the source code, I solved this problem as follows.\r\n\r\nIn the source code file: arrow_dataset.py -> _map_single(...)\r\n\r\nchange\r\n```python\r\nif update_data and tmp_file is not None:\r\n    shutil.move(tmp_file.name, cache_file_name)\r\n```\r\nto\r\n```python\r\ntmp_file.close()\r\nif update_data and tmp_file is not None:\r\n    shutil.move(tmp_file.name, cache_file_name)\r\n```\r\n\r\nThen it works without needing multiple times runs to avoid the permission error.\r\nI know this solution is unusual since it changes the source code. Hopefully, the lib's contributors can have better solutions in the future.\r\n""
 '@wangcongcong123 thanks  for sharing.\n(BTW I also solved it locally on windows by putting the problematic line under try except and not using cache... On windows I just needed 1% of the dataset anyway)']","Hi, I consistently get the following error when developing in my PC (windows 10):

```
    train_dataset = train_dataset.map(convert_to_features, batched=True)
  File ""C:\Users\saareliad\AppData\Local\Continuum\miniconda3\envs\py38\lib\site-packages\nlp\arrow_dataset.py"", line 970, in map
    shutil.move(tmp_file.name, cache_file_name)
  File ""C:\Users\saareliad\AppData\Local\Continuum\miniconda3\envs\py38\lib\shutil.py"", line 803, in move
    os.unlink(src)
PermissionError: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\Users\\saareliad\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\\408a8fa46a1e2805445b793f1022e743428ca739a34809fce872f0c7f17b44ab\\tmpsau1bep1'

```"
https://github.com/huggingface/datasets/issues/580,"nlp re-creates already-there caches when using a script, but not within a shell","[""Couln't reproduce on my side :/ \r\nlet me know if you manage to reproduce on another env (colab for example)""
 'Fixed with a clean re-install!']","`nlp` keeps creating new caches for the same file when launching `filter` from a script, and behaves correctly from within the shell.

Example: try running

```
import nlp

hans_easy_data = nlp.load_dataset('hans', split=""validation"").filter(lambda x: x['label'] == 0)
hans_hard_data = nlp.load_dataset('hans', split=""validation"").filter(lambda x: x['label'] == 1)
```

twice. If launched from a `file.py` script, the cache will be re-created the second time. If launched as 3 shell/`ipython` commands, `nlp` will correctly re-use the cache.
As observed with @lhoestq."
https://github.com/huggingface/datasets/issues/577,Some languages in wikipedia dataset are not loading,"['Some wikipedia languages have already been processed by us and are hosted on our google storage. This is the case for ""fr"" and ""en"" for example.\r\n\r\nFor other smaller languages (in terms of bytes), they are directly downloaded and parsed from the wikipedia dump site.\r\nParsing can take some time for languages with hundreds of MB of xml.\r\n\r\nLet me know if you encounter an error or if you feel that is is taking too long for you.\r\nWe could process those that really take too much time'
 'Ok, thanks for clarifying, that makes sense. I will time those examples later today and post back here.\r\n\r\nAlso, it seems that not all dumps should use the same date. For instance, I was checking the Spanish dump doing the following:\r\n```\r\ndata = nlp.load_dataset(\'wikipedia\', \'20200501.es\', beam_runner=\'DirectRunner\', split=\'train\')\r\n```\r\n\r\nI got the error below because this URL does not exist: https://dumps.wikimedia.org/eswiki/20200501/dumpstatus.json. So I checked the actual available dates here https://dumps.wikimedia.org/eswiki/ and there is no 20200501. If one tries for a date available in the link, then the nlp library does not allow such a request because is not in the list of expected datasets.\r\n\r\n```\r\nDownloading and preparing dataset wikipedia/20200501.es (download: Unknown size, generated: Unknown size, post-processed: Unknown sizetotal: Unknown size) to /home/gaguilar/.cache/huggingface/datasets/wikipedia/20200501.es/1.0.0/7be7f4324255faf70687be8692de57cf79197afdc33ff08d6a04ed602df32d50...\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/gaguilar/.conda/envs/pytorch/lib/python3.8/site-packages/nlp/load.py"", line 548, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File ""/home/gaguilar/.conda/envs/pytorch/lib/python3.8/site-packages/nlp/builder.py"", line 462, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File ""/home/gaguilar/.conda/envs/pytorch/lib/python3.8/site-packages/nlp/builder.py"", line 965, in _download_and_prepare\r\n    super(BeamBasedBuilder, self)._download_and_prepare(\r\n  File ""/home/gaguilar/.conda/envs/pytorch/lib/python3.8/site-packages/nlp/builder.py"", line 518, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File ""/home/gaguilar/.conda/envs/pytorch/lib/python3.8/site-packages/nlp/datasets/wikipedia/7be7f4324255faf70687be8692de57cf79197afdc33ff08d6a04ed602df32d50/wikipedia.py"", line 422, in _split_generators\r\n    downloaded_files = dl_manager.download_and_extract({""info"": info_url})\r\n  File ""/home/gaguilar/.conda/envs/pytorch/lib/python3.8/site-packages/nlp/utils/download_manager.py"", line 220, in download_and_extract\r\n    return self.extract(self.download(url_or_urls))\r\n  File ""/home/gaguilar/.conda/envs/pytorch/lib/python3.8/site-packages/nlp/utils/download_manager.py"", line 155, in download\r\n    downloaded_path_or_paths = map_nested(\r\n  File ""/home/gaguilar/.conda/envs/pytorch/lib/python3.8/site-packages/nlp/utils/py_utils.py"", line 163, in map_nested\r\n    return {\r\n  File ""/home/gaguilar/.conda/envs/pytorch/lib/python3.8/site-packages/nlp/utils/py_utils.py"", line 164, in <dictcomp>\r\n    k: map_nested(\r\n  File ""/home/gaguilar/.conda/envs/pytorch/lib/python3.8/site-packages/nlp/utils/py_utils.py"", line 191, in map_nested\r\n    return function(data_struct)\r\n  File ""/home/gaguilar/.conda/envs/pytorch/lib/python3.8/site-packages/nlp/utils/download_manager.py"", line 156, in <lambda>\r\n    lambda url: cached_path(url, download_config=self._download_config,), url_or_urls,\r\n  File ""/home/gaguilar/.conda/envs/pytorch/lib/python3.8/site-packages/nlp/utils/file_utils.py"", line 191, in cached_path\r\n    output_path = get_from_cache(\r\n  File ""/home/gaguilar/.conda/envs/pytorch/lib/python3.8/site-packages/nlp/utils/file_utils.py"", line 356, in get_from_cache\r\n    raise ConnectionError(""Couldn\'t reach {}"".format(url))\r\nConnectionError: Couldn\'t reach https://dumps.wikimedia.org/eswiki/20200501/dumpstatus.json\r\n```'
 'Thanks ! This will be very helpful.\r\n\r\nAbout the date issue, I think it\'s possible to use another date with\r\n\r\n```python\r\nload_dataset(""wikipedia"", language=""es"", date=""..."", beam_runner=""..."")\r\n```\r\n\r\nHowever we\'ve not processed wikipedia dumps for other dates than 20200501 (yet ?)\r\n\r\nOne more thing that is specific to 20200501.es: it was available once but the `mwparserfromhell` was not able to parse it for some reason, so we didn\'t manage to get a processed version of 20200501.es (see #321 )'
 'Cool! Thanks for the trick regarding different dates!\r\n\r\nI checked the download/processing time for retrieving the Arabic Wikipedia dump, and it took about 3.2 hours. I think that this may be a bit impractical when it comes to working with multiple languages (although I understand that storing those datasets in your Google storage may not be very appealing either). \r\n\r\nFor the record, here\'s what I did:\r\n```python\r\nimport nlp\r\nimport time\r\n\r\ndef timeit(filename):\r\n    elapsed = time.time()\r\n    data = nlp.load_dataset(\'wikipedia\', filename, beam_runner=\'DirectRunner\', split=\'train\')\r\n    elapsed = time.time() - elapsed\r\n    print(f""Loading the \'{filename}\' data took {elapsed:,.1f} seconds..."")\r\n    return data\r\n\r\ndata = timeit(\'20200501.ar\')\r\n```\r\n\r\nHere\'s the output:\r\n```\r\nDownloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13.0k/13.0k [00:00<00:00, 8.34MB/s]\r\nDownloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28.7k/28.7k [00:00<00:00, 954kB/s]\r\nDownloading and preparing dataset wikipedia/20200501.ar (download: Unknown size, generated: Unknown size, post-processed: Unknown sizetotal: Unknown size) to /home/gaguil20/.cache/huggingface/datasets/wikipedia/20200501.ar/1.0.0/7be7f4324255faf70687be8692de57cf79197afdc33ff08d6a04ed602df32d50...\r\nDownloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 47.4k/47.4k [00:00<00:00, 1.40MB/s]\r\nDownloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79.8M/79.8M [00:15<00:00, 5.13MB/s]\r\nDownloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 171M/171M [00:33<00:00, 5.13MB/s]\r\nDownloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 103M/103M [00:20<00:00, 5.14MB/s]\r\nDownloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 227M/227M [00:44<00:00, 5.06MB/s]\r\nDownloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 140M/140M [00:28<00:00, 4.96MB/s]\r\nDownloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 160M/160M [00:30<00:00, 5.20MB/s]\r\nDownloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 97.5M/97.5M [00:19<00:00, 5.06MB/s]\r\nDownloading: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 222M/222M [00:42<00:00, 5.21MB/s]\r\n100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [03:16<00:00, 196.39s/sources]\r\nDataset wikipedia downloaded and prepared to /home/gaguil20/.cache/huggingface/datasets/wikipedia/20200501.ar/1.0.0/7be7f4324255faf70687be8692de57cf79197afdc33ff08d6a04ed602df32d50. Subsequent calls will reuse this data.\r\nLoading the \'20200501.ar\' data took 11,582.7 seconds...\r\n````'
 '> About the date issue, I think it\'s possible to use another date with\r\n> ```python\r\n> load_dataset(""wikipedia"", language=""es"", date=""..."", beam_runner=""..."")\r\n> ```\r\n\r\nI tried your suggestion about the date and the function does not accept the language and date keywords. I tried both on `nlp` v0.4 and the new `datasets` library (v1.0.2):\r\n```\r\nload_dataset(""wikipedia"", language=""es"", date=""20200601"", beam_runner=\'DirectRunner\', split=\'train\')\r\n```\r\nFor now, my quick workaround to keep things moving was to simply change the date inside the library at this line: [https://github.com/huggingface/datasets/blob/master/datasets/wikipedia/wikipedia.py#L403](https://github.com/huggingface/datasets/blob/master/datasets/wikipedia/wikipedia.py#L403)\r\n\r\nNote that the date and languages are valid: [https://dumps.wikimedia.org/eswiki/20200601/dumpstatus.json](https://dumps.wikimedia.org/eswiki/20200601/dumpstatus.json)\r\n\r\nAny suggestion is welcome :) @lhoestq \r\n\r\n\r\n## **[UPDATE]**\r\n\r\nThe workaround I mentioned fetched the data, but then I faced another issue (even the log says to report this as bug):\r\n```\r\nERROR:root:mwparserfromhell ParseError: This is a bug and should be reported. Info: C tokenizer exited with non-empty token stack.\r\n```\r\n\r\nHere\'s the full stack (which says that there is a key error caused by this key: `KeyError: \'000nbsp\'`):\r\n\r\n```Downloading and preparing dataset wikipedia/20200601.es (download: Unknown size, generated: Unknown size, post-processed: Unknown sizetotal: Unknown size) to /home/gustavoag/.cache/huggingface/datasets/wikipedia/20200601.es/1.0.0/7be7f4324255faf70687be8692de57cf79197afdc33ff08d6a04ed602df32d50...\r\nDownloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 74.7k/74.7k [00:00<00:00, 1.53MB/s]\r\nDownloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 232M/232M [00:48<00:00, 4.75MB/s]\r\nDownloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 442M/442M [01:39<00:00, 4.44MB/s]\r\nDownloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 173M/173M [00:33<00:00, 5.12MB/s]\r\nDownloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 344M/344M [01:14<00:00, 4.59MB/s]\r\nDownloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 541M/541M [01:59<00:00, 4.52MB/s]\r\nDownloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 476M/476M [01:31<00:00, 5.18MB/s]\r\nDownloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 545M/545M [02:02<00:00, 4.46MB/s]\r\nDownloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 299M/299M [01:01<00:00, 4.89MB/s]\r\nDownloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9.60M/9.60M [00:01<00:00, 4.84MB/s]\r\nDownloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 423M/423M [01:36<00:00, 4.38MB/s]\r\nWARNING:apache_beam.options.pipeline_options:Discarding unparseable args: [\'--lang\', \'es\', \'--date\', \'20200601\', \'--tokenizer\', \'bert-base-multilingual-cased\', \'--cache\', \'train\', \'valid\', \'--max_dataset_length\', \'200000\', \'10000\']\r\n\r\nERROR:root:mwparserfromhell ParseError: This is a bug and should be reported. Info: C tokenizer exited with non-empty token stack.\r\nERROR:root:mwparserfromhell ParseError: This is a bug and should be reported. Info: C tokenizer exited with non-empty token stack.\r\nERROR:root:mwparserfromhell ParseError: This is a bug and should be reported. Info: C tokenizer exited with non-empty token stack.\r\nERROR:root:mwparserfromhell ParseError: This is a bug and should be reported. Info: C tokenizer exited with non-empty token stack.\r\nTraceback (most recent call last):\r\n  File ""apache_beam/runners/common.py"", line 961, in apache_beam.runners.common.DoFnRunner.process\r\n  File ""apache_beam/runners/common.py"", line 553, in apache_beam.runners.common.SimpleInvoker.invoke_process\r\n  File ""apache_beam/runners/common.py"", line 1095, in apache_beam.runners.common._OutputProcessor.process_outputs\r\n  File ""/home/gustavoag/anaconda3/envs/pytorch/lib/python3.8/site-packages/nlp/datasets/wikipedia/7be7f4324255faf70687be8692de57cf79197afdc33ff08d6a04ed602df32d50/wikipedia.py"", line 500, in _clean_content\r\n    text = _parse_and_clean_wikicode(raw_content, parser=mwparserfromhell)\r\n  File ""/home/gustavoag/anaconda3/envs/pytorch/lib/python3.8/site-packages/nlp/datasets/wikipedia/7be7f4324255faf70687be8692de57cf79197afdc33ff08d6a04ed602df32d50/wikipedia.py"", line 556, in _parse_and_clean_wikicode\r\n    section_text.append(section.strip_code().strip())\r\n  File ""/home/gustavoag/anaconda3/envs/pytorch/lib/python3.8/site-packages/mwparserfromhell/wikicode.py"", line 643, in strip_code\r\n    stripped = node.__strip__(**kwargs)\r\n  File ""/home/gustavoag/anaconda3/envs/pytorch/lib/python3.8/site-packages/mwparserfromhell/nodes/html_entity.py"", line 63, in __strip__\r\n    return self.normalize()\r\n  File ""/home/gustavoag/anaconda3/envs/pytorch/lib/python3.8/site-packages/mwparserfromhell/nodes/html_entity.py"", line 178, in normalize\r\n    return chrfunc(htmlentities.name2codepoint[self.value])\r\nKeyError: \'000nbsp\'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File ""/home/gustavoag/anaconda3/envs/pytorch/lib/python3.8/runpy.py"", line 194, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File ""/home/gustavoag/anaconda3/envs/pytorch/lib/python3.8/runpy.py"", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File ""/raid/data/gustavoag/projects/char2subword/research/preprocessing/split_wiki.py"", line 96, in <module>\r\n    main()\r\n  File ""/raid/data/gustavoag/projects/char2subword/research/preprocessing/split_wiki.py"", line 65, in main\r\n    data = nlp.load_dataset(\'wikipedia\', f\'{args.date}.{args.lang}\', beam_runner=\'DirectRunner\', split=\'train\')\r\n  File ""/home/gustavoag/anaconda3/envs/pytorch/lib/python3.8/site-packages/nlp/load.py"", line 548, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File ""/home/gustavoag/anaconda3/envs/pytorch/lib/python3.8/site-packages/nlp/builder.py"", line 462, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File ""/home/gustavoag/anaconda3/envs/pytorch/lib/python3.8/site-packages/nlp/builder.py"", line 969, in _download_and_prepare\r\n    pipeline_results = pipeline.run()\r\n  File ""/home/gustavoag/anaconda3/envs/pytorch/lib/python3.8/site-packages/apache_beam/pipeline.py"", line 534, in run\r\n    return self.runner.run_pipeline(self, self._options)\r\n  File ""/home/gustavoag/anaconda3/envs/pytorch/lib/python3.8/site-packages/apache_beam/runners/direct/direct_runner.py"", line 119, in run_pipeline\r\n    return runner.run_pipeline(pipeline, options)\r\n  File ""/home/gustavoag/anaconda3/envs/pytorch/lib/python3.8/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py"", line 172, in run_pipeline\r\n    self._latest_run_result = self.run_via_runner_api(\r\n  File ""/home/gustavoag/anaconda3/envs/pytorch/lib/python3.8/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py"", line 183, in run_via_runner_api\r\n    return self.run_stages(stage_context, stages)\r\n  File ""/home/gustavoag/anaconda3/envs/pytorch/lib/python3.8/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py"", line 338, in run_stages\r\n    stage_results = self._run_stage(\r\n  File ""/home/gustavoag/anaconda3/envs/pytorch/lib/python3.8/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py"", line 512, in _run_stage\r\n    last_result, deferred_inputs, fired_timers = self._run_bundle(\r\n  File ""/home/gustavoag/anaconda3/envs/pytorch/lib/python3.8/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py"", line 556, in _run_bundle\r\n    result, splits = bundle_manager.process_bundle(\r\n  File ""/home/gustavoag/anaconda3/envs/pytorch/lib/python3.8/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py"", line 940, in process_bundle\r\n    for result, split_result in executor.map(execute, zip(part_inputs,  # pylint: disable=zip-builtin-not-iterating\r\n  File ""/home/gustavoag/anaconda3/envs/pytorch/lib/python3.8/concurrent/futures/_base.py"", line 611, in result_iterator\r\n    yield fs.pop().result()\r\n  File ""/home/gustavoag/anaconda3/envs/pytorch/lib/python3.8/concurrent/futures/_base.py"", line 439, in result\r\n    return self.__get_result()\r\n  File ""/home/gustavoag/anaconda3/envs/pytorch/lib/python3.8/concurrent/futures/_base.py"", line 388, in __get_result\r\n    raise self._exception\r\n  File ""/home/gustavoag/anaconda3/envs/pytorch/lib/python3.8/site-packages/apache_beam/utils/thread_pool_executor.py"", line 44, in run\r\n    self._future.set_result(self._fn(*self._fn_args, **self._fn_kwargs))\r\n  File ""/home/gustavoag/anaconda3/envs/pytorch/lib/python3.8/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py"", line 932, in execute\r\n    return bundle_manager.process_bundle(\r\n  File ""/home/gustavoag/anaconda3/envs/pytorch/lib/python3.8/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py"", line 837, in process_bundle\r\n    result_future = self._worker_handler.control_conn.push(process_bundle_req)\r\n  File ""/home/gustavoag/anaconda3/envs/pytorch/lib/python3.8/site-packages/apache_beam/runners/portability/fn_api_runner/worker_handlers.py"", line 352, in push\r\n    response = self.worker.do_instruction(request)\r\n  File ""/home/gustavoag/anaconda3/envs/pytorch/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 479, in do_instruction\r\n    return getattr(self, request_type)(\r\n  File ""/home/gustavoag/anaconda3/envs/pytorch/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 515, in process_bundle\r\n    bundle_processor.process_bundle(instruction_id))\r\n  File ""/home/gustavoag/anaconda3/envs/pytorch/lib/python3.8/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 977, in process_bundle\r\n    input_op_by_transform_id[element.transform_id].process_encoded(\r\n  File ""/home/gustavoag/anaconda3/envs/pytorch/lib/python3.8/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 218, in process_encoded\r\n    self.output(decoded_value)\r\n  File ""apache_beam/runners/worker/operations.py"", line 330, in apache_beam.runners.worker.operations.Operation.output\r\n  File ""apache_beam/runners/worker/operations.py"", line 332, in apache_beam.runners.worker.operations.Operation.output\r\n  File ""apache_beam/runners/worker/operations.py"", line 195, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive\r\n  File ""apache_beam/runners/worker/operations.py"", line 670, in apache_beam.runners.worker.operations.DoOperation.process\r\n  File ""apache_beam/runners/worker/operations.py"", line 671, in apache_beam.runners.worker.operations.DoOperation.process\r\n  File ""apache_beam/runners/common.py"", line 963, in apache_beam.runners.common.DoFnRunner.process\r\n  File ""apache_beam/runners/common.py"", line 1030, in apache_beam.runners.common.DoFnRunner._reraise_augmented\r\n  File ""apache_beam/runners/common.py"", line 961, in apache_beam.runners.common.DoFnRunner.process\r\n  File ""apache_beam/runners/common.py"", line 553, in apache_beam.runners.common.SimpleInvoker.invoke_process\r\n  File ""apache_beam/runners/common.py"", line 1122, in apache_beam.runners.common._OutputProcessor.process_outputs\r\n  File ""apache_beam/runners/worker/operations.py"", line 195, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive\r\n  File ""apache_beam/runners/worker/operations.py"", line 670, in apache_beam.runners.worker.operations.DoOperation.process\r\n  File ""apache_beam/runners/worker/operations.py"", line 671, in apache_beam.runners.worker.operations.DoOperation.process\r\n  File ""apache_beam/runners/common.py"", line 963, in apache_beam.runners.common.DoFnRunner.process\r\n  File ""apache_beam/runners/common.py"", line 1030, in apache_beam.runners.common.DoFnRunner._reraise_augmented\r\n  File ""apache_beam/runners/common.py"", line 961, in apache_beam.runners.common.DoFnRunner.process\r\n  File ""apache_beam/runners/common.py"", line 553, in apache_beam.runners.common.SimpleInvoker.invoke_process\r\n  File ""apache_beam/runners/common.py"", line 1122, in apache_beam.runners.common._OutputProcessor.process_outputs\r\n  File ""apache_beam/runners/worker/operations.py"", line 195, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive\r\n  File ""apache_beam/runners/worker/operations.py"", line 670, in apache_beam.runners.worker.operations.DoOperation.process\r\n  File ""apache_beam/runners/worker/operations.py"", line 671, in apache_beam.runners.worker.operations.DoOperation.process\r\n  File ""apache_beam/runners/common.py"", line 963, in apache_beam.runners.common.DoFnRunner.process\r\n  File ""apache_beam/runners/common.py"", line 1045, in apache_beam.runners.common.DoFnRunner._reraise_augmented\r\n  File ""/home/gustavoag/anaconda3/envs/pytorch/lib/python3.8/site-packages/future/utils/__init__.py"", line 446, in raise_with_traceback\r\n    raise exc.with_traceback(traceback)\r\n  File ""apache_beam/runners/common.py"", line 961, in apache_beam.runners.common.DoFnRunner.process\r\n  File ""apache_beam/runners/common.py"", line 553, in apache_beam.runners.common.SimpleInvoker.invoke_process\r\n  File ""apache_beam/runners/common.py"", line 1095, in apache_beam.runners.common._OutputProcessor.process_outputs\r\n  File ""/home/gustavoag/anaconda3/envs/pytorch/lib/python3.8/site-packages/nlp/datasets/wikipedia/7be7f4324255faf70687be8692de57cf79197afdc33ff08d6a04ed602df32d50/wikipedia.py"", line 500, in _clean_content\r\n    text = _parse_and_clean_wikicode(raw_content, parser=mwparserfromhell)\r\n  File ""/home/gustavoag/anaconda3/envs/pytorch/lib/python3.8/site-packages/nlp/datasets/wikipedia/7be7f4324255faf70687be8692de57cf79197afdc33ff08d6a04ed602df32d50/wikipedia.py"", line 556, in _parse_and_clean_wikicode\r\n    section_text.append(section.strip_code().strip())\r\n  File ""/home/gustavoag/anaconda3/envs/pytorch/lib/python3.8/site-packages/mwparserfromhell/wikicode.py"", line 643, in strip_code\r\n    stripped = node.__strip__(**kwargs)\r\n  File ""/home/gustavoag/anaconda3/envs/pytorch/lib/python3.8/site-packages/mwparserfromhell/nodes/html_entity.py"", line 63, in __strip__\r\n    return self.normalize()\r\n  File ""/home/gustavoag/anaconda3/envs/pytorch/lib/python3.8/site-packages/mwparserfromhell/nodes/html_entity.py"", line 178, in normalize\r\n    return chrfunc(htmlentities.name2codepoint[self.value])\r\nKeyError: ""000nbsp [while running \'train/Clean content\']""```'
 '@lhoestq Any updates on this? I have similar issues with the Romanian dump, tnx.'
 'Hey @gaguilar ,\r\n\r\nI just found the [""char2subword"" paper](https://arxiv.org/pdf/2010.12730.pdf) and I\'m really interested in trying it out on own vocabs/datasets like for historical texts (I\'ve already [trained some lms](https://github.com/stefan-it/europeana-bert) on newspaper articles with OCR errors).\r\n\r\nDo you plan to release the code for your paper or is it possible to get the implementation 🤔 Many thanks :hugs: '
 'Hi @stefan-it! Thanks for your interest in our work! We do plan to release the code, but we will make it available once the paper has been published at a conference. Sorry for the inconvenience!\r\n\r\nHi @lhoestq, do you have any insights for this issue by any chance? Thanks!'
 ""This is an issue on the `mwparserfromhell` side. You could try to update `mwparserfromhell` and see if it fixes the issue. If it doesn't we'll have to create an issue on their repo for them to fix it.\r\nBut first let's see if the latest version of `mwparserfromhell` does the job.""
 'I think the work around  as suggested in the issue [#886] is not working for several languages, such as `id`. For example, I tried all the dates to download dataset for `id` langauge from the following link: (https://github.com/huggingface/datasets/pull/886) [https://dumps.wikimedia.org/idwiki/](https://dumps.wikimedia.org/idwiki/ )\r\n\r\n>  >>> dataset = load_dataset(\'wikipedia\', language=\'id\', date=""20210501"",  beam_runner=\'DirectRunner\')\r\nWARNING:datasets.builder:Using custom data configuration 20210501.id-date=20210501,language=id\r\nDownloading and preparing dataset wikipedia/20210501.id (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /Users/.cache/huggingface/datasets/wikipedia/20210501.id-date=20210501,language=id/0.0.0/2fe8db1405aef67dff9fcc51e133e1f9c5b0106f9d9e9638188176d278fd5ff1...\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/Users/opt/anaconda3/envs/proj/lib/python3.9/site-packages/datasets/load.py"", line 745, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File ""/Users/opt/anaconda3/envs/proj/lib/python3.9/site-packages/datasets/builder.py"", line 574, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File ""/Users/opt/anaconda3/envs/proj/lib/python3.9/site-packages/datasets/builder.py"", line 1139, in _download_and_prepare\r\n    super(BeamBasedBuilder, self)._download_and_prepare(\r\n  File ""/Users/opt/anaconda3/envs/proj/lib/python3.9/site-packages/datasets/builder.py"", line 630, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File ""/Users/.cache/huggingface/modules/datasets_modules/datasets/wikipedia/2fe8db1405aef67dff9fcc51e133e1f9c5b0106f9d9e9638188176d278fd5ff1/wikipedia.py"", line 420, in _split_generators\r\n    downloaded_files = dl_manager.download_and_extract({""info"": info_url})\r\n  File ""/Users/opt/anaconda3/envs/proj/lib/python3.9/site-packages/datasets/utils/download_manager.py"", line 287, in download_and_extract\r\n    return self.extract(self.download(url_or_urls))\r\n  File ""/Users/opt/anaconda3/envs/proj/lib/python3.9/site-packages/datasets/utils/download_manager.py"", line 195, in download\r\n    downloaded_path_or_paths = map_nested(\r\n  File ""/Users/opt/anaconda3/envs/proj/lib/python3.9/site-packages/datasets/utils/py_utils.py"", line 203, in map_nested\r\n    mapped = [\r\n  File ""/Users/opt/anaconda3/envs/proj/lib/python3.9/site-packages/datasets/utils/py_utils.py"", line 204, in <listcomp>\r\n    _single_map_nested((function, obj, types, None, True)) for obj in tqdm(iterable, disable=disable_tqdm)\r\n  File ""/Users/opt/anaconda3/envs/proj/lib/python3.9/site-packages/datasets/utils/py_utils.py"", line 142, in _single_map_nested\r\n    return function(data_struct)\r\n  File ""/Users/opt/anaconda3/envs/proj/lib/python3.9/site-packages/datasets/utils/download_manager.py"", line 218, in _download\r\n    return cached_path(url_or_filename, download_config=download_config)\r\n  File ""/Users/opt/anaconda3/envs/proj/lib/python3.9/site-packages/datasets/utils/file_utils.py"", line 281, in cached_path\r\n    output_path = get_from_cache(\r\n  File ""/Users/opt/anaconda3/envs/proj/lib/python3.9/site-packages/datasets/utils/file_utils.py"", line 623, in get_from_cache\r\n    raise ConnectionError(""Couldn\'t reach {}"".format(url))\r\nConnectionError: Couldn\'t reach https://dumps.wikimedia.org/idwiki/20210501/dumpstatus.json\r\n\r\nMoreover the downloading speed for `non-en` language is very very slow. And interestingly the download stopped after approx a couple minutes due to the read time-out. I tried numerous times and the results is same. Is there any feasible way to download non-en language using huggingface?\r\n\r\n> File ""/Users/miislamg/opt/anaconda3/envs/proj-semlm/lib/python3.9/site-packages/requests/models.py"", line 760, in generate\r\n    raise ConnectionError(e)\r\nrequests.exceptions.ConnectionError: HTTPSConnectionPool(host=\'dumps.wikimedia.org\', port=443): Read timed out.\r\nDownloading:   7%|████████▎                                                                                                                   | 10.2M/153M [03:35<50:07, 47.4kB/s]'
 'Hi ! The link https://dumps.wikimedia.org/idwiki/20210501/dumpstatus.json seems to be working fine for me.\r\n\r\nRegarding the time outs, it must come either from an issue on the wikimedia host side, or from your internet connection.\r\nFeel free to try again several times.'
 'I was trying to download dataset for `es` language, however I am getting the following error:\r\n```\r\ndataset = load_dataset(\'wikipedia\', language=\'es\', date=""20210320"", beam_runner=\'DirectRunner\') \r\n```\r\n\r\n```\r\nDownloading and preparing dataset wikipedia/20210320.es (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /scratch/user_name/datasets/wikipedia/20210320.es-date=20210320,language=es/0.0.0/2fe8db1405aef67dff9fcc51e133e1f9c5b0106f9d9e9638188176d278fd5ff1...\r\nTraceback (most recent call last):\r\n  File ""apache_beam/runners/common.py"", line 1233, in apache_beam.runners.common.DoFnRunner.process\r\n  File ""apache_beam/runners/common.py"", line 581, in apache_beam.runners.common.SimpleInvoker.invoke_process\r\n  File ""apache_beam/runners/common.py"", line 1368, in apache_beam.runners.common._OutputProcessor.process_outputs\r\n  File ""/scratch/user_name/modules/datasets_modules/datasets/wikipedia/2fe8db1405aef67dff9fcc51e133e1f9c5b0106f9d9e9638188176d278fd5ff1/wikipedia.py"", line 492, in _clean_content\r\n    text = _parse_and_clean_wikicode(raw_content, parser=mwparserfromhell)\r\n  File ""/scratch/user_name/modules/datasets_modules/datasets/wikipedia/2fe8db1405aef67dff9fcc51e133e1f9c5b0106f9d9e9638188176d278fd5ff1/wikipedia.py"", line 548, in _parse_and_clean_wikicode\r\n    section_text.append(section.strip_code().strip())\r\n  File ""/opt/conda/lib/python3.7/site-packages/mwparserfromhell/wikicode.py"", line 639, in strip_code\r\n    stripped = node.__strip__(**kwargs)\r\n  File ""/opt/conda/lib/python3.7/site-packages/mwparserfromhell/nodes/html_entity.py"", line 60, in __strip__\r\n    return self.normalize()\r\n  File ""/opt/conda/lib/python3.7/site-packages/mwparserfromhell/nodes/html_entity.py"", line 150, in normalize\r\n    return chr(htmlentities.name2codepoint[self.value])\r\nKeyError: \'000nbsp\'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File ""download_dataset_all.py"", line 8, in <module>\r\n    dataset = load_dataset(\'wikipedia\', language=language, date=""20210320"", beam_runner=\'DirectRunner\') \r\n  File ""/opt/conda/lib/python3.7/site-packages/datasets/load.py"", line 748, in load_dataset\r\n    use_auth_token=use_auth_token,\r\n  File ""/opt/conda/lib/python3.7/site-packages/datasets/builder.py"", line 575, in download_and_prepare\r\n    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n  File ""/opt/conda/lib/python3.7/site-packages/datasets/builder.py"", line 1152, in _download_and_prepare\r\n    pipeline_results = pipeline.run()\r\n  File ""/opt/conda/lib/python3.7/site-packages/apache_beam/pipeline.py"", line 564, in run\r\n    return self.runner.run_pipeline(self, self._options)\r\n  File ""/opt/conda/lib/python3.7/site-packages/apache_beam/runners/direct/direct_runner.py"", line 131, in run_pipeline\r\n    return runner.run_pipeline(pipeline, options)\r\n  File ""/opt/conda/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py"", line 190, in run_pipeline\r\n    pipeline.to_runner_api(default_environment=self._default_environment))\r\n  File ""/opt/conda/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py"", line 200, in run_via_runner_api\r\n    return self.run_stages(stage_context, stages)\r\n  File ""/opt/conda/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py"", line 366, in run_stages\r\n    bundle_context_manager,\r\n  File ""/opt/conda/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py"", line 562, in _run_stage\r\n    bundle_manager)\r\n  File ""/opt/conda/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py"", line 602, in _run_bundle\r\n    data_input, data_output, input_timers, expected_timer_output)\r\n  File ""/opt/conda/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py"", line 903, in process_bundle\r\n    result_future = self._worker_handler.control_conn.push(process_bundle_req)\r\n  File ""/opt/conda/lib/python3.7/site-packages/apache_beam/runners/portability/fn_api_runner/worker_handlers.py"", line 378, in push\r\n    response = self.worker.do_instruction(request)\r\n  File ""/opt/conda/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 610, in do_instruction\r\n    getattr(request, request_type), request.instruction_id)\r\n  File ""/opt/conda/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 647, in process_bundle\r\n    bundle_processor.process_bundle(instruction_id))\r\n  File ""/opt/conda/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 1001, in process_bundle\r\n    element.data)\r\n  File ""/opt/conda/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 229, in process_encoded\r\n    self.output(decoded_value)\r\n  File ""apache_beam/runners/worker/operations.py"", line 356, in apache_beam.runners.worker.operations.Operation.output\r\n  File ""apache_beam/runners/worker/operations.py"", line 358, in apache_beam.runners.worker.operations.Operation.output\r\n  File ""apache_beam/runners/worker/operations.py"", line 220, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive\r\n  File ""apache_beam/runners/worker/operations.py"", line 717, in apache_beam.runners.worker.operations.DoOperation.process\r\n  File ""apache_beam/runners/worker/operations.py"", line 718, in apache_beam.runners.worker.operations.DoOperation.process\r\n  File ""apache_beam/runners/common.py"", line 1235, in apache_beam.runners.common.DoFnRunner.process\r\n  File ""apache_beam/runners/common.py"", line 1300, in apache_beam.runners.common.DoFnRunner._reraise_augmented\r\n  File ""apache_beam/runners/common.py"", line 1233, in apache_beam.runners.common.DoFnRunner.process\r\n  File ""apache_beam/runners/common.py"", line 581, in apache_beam.runners.common.SimpleInvoker.invoke_process\r\n  File ""apache_beam/runners/common.py"", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs\r\n  File ""apache_beam/runners/worker/operations.py"", line 220, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive\r\n  File ""apache_beam/runners/worker/operations.py"", line 717, in apache_beam.runners.worker.operations.DoOperation.process\r\n  File ""apache_beam/runners/worker/operations.py"", line 718, in apache_beam.runners.worker.operations.DoOperation.process\r\n  File ""apache_beam/runners/common.py"", line 1235, in apache_beam.runners.common.DoFnRunner.process\r\n  File ""apache_beam/runners/common.py"", line 1300, in apache_beam.runners.common.DoFnRunner._reraise_augmented\r\n  File ""apache_beam/runners/common.py"", line 1233, in apache_beam.runners.common.DoFnRunner.process\r\n  File ""apache_beam/runners/common.py"", line 581, in apache_beam.runners.common.SimpleInvoker.invoke_process\r\n  File ""apache_beam/runners/common.py"", line 1395, in apache_beam.runners.common._OutputProcessor.process_outputs\r\n  File ""apache_beam/runners/worker/operations.py"", line 220, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive\r\n  File ""apache_beam/runners/worker/operations.py"", line 717, in apache_beam.runners.worker.operations.DoOperation.process\r\n  File ""apache_beam/runners/worker/operations.py"", line 718, in apache_beam.runners.worker.operations.DoOperation.process\r\n  File ""apache_beam/runners/common.py"", line 1235, in apache_beam.runners.common.DoFnRunner.process\r\n  File ""apache_beam/runners/common.py"", line 1315, in apache_beam.runners.common.DoFnRunner._reraise_augmented\r\n  File ""/opt/conda/lib/python3.7/site-packages/future/utils/__init__.py"", line 446, in raise_with_traceback\r\n    raise exc.with_traceback(traceback)\r\n  File ""apache_beam/runners/common.py"", line 1233, in apache_beam.runners.common.DoFnRunner.process\r\n  File ""apache_beam/runners/common.py"", line 581, in apache_beam.runners.common.SimpleInvoker.invoke_process\r\n  File ""apache_beam/runners/common.py"", line 1368, in apache_beam.runners.common._OutputProcessor.process_outputs\r\n  File ""/scratch/user_name/modules/datasets_modules/datasets/wikipedia/2fe8db1405aef67dff9fcc51e133e1f9c5b0106f9d9e9638188176d278fd5ff1/wikipedia.py"", line 492, in _clean_content\r\n    text = _parse_and_clean_wikicode(raw_content, parser=mwparserfromhell)\r\n  File ""/scratch/user_name/modules/datasets_modules/datasets/wikipedia/2fe8db1405aef67dff9fcc51e133e1f9c5b0106f9d9e9638188176d278fd5ff1/wikipedia.py"", line 548, in _parse_and_clean_wikicode\r\n    section_text.append(section.strip_code().strip())\r\n  File ""/opt/conda/lib/python3.7/site-packages/mwparserfromhell/wikicode.py"", line 639, in strip_code\r\n    stripped = node.__strip__(**kwargs)\r\n  File ""/opt/conda/lib/python3.7/site-packages/mwparserfromhell/nodes/html_entity.py"", line 60, in __strip__\r\n    return self.normalize()\r\n  File ""/opt/conda/lib/python3.7/site-packages/mwparserfromhell/nodes/html_entity.py"", line 150, in normalize\r\n    return chr(htmlentities.name2codepoint[self.value])\r\nKeyError: ""000nbsp [while running \'train/Clean content\']""\r\n```'
 'Hi ! This looks related to this issue: https://github.com/huggingface/datasets/issues/1994\r\nBasically the parser that is used (mwparserfromhell) has some issues for some pages in `es`.\r\nWe already reported some issues for `es` on their repo at https://github.com/earwig/mwparserfromhell/issues/247 but it looks like there are still a few issues. Might be a good idea to open a new issue on the mwparserfromhell repo']","Hi,

I am working with the `wikipedia` dataset and I have a script that goes over 92 of the available languages in that dataset. So far I have detected that `ar`, `af`, `an` are not loading. Other languages like `fr` and `en` are working fine. Here's how I am loading them:

```
import nlp

langs = ['ar'. 'af', 'an']

for lang in langs:
    data = nlp.load_dataset('wikipedia', f'20200501.{lang}', beam_runner='DirectRunner', split='train') 
    print(lang, len(data))
```

Here's what I see for 'ar' (it gets stuck there):
```
Downloading and preparing dataset wikipedia/20200501.ar (download: Unknown size, generated: Unknown size, post-processed: Unknown sizetotal: Unknown size) to /home/gaguilar/.cache/huggingface/datasets/wikipedia/20200501.ar/1.0.0/7be7f4324255faf70687be8692de57cf79197afdc33ff08d6a04ed602df32d50...
```

Note that those languages are indeed in the list of expected languages. Any suggestions on how to work around this? Thanks!"
https://github.com/huggingface/datasets/issues/575,"Couldn't reach certain URLs and for the ones that can be reached, code just blocks after downloading.","[""Update:\r\n\r\nThe imdb download completed after a long time (about 45 mins). Ofcourse once download loading was instantaneous. Also, the loaded object was of type `arrow_dataset`. \r\n\r\nThe urls for glue still doesn't work though.""
 ""Thanks for the report, I'll give a look!""
 'I am also seeing a similar error when running the following:\r\n\r\n```\r\nimport nlp\r\ndataset = load_dataset(\'cola\')\r\n```\r\nError:\r\n```\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/js11133/.conda/envs/jiant/lib/python3.8/site-packages/nlp/load.py"", line 509, in load_dataset\r\n    module_path = prepare_module(path, download_config=download_config, dataset=True)\r\n  File ""/home/js11133/.conda/envs/jiant/lib/python3.8/site-packages/nlp/load.py"", line 248, in prepare_module\r\n    local_path = cached_path(file_path, download_config=download_config)\r\n  File ""/home/js11133/.conda/envs/jiant/lib/python3.8/site-packages/nlp/utils/file_utils.py"", line 191, in cached_path\r\n    output_path = get_from_cache(\r\n  File ""/home/js11133/.conda/envs/jiant/lib/python3.8/site-packages/nlp/utils/file_utils.py"", line 356, in get_from_cache\r\n    raise ConnectionError(""Couldn\'t reach {}"".format(url))\r\nConnectionError: Couldn\'t reach https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/cola/cola.py\r\n```'
 '@jeswan `""cola""` is not a valid dataset identifier (you can check the up-to-date list on https://huggingface.co/datasets) but you can find cola inside glue.'
 'Ah right. Thanks!'
 'Hi. Closing this one since #626 updated the glue urls.\r\n\r\n> 1. Why is it still blocking? Is it still downloading?\r\n\r\nAfter downloading it generates the arrow file by iterating through the examples.\r\nThe number of examples processed by second is shown during the processing (not sure why it was not the case for you)\r\n\r\n> 2. I specified split as train, so why is the test folder being populated?\r\n\r\nIt downloads every split\r\n\r\n\r\n\r\n']","Hi,

I'm following the [quick tour](https://huggingface.co/nlp/quicktour.html) and tried to load the glue dataset:
```
>>> from nlp import load_dataset
>>> dataset = load_dataset('glue', 'mrpc', split='train')
```

However, this ran into a `ConnectionError` saying it could not reach the URL (just pasting the last few lines):
```

/net/vaosl01/opt/NFS/su0/miniconda3/envs/hf/lib/python3.7/site-packages/nlp/utils/file_utils.py in get_from_cache(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only)
    354                 "" to False.""
    355             )
--> 356         raise ConnectionError(""Couldn't reach {}"".format(url))
    357 
    358     # From now on, connected is True.

ConnectionError: Couldn't reach https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2Fmrpc_dev_ids.tsv?alt=media&token=ec5c0836-31d5-48f4-b431-7480817f1adc
```

I tried glue with cola and sst2. I got the same error, just instead of mrpc in the URL, it was replaced with cola and sst2.

Since this was not working, I thought I'll try another dataset. So I tried downloading the imdb dataset:
```
ds = load_dataset('imdb', split='train')
```
This downloads the data, but it just blocks after that:
```
Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4.56k/4.56k [00:00<00:00, 1.38MB/s]
Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2.07k/2.07k [00:00<00:00, 1.15MB/s]
Downloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.06 MiB, post-processed: Unknown sizetotal: 207.28 MiB) to /net/vaosl01/opt/NFS/su0/huggingface/datasets/imdb/plain_text/1.0.0/76cdbd7249ea3548c928bbf304258dab44d09cd3638d9da8d42480d1d1be3743...
Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 84.1M/84.1M [00:07<00:00, 11.1MB/s]
```

I checked the folder `$HF_HOME/datasets/downloads/extracted/<id>/aclImdb`. This folder is constantly growing in size. When I navigated to the train folder within, there was no file. However, the test folder seemed to be populating. The last time I checked it was 327M. I thought the Imdb dataset was smaller than that. My questions are:
1. Why is it still blocking? Is it still downloading?
2. I specified split as train, so why is the test folder being populated?
3. I read somewhere that after downloading, `nlp` converts the text files into some sort of `arrow` files, which will also take a while. Is this also happening here?

Thanks.
"
https://github.com/huggingface/datasets/issues/568,`metric.compute` throws `ArrowInvalid` error,"['Hmm might be related to what we are solving in #564'
 ""Could you try to update to `datasets>=1.0.0` (we changed the name of the library) and try again ?\r\nIf is was related to the distributed setup settings it must be fixed.\r\nIf it was related to empty metric inputs it's going to be fixed in #654 ""
 'Closing this one as it was fixed in #654 \r\nFeel free to re-open if you have other questions']","I get the following error with `rouge.compute`. It happens only with distributed training, and it occurs randomly I can't easily reproduce it. This is using `nlp==0.4.0`

```
  File ""/home/beltagy/trainer.py"", line 92, in validation_step
    rouge_scores = rouge.compute(predictions=generated_str, references=gold_str, rouge_types=['rouge2', 'rouge1', 'rougeL'])
  File ""/home/beltagy/miniconda3/envs/allennlp/lib/python3.7/site-packages/nlp/metric.py"", line 224, in compute
    self.finalize(timeout=timeout)
  File ""/home/beltagy/miniconda3/envs/allennlp/lib/python3.7/site-packages/nlp/metric.py"", line 213, in finalize
    self.data = Dataset(**reader.read_files(node_files))
  File ""/home/beltagy/miniconda3/envs/allennlp/lib/python3.7/site-packages/nlp/arrow_reader.py"", line 217, in read_files
    dataset_kwargs = self._read_files(files=files, info=self._info, original_instructions=original_instructions)
  File ""/home/beltagy/miniconda3/envs/allennlp/lib/python3.7/site-packages/nlp/arrow_reader.py"", line 162, in _read_files
    pa_table: pa.Table = self._get_dataset_from_filename(f_dict)
  File ""/home/beltagy/miniconda3/envs/allennlp/lib/python3.7/site-packages/nlp/arrow_reader.py"", line 276, in _get_dataset_from_filename
    f = pa.ipc.open_stream(mmap)
  File ""/home/beltagy/miniconda3/envs/allennlp/lib/python3.7/site-packages/pyarrow/ipc.py"", line 173, in open_stream
    return RecordBatchStreamReader(source)
  File ""/home/beltagy/miniconda3/envs/allennlp/lib/python3.7/site-packages/pyarrow/ipc.py"", line 64, in __init__
    self._open(source)
  File ""pyarrow/ipc.pxi"", line 469, in pyarrow.lib._RecordBatchStreamReader._open
  File ""pyarrow/error.pxi"", line 122, in pyarrow.lib.pyarrow_internal_check_status
  File ""pyarrow/error.pxi"", line 84, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: Tried reading schema message, was null or length 0
```"
https://github.com/huggingface/datasets/issues/565,No module named 'nlp.logging',"['Thanks for reporting.\r\n\r\nApparently this is a versioning issue: the lib downloaded the `bleurt` script from the master branch where we did this change recently. We\'ll fix that in a new release this week or early next week. Cc @thomwolf \r\n\r\nUntil that, I\'d suggest you to download the right bleurt folder from github ([this one](https://github.com/huggingface/nlp/tree/0.4.0/metrics/bleurt)) and do\r\n\r\n```python\r\nfrom nlp import load_metric\r\n\r\nbleurt = load_metric(""path/to/bleurt/folder"")\r\n```\r\n\r\nTo download it you can either clone the repo or download the `bleurt.py` file and place it in a folder named `bleurt` '
 ""Actually we can fix this on our side, this script didn't had to be updated. I'll do it in a few minutes""]","Hi, I am using nlp version 0.4.0. Trying to use bleurt as an eval metric, however, the bleurt script imports nlp.logging which creates the following error. What am I missing?

```
>>> import nlp
2020-09-02 13:47:09.210310: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
>>> bleurt = nlp.load_metric(""bleurt"")
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/melody/anaconda3/envs/transformers/lib/python3.6/site-packages/nlp/load.py"", line 443, in load_metric
    metric_cls = import_main_class(module_path, dataset=False)
  File ""/home/melody/anaconda3/envs/transformers/lib/python3.6/site-packages/nlp/load.py"", line 61, in import_main_class
    module = importlib.import_module(module_path)
  File ""/home/melody/anaconda3/envs/transformers/lib/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 665, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 678, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""/home/melody/anaconda3/envs/transformers/lib/python3.6/site-packages/nlp/metrics/bleurt/43448cf2959ea81d3ae0e71c5c8ee31dc15eed9932f197f5f50673cbcecff2b5/bleurt.py"", line 20, in <module>
    from nlp.logging import get_logger
ModuleNotFoundError: No module named 'nlp.logging'
```

Just to show once again that I can't import the logging module:

```
>>> import nlp
2020-09-02 13:48:38.190621: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
>>> nlp.__version__
'0.4.0'
>>> from nlp.logging import get_logger
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ModuleNotFoundError: No module named 'nlp.logging'
```"
https://github.com/huggingface/datasets/issues/560,Using custom DownloadConfig results in an error,"['From my limited understanding, part of the issue seems related to the `prepare_module`  and `download_and_prepare` functions each handling the case where no config is passed. For example, `prepare_module` does mutate the object passed and forces the flags `extract_compressed_file` and `force_extract` to `True`.\r\n\r\nSee:\r\n* https://github.com/huggingface/nlp/blob/5fb61e1012bda724a9b6b847307d90a1380abfa5/src/nlp/load.py#L227\r\n* https://github.com/huggingface/nlp/blob/5fb61e1012bda724a9b6b847307d90a1380abfa5/src/nlp/builder.py#L388\r\n\r\nMaybe a cleaner solution would be to always instantiate a default `DownloadConfig` object at the top-level, have it as non-optional for the lower-level functions and treat it as immutable. '
 ""Thanks for the report, I'll take a look.\r\n\r\nWhat is your specific use-case for providing a DownloadConfig object?\r\n""
 ""Thanks. Our use case involves running a training job behind a corporate firewall with no access to any external resources (S3, GCP or other web resources).\r\n\r\nI was thinking about a 2-steps process:\r\n1) Download the resources / artifacts using some secure corporate channel, ie run `nlp.load_dataset()` without a specific `DownloadConfig`. After that, collect the files from the `$HF_HOME` folder\r\n2) Copy the `$HF_HOME` folder in the firewalled environment. Run `nlp.load_dataset()` with a custom config `DownloadConfig(local_files_only=True)`\r\n\r\nHowever this ends up a bit clunky in practice, even when solving the `DownloadConfig` issue above. For example, the `filename` hash computed in `get_from_cache()` differs in the `local_files_only=False` vs `local_files_only=True` case (local case defaults `etag` to `None`, which results in a different hash). So effectively step 2) above doesn't work because the hash computed differs from the hash in the cache folder. Some hacks / workaround are possible but this solution becomes very convoluted.\r\nhttps://github.com/huggingface/nlp/blob/c214aa5a4430c1df1bcd0619fd94d6abdf9d2da7/src/nlp/utils/file_utils.py#L417\r\n\r\nWould you recommend a different path?\r\n""
 'I see.\r\n\r\nProbably the easiest way for you would be that we add simple serialization/deserialization methods to the Dataset and DatasetDict objects once the data files have been downloaded and all the dataset is processed.\r\n\r\nWhat do you think @lhoestq ?'
 'This use-case will be solved with #571 '
 'Thank you very much @thomwolf and @lhoestq we will give it a try']","## Version / Environment

Ubuntu 18.04
Python 3.6.8
nlp 0.4.0

## Description

Loading `imdb` dataset works fine when when I don't specify any `download_config` argument. When I create a custom `DownloadConfig` object and pass it to the `nlp.load_dataset` function, this results in an error.

## How to reproduce

### Example without DownloadConfig --> works

```python
import os

os.environ[""HF_HOME""] = ""/data/hf-test-without-dl-config-01/""

import logging
import nlp

logging.basicConfig(level=logging.INFO)

if __name__ == ""__main__"":
    imdb = nlp.load_dataset(path=""imdb"")
```

### Example with DownloadConfig --> doesn't work

```python
import os

os.environ[""HF_HOME""] = ""/data/hf-test-with-dl-config-01/""

import logging
import nlp
from nlp.utils import DownloadConfig

logging.basicConfig(level=logging.INFO)

if __name__ == ""__main__"":
    download_config = DownloadConfig()
    imdb = nlp.load_dataset(path=""imdb"", download_config=download_config)
```

Error traceback:

```
Traceback (most recent call last):
  File ""/.../example_with_dl_config.py"", line 13, in <module>
    imdb = nlp.load_dataset(path=""imdb"", download_config=download_config)
  File ""/.../python3.6/python3.6/site-packages/nlp/load.py"", line 549, in load_dataset
    download_config=download_config, download_mode=download_mode, ignore_verifications=ignore_verifications,
  File ""/.../python3.6/python3.6/site-packages/nlp/builder.py"", line 463, in download_and_prepare
    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
  File ""/.../python3.6/python3.6/site-packages/nlp/builder.py"", line 518, in _download_and_prepare
    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)
  File ""/.../python3.6/python3.6/site-packages/nlp/datasets/imdb/76cdbd7249ea3548c928bbf304258dab44d09cd3638d9da8d42480d1d1be3743/imdb.py"", line 86, in _split_generators
    arch_path = dl_manager.download_and_extract(_DOWNLOAD_URL)
  File ""/.../python3.6/python3.6/site-packages/nlp/utils/download_manager.py"", line 220, in download_and_extract
    return self.extract(self.download(url_or_urls))
  File ""/.../python3.6/python3.6/site-packages/nlp/utils/download_manager.py"", line 158, in download
    self._record_sizes_checksums(url_or_urls, downloaded_path_or_paths)
  File ""/.../python3.6/python3.6/site-packages/nlp/utils/download_manager.py"", line 108, in _record_sizes_checksums
    self._recorded_sizes_checksums[url] = get_size_checksum_dict(path)
  File ""/.../python3.6/python3.6/site-packages/nlp/utils/info_utils.py"", line 79, in get_size_checksum_dict
    with open(path, ""rb"") as f:
IsADirectoryError: [Errno 21] Is a directory: '/data/hf-test-with-dl-config-01/datasets/extracted/b6802c5b61824b2c1f7dbf7cda6696b5f2e22214e18d171ce1ed3be90c931ce5'
```

"
https://github.com/huggingface/datasets/issues/554,nlp downloads to its module path,"['Indeed this is a known issue arising from the fact that we try to be compatible with cloupickle.\r\n\r\nDoes this also happen if you are installing in a virtual environment?'
 '> Indeed this is a know issue with the fact that we try to be compatible with cloupickle.\r\n> \r\n> Does this also happen if you are installing in a virtual environment?\r\n\r\nThen it would work, because the package is in a writable path.'
 ""If it's fine for you then this is the recommended way to solve this issue.""
 ""> If it's fine for you then this is the recommended way to solve this issue.\r\n\r\nI don't want to use a virtual environment, because Nix is fully reproducible, and virtual environments are not. And I am the maintainer of the `transformers` in nixpkgs, so sooner or later I will have to package `nlp`, since it is becoming a dependency of `transformers` ;).""
 ""Ok interesting. We could have another check to see if it's possible to download and import the datasets script at another location than the module path. I think this would probably involve tweaking the python system path dynamically.\r\n\r\nI don't know anything about Nix so if you want to give this a try your self we can guide you or you can give us more information on your general project and how this works.\r\n\r\nRegarding `nlp` and `transformers`, we are not sure `nlp` will become a required dependency for `transformers`.  It will probably be used a lot in the examples but I think it probably won't be a required dependency for the main package since we try to keep it as light as possible in terms of deps.\r\n\r\nHappy to help you make all these things work better for your use-case ""
 '@danieldk modules are now installed in a different location (by default in the cache directory of the lib, in `~/.cache/huggingface/modules`). You can also change that using the environment variable `HF_MODULES_PATH`\r\n\r\nFeel free to play with this change from the master branch for now, and let us know if it sounds good for you :)\r\nWe plan to do a release in the next coming days'
 'Awesome! I’ll hopefully have some time in the coming days to try this.'
 '> Feel free to play with this change from the master branch for now, and let us know if it sounds good for you :)\r\n> We plan to do a release in the next coming days\r\n\r\nThanks for making this change! I just packaged the latest commit on master and it works like a charm now! :partying_face: ']","I am trying to package `nlp` for Nix, because it is now an optional dependency for `transformers`. The problem that I encounter is that the `nlp` library downloads to the module path, which is typically not writable in most package management systems:

```>>> import nlp
>>> squad_dataset = nlp.load_dataset('squad')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/nix/store/2yhik0hhqayksmkkfb0ylqp8cf5wa5wp-python3-3.8.5-env/lib/python3.8/site-packages/nlp/load.py"", line 530, in load_dataset
    module_path, hash = prepare_module(path, download_config=download_config, dataset=True)
  File ""/nix/store/2yhik0hhqayksmkkfb0ylqp8cf5wa5wp-python3-3.8.5-env/lib/python3.8/site-packages/nlp/load.py"", line 329, in prepare_module
    os.makedirs(main_folder_path, exist_ok=True)
  File ""/nix/store/685kq8pyhrvajah1hdsfn4q7gm3j4yd4-python3-3.8.5/lib/python3.8/os.py"", line 223, in makedirs
    mkdir(name, mode)
OSError: [Errno 30] Read-only file system: '/nix/store/2yhik0hhqayksmkkfb0ylqp8cf5wa5wp-python3-3.8.5-env/lib/python3.8/site-packages/nlp/datasets/squad'
```

Do you have any suggested workaround for this issue?

Perhaps overriding the default value for `force_local_path` of `prepare_module`?"
https://github.com/huggingface/datasets/issues/546,Very slow data loading on large dataset,"[""When you load a text file for the first time with `nlp`, the file is converted into Apache Arrow format. Arrow allows to use memory-mapping, which means that you can load an arbitrary large dataset.\r\n\r\nNote that as soon as the conversion has been done once, the next time you'll load the dataset it will be much faster.\r\n\r\nHowever for a 1TB dataset, the conversion can indeed take time. You could try to load parts of it in parallel, and then use `nlp.concatenate_datasets` to get your full dataset.""
 'Humm, we can give a look at these large scale datasets indeed.\r\n\r\nDo you mind sharing a few stats on your dataset so I can try to test on a similar one?\r\n\r\nIn particular some orders of magnitudes for the number of files, number of lines per files, line lengths.'
 '@lhoestq Yes, I understand that the first time requires more time. The concatenate_datasets seems to be a workaround, but I believe a multi-processing method should be integrated into load_dataset to make it easier and more efficient for users.\r\n\r\n@thomwolf Sure, here are the statistics:\r\nNumber of lines: 4.2 Billion\r\nNumber of files: 6K\r\nNumber of tokens: 800 Billion\r\nThe number of lines is distributed equally across these 6k files.\r\nThe line length varies between 100 tokens to 40k tokens.\r\n'
 '@agemagician you can give a try at a multithreaded version if you want (currently on the #548).\r\n\r\nTo test it, you just need to copy the new `text` processing script which is [here](https://github.com/huggingface/nlp/blob/07d92a82b7594498ff702f3cca55c074e2052257/datasets/text/text.py) somewhere on your drive and give it\'s local path instead of `text` to `load_dataset`. E.g. in your example:\r\n```python\r\ntrain_files = glob.glob(""xxx/*.txt"",recursive=True)\r\nrandom.shuffle(train_files)\r\n\r\nprint(train_files)\r\n\r\ndataset = nlp.load_dataset(\'./datasets/text.py\',   # path to where you\'ve dowloaded the multi-threaded text loading script\r\n                           data_files=train_files,\r\n                           name=""customDataset"",\r\n                           version=""1.0.0"",\r\n                           cache_dir=""xxx/nlp"")\r\n```'
 'I have already generated the dataset, but now I tried to reload it and it is still very slow.\r\n\r\nI also have installed your commit and it is slow, even after the dataset was already generated.\r\n`pip install git+https://github.com/huggingface/nlp.git@07d92a82b7594498ff702f3cca55c074e2052257`\r\n\r\nIt uses only a single thread.\r\n\r\nDid I miss something ?'
 'As mentioned in #548 , each time you call `load_dataset` with `data_files=`, they are hashed to get the cache directory name. Hashing can be too slow with 1TB of data. I feel like we should have a faster way of getting a hash that identifies the input data files'
 'I believe this is really a very important feature, otherwise, we will still have the issue of too slow loading problems even if the data cache generation is fast.'
 ""Hmm ok then maybe it's the hashing step indeed.\r\n\r\nLet's see if we can improve this as well.\r\n\r\n(you will very likely have to regenerate your dataset if we change this part of the lib though since I expect modifications on this part of the lib to results in new hashes)""
 ""Also, @agemagician you have to follow the step I indicate in my previous message [here](https://github.com/huggingface/nlp/issues/546#issuecomment-684648927) to use the new text loading script.\r\n\r\nJust doing `pip install git+https://github.com/huggingface/nlp.git@07d92a82b7594498ff702f3cca55c074e2052257` like you did won't use the new script (they are not inside the library but hosted on our hub).""
 'No problem, I will regenerate it. This will make us see if we solved both issues and now both the data generation step, as well as the hashing step, is fast.'
 'Any news for the hashing ?' ""I'm working on it today :)""
 ""Ok so now the text files won't be hashed.\r\n\r\nI also updated #548 to include this change.\r\nLet us know if it helps @agemagician :)""
 'Perfect thanks for your amazing work.']","I made a simple python script to check the NLP library speed, which loads 1.1 TB of textual data.
It has been 8 hours and still, it is on the loading steps.
It does work when the text dataset size is small about  1 GB, but it doesn't scale.
It also uses a single thread during the data loading step.

```
train_files = glob.glob(""xxx/*.txt"",recursive=True)
random.shuffle(train_files)

print(train_files)

dataset = nlp.load_dataset('text', 
                           data_files=train_files,
                           name=""customDataset"",
                           version=""1.0.0"",
                           cache_dir=""xxx/nlp"")
```

Is there something that I am missing ?"
https://github.com/huggingface/datasets/issues/545,New release coming up for this library,['Update: release is planed mid-next week.'],"Hi all,
A few words on the roadmap for this library.

The next release will be a big one and is planed at the end of this week.

In addition to the support for indexed datasets (useful for non-parametric models like REALM, RAG, DPR, knn-LM and many other fast dataset retrieval technics), it will:
- have support for multi-modal datasets
- include various significant improvements on speed for standard processing (map, shuffling, ...)
- have a better support for metrics (better caching, and a robust API) and a bigger focus on reproductibility
- change the name to the final name (voted by the community): `datasets`
- be the 1.0.0 release as we think the API will be mostly stabilized from now on"
https://github.com/huggingface/datasets/issues/543,nlp.load_dataset is not safe for multi processes when loading from local files,"[""I'll take a look!""]","Loading from local files, e.g., `dataset = nlp.load_dataset('csv', data_files=['file_1.csv', 'file_2.csv'])`
concurrently from multiple processes, will raise `FileExistsError` from builder's line 430, https://github.com/huggingface/nlp/blob/6655008c738cb613c522deb3bd18e35a67b2a7e5/src/nlp/builder.py#L423-L438

Likely because multiple processes step into download_and_prepare, https://github.com/huggingface/nlp/blob/6655008c738cb613c522deb3bd18e35a67b2a7e5/src/nlp/load.py#L550-L554

This can happen when launching distributed training with commands like `python -m torch.distributed.launch --nproc_per_node 4` on a new collection of files never loaded before.

I can create a PR that puts in some file locks. It would be helpful if I can be informed of the convention for naming and placement of the lock."
https://github.com/huggingface/datasets/issues/539,[Dataset] `NonMatchingChecksumError` due to an update in the LinCE benchmark data,"[""Hi @gaguilar \r\n\r\nIf you want to take care of this, it very simple, you just need to regenerate the `dataset_infos.json` file as indicated [in the doc](https://huggingface.co/nlp/share_dataset.html#adding-metadata) by [installing from source](https://huggingface.co/nlp/installation.html#installing-from-source) and running the following command from the root of the repo:\r\n```bash\r\npython nlp-cli test ./datasets/lince --save_infos --all_configs\r\n```\r\nAnd then you can open a pull-request with the updated json file.\r\n\r\nOtherwise we'll do it sometime this week.""
 'Hi @thomwolf \r\n\r\nThanks for the details! I just created a PR with the updated `dataset_infos.json` file (#550).'
 'Thanks for updating the json file. Closing this one']","Hi,

There is a `NonMatchingChecksumError` error for the `lid_msaea` (language identification for Modern Standard Arabic - Egyptian Arabic) dataset from the LinCE benchmark due to a minor update on that dataset. 

How can I update the checksum of the library to solve this issue? The error is below and it also appears in the [nlp viewer](https://huggingface.co/nlp/viewer/?dataset=lince&config=lid_msaea):

```python
import nlp
nlp.load_dataset('lince', 'lid_msaea')
```

Output:
```
NonMatchingChecksumError: ['https://ritual.uh.edu/lince/libaccess/eyJ1c2VybmFtZSI6ICJodWdnaW5nZmFjZSBubHAiLCAidXNlcl9pZCI6IDExMSwgImVtYWlsIjogImR1bW15QGVtYWlsLmNvbSJ9/lid_msaea.zip']
Traceback:
File ""/home/sasha/streamlit/lib/streamlit/ScriptRunner.py"", line 322, in _run_script
    exec(code, module.__dict__)
File ""/home/sasha/nlp-viewer/run.py"", line 196, in <module>
    dts, fail = get(str(option.id), str(conf_option.name) if conf_option else None)
File ""/home/sasha/streamlit/lib/streamlit/caching.py"", line 591, in wrapped_func
    return get_or_create_cached_value()
File ""/home/sasha/streamlit/lib/streamlit/caching.py"", line 575, in get_or_create_cached_value
    return_value = func(*args, **kwargs)
File ""/home/sasha/nlp-viewer/run.py"", line 150, in get
    builder_instance.download_and_prepare()
File ""/home/sasha/.local/share/virtualenvs/lib-ogGKnCK_/lib/python3.7/site-packages/nlp/builder.py"", line 432, in download_and_prepare
    download_config.force_download = download_mode == FORCE_REDOWNLOAD
File ""/home/sasha/.local/share/virtualenvs/lib-ogGKnCK_/lib/python3.7/site-packages/nlp/builder.py"", line 469, in _download_and_prepare
File ""/home/sasha/.local/share/virtualenvs/lib-ogGKnCK_/lib/python3.7/site-packages/nlp/utils/info_utils.py"", line 36, in verify_checksums
    raise NonMatchingChecksumError(str(bad_urls))
```

Thank you in advance!

@lhoestq "
https://github.com/huggingface/datasets/issues/537,[Dataset] RACE dataset Checksums error,"['`NonMatchingChecksumError` means that the checksum of the downloaded file is not the expected one.\r\nEither the file you downloaded was corrupted along the way, or the host updated the file.\r\nCould you try to clear your cache and run `load_dataset` again ? If the error is still there, it means that there was an update in the data, and we may have to update the expected checksum value.'
 'I just cleared the cache an run it again. The error persists ):\r\n\r\n```\r\n nlp (master) $ rm -rf  /Users/abarbosa/.cache/huggingface/\r\n nlp (master) $ python\r\nPython 3.8.5 (default, Aug  5 2020, 03:39:04)\r\n[Clang 10.0.0 ] :: Anaconda, Inc. on darwin\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n>>> import nlp\r\n>>> dataset = nlp.load_dataset(""race"")\r\nDownloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4.39k/4.39k [00:00<00:00, 661kB/s]\r\nDownloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.81k/1.81k [00:00<00:00, 644kB/s]\r\nUsing custom data configuration default\r\nDownloading and preparing dataset race/default (download: 84.52 MiB, generated: 132.61 MiB, post-processed: Unknown size, total: 217.13 MiB) to /Users/abarbosa/.cache/huggingface/datasets/race/default/0.1.0/5461327f1a83549ca0d845a3159c806d2baf4f8d0d8f7d657157ce7cdf3899c2...\r\nDownloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25.4M/25.4M [01:03<00:00, 401kB/s]\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/Users/abarbosa/Documents/nlp/src/nlp/load.py"", line 550, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File ""/Users/abarbosa/Documents/nlp/src/nlp/builder.py"", line 471, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File ""/Users/abarbosa/Documents/nlp/src/nlp/builder.py"", line 530, in _download_and_prepare\r\n    verify_checksums(\r\n  File ""/Users/abarbosa/Documents/nlp/src/nlp/utils/info_utils.py"", line 38, in verify_checksums\r\n    raise NonMatchingChecksumError(error_msg + str(bad_urls))\r\nnlp.utils.info_utils.NonMatchingChecksumError: Checksums didn\'t match for dataset source files:\r\n[\'http://www.cs.cmu.edu/~glai1/data/race/RACE.tar.gz\']\r\n>>>\r\n```'
 'Dealing with the same issue please update the checksum on nlp library end. The data seems to have changed on their end.'
 'We have a discussion on this datasets here: https://github.com/huggingface/nlp/pull/540\r\n\r\nFeel free to participate if you have some opinion on the scope of data which should be included in this dataset.'
 ""At least for me, the file that was downloaded from CMU isn't the complete dataset, but a small subset of it (~25MB vs ~85MB). I've previously downloaded the dataset directly, so for my personal needs I could just swap out the corrupted file with the correct one. Perhaps you could host it like you do for the Wikipedia and BookCorpus datasets.\r\n\r\n""
 ""> At least for me, the file that was downloaded from CMU isn't the complete dataset, but a small subset of it (~25MB vs ~85MB). I've previously downloaded the dataset directly, so for my personal needs I could just swap out the corrupted file with the correct one. Perhaps you could host it like you do for the Wikipedia and BookCorpus datasets.\r\n\r\nCould you upload this please?""
 '> > At least for me, the file that was downloaded from CMU isn\'t the complete dataset, but a small subset of it (~25MB vs ~85MB). I\'ve previously downloaded the dataset directly, so for my personal needs I could just swap out the corrupted file with the correct one. Perhaps you could host it like you do for the Wikipedia and BookCorpus datasets.\r\n> \r\n> Could you upload this please?\r\n\r\nNot sure if I can upload it according to their license (""You agree not to reproduce, duplicate, copy, sell, trade, resell or exploit for any commercial purpose, any portion of the contexts and any portion of derived data."").'
 'I managed to fix it in #540 :)'
 'Closing since @540 is merged\r\n\r\nThanks again @abarbosa94 ']","Hi there, I just would like to use this awesome lib to perform a dataset fine-tuning on RACE dataset. I have performed the following steps:

```
dataset = nlp.load_dataset(""race"")
len(dataset[""train""]), len(dataset[""validation""])
```

But then I got the following error:

```
---------------------------------------------------------------------------
NonMatchingChecksumError                  Traceback (most recent call last)
<ipython-input-15-8bf7603ce0ed> in <module>
----> 1 dataset = nlp.load_dataset(""race"")
      2 len(dataset[""train""]), len(dataset[""validation""])

~/miniconda3/envs/masters/lib/python3.8/site-packages/nlp/load.py in load_dataset(path, name, version, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, save_infos, **config_kwargs)
    546 
    547     # Download and prepare data
--> 548     builder_instance.download_and_prepare(
    549         download_config=download_config, download_mode=download_mode, ignore_verifications=ignore_verifications,
    550     )

~/miniconda3/envs/masters/lib/python3.8/site-packages/nlp/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, **download_and_prepare_kwargs)
    460                         logger.info(""Dataset not on Hf google storage. Downloading and preparing it from source"")
    461                 if not downloaded_from_gcs:
--> 462                     self._download_and_prepare(
    463                         dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
    464                     )

~/miniconda3/envs/masters/lib/python3.8/site-packages/nlp/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)
    519         # Checksums verification
    520         if verify_infos:
--> 521             verify_checksums(
    522                 self.info.download_checksums, dl_manager.get_recorded_sizes_checksums(), ""dataset source files""
    523             )

~/miniconda3/envs/masters/lib/python3.8/site-packages/nlp/utils/info_utils.py in verify_checksums(expected_checksums, recorded_checksums, verification_name)
     36     if len(bad_urls) > 0:
     37         error_msg = ""Checksums didn't match"" + for_verification_name + "":\n""
---> 38         raise NonMatchingChecksumError(error_msg + str(bad_urls))
     39     logger.info(""All the checksums matched successfully"" + for_verification_name)
     40 

NonMatchingChecksumError: Checksums didn't match for dataset source files:
['http://www.cs.cmu.edu/~glai1/data/race/RACE.tar.gz']
```"
https://github.com/huggingface/datasets/issues/534,`list_datasets()` is broken.,"['Thanks for reporting !\r\nThis has been fixed in #475 and the fix will be available in the next release'
 'What you can do instead to get the list of the datasets is call\r\n\r\n```python\r\nprint([dataset.id for dataset in nlp.list_datasets()])\r\n```'
 'Thanks @lhoestq . ']","version = '0.4.0'

`list_datasets()` is broken. It results in the following error : 

```
In [3]: nlp.list_datasets()
Out[3]: ---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
~/.virtualenvs/san-lgUCsFg_/lib/python3.8/site-packages/IPython/core/formatters.py in __call__(self, obj)
    700                 type_pprinters=self.type_printers,
    701                 deferred_pprinters=self.deferred_printers)
--> 702             printer.pretty(obj)
    703             printer.flush()
    704             return stream.getvalue()

~/.virtualenvs/san-lgUCsFg_/lib/python3.8/site-packages/IPython/lib/pretty.py in pretty(self, obj)
    375                 if cls in self.type_pprinters:
    376                     # printer registered in self.type_pprinters
--> 377                     return self.type_pprinters[cls](obj, self, cycle)
    378                 else:
    379                     # deferred printer

~/.virtualenvs/san-lgUCsFg_/lib/python3.8/site-packages/IPython/lib/pretty.py in inner(obj, p, cycle)
    553                 p.text(',')
    554                 p.breakable()
--> 555             p.pretty(x)
    556         if len(obj) == 1 and type(obj) is tuple:
    557             # Special case for 1-item tuples.

~/.virtualenvs/san-lgUCsFg_/lib/python3.8/site-packages/IPython/lib/pretty.py in pretty(self, obj)
    392                         if cls is not object \
    393                                 and callable(cls.__dict__.get('__repr__')):
--> 394                             return _repr_pprint(obj, self, cycle)
    395
    396             return _default_pprint(obj, self, cycle)

~/.virtualenvs/san-lgUCsFg_/lib/python3.8/site-packages/IPython/lib/pretty.py in _repr_pprint(obj, p, cycle)
    698     """"""A pprint that just redirects to the normal repr function.""""""
    699     # Find newlines and replace them with p.break_()
--> 700     output = repr(obj)
    701     lines = output.splitlines()
    702     with p.group():

~/.virtualenvs/san-lgUCsFg_/lib/python3.8/site-packages/nlp/hf_api.py in __repr__(self)
    110
    111     def __repr__(self):
--> 112         single_line_description = self.description.replace(""\n"", """")
    113         return f""nlp.ObjectInfo(id='{self.id}', description='{single_line_description}', files={self.siblings})""
    114

AttributeError: 'NoneType' object has no attribute 'replace'
```"
https://github.com/huggingface/datasets/issues/532,File exists error when used with TPU,"['I am facing probably facing similar issues with \r\n\r\n`wiki40b_en_100_0`'
 'Could you try to run `dataset = load_dataset(""text"", data_files=file_path, split=""train"")` once before calling the script ?\r\n\r\nIt looks like several processes try to create the dataset in arrow format at the same time. If the dataset is already created it should be fine'
 'Thanks! I tested on 328MB text data on `n1-standard-8 (8 vCPUs, 30 GB memory)`. The main script ran without any issue, but it seems to require a huge space in the drive.\r\n\r\nAs suggested, I ran the following script before running the pre-training command with `xla_spawn.py`.\r\n\r\n```python\r\nfrom nlp import load_dataset\r\n\r\nfile_path=""your_file_name""\r\nload_dataset(""text"", data_files=file_path, split=""train"")\r\n```\r\nThis will create `text-train.arrow` under the default cache directory. Then, I run the script with `xla_spawn.py`. It will load data from the cached file. My understanding is that there\'s no other way but to do this two-step process with the current version (0.4) of `nlp`.\r\n\r\nDuring another caching process that happens in the main script:\r\n\r\n```\r\n08/26/2020 09:19:51 - INFO - nlp.utils.info_utils -   All the checksums matched successfully for post processing resources\r\n08/26/2020 09:19:53 - INFO - nlp.arrow_dataset -   Caching processed dataset at /home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d/cache-f90f341e5308a7469\r\n8d872bcc88f9c0e.arrow\r\n```\r\n\r\n`nlp` generates a temporary file per core, each of which is three times larger than the original text data. If each process is actually writing on the disk, you will need a huge amount of space in your drive. (Maybe I\'m missing something.)\r\n\r\n```\r\n-rw-r--r-- 1 ***** *****  674 Aug 26 09:19 dataset_info.json\r\n-rw-r--r-- 1 ***** *****    0 Aug 26 09:19 LICENSE\r\n-rw-r--r-- 1 ***** ***** 332M Aug 26 09:10 text-train.arrow\r\n-rw------- 1 ***** ***** 940M Aug 26 09:31 tmp0k43sazw\r\n-rw------- 1 ***** ***** 940M Aug 26 09:31 tmp7sxs9mj5\r\n-rw------- 1 ***** ***** 939M Aug 26 09:31 tmpbbiqw2vp\r\n-rw------- 1 ***** ***** 937M Aug 26 09:31 tmpjxb5ptyu\r\n-rw------- 1 ***** ***** 933M Aug 26 09:31 tmpk3hkdh0e\r\n-rw------- 1 ***** ***** 944M Aug 26 09:31 tmpnoalwftz\r\n-rw------- 1 ***** ***** 931M Aug 26 09:31 tmpuxdr_dz3\r\n-rw------- 1 ***** ***** 945M Aug 26 09:31 tmpxjyuy6dk\r\n```\r\nAfter the caching process, they seem to be merged into one file.\r\n\r\n```\r\n-rw------- 1  ***** ***** 989M Aug 26 09:32 cache-f90f341e5308a74698d872bcc88f9c0e.arrow\r\n-rw-r--r-- 1  ***** *****  674 Aug 26 09:19 dataset_info.json\r\n-rw-r--r-- 1  ***** *****    0 Aug 26 09:19 LICENSE\r\n-rw-r--r-- 1  ***** ***** 332M Aug 26 09:10 text-train.arrow\r\n```'
 ""Again it looks like every process tries to tokenize the full dataset at the same time.\r\nIf you do the tokenization before calling `xla_spawn.py` once, then each process will then use the tokenized cached file `cache-f90f341e5308a74698d872bcc88f9c0e.arrow` and not recompute it.\r\n\r\nNot sure if there's a better way to do that cc @julien-c @thomwolf ""
 ""I wrote a separate script just for preparing a cached file, including tokenization. Each process did use the tokenized cached file.\r\n\r\nCurrently I'm testing the pipeline on 24GB text data. It took about 1.5 hour to create a cached file on `n1-highmem-16 (16 vCPUs, 104 GB memory)`. I assume loading this cached file in the main script with `xla_spawn.py` won't be an issue (even if there are 8 processes).\r\n\r\n```\r\ntotal 98G\r\ndrwxr-xr-x 2 ***** ***** 4.0K Aug 26 13:38 .\r\ndrwxr-xr-x 3 ***** ***** 4.0K Aug 26 12:24 ..\r\n-rw------- 1 ***** *****  74G Aug 26 13:38 cache-a7aa04134ba7b1aff5d9710f14a4e334.arrow\r\n-rw-r--r-- 1 ***** *****  681 Aug 26 12:24 dataset_info.json\r\n-rw-r--r-- 1 ***** *****    0 Aug 26 12:24 LICENSE\r\n-rw-r--r-- 1 ***** *****  25G Aug 26 12:24 text-train.arrow\r\n```""
 'Yes loading the cached file should be fine from different processes'
 ""Sorry, I thought it was working, but actually the second call doesn't use the cached file that was generated separately, and it will generate another cache-****.arrorw file with a different name. If I run the training script again (with `xla_spawn.py`), it will use the second cached file, which was generated by the training script itself in the previous run.\r\n\r\n```\r\ndrwxr-xr-x 2 ***** ***** 4.0K Aug 26 15:35 .\r\ndrwxr-xr-x 3 ***** ***** 4.0K Aug 26 15:29 ..\r\n-rw------- 1 ***** *****  99M Aug 26 15:35 cache-0d77dfce704493dbe63f071eed6a5431.arrow\r\n-rw------- 1 ***** *****  99M Aug 26 15:29 cache-69633651476e943b93c89ace715f9487.arrow\r\n-rw-r--r-- 1 ***** *****  670 Aug 26 15:33 dataset_info.json\r\n-rw-r--r-- 1 ***** *****    0 Aug 26 15:33 LICENSE\r\n-rw-r--r-- 1 ***** *****  33M Aug 26 15:29 text-train.arrow\r\n```""
 'So if I understand correctly it means that the cached file generated by your separated script is different by the one used by the training script ?'
 'Yes.\r\n\r\n1. `cache-69633651476e943b93c89ace715f9487.arrow` was generated with a separate script. \r\n2. I ran the entire script with `xla_spawn.py`.\r\n3. `cache-69633651476e943b93c89ace715f9487.arrow` is not used.\r\n4. `cache-0d77dfce704493dbe63f071eed6a5431.arrow` is created.\r\n5. training starts...\r\n\r\nNow, if I kill the process at step 5, and do the step 2 again, it will use `cache-0d77dfce704493dbe63f071eed6a5431.arrow` (cached file created at step 4) without any issue.\r\n\r\nI used the following to generate the first cached file.\r\n```python\r\ndataset = load_dataset(""text"", data_files=file_path, split=""train"")\r\ndataset = dataset.map(lambda ex: tokenizer(ex[""text""], add_special_tokens=True,\r\n                                        truncation=True, max_length=args.block_size), batched=True)\r\ndataset.set_format(type=\'torch\', columns=[\'input_ids\'])\r\n```'
 ""1.  Here's the log from the first step.\r\n```\r\nDownloading and preparing dataset text/default-e84dd29acc4ad9ef (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/*****/.cache/huggingface/datasets/text/default-e84dd29acc4ad9ef/0.0.0/\r\n447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d...\r\nDataset text downloaded and prepared to /home/*****/.cache/huggingface/datasets/text/default-e84dd29acc4ad9ef/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d. Subsequent calls will reuse this data.\r\n```\r\nThere's a file named `cache-7b1440ba7077af0f0d9035b5a55d01fc.arrow`, so it did create a cached file.\r\n```\r\ndrwxr-xr-x 2 ***** ***** 4.0K Aug 26 15:59 .\r\ndrwxr-xr-x 3 ***** ***** 4.0K Aug 26 15:58 ..\r\n-rw------- 1 ***** *****  99M Aug 26 15:59 cache-7b1440ba7077af0f0d9035b5a55d01fc.arrow\r\n-rw-r--r-- 1 ***** *****  670 Aug 26 15:58 dataset_info.json\r\n-rw-r--r-- 1 ***** *****    0 Aug 26 15:58 LICENSE\r\n-rw-r--r-- 1 ***** *****  33M Aug 26 15:58 text-train.arrow\r\n```\r\n2. Ideally, `cache-7b1440ba7077af0f0d9035b5a55d01fc.arrow` should be used in `run_language_modeling.py` (modified version using `nlp`) with `xla_spawn.py`. But it looks like it's creating a new cached file.\r\n\r\n```\r\n08/26/2020 16:13:03 - INFO - filelock -   Lock 139635836351096 released on /home/*****/.cache/huggingface/datasets/3e34209a2741375a1db1ff03bf1abba1a9bd0e6016912d3ead0114b9d1ca2685.202fa4f84f552bff1f5400ae012663839c61efb3de068c6c8722d34ac0ea6192\r\n.py.lock\r\n08/26/2020 16:13:03 - WARNING - nlp.builder -   Using custom data configuration default\r\n08/26/2020 16:13:03 - INFO - nlp.builder -   Overwrite dataset info from restored data version.\r\n08/26/2020 16:13:03 - INFO - nlp.info -   Loading Dataset info from /home/*****/.cache/huggingface/datasets/text/default-e84dd29acc4ad9ef/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d\r\n08/26/2020 16:13:03 - INFO - nlp.builder -   Reusing dataset text (/home/*****/.cache/huggingface/datasets/text/default-e84dd29acc4ad9ef/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d)\r\n08/26/2020 16:13:03 - INFO - nlp.builder -   Constructing Dataset for split train, from /home/*****/.cache/huggingface/datasets/text/default-e84dd29acc4ad9ef/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d\r\n08/26/2020 16:13:03 - INFO - nlp.utils.info_utils -   All the checksums matched successfully for post processing resources\r\n08/26/2020 16:13:03 - INFO - nlp.builder -   Overwrite dataset info from restored data version.\r\n08/26/2020 16:13:03 - INFO - nlp.info -   Loading Dataset info from /home/*****/.cache/huggingface/datasets/text/default-e84dd29acc4ad9ef/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d\r\n08/26/2020 16:13:03 - INFO - nlp.builder -   Reusing dataset text (/home/*****/.cache/huggingface/datasets/text/default-e84dd29acc4ad9ef/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d)\r\n08/26/2020 16:13:03 - INFO - nlp.builder -   Constructing Dataset for split train, from /home/*****/.cache/huggingface/datasets/text/default-e84dd29acc4ad9ef/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d\r\n08/26/2020 16:13:03 - INFO - nlp.utils.info_utils -   All the checksums matched successfully for post processing resources\r\n08/26/2020 16:13:05 - INFO - nlp.arrow_dataset -   Caching processed dataset at /home/*****/.cache/huggingface/datasets/text/default-e84dd29acc4ad9ef/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d/cache-0d77dfce704493dbe\r\n63f071eed6a5431.arrow\r\n^M  0%|          | 0/100 [00:00<?, ?it/s]08/26/2020 16:13:05 - INFO - nlp.arrow_dataset -   Caching processed dataset at /home/*****/.cache/huggingface/datasets/text/default-e84dd29acc4ad9ef/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6\r\nfe661fe4d070d380d/cache-0d77dfce704493dbe63f071eed6a5431.arrow\r\n```\r\n\r\nThere are two cached files in the directory:\r\n\r\n```\r\ndrwxr-xr-x 2 ***** ***** 4.0K Aug 26 16:14 .\r\ndrwxr-xr-x 3 ***** ***** 4.0K Aug 26 15:58 ..\r\n-rw------- 1 ***** *****  99M Aug 26 16:14 cache-0d77dfce704493dbe63f071eed6a5431.arrow\r\n-rw------- 1 ***** *****  99M Aug 26 15:59 cache-7b1440ba7077af0f0d9035b5a55d01fc.arrow\r\n-rw-r--r-- 1 ***** *****  670 Aug 26 16:13 dataset_info.json\r\n-rw-r--r-- 1 ***** *****    0 Aug 26 16:13 LICENSE\r\n-rw-r--r-- 1 ***** *****  33M Aug 26 15:58 text-train.arrow\r\n```\r\n\r\nIf I kill the process, and run it again, it will use the second cached file.\r\n\r\n```\r\n08/26/2020 16:19:52 - WARNING - nlp.builder -   Using custom data configuration default\r\n08/26/2020 16:19:52 - INFO - nlp.builder -   Overwrite dataset info from restored data version.\r\n08/26/2020 16:19:52 - INFO - nlp.info -   Loading Dataset info from /home/*****/.cache/huggingface/datasets/text/default-e84dd29acc4ad9ef/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d\r\n08/26/2020 16:19:52 - INFO - nlp.builder -   Reusing dataset text (/home/*****/.cache/huggingface/datasets/text/default-e84dd29acc4ad9ef/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d)\r\n08/26/2020 16:19:52 - INFO - nlp.builder -   Constructing Dataset for split train, from /home/*****/.cache/huggingface/datasets/text/default-e84dd29acc4ad9ef/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d\r\n08/26/2020 16:19:52 - INFO - nlp.utils.info_utils -   All the checksums matched successfully for post processing resources\r\n08/26/2020 16:19:53 - INFO - nlp.arrow_dataset -   Loading cached processed dataset at /home/*****/.cache/huggingface/datasets/text/default-e84dd29acc4ad9ef/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d/cache-0d77dfce70\r\n4493dbe63f071eed6a5431.arrow\r\n08/26/2020 16:19:53 - INFO - nlp.arrow_dataset -   Set __getitem__(key) output type to torch for ['input_ids'] columns  (when key is int or slice) and don't output other (un-formatted) columns.\r\n```""
 'Thanks for all the details.\r\nThe two cached files are supposed to be the same. I suspect that the caching has a problem with the tokenizer.\r\nWhich tokenizer did you use ?'
 'I trained a byte-level BPE tokenizer on my data with `tokenziers` library following this [example](https://github.com/huggingface/tokenizers/blob/master/bindings/python/examples/train_bytelevel_bpe.py).\r\n\r\nAnd I put these model files in a directory named `""model_name""`. I also put config.json, which is the original RoBERTa config file.\r\n\r\n```bash\r\n%ls  model_name\r\nconfig.json     merges.txt      vocab.json\r\n```\r\n\r\n[This](https://github.com/huggingface/transformers/blob/4bd7be9a4268221d2a0000c7e8033aaeb365c03b/examples/language-modeling/run_language_modeling.py#L196) is the line where `run_language_modeling.py` loads the tokenier.\r\n\r\n```python\r\ntokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, cache_dir=model_args.cache_dir)\r\n```\r\n\r\nI use `""model_name""` for `model_args.tokenizer_name`. I don\'t specify `model_args.cache_dir`. It is \'None\' by default.'
 ""In my separated script for caching, I'm using `use_fast=True` when initializing a tokenizer.\r\n\r\n```python\r\ntokenizer = AutoTokenizer.from_pretrained(args.config_name, use_fast=True)\r\n```\r\nI wasn't using that option in the main script. That could be the reason...""
 'Yea it could definitely explain why you have two different cache files.\r\nLet me know if using the same tokenizers on both sides fixes the issue'
 'It still creates a new file even if I remove `use_fast=True`... \r\n\r\nHere\'s the script used to create a cached file.\r\n```python \r\n#!/usr/bin/env python3\r\n\r\nimport argparse\r\n\r\nfrom transformers import AutoTokenizer\r\n\r\nfrom nlp import load_dataset\r\n\r\n\r\ndef main():\r\n    parser = argparse.ArgumentParser(description=\'description\')\r\n    parser.add_argument(\'--config_name\', type=str, help=\'Pretrained config name or path if not the same as model_name\')\r\n    parser.add_argument(\'--data_file\', type=str, help=\'The input data file (a text file).\')\r\n    parser.add_argument(\'--block_size\', type=int, default=-1, help=\'The training dataset will be truncated in block of this size for training\')\r\n    args = parser.parse_args()\r\n\r\n    tokenizer = AutoTokenizer.from_pretrained(args.config_name)\r\n\r\n    dataset = load_dataset(""text"", data_files=args.data_file, split=""train"")\r\n    dataset = dataset.map(lambda ex: tokenizer(ex[""text""], add_special_tokens=True,\r\n                                truncation=True, max_length=args.block_size), batched=True)\r\n    dataset.set_format(type=\'torch\', columns=[\'input_ids\'])\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    main()\r\n```\r\n\r\nHere\'s how the data is loaded in the modified `run_language_modeling.py`. [[original function](https://github.com/huggingface/transformers/blob/971d1802d009d9996b36a34a34477cee849ef39f/examples/language-modeling/run_language_modeling.py#L128-L135)]\r\n\r\n```python\r\ndef get_dataset(args: DataTrainingArguments, tokenizer: PreTrainedTokenizer, evaluate=False):\r\n    file_path = args.eval_data_file if evaluate else args.train_data_file\r\n    split = ""validation"" if evaluate else ""train""\r\n    if args.line_by_line:\r\n        # return LineByLineTextDataset(tokenizer=tokenizer, file_path=file_path, block_size=args.block_size)\r\n        dataset = load_dataset(""text"", data_files=file_path, split=""train"")\r\n        dataset = dataset.map(lambda ex: tokenizer(ex[""text""], add_special_tokens=True,\r\n                              truncation=True, max_length=args.block_size), batched=True)\r\n        dataset.set_format(type=\'torch\', columns=[\'input_ids\'])\r\n        return dataset\r\n\r\n    else:\r\n        return TextDataset(\r\n            tokenizer=tokenizer, file_path=file_path, block_size=args.block_size, overwrite_cache=args.overwrite_cache\r\n        )\r\n```\r\n\r\nProbably I don\'t need this part in the main script,\r\n\r\n```python\r\ndataset = dataset.map(lambda ex: tokenizer(ex[""text""], add_special_tokens=True,\r\n                              truncation=True, max_length=args.block_size), batched=True)\r\n        dataset.set_format(type=\'torch\', columns=[\'input_ids\'])\r\n```\r\nand simply do this?\r\n```python\r\ndataset = load_dataset(""text"", data_files=file_path, split=""train"")\r\nreturn dataset\r\n```'
 'You need this part in the main script or it will use the dataset that is not tokenized\r\n\r\n'
 'I can see that the tokenizer in `run_language_modeling.py` is not instantiated the same way as in your separated script.\r\nIndeed we can see L196:\r\n```python\r\ntokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, cache_dir=model_args.cache_dir)\r\n```\r\nCould you try to make it so they are instantiated the exact same way please ?'
 'I updated my separated script, but it\'s creating a cached file again. If I don\'t use the `model_args.cache_dir`, both will get `None`, so they should be the same.\r\n\r\n```python\r\n#!/usr/bin/env python3\r\nimport argparse\r\n\r\nfrom transformers import AutoTokenizer\r\nfrom nlp import load_dataset\r\n\r\ndef main():\r\n    parser = argparse.ArgumentParser(description=\'description\')\r\n    parser.add_argument(\'--tokenizer_name\', type=str, help=\'Pretrained tokenizer name or path if not the same as model_name\')\r\n    parser.add_argument(\'--data_file\', type=str, help=\'The input data file (a text file).\')\r\n    parser.add_argument(\'--cache_dir\', type=str, default=None, help=\'Where do you want to store the pretrained models downloaded from s3\')\r\n    parser.add_argument(\'--block_size\', type=int, default=-1, help=\'The training dataset will be truncated in block of this size for training\')\r\n\r\n    model_args = parser.parse_args()\r\n\r\n    tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, cache_dir=model_args.cache_dir)\r\n\r\n    dataset = load_dataset(""text"", data_files=model_args.data_file, split=""train"")\r\n    dataset = dataset.map(lambda ex: tokenizer(ex[""text""], add_special_tokens=True,\r\n                                truncation=True, max_length=model_args.block_size), batched=True)\r\n    dataset.set_format(type=\'torch\', columns=[\'input_ids\'])\r\n\r\nif __name__ == ""__main__"":\r\n    main()\r\n```\r\n\r\nIs there a way to specify the cache file to load, and skip the re-computation?'
 'Could you also check that the `args.block_size` used in the lambda function is the same as well ?'
 'Here\'s a minimal working example to reproduce this issue.\r\n\r\nAssumption:\r\n- You have access to TPU.\r\n- You have installed `transformers` and `nlp`.\r\n- You have tokenizer files (`config.json`, `merges.txt`, `vocab.json`) under the directory named `model_name`.\r\n- You have `xla_spawn.py` (Download from https://github.com/huggingface/transformers/blob/master/examples/xla_spawn.py).\r\n- You have saved the following script as `prepare_cached_dataset.py`.\r\n\r\n```python\r\n#!/usr/bin/env python3\r\nimport argparse\r\nfrom transformers import AutoTokenizer\r\nfrom nlp import load_dataset\r\n\r\ndef main():\r\n    parser = argparse.ArgumentParser(description=\'description\')\r\n    parser.add_argument(\'--tokenizer_name\', type=str, help=\'Pretrained tokenizer name or path if not the same as model_name\')\r\n    parser.add_argument(\'--data_file\', type=str, help=\'The input data file (a text file).\')\r\n    parser.add_argument(\'--cache_dir\', type=str, default=None, help=\'Where do you want to store the pretrained models downloaded from s3\')\r\n    parser.add_argument(\'--block_size\', type=int, default=-1, help=\'The training dataset will be truncated in block of this size for training\')\r\n    parser.add_argument(\'--tpu_num_cores\', type=int, default=1, help=\'Number of TPU cores to use (1 or 8). For xla_apwan.py\')\r\n    model_args = parser.parse_args()\r\n    \r\n    tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, cache_dir=model_args.cache_dir, use_fast=True)\r\n    \r\n    dataset = load_dataset(""text"", data_files=model_args.data_file, split=""train"")\r\n    dataset = dataset.map(lambda ex: tokenizer(ex[""text""], add_special_tokens=True,\r\n                                truncation=True, max_length=model_args.block_size), batched=True)\r\n    dataset.set_format(type=\'torch\', columns=[\'input_ids\'])\r\n\r\ndef _mp_fn(index):\r\n    # For xla_spawn (TPUs)\r\n    main()\r\n\r\nif __name__ == ""__main__"":\r\n    main()\r\n```\r\n\r\n- Run the following command. Replace `your_training_data` with some text file.\r\n\r\n```bash\r\nexport TRAIN_DATA=your_training_data\r\n\r\npython prepare_cached_dataset.py \\\r\n--tokenizer_name=model_name \\\r\n--block_size=512 \\\r\n--data_file=$TRAIN_DATA\r\n```\r\n- Check the cached directory.\r\n```bash\r\nls -lha /home/*****/.cache/huggingface/datasets/text/default-e84dd29acc4ad9ef/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d\r\ntotal 132M\r\ndrwxr-xr-x 2 ***** ***** 4.0K Aug 28 13:08 .\r\ndrwxr-xr-x 3 ***** ***** 4.0K Aug 28 13:08 ..\r\n-rw------- 1 ***** *****  99M Aug 28 13:08 cache-bfc7cb0702426d19242db5e8c079f04b.arrow\r\n-rw-r--r-- 1 ***** *****  670 Aug 28 13:08 dataset_info.json\r\n-rw-r--r-- 1 ***** *****    0 Aug 28 13:08 LICENSE\r\n-rw-r--r-- 1 ***** *****  33M Aug 28 13:08 text-train.arrow\r\n```\r\n\r\n- Run the same script again. (The output should be just `Using custom data configuration default`.)\r\n```\r\npython prepare_cached_dataset.py \\\r\n--tokenizer_name=model_name \\\r\n--block_size=512 \\\r\n--data_file=$TRAIN_DATA\r\n```\r\n- Check the cached directory.\r\n```bash\r\nls -lha /home/*****/.cache/huggingface/datasets/text/default-e84dd29acc4ad9ef/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d\r\ntotal 132M\r\ndrwxr-xr-x 2 ***** ***** 4.0K Aug 28 13:08 .\r\ndrwxr-xr-x 3 ***** ***** 4.0K Aug 28 13:08 ..\r\n-rw------- 1 ***** *****  99M Aug 28 13:08 cache-bfc7cb0702426d19242db5e8c079f04b.arrow\r\n-rw-r--r-- 1 ***** *****  670 Aug 28 13:20 dataset_info.json\r\n-rw-r--r-- 1 ***** *****    0 Aug 28 13:20 LICENSE\r\n-rw-r--r-- 1 ***** *****  33M Aug 28 13:08 text-train.arrow\r\n```\r\n- The cached file (`cache-bfc7cb0702426d19242db5e8c079f04b.arrow`) is reused.\r\n- Now, run this script with `xla_spawn.py`. Ideally, it should reuse the cached file, however, you will see each process is creating a cache file again.\r\n\r\n```bash\r\npython xla_spawn.py --num_cores 8 \\\r\nprepare_cached_dataset.py \\\r\n--tokenizer_name=model_name \\\r\n--block_size=512 \\\r\n--data_file=$TRAIN_DATA\r\n```\r\n\r\n- Check the cached directory. There are two arrrow files.\r\n```bash\r\nls -lha /home/*****/.cache/huggingface/datasets/text/default-e84dd29acc4ad9ef/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d\r\ntotal 230M\r\ndrwxr-xr-x 2 ***** ***** 4.0K Aug 28 13:25 .\r\ndrwxr-xr-x 3 ***** ***** 4.0K Aug 28 13:08 ..\r\n-rw------- 1 ***** *****  99M Aug 28 13:08 cache-bfc7cb0702426d19242db5e8c079f04b.arrow\r\n-rw------- 1 ***** *****  99M Aug 28 13:25 cache-e0e2313e49c8a110aafcc8133154c19a.arrow\r\n-rw-r--r-- 1 ***** *****  670 Aug 28 13:24 dataset_info.json\r\n-rw-r--r-- 1 ***** *****    0 Aug 28 13:24 LICENSE\r\n-rw-r--r-- 1 ***** *****  33M Aug 28 13:08 text-train.arrow\r\n```\r\n'
 'I ended up specifying the `cache_file_name` argument when I call `map` function.\r\n\r\n```python\r\ndataset = dataset.map(lambda ex: tokenizer(ex[""text""], add_special_tokens=True, truncation=True, max_length=args.block_size),\r\n                      batched=True,\r\n                      cache_file_name=cache_file_name)\r\n```\r\n\r\nNote:\r\n- `text` dataset in `nlp` does not strip `""\\n""`.  If you want the same output as in [`LineByLineTextDataset`](https://github.com/huggingface/transformers/blob/afc4ece462ad83a090af620ff4da099a0272e171/src/transformers/data/datasets/language_modeling.py#L88-L111), you would need to create your own dataset class where you replace `line` to `line.strip()` [here](https://github.com/huggingface/nlp/blob/master/datasets/text/text.py#L35).\r\n']","Hi,

I'm getting a ""File exists"" error when I use [text dataset](https://github.com/huggingface/nlp/tree/master/datasets/text) for pre-training a RoBERTa model using `transformers` (3.0.2) and `nlp`(0.4.0) on a VM with TPU (v3-8).

I modified [line 131 in the original `run_language_modeling.py`](https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_language_modeling.py#L131) as follows:

```python
# line 131: return LineByLineTextDataset(tokenizer=tokenizer, file_path=file_path, block_size=args.block_size)
dataset = load_dataset(""text"", data_files=file_path, split=""train"")
dataset = dataset.map(lambda ex: tokenizer(ex[""text""], add_special_tokens=True,
                                        truncation=True, max_length=args.block_size), batched=True)
dataset.set_format(type='torch', columns=['input_ids'])
return dataset
```

When I run this with [`xla_spawn.py`](https://github.com/huggingface/transformers/blob/master/examples/xla_spawn.py), I get the following error (it produces one message per core in TPU, which I believe is fine).

It seems the current version doesn't take into account distributed training processes as in [this example](https://github.com/huggingface/transformers/blob/a573777901e662ec2e565be312ffaeedef6effec/src/transformers/data/datasets/language_modeling.py#L35-L38)?

```
08/25/2020 13:59:41 - WARNING - nlp.builder -   Using custom data configuration default
08/25/2020 13:59:43 - INFO - nlp.builder -   Generating dataset text (/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d)
08/25/2020 13:59:43 - INFO - nlp.builder -   Generating dataset text (/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d)
08/25/2020 13:59:43 - INFO - nlp.builder -   Generating dataset text (/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d)
08/25/2020 13:59:43 - INFO - nlp.builder -   Generating dataset text (/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d)
08/25/2020 13:59:43 - INFO - nlp.builder -   Generating dataset text (/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d)
08/25/2020 13:59:43 - INFO - nlp.builder -   Generating dataset text (/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d)
08/25/2020 13:59:43 - INFO - nlp.builder -   Generating dataset text (/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d)
08/25/2020 13:59:43 - INFO - nlp.builder -   Generating dataset text (/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d)
Downloading and preparing dataset text/default-b0932b2bdbb63283 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/
447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d...
Downloading and preparing dataset text/default-b0932b2bdbb63283 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/
447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d...
Downloading and preparing dataset text/default-b0932b2bdbb63283 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/
447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d...
Downloading and preparing dataset text/default-b0932b2bdbb63283 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/
447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d...
Downloading and preparing dataset text/default-b0932b2bdbb63283 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/
447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d...
Downloading and preparing dataset text/default-b0932b2bdbb63283 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/
447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d...
Exception in device=TPU:6: [Errno 17] File exists: '/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d.incomplete'
Exception in device=TPU:4: [Errno 17] File exists: '/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d.incomplete'
Exception in device=TPU:1: [Errno 17] File exists: '/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d.incomplete'
Downloading and preparing dataset text/default-b0932b2bdbb63283 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/
447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d...
Exception in device=TPU:7: [Errno 17] File exists: '/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d.incomplete'
Exception in device=TPU:3: [Errno 17] File exists: '/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d.incomplete'
Downloading and preparing dataset text/default-b0932b2bdbb63283 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/
447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d...
Exception in device=TPU:2: [Errno 17] File exists: '/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d.incomplete'
Exception in device=TPU:0: [Errno 17] File exists: '/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d.incomplete'
Traceback (most recent call last):
  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/torch_xla/distributed/xla_multiprocessing.py"", line 231, in _start_fn
    fn(gindex, *args)
  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/torch_xla/distributed/xla_multiprocessing.py"", line 231, in _start_fn
    fn(gindex, *args)
  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/torch_xla/distributed/xla_multiprocessing.py"", line 231, in _start_fn
    fn(gindex, *args)
  File ""/home/*****/huggingface_roberta/run_language_modeling.py"", line 300, in _mp_fn
    main()
  File ""/home/*****/huggingface_roberta/run_language_modeling.py"", line 300, in _mp_fn
    main()
  File ""/home/*****/huggingface_roberta/run_language_modeling.py"", line 300, in _mp_fn
    main()
  File ""/home/*****/huggingface_roberta/run_language_modeling.py"", line 240, in main
    train_dataset = get_dataset(data_args, tokenizer=tokenizer) if training_args.do_train else None
  File ""/home/*****/huggingface_roberta/run_language_modeling.py"", line 240, in main
    train_dataset = get_dataset(data_args, tokenizer=tokenizer) if training_args.do_train else None
  File ""/home/*****/huggingface_roberta/run_language_modeling.py"", line 240, in main
    train_dataset = get_dataset(data_args, tokenizer=tokenizer) if training_args.do_train else None
  File ""/home/*****/huggingface_roberta/run_language_modeling.py"", line 134, in get_dataset
    dataset = load_dataset(""text"", data_files=file_path, split=""train"")
  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/nlp/load.py"", line 546, in load_dataset
    download_config=download_config, download_mode=download_mode, ignore_verifications=ignore_verifications,
  File ""/home/*****/huggingface_roberta/run_language_modeling.py"", line 134, in get_dataset
    dataset = load_dataset(""text"", data_files=file_path, split=""train"")
  File ""/home/*****/huggingface_roberta/run_language_modeling.py"", line 134, in get_dataset
    dataset = load_dataset(""text"", data_files=file_path, split=""train"")
  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/nlp/builder.py"", line 450, in download_and_prepare
    with incomplete_dir(self._cache_dir) as tmp_data_dir:
Traceback (most recent call last):
  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/nlp/load.py"", line 546, in load_dataset
    download_config=download_config, download_mode=download_mode, ignore_verifications=ignore_verifications,
  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/contextlib.py"", line 81, in __enter__
    return next(self.gen)
  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/nlp/load.py"", line 546, in load_dataset
    download_config=download_config, download_mode=download_mode, ignore_verifications=ignore_verifications,
  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/torch_xla/distributed/xla_multiprocessing.py"", line 231, in _start_fn
    fn(gindex, *args)
  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/nlp/builder.py"", line 450, in download_and_prepare
    with incomplete_dir(self._cache_dir) as tmp_data_dir:
  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/nlp/builder.py"", line 422, in incomplete_dir
    os.makedirs(tmp_dir)
  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/nlp/builder.py"", line 450, in download_and_prepare
    with incomplete_dir(self._cache_dir) as tmp_data_dir:
  File ""/home/*****/huggingface_roberta/run_language_modeling.py"", line 300, in _mp_fn
      main()
  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/contextlib.py"", line 81, in __enter__
    return next(self.gen)
  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/os.py"", line 220, in makedirs
    mkdir(name, mode)
  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/contextlib.py"", line 81, in __enter__
    return next(self.gen)
  File ""/home/*****/huggingface_roberta/run_language_modeling.py"", line 240, in main
    train_dataset = get_dataset(data_args, tokenizer=tokenizer) if training_args.do_train else None
  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/nlp/builder.py"", line 422, in incomplete_dir
    os.makedirs(tmp_dir)
  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/torch_xla/distributed/xla_multiprocessing.py"", line 231, in _start_fn
    fn(gindex, *args)
  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/nlp/builder.py"", line 422, in incomplete_dir
    os.makedirs(tmp_dir)
  File ""/home/*****/huggingface_roberta/run_language_modeling.py"", line 134, in get_dataset
    dataset = load_dataset(""text"", data_files=file_path, split=""train"")
  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/os.py"", line 220, in makedirs
    mkdir(name, mode)
  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/nlp/load.py"", line 546, in load_dataset
    download_config=download_config, download_mode=download_mode, ignore_verifications=ignore_verifications,
FileExistsError: [Errno 17] File exists: '/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d.incomplete'
  File ""/home/*****/huggingface_roberta/run_language_modeling.py"", line 300, in _mp_fn
    main()
  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/nlp/builder.py"", line 450, in download_and_prepare
    with incomplete_dir(self._cache_dir) as tmp_data_dir:
  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/os.py"", line 220, in makedirs
    mkdir(name, mode)
FileExistsError: [Errno 17] File exists: '/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d.incomplete'
  File ""/home/*****/huggingface_roberta/run_language_modeling.py"", line 240, in main
    train_dataset = get_dataset(data_args, tokenizer=tokenizer) if training_args.do_train else None
  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/contextlib.py"", line 81, in __enter__
    return next(self.gen)
FileExistsError: [Errno 17] File exists: '/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d.incomplete'
  File ""/home/*****/huggingface_roberta/run_language_modeling.py"", line 134, in get_dataset
    dataset = load_dataset(""text"", data_files=file_path, split=""train"")
  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/nlp/builder.py"", line 422, in incomplete_dir
    os.makedirs(tmp_dir)
  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/nlp/load.py"", line 546, in load_dataset
    download_config=download_config, download_mode=download_mode, ignore_verifications=ignore_verifications,
  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/os.py"", line 220, in makedirs
    mkdir(name, mode)
      File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/nlp/builder.py"", line 450, in download_and_prepare
    with incomplete_dir(self._cache_dir) as tmp_data_dir:
  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/contextlib.py"", line 81, in __enter__
    return next(self.gen)
FileExistsError: [Errno 17] File exists: '/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d.incomplete'
  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/nlp/builder.py"", line 422, in incomplete_dir
    os.makedirs(tmp_dir)
  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/os.py"", line 220, in makedirs
    mkdir(name, mode)
Traceback (most recent call last):
FileExistsError: [Errno 17] File exists: '/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d.incomplete'
Traceback (most recent call last):
  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/torch_xla/distributed/xla_multiprocessing.py"", line 231, in _start_fn
    fn(gindex, *args)
  File ""/home/*****/huggingface_roberta/run_language_modeling.py"", line 300, in _mp_fn
    main()
  File ""/home/*****/huggingface_roberta/run_language_modeling.py"", line 240, in main
    train_dataset = get_dataset(data_args, tokenizer=tokenizer) if training_args.do_train else None
  File ""/home/*****/huggingface_roberta/run_language_modeling.py"", line 134, in get_dataset
    dataset = load_dataset(""text"", data_files=file_path, split=""train"")
  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/torch_xla/distributed/xla_multiprocessing.py"", line 231, in _start_fn
    fn(gindex, *args)
  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/nlp/load.py"", line 546, in load_dataset
    download_config=download_config, download_mode=download_mode, ignore_verifications=ignore_verifications,
  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/nlp/builder.py"", line 450, in download_and_prepare
    with incomplete_dir(self._cache_dir) as tmp_data_dir:
  File ""/home/*****/huggingface_roberta/run_language_modeling.py"", line 300, in _mp_fn
    main()
  File ""/home/*****/huggingface_roberta/run_language_modeling.py"", line 240, in main
    train_dataset = get_dataset(data_args, tokenizer=tokenizer) if training_args.do_train else None
  File ""/home/*****/huggingface_roberta/run_language_modeling.py"", line 134, in get_dataset
    dataset = load_dataset(""text"", data_files=file_path, split=""train"")
  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/nlp/load.py"", line 546, in load_dataset
    download_config=download_config, download_mode=download_mode, ignore_verifications=ignore_verifications,
  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/nlp/builder.py"", line 450, in download_and_prepare
    with incomplete_dir(self._cache_dir) as tmp_data_dir:
  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/contextlib.py"", line 81, in __enter__
    return next(self.gen)
  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/site-packages/nlp/builder.py"", line 422, in incomplete_dir
    os.makedirs(tmp_dir)
  File ""/anaconda3/envs/torch-xla-1.6/lib/python3.6/os.py"", line 220, in makedirs
    mkdir(name, mode)
FileExistsError: [Errno 17] File exists: '/home/*****/.cache/huggingface/datasets/text/default-b0932b2bdbb63283/0.0.0/447f2bcfa2a721a37bc8fdf23800eade1523cf07f7eada6fe661fe4d070d380d.incomplete'
```

"
https://github.com/huggingface/datasets/issues/525,wmt download speed example,"[""Thanks for creating the issue :)\r\nThe download link for wmt-en-de raw looks like a mirror. We should use that instead of the current url.\r\nIs this mirror official ?\r\n\r\nAlso it looks like for `ro-en` it tried to download other languages. If we manage to only download the one that is asked it'd be cool\r\n\r\nAlso cc @patrickvonplaten ""
 'Mirror is not official.'
 'Shall we host the files ourselves or it is fine to use this mirror in your opinion ?'
 'Should we add an argument in `load_dataset` to override some URL with a custom URL (e.g. mirror) or a local path?\r\n\r\nThis could also be used to provide local files instead of the original files as requested by some users (e.g. when you made a dataset with the same format than SQuAD and what to use it instead of the official dataset files).'
 ""@lhoestq I think we should host it ourselves. I'll put the subset of wmt (without preprocessed files) that we need on s3 and post a link over the weekend.""
 'Is there a solution yet? The download speed is still too slow. 60-70kbps download for wmt16 and around 100kbps for wmt19. @sshleifer '
 ""I'm working on mirror links which will provide high download speed :)\r\nSee https://github.com/huggingface/datasets/issues/1892""]","Continuing from the slack 1.0 roadmap thread w @lhoestq , I realized the slow downloads is only a thing sometimes. Here are a few examples, I suspect there are multiple issues. All commands were run from the same gcp us-central-1f machine.

```
import nlp
nlp.load_dataset('wmt16', 'de-en')
```
Downloads at 49.1 KB/S

Whereas 
```
pip install gdown # download from google drive
!gdown https://drive.google.com/uc?id=1iO7um-HWoNoRKDtw27YUSgyeubn9uXqj
```
Downloads at 127 MB/s. (The file is a copy of wmt-en-de raw).


```
nlp.load_dataset('wmt16', 'ro-en')
```
goes at 27 MB/s, much faster. 

if we wget the same data from s3 is the same download speed, but ¼ the file size:
```
wget https://s3.amazonaws.com/datasets.huggingface.co/translation/wmt_en_ro_packed_200_rand.tgz
```

Finally,
```
nlp.load_dataset('wmt19', 'zh-en')
```
Starts fast, but broken. (duplicate of #493 )


"
https://github.com/huggingface/datasets/issues/524,Some docs are missing parameter names,"['Indeed, good catch!']","See https://huggingface.co/nlp/master/package_reference/main_classes.html#nlp.Dataset.map. I believe this is because the parameter names are enclosed in backticks in the docstrings, maybe it's an old docstring format that doesn't work with the current Sphinx version."
https://github.com/huggingface/datasets/issues/522,dictionnary typo in docs,['Thanks!'],"Many places dictionary is spelled dictionnary, not sure if its on purpose or not.
Fixed in this pr:  
https://github.com/huggingface/nlp/pull/521 "
https://github.com/huggingface/datasets/issues/519,[BUG] Metrics throwing new error on master since 0.4.0,"['Update - maybe this is only failing on bleu because I was not tokenizing inputs to the metric'
 'Closing - seems to be just forgetting to tokenize. And found the helpful discussion in #137 ']","The following error occurs when passing in references of type `List[List[str]]` to metrics like bleu.
Wasn't happening on 0.4.0 but happening now on master.

```
  File ""/usr/local/lib/python3.7/site-packages/nlp/metric.py"", line 226, in compute
    self.add_batch(predictions=predictions, references=references)
  File ""/usr/local/lib/python3.7/site-packages/nlp/metric.py"", line 242, in add_batch
    batch = self.info.features.encode_batch(batch)
  File ""/usr/local/lib/python3.7/site-packages/nlp/features.py"", line 527, in encode_batch
    encoded_batch[key] = [encode_nested_example(self[key], cast_to_python_objects(obj)) for obj in column]
  File ""/usr/local/lib/python3.7/site-packages/nlp/features.py"", line 527, in <listcomp>
    encoded_batch[key] = [encode_nested_example(self[key], cast_to_python_objects(obj)) for obj in column]
  File ""/usr/local/lib/python3.7/site-packages/nlp/features.py"", line 456, in encode_nested_example
    raise ValueError(""Got a string but expected a list instead: '{}'"".format(obj))
```"
https://github.com/huggingface/datasets/issues/517,add MLDoc dataset,"['Any updates on this?'
 'This request is still an open issue waiting to be addressed by any community member, @GuillemGSubies.']","Hi,

I am recommending that someone add MLDoc, a multilingual news topic classification dataset.

- Here's a link to the Github: https://github.com/facebookresearch/MLDoc
- and the paper: http://www.lrec-conf.org/proceedings/lrec2018/pdf/658.pdf

Looks like the dataset contains news stories in multiple languages that can be classified into four hierarchical groups: CCAT (Corporate/Industrial), ECAT (Economics), GCAT (Government/Social) and MCAT (Markets). There are 13 languages: Dutch, French, German, Chinese, Japanese, Russian, Portuguese, Spanish, Latin American Spanish, Italian, Danish, Norwegian, and Swedish"
https://github.com/huggingface/datasets/issues/514,dataset.shuffle(keep_in_memory=True) is never allowed,"['This seems to be fixed in #513 for the filter function, replacing `cache_file_name` with `indices_cache_file_name` in the assert. Although not for the `map()` function @thomwolf '
 ""Maybe I'm a bit tired but I fail to see the issue here.\r\n\r\nSince `cache_file_name` is `None` by default, if you set `keep_in_memory` to `True`, the assert should pass, no?""
 'I failed to realise that this only applies to `shuffle()`. Whenever `keep_in_memory` is set to True, this is passed on to the `select()` function. However, if `cache_file_name` is None, it will be defined in the `shuffle()` function before it is passed on to `select()`. \r\n\r\nThus, `select()` is called with `keep_in_memory=True` and a not None value for `cache_file_name`. \r\nThis is essentially fixed in #513 \r\n\r\nEasily reproducible:\r\n```python\r\n>>> import nlp\r\n>>> data = nlp.load_dataset(""cosmos_qa"", split=""train"")\r\nUsing custom data configuration default\r\n>>> data.shuffle(keep_in_memory=True)\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/vegarab/.conda/envs/torch/lib/python3.7/site-packages/nlp/arrow_dataset.py"", line 1398, in shuffle\r\n    verbose=verbose,\r\n  File ""/home/vegarab/.conda/envs/torch/lib/python3.7/site-packages/nlp/arrow_dataset.py"", line 1178, in select\r\n    ), ""Please use either `keep_in_memory` or `cache_file_name` but not both.""\r\nAssertionError: Please use either `keep_in_memory` or `cache_file_name` but not both.\r\n>>>data.select([0], keep_in_memory=True)\r\n# No error\r\n```'
 'Oh yes ok got it thanks. Should be fixed if we are happy with #513 indeed.'
 ""My bad. This is actually not fixed in #513. Sorry about that...\r\nThe new `indices_cache_file_name` is set to a non-None value in the new `shuffle()` as well. \r\n\r\nThe buffer and caching mechanisms used in the `select()` function are too intricate for me to understand why the check is there at all. I've removed it in my local build and it seems to be working fine for my project, without really considering other implications of the change. \r\n\r\n""
 ""Ok I'll investigate and add a series of tests on the `keep_in_memory=True` settings which is under-tested atm""
 'Hey, still seeing this issue with the latest version.']","As of commit ef4aac2, the usage of the parameter `keep_in_memory=True` is never possible: `dataset.select(keep_in_memory=True)`

The commit added the lines
```python
# lines 994-996 in src/nlp/arrow_dataset.py
       assert (
            not keep_in_memory or cache_file_name is None
        ), ""Please use either `keep_in_memory` or `cache_file_name` but not both.""
```

This affects both `shuffle()` as `select()` is a sub-routine, and `map()` that has the same check. 

I'd love to fix this myself, but unsure what the intention of the assert is given the rest of the logic in the function concerning `ccache_file_name` and `keep_in_memory`."
https://github.com/huggingface/datasets/issues/511,dataset.shuffle() and select() resets format. Intended?,"[""Hi @vegarab yes feel free to open a discussion here.\r\n\r\nThis design choice was not very much thought about.\r\n\r\nSince `dataset.select()` (like all the method without a trailing underscore) is non-destructive and returns a new dataset it has most of its properties initialized from scratch (except the table and infos).\r\n\r\nThinking about it I don't see a strong reason against transmitting the format from the parent dataset to its newly created child. It's probably what's expected by the user in most cases. What do you think @lhoestq?\r\n\r\nBy the way, I've been working today on a refactoring of all the samples re-ordering/selection methods (`select`, `sort`, `shuffle`, `shard`, `train_test_split`). The idea is to speed them up by a lot (like, really a lot) by working as much as possible with an indices mapping table instead of doing a deep copy of the full dataset as we've been doing currently. You can give it a look and try it here: https://github.com/huggingface/nlp/pull/513\r\nFeedbacks are very much welcome""
 ""I think it's ok to keep the format.\r\nIf we want to have this behavior for `.map` too we just have to make sure it doesn't keep a column that's been removed.""
 'Shall we have this in the coming release by the way @lhoestq ?'
 'Yes sure !'
 'Since datasets 1.0.0 the format is not reset anymore.\r\nClosing this one, but feel free to re-open if you have other questions']","Calling `dataset.shuffle()` or `dataset.select()` on a dataset resets its format set by `dataset.set_format()`. Is this intended or an oversight?

When working on quite large datasets that require a lot of preprocessing I find it convenient to save the processed dataset to file using `torch.save(""dataset.pt"")`. Later loading the dataset object using `torch.load(""dataset.pt"")`, which conserves the defined format before saving. 
I do shuffling and selecting (for controlling dataset size) after loading the data from .pt-file, as it's convenient whenever you train multiple models with varying sizes of the same dataset. 

The obvious workaround for this is to set the format again after using `dataset.select()` or `dataset.shuffle()`.

_I guess this is more of a discussion on the design philosophy of the functions. Please let me know if this is not the right channel for these kinds of discussions or if they are not wanted at all!_

####  How to reproduce:

```python
import nlp
from transformers import T5Tokenizer

tokenizer = T5Tokenizer.from_pretrained(""t5-base"")
def create_features(batch):
    context_encoding = tokenizer.batch_encode_plus(batch[""context""])
    return {""input_ids"": context_encoding[""input_ids""]}

dataset = nlp.load_dataset(""cosmos_qa"", split=""train"")
dataset = dataset.map(create_features, batched=True)
dataset.set_format(type=""torch"", columns=[""input_ids""])
dataset[0]
# {'input_ids': tensor([ 1804,  3525,  1602,  ...   0,     0])}

dataset = dataset.shuffle()
dataset[0]
# {'id': '3Q9(...)20', 'context': ""Good Old War an (...) play ?', 'answer0': 'None of the above choices .', 'answer1': 'This person likes music and likes to see the show , they will see other bands play .', (...) 'input_ids': [1804, 3525, 1602, ... , 0, 0]}

```"
https://github.com/huggingface/datasets/issues/510,Version of numpy to use the library,"[""Seems like this method was added in 1.17. I'll add a requirement on this.""
 'Thank you so much. After upgrading the numpy library, it worked.']","Thank you so much for your excellent work! I would like to use nlp library in my project. While importing nlp, I am receiving the following error `AttributeError: module 'numpy.random' has no attribute 'Generator'` Numpy version in my project is 1.16.0. May I learn which numpy version is used for the nlp library.

Thanks in advance."
https://github.com/huggingface/datasets/issues/509,Converting TensorFlow dataset example,"[""Do you want to convert a dataset script to the tfds format ?\r\nIf so, we currently have a comversion script nlp/commands/convert.py but it is a conversion script that goes from tfds to nlp.\r\nI think it shouldn't be too hard to do the changes in reverse (at some manual adjustments).\r\nIf you manage to make it work in reverse, feel free to open a PR to share it with the community :)""
 'In our docs: [Using a Dataset with PyTorch/Tensorflow](https://huggingface.co/docs/datasets/torch_tensorflow.html).']","Hi,
I want to use TensorFlow datasets with this repo, I noticed you made some conversion script,
can you give a simple example of using it?

Thanks
"
https://github.com/huggingface/datasets/issues/508,TypeError: Receiver() takes no arguments,"['Which version of Apache Beam do you have (can you copy your full environment info here)?'
 'apache-beam==2.23.0\r\nnlp==0.4.0\r\n\r\nFor me this was resolved by running the same python script on Linux (or really WSL). '
 ""Do you manage to run a dummy beam pipeline with python on windows ? \r\nYou can test a dummy pipeline with [this code](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/wordcount_minimal.py)\r\n\r\nIf you get the same error, it means that the issue comes from apache beam.\r\nOtherwise we'll investigate what went wrong here""
 'Still, same error, so I guess it is on apache beam then. \r\nThanks for the investigation.'
 'Thanks for trying\r\nLet us know if you find clues of what caused this issue, or if you find a fix']","I am trying to load a wikipedia data set

```
import nlp
from nlp import load_dataset

dataset = load_dataset(""wikipedia"", ""20200501.en"", split=""train"", cache_dir=data_path, beam_runner='DirectRunner')
#dataset = load_dataset('wikipedia', '20200501.sv', cache_dir=data_path, beam_runner='DirectRunner')
```

This fails in the apache beam runner. 

```
Traceback (most recent call last):
  File ""D:/ML/wikiembedding/gpt2_sv.py"", line 36, in <module>
    dataset = load_dataset(""wikipedia"", ""20200501.en"", split=""train"", cache_dir=my_cache_dir, beam_runner='DirectRunner')
  File ""C:\Users\seto\AppData\Local\Programs\Python\Python38\lib\site-packages\nlp\load.py"", line 548, in load_dataset
    builder_instance.download_and_prepare(
  File ""C:\Users\seto\AppData\Local\Programs\Python\Python38\lib\site-packages\nlp\builder.py"", line 462, in download_and_prepare
    self._download_and_prepare(
  File ""C:\Users\seto\AppData\Local\Programs\Python\Python38\lib\site-packages\nlp\builder.py"", line 969, in _download_and_prepare
    pipeline_results = pipeline.run()
  File ""C:\Users\seto\AppData\Local\Programs\Python\Python38\lib\site-packages\apache_beam\pipeline.py"", line 534, in run
    return self.runner.run_pipeline(self, self._options)
....
  File ""C:\Users\seto\AppData\Local\Programs\Python\Python38\lib\site-packages\apache_beam\runners\worker\bundle_processor.py"", line 218, in process_encoded
    self.output(decoded_value)
  File ""C:\Users\seto\AppData\Local\Programs\Python\Python38\lib\site-packages\apache_beam\runners\worker\operations.py"", line 332, in output
    cython.cast(Receiver, self.receivers[output_index]).receive(windowed_value)
  File ""C:\Users\seto\AppData\Local\Programs\Python\Python38\lib\site-packages\Cython\Shadow.py"", line 167, in cast
    return type(*args)
TypeError: Receiver() takes no arguments

```

This is run on a Windows 10 machine with python 3.8. I get the same error loading the swedish wikipedia dump."
https://github.com/huggingface/datasets/issues/507,Errors when I use ,"['Looks like an issue with 3.0.2 transformers version. Works fine when I use ""master"" version of transformers.']","I tried the following example code from https://huggingface.co/deepset/roberta-base-squad2 and got errors 
I am using **transformers 3.0.2** code .


from transformers.pipelines import pipeline
from transformers.modeling_auto import AutoModelForQuestionAnswering
from transformers.tokenization_auto import AutoTokenizer

model_name = ""deepset/roberta-base-squad2""

nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)
QA_input = {
    'question': 'Why is model conversion important?',
    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'
}
res = nlp(QA_input)

The errors are :

res = nlp(QA_input)
  File "".local/lib/python3.6/site-packages/transformers/pipelines.py"", line 1316, in __call__
    for s, e, score in zip(starts, ends, scores)
  File "".local/lib/python3.6/site-packages/transformers/pipelines.py"", line 1316, in <listcomp>
    for s, e, score in zip(starts, ends, scores)
KeyError: 0

"
https://github.com/huggingface/datasets/issues/501,Caching doesn't work for map (non-deterministic),"[""Thanks for reporting !\r\n\r\nTo store the cache file, we compute a hash of the function given in `.map`, using our own hashing function.\r\nThe hash doesn't seem to stay the same over sessions for the tokenizer.\r\nApparently this is because of the regex at `tokenizer.pat` is not well supported by our hashing function.\r\n\r\nI'm working on a fix""
 'Thanks everyone. Works great now.']","The caching functionality doesn't work reliably when tokenizing a dataset. Here's a small example to reproduce it. 

```python
import nlp
import transformers

def main():
    ds = nlp.load_dataset(""reddit"", split=""train[:500]"")

    tokenizer = transformers.AutoTokenizer.from_pretrained(""gpt2"")

    def convert_to_features(example_batch):
        input_str = example_batch[""body""]
        encodings = tokenizer(input_str, add_special_tokens=True, truncation=True)
        return encodings

    ds = ds.map(convert_to_features, batched=True)

if __name__ == ""__main__"":
    main()
```

Roughly 3/10 times, this example recomputes the tokenization.

Is this expected behaviour?"
https://github.com/huggingface/datasets/issues/492,nlp.Features does not distinguish between nullable and non-nullable types in PyArrow schema,"['In 0.4.0, the assertion in `concatenate_datasets ` is on the features, and not the schema.\r\nCould you try to update `nlp` ?\r\n\r\nAlso, since 0.4.0, you can use `dset_wikipedia.cast_(dset_books.features)` to avoid the schema cast hack.'
 'Or maybe the assertion comes from elsewhere ?'
 ""I'm using the master branch. The assertion failure comes from the underlying `pa.concat_tables()`, which is in the pyarrow package. That method does check schemas.\r\n\r\nSince `features.type` does not contain information about nullable vs non-nullable features, the `cast_()` method won't resolve the schema mismatch. There is information in a schema which is not stored in features.""
 ""I'm doing a refactor of type inference in #363 . Both text fields should match after that""
 'By default nullable will be set to True'
 'It should be good now. I was able to run\r\n\r\n```python\r\n>>> from nlp import concatenate_datasets, load_dataset\r\n>>>\r\n>>> bookcorpus = load_dataset(""bookcorpus"", split=""train"")\r\n>>> wiki = load_dataset(""wikipedia"", ""20200501.en"", split=""train"")\r\n>>> wiki.remove_columns_(""title"")  # only keep the text\r\n>>>\r\n>>> assert bookcorpus.features.type == wiki.features.type\r\n>>> bert_dataset = concatenate_datasets([bookcorpus, wiki])\r\n```'
 'Thanks!']","Here's the code I'm trying to run:

```python
dset_wikipedia = nlp.load_dataset(""wikipedia"", ""20200501.en"", split=""train"", cache_dir=args.cache_dir)
dset_wikipedia.drop(columns=[""title""])
dset_wikipedia.features.pop(""title"")
dset_books = nlp.load_dataset(""bookcorpus"", split=""train"", cache_dir=args.cache_dir)
dset = nlp.concatenate_datasets([dset_wikipedia, dset_books])
```

This fails because they have different schemas, despite having identical features.

```python
assert dset_wikipedia.features == dset_books.features # True
assert dset_wikipedia._data.schema == dset_books._data.schema # False
```

The Wikipedia dataset has 'text: string', while the BookCorpus dataset has 'text: string not null'. Currently I hack together a working schema match with the following line, but it would be better if this was handled in Features themselves.

```python
dset_wikipedia._data = dset_wikipedia.data.cast(dset_books._data.schema)
```
"
https://github.com/huggingface/datasets/issues/491,No 0.4.0 release on GitHub,"['I did the release on github, and updated the doc :)\r\nSorry for the delay'
 'Thanks!']","0.4.0 was released on PyPi, but not on GitHub. This means [the documentation](https://huggingface.co/nlp/) is still displaying from 0.3.0, and that there's no tag to easily clone the 0.4.0 version of the repo."
https://github.com/huggingface/datasets/issues/489,ug,['whoops' 'please delete this'],
https://github.com/huggingface/datasets/issues/488,issues with downloading datasets for wmt16 and wmt19,"['I found `UNv1.0.en-ru.tar.gz` here: https://conferences.unite.un.org/uncorpus/en/downloadoverview, so it can be reconstructed with:\r\n```\r\nwget -c https://stuncorpusprod.blob.core.windows.net/corpusfiles/UNv1.0.en-ru.tar.gz.00\r\nwget -c https://stuncorpusprod.blob.core.windows.net/corpusfiles/UNv1.0.en-ru.tar.gz.01\r\nwget -c https://stuncorpusprod.blob.core.windows.net/corpusfiles/UNv1.0.en-ru.tar.gz.02\r\ncat UNv1.0.en-ru.tar.gz.0* > UNv1.0.en-ru.tar.gz\r\n```\r\nit has other languages as well, in case https://storage.googleapis.com/tfdataset-data/downloadataset/uncorpus/ is gone'
 ""Further, `nlp.load_dataset('wmt19', 'ru-en')` has only the `train` and `val` datasets. `test` is missing.\r\n\r\nFixed locally for summarization needs, by running:\r\n```\r\npip install sacrebleu\r\nsacrebleu -t wmt19 -l ru-en --echo src > test.source\r\nsacrebleu -t wmt19 -l ru-en --echo ref > test.target\r\n```\r\nh/t @sshleifer ""]","I  have encountered multiple issues while trying to:
```
import nlp
dataset = nlp.load_dataset('wmt16', 'ru-en')
metric = nlp.load_metric('wmt16')
```
1. I had to do `pip install -e "".[dev]"" ` on master, currently released nlp didn't work (sorry, didn't save the error) - I went back to the released version and now it worked. So it must have been some outdated dependencies that  `pip install -e "".[dev]"" ` fixed.

2. it was downloading at 60kbs - almost 5 hours to get the dataset. It was downloading all pairs and not just the one I asked for. 

I tried the same code with `wmt19` in parallel and it took a few secs to download and it only fetched data for the requested pair. (but it failed too, see below)

3. my machine has crushed and when I retried I got:

```
Traceback (most recent call last):
  File ""./download.py"", line 9, in <module>
    dataset = nlp.load_dataset('wmt16', 'ru-en')
  File ""/mnt/nvme1/code/huggingface/nlp-master/src/nlp/load.py"", line 549, in load_dataset
    download_config=download_config, download_mode=download_mode, ignore_verifications=ignore_verifications,
  File ""/mnt/nvme1/code/huggingface/nlp-master/src/nlp/builder.py"", line 449, in download_and_prepare
    with incomplete_dir(self._cache_dir) as tmp_data_dir:
  File ""/home/stas/anaconda3/envs/main/lib/python3.7/contextlib.py"", line 112, in __enter__
    return next(self.gen)
  File ""/mnt/nvme1/code/huggingface/nlp-master/src/nlp/builder.py"", line 422, in incomplete_dir
    os.makedirs(tmp_dir)
  File ""/home/stas/anaconda3/envs/main/lib/python3.7/os.py"", line 221, in makedirs
    mkdir(name, mode)
FileExistsError: [Errno 17] File exists: '/home/stas/.cache/huggingface/datasets/wmt16/ru-en/1.0.0/4d8269cdd971ed26984a9c0e4a158e0c7afc8135fac8fb8ee43ceecf38fd422d.incomplete'
```
it can't handle resumes. but neither allows a new start. Had to delete it manually.

4. and finally when it downloaded the dataset, it then failed to fetch the metrics:
```
Traceback (most recent call last):
  File ""./download.py"", line 15, in <module>
    metric = nlp.load_metric('wmt16')
  File ""/mnt/nvme1/code/huggingface/nlp-master/src/nlp/load.py"", line 442, in load_metric
    module_path, hash = prepare_module(path, download_config=download_config, dataset=False)
  File ""/mnt/nvme1/code/huggingface/nlp-master/src/nlp/load.py"", line 258, in prepare_module
    local_path = cached_path(file_path, download_config=download_config)
  File ""/mnt/nvme1/code/huggingface/nlp-master/src/nlp/utils/file_utils.py"", line 198, in cached_path
    local_files_only=download_config.local_files_only,
  File ""/mnt/nvme1/code/huggingface/nlp-master/src/nlp/utils/file_utils.py"", line 356, in get_from_cache
    raise ConnectionError(""Couldn't reach {}"".format(url))
ConnectionError: Couldn't reach https://s3.amazonaws.com/datasets.huggingface.co/nlp/metrics/wmt16/wmt16.py
```

5. If I run the same code with `wmt19`, it fails too:

```
ConnectionError: Couldn't reach https://storage.googleapis.com/tfdataset-data/downloadataset/uncorpus/UNv1.0.en-ru.tar.gz
```"
https://github.com/huggingface/datasets/issues/486,Bookcorpus data contains pretokenized text,"[""Yes indeed it looks like some `'` and spaces are missing (for example in `dont` or `didnt`).\r\nDo you know if there exist some copies without this issue ?\r\nHow would you fix this issue on the current data exactly ? I can see that the data is raw text (not tokenized) so I'm not sure I understand how you would do it. Could you provide more details ?""
 'I\'m afraid that I don\'t know how to obtain the original BookCorpus data. I believe this version came from an anonymous Google Drive link posted in another issue.\r\n\r\nGoing through the raw text in this version, it\'s apparent that NLTK\'s TreebankWordTokenizer was applied on it (I gave some examples in my original post), followed by:\r\n`\' \'.join(tokens)`\r\nYou can retrieve the tokenization by splitting on whitespace. You can then ""detokenize"" it with TreebankWordDetokenizer class of NLTK (though, as I suggested, use the fixed version in my repo). This will bring the text closer to its original form, but some steps of TreebankWordTokenizer are destructive, so it wouldn\'t be one-to-one. Something along the lines of the following should work:\r\n```\r\ntreebank_detokenizer = nltk.tokenize.treebank.TreebankWordDetokenizer()\r\ndb = nlp.load_dataset(\'bookcorpus\', split=nlp.Split.TRAIN)\r\ndb = db.map(lambda x: treebank_detokenizer.detokenize(x[\'text\'].split()))\r\n```\r\n\r\nRegarding other issues beyond the above, I\'m afraid that I can\'t help with that.'
 ""Ok I get it, that would be very cool indeed\r\n\r\nWhat kinds of patterns the detokenizer can't retrieve ?""
 'The TreebankTokenizer makes some assumptions about whitespace, parentheses, quotation marks, etc. For instance, while tokenizing the following text:\r\n```\r\nDwayne ""The Rock"" Johnson\r\n```\r\nwill result in:\r\n```\r\nDwayne `` The Rock \'\' Johnson\r\n```\r\nwhere the left and right quotation marks are turned into distinct symbols. Upon reconstruction, we can attach the left part to its token on the right, and respectively for the right part. However, the following texts would be tokenized exactly the same:\r\n```\r\nDwayne "" The Rock "" Johnson\r\nDwayne "" The Rock"" Johnson\r\nDwayne     "" The Rock"" Johnson\r\n...\r\n```\r\nIn the above examples, the detokenizer would correct these inputs into the canonical text\r\n```\r\nDwayne ""The Rock"" Johnson\r\n```\r\nHowever, there are cases where there the solution cannot easily be inferred (at least without a true LM - this tokenizer is just a bunch of regexes). For instance, in cases where you have a fragment that contains the end of quote, but not its beginning, plus an accidental space:\r\n```\r\n... and it sounds fantastic, "" he said.\r\n```\r\nIn the above case, the tokenizer would assume that the quotes refer to the next token, and so upon detokenization it will result in the following mistake:\r\n```\r\n... and it sounds fantastic, ""he said.\r\n```\r\n\r\nWhile these are all odd edge cases (the basic assumptions do make sense), in noisy data they can occur, which is why I mentioned that the detokenizer cannot restore the original perfectly.\r\n'
 'To confirm, since this is preprocessed, this was not the exact version of the Book Corpus used to actually train the models described here (particularly Distilbert)?  https://huggingface.co/datasets/bookcorpus\r\n\r\nOr does this preprocessing exactly match that of the papers?'
 'I believe these are just artifacts of this particular source. It might be better to crawl it again, or use another preprocessed source, as found here: https://github.com/soskek/bookcorpus '
 'Yes actually the BookCorpus on hugginface is based on [this](https://github.com/soskek/bookcorpus/issues/24#issuecomment-643933352). And I kind of regret naming it as ""BookCorpus"" instead of something like ""BookCorpusLike"".\r\n\r\nBut there is a good news ! @shawwn has replicated BookCorpus in his way, and also provided a link to download the plain text files. see [here](https://github.com/soskek/bookcorpus/issues/27). There is chance we can have a ""OpenBookCorpus"" !']","It seem that the bookcoprus data downloaded through the library was pretokenized with NLTK's Treebank tokenizer, which changes the text in incompatible ways to how, for instance, BERT's wordpiece tokenizer works. For example, ""didn't"" becomes ""did"" + ""n't"", and double quotes are changed to `` and '' for start and end quotes, respectively.

On my own projects, I just run the data through NLTK's TreebankWordDetokenizer to reverse the tokenization (as best as possible). I think it would be beneficial to apply this transformation directly on your remote cached copy of the dataset. If you choose to do so, I would also suggest to use my fork of NLTK that fixes several bugs in their detokenizer (I've opened a pull-request, but they've yet to respond): https://github.com/nltk/nltk/pull/2575"
https://github.com/huggingface/datasets/issues/483,rotten tomatoes movie review dataset taken down,"['found a mirror: https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz'
 'fixed in #484 '
 'Closing this one. Thanks again @jxmorris12 for taking care of this :)']","In an interesting twist of events, the individual who created the movie review seems to have left Cornell, and their webpage has been removed, along with the movie review dataset (http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz). It's not downloadable anymore."
https://github.com/huggingface/datasets/issues/482,Bugs : dataset.map() is frozen on ELI5,"[""This comes from an overflow in pyarrow's array.\r\nIt is stuck inside the loop that reduces the batch size to avoid the overflow.\r\nI'll take a look""
 ""I created a PR to fix the issue.\r\nIt was due to an overflow check that handled badly an empty list.\r\n\r\nYou can try the changes by using \r\n```\r\n!pip install git+https://github.com/huggingface/nlp.git@fix-bad-type-in-overflow-check\r\n```\r\n\r\nAlso I noticed that the first 1000 examples have an empty list in the `title_urls` field. The feature type inference in `.map` will consider it `null` because of that, and it will crash when it encounter the next example with a `title_urls` that is not empty.\r\n\r\nTherefore to fix that, what you can do for now is increase the writer batch size so that the feature inference will take into account at least one example with a non-empty `title_urls`:\r\n\r\n```python\r\n# default batch size is 1_000 and it's not enough for feature type inference because of empty lists\r\nvalid_dataset = valid_dataset.map(make_input_target, writer_batch_size=3_000) \r\n```\r\n\r\nI was able to run the frozen cell with these changes.""
 '@lhoestq Perfect and thank you very much!!\r\nClose the issue.'
 '@lhoestq mapping the function `make_input_target` was passed by your fixing.\r\n\r\nHowever, there is another error in the final step of `valid_dataset.map(convert_to_features, batched=True)`\r\n\r\n`ArrowInvalid: Could not convert Thepiratebay.vg with type str: converting to null type`\r\n(The [same colab notebook above with new error message](https://colab.research.google.com/drive/14wttOTv3ky74B_c0kv5WrbgQjCF2fYQk?usp=sharing#scrollTo=5sRrJ3_C8rLt))\r\n\r\nDo you have some ideas? (I am really sorry I could not debug it by myself since I never used `pyarrow` before) \r\nNote that `train_dataset.map(convert_to_features, batched=True)` can be run successfully even though train_dataset is 27x bigger than `valid_dataset` so I believe the problem lies in some field of `valid_dataset` again .'
 ""I got this issue too and fixed it by specifying `writer_batch_size=3_000` in `.map`.\r\nThis is because Arrow didn't expect `Thepiratebay.vg` in `title_urls `, as all previous examples have empty lists in `title_urls `""
 'I am clear now . Thank so much again Quentin!']","Hi Huggingface Team!

Thank you guys once again for this amazing repo.

I have tried to prepare ELI5 to train with T5, based on [this wonderful notebook of Suraj Patil](https://github.com/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb) 

However, when I run `dataset.map()` on ELI5 to prepare `input_text, target_text`, `dataset.map` is **frozen** in the first hundreds examples. On the contrary, this works totally fine on SQUAD (80,000 examples). Both `nlp` version 0.3.0 and 0.4.0 cause frozen process . Also try various `pyarrow` versions from 0.16.0 / 0.17.0 / 1.0.0 also have the same frozen process.

Reproducible code can be found on [this colab notebook ](https://colab.research.google.com/drive/14wttOTv3ky74B_c0kv5WrbgQjCF2fYQk?usp=sharing), where I also show that the same mapping function works fine on SQUAD, so the problem is likely due to ELI5 somehow.

----------------------------------------
**More Info :** instead of `map`, if I run `for` loop and apply function by myself, there's no error and can finish within 10 seconds. However, `nlp dataset` is immutable (I couldn't manually assign a new key-value to `dataset `object)

I also notice that SQUAD texts are quite clean while ELI5 texts contain many special characters, not sure if this is the cause ?"
https://github.com/huggingface/datasets/issues/478,Export TFRecord to GCP bucket,"['Nevermind, I restarted my python session and it worked fine...\r\n\r\n---\r\n\r\nI had an authentification error, and I authenticated from another terminal. After that, no more error but it was not working. Restarting the sessions makes it work :)']","Previously, I was writing TFRecords manually to GCP bucket with : `with tf.io.TFRecordWriter('gs://my_bucket/x.tfrecord')`

Since `0.4.0` is out with the `export()` function, I tried it. But it seems TFRecords cannot be directly written to GCP bucket.

`dataset.export('local.tfrecord')` works fine,  
but `dataset.export('gs://my_bucket/x.tfrecord')` does not work. 

There is no error message, I just can't find the file on my bucket...

---

Looking at the code, `nlp` is using `tf.data.experimental.TFRecordWriter`, while I was using `tf.io.TFRecordWriter`.  

**What's the difference between those 2 ? How can I write TFRecords files directly to GCP bucket ?**

@jarednielsen @lhoestq "
https://github.com/huggingface/datasets/issues/477,Overview.ipynb throws exceptions with nlp 0.4.0,"[""Thanks for reporting this issue\r\n\r\nThere was a bug where numpy arrays would get returned instead of tensorflow tensors.\r\nThis is fixed on master.\r\n\r\nI tried to re-run the colab and encountered this error instead:\r\n\r\n```\r\nAttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'to_tensor'\r\n```\r\n\r\nThis is because the dataset returns a Tensor and not a RaggedTensor.\r\nBut I think we should always return a RaggedTensor unless the length of the sequence is fixed (it that case they can be stack into a Tensor).""
 'Hi, I got another error (on Colab):\r\n\r\n```python\r\n# You can read a few attributes of the datasets before loading them (they are python dataclasses)\r\nfrom dataclasses import asdict\r\n\r\nfor key, value in asdict(datasets[6]).items():\r\n    print(\'👉 \' + key + \': \' + str(value))\r\n\r\n---------------------------------------------------------------------------\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n\r\n<ipython-input-6-b8ace6c227a2> in <module>()\r\n      2 from dataclasses import asdict\r\n      3 \r\n----> 4 for key, value in asdict(datasets[6]).items():\r\n      5     print(\'👉 \' + key + \': \' + str(value))\r\n\r\n/usr/local/lib/python3.6/dist-packages/dataclasses.py in asdict(obj, dict_factory)\r\n   1008     """"""\r\n   1009     if not _is_dataclass_instance(obj):\r\n-> 1010         raise TypeError(""asdict() should be called on dataclass instances"")\r\n   1011     return _asdict_inner(obj, dict_factory)\r\n   1012 \r\n\r\nTypeError: asdict() should be called on dataclass instances\r\n```'
 ""Indeed we'll update the cola with the new release coming up this week.""]","with nlp 0.4.0, the TensorFlow example in Overview.ipynb throws the following exceptions:


---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-5-48907f2ad433> in <module>
----> 1 features = {x: train_tf_dataset[x].to_tensor(default_value=0, shape=[None, tokenizer.max_len]) for x in columns[:3]}
      2 labels = {""output_1"": train_tf_dataset[""start_positions""].to_tensor(default_value=0, shape=[None, 1])}
      3 labels[""output_2""] = train_tf_dataset[""end_positions""].to_tensor(default_value=0, shape=[None, 1])
      4 tfdataset = tf.data.Dataset.from_tensor_slices((features, labels)).batch(8)

<ipython-input-5-48907f2ad433> in <dictcomp>(.0)
----> 1 features = {x: train_tf_dataset[x].to_tensor(default_value=0, shape=[None, tokenizer.max_len]) for x in columns[:3]}
      2 labels = {""output_1"": train_tf_dataset[""start_positions""].to_tensor(default_value=0, shape=[None, 1])}
      3 labels[""output_2""] = train_tf_dataset[""end_positions""].to_tensor(default_value=0, shape=[None, 1])
      4 tfdataset = tf.data.Dataset.from_tensor_slices((features, labels)).batch(8)

AttributeError: 'numpy.ndarray' object has no attribute 'to_tensor'"
https://github.com/huggingface/datasets/issues/474,test_load_real_dataset when config has BUILDER_CONFIGS that matter,"['The `data_dir` parameter has been removed. Now the error is `ValueError: Config name is missing`\r\n\r\nAs mentioned in #470 I think we can have one test with the first config of BUILDER_CONFIGS, and another test that runs all of the configs in BUILDER_CONFIGS'
 'This was fixed in #527 \r\n\r\nClosing this one, but feel free to re-open if you have other questions']","It a dataset has custom `BUILDER_CONFIGS` with non-keyword arguments (or keyword arguments with non default values), the config is not loaded during the test and causes an error.
I think the problem is that `test_load_real_dataset` calls `load_dataset` with `data_dir=temp_data_dir` ([here](https://github.com/huggingface/nlp/blob/master/tests/test_dataset_common.py#L200)). This causes [this line](https://github.com/huggingface/nlp/blob/master/src/nlp/builder.py#L201) to always be false because `config_kwargs` is not `None`. [This line](https://github.com/huggingface/nlp/blob/master/src/nlp/builder.py#L222) will be run instead, which doesn't use `BUILDER_CONFIGS`.

For an example, you can try running the test for lince:
` RUN_SLOW=1 pytest tests/test_dataset_common.py::LocalDatasetTest::test_load_real_dataset_lince`
which yields
> E           TypeError: __init__() missing 3 required positional arguments: 'colnames', 'classes', and 'label_column'"
https://github.com/huggingface/datasets/issues/469,invalid data type 'str' at _convert_outputs in arrow_dataset.py,"['Hi ! Did you try to set the output format to pytorch ? (or tensorflow if you\'re using tensorflow)\r\nIt can be done with `dataset.set_format(""torch"", columns=columns)`  (or ""tensorflow"").\r\n\r\nNote that for pytorch, string columns can\'t be converted to `torch.Tensor`, so you have to specify in `columns=` the list of columns you want to keep (`input_ids` for example)'
 ""Hello . Yes, I did set the output format as below for the two columns \r\n\r\n  `train_dataset.set_format('torch',columns=['Text','Label'])`\r\n ""
 'I think you\'re having this issue because you try to format strings as pytorch tensors, which is not possible.\r\nIndeed by having ""Text"" in `columns=[\'Text\',\'Label\']`, you try to convert the text values to pytorch tensors.\r\n\r\nInstead I recommend you to first tokenize your dataset using a tokenizer from transformers. For example\r\n\r\n```python\r\nfrom transformers import BertTokenizer\r\ntokenizer = BertTokenizer.from_pretrained(""bert-base-uncased"")\r\n\r\ntrain_dataset.map(lambda x: tokenizer(x[""Text""]), batched=True)\r\ntrain_dataset.set_format(""torch"", column=[""input_ids""])\r\n```\r\n\r\nAnother way to fix your issue would be to not set the format to pytorch, and leave the dataset as it is by default. In that case, the strings are returned normally when you get examples from your dataloader. It means that you would have to tokenize the examples in the training loop (or using a data collator) though.\r\n\r\nLet me know if you have other questions'
 ""Hi, actually the thing is I am getting the same error and even after tokenizing them I am passing them through batch_encode_plus.\r\nI dont know what seems to be the problem is. I even converted it into 'pt' while passing them through batch_encode_plus but when I am evaluating my model , i am getting this error\r\n\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-145-ca218223c9fc> in <module>()\r\n----> 1 val_loss, predictions, true_val = evaluate(dataloader_validation)\r\n      2 val_f1 = f1_score_func(predictions, true_val)\r\n      3 tqdm.write(f'Validation loss: {val_loss}')\r\n      4 tqdm.write(f'F1 Score (Weighted): {val_f1}')\r\n\r\n6 frames\r\n/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataset.py in <genexpr>(.0)\r\n    160 \r\n    161     def __getitem__(self, index):\r\n--> 162         return tuple(tensor[index] for tensor in self.tensors)\r\n    163 \r\n    164     def __len__(self):\r\n\r\nTypeError: new(): invalid data type 'str' ""
 ""> Hi, actually the thing is I am getting the same error and even after tokenizing them I am passing them through batch_encode_plus.\r\n> I dont know what seems to be the problem is. I even converted it into 'pt' while passing them through batch_encode_plus but when I am evaluating my model , i am getting this error\r\n> \r\n> TypeError Traceback (most recent call last)\r\n> in ()\r\n> ----> 1 val_loss, predictions, true_val = evaluate(dataloader_validation)\r\n> 2 val_f1 = f1_score_func(predictions, true_val)\r\n> 3 tqdm.write(f'Validation loss: {val_loss}')\r\n> 4 tqdm.write(f'F1 Score (Weighted): {val_f1}')\r\n> \r\n> 6 frames\r\n> /usr/local/lib/python3.6/dist-packages/torch/utils/data/dataset.py in (.0)\r\n> 160\r\n> 161 def **getitem**(self, index):\r\n> --> 162 return tuple(tensor[index] for tensor in self.tensors)\r\n> 163\r\n> 164 def **len**(self):\r\n> \r\n> TypeError: new(): invalid data type 'str'\r\n\r\nI got the same error and fix it .\r\nyou can check your input where there may be string contained.\r\nsuch as\r\n```\r\na = [1,2,3,4,'<unk>']\r\ntorch.tensor(a)\r\n```""
 ""I didn't know tokenizers could return strings in the token ids. Which tokenizer are you using to get this @Doragd ?""
 ""> I didn't know tokenizers could return strings in the token ids. Which tokenizer are you using to get this @Doragd ?\r\n\r\ni'm sorry that i met this issue in another place (not in huggingface repo). ""
 '@akhilkapil do you have strings in your dataset ? When you set the dataset format to ""pytorch"" you should exclude columns with strings as pytorch can\'t make tensors out of strings']","I trying to build multi label text classifier model using Transformers lib. 

I'm using Transformers NLP to load the data set, while calling trainer.train() method. It throws the following error 

File ""C:\***\arrow_dataset.py"", line 343, in _convert_outputs
    v = command(v)
TypeError: new(): invalid data type 'str'

I'm using pyarrow 1.0.0.  And I have simple custom data set with Text and Integer Label.  
Ex: Data
 Text ,     Label  #Column Header
 I'm facing an Network issue, 1
 I forgot my password, 2

Error StackTrace:

File ""C:\**\transformers\trainer.py"", line 492, in train
    for step, inputs in enumerate(epoch_iterator):
  File ""C:\**\tqdm\std.py"", line 1104, in __iter__
    for obj in iterable:
  File ""C:\**\torch\utils\data\dataloader.py"", line 345, in __next__
    data = self._next_data()
  File ""C:\**\torch\utils\data\dataloader.py"", line 385, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File ""C:\**\torch\utils\data\_utils\fetch.py"", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File ""C:\**\torch\utils\data\_utils\fetch.py"", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File ""C:\**\nlp\arrow_dataset.py"", line 414, in __getitem__
    output_all_columns=self._output_all_columns,
  File ""C:\**\nlp\arrow_dataset.py"", line 403, in _getitem
    outputs, format_type=format_type, format_columns=format_columns, output_all_columns=output_all_columns
  File ""C:\**\nlp\arrow_dataset.py"", line 343, in _convert_outputs
    v = command(v)
TypeError: new(): invalid data type 'str'
 
"
https://github.com/huggingface/datasets/issues/468,UnicodeDecodeError while loading PAN-X task of XTREME dataset,"['Indeed. Solution 1 is the simplest.\r\n\r\nThis is actually a recurring problem.\r\nI think we should scan all the datasets with regexpr to fix the use of `open()` without encodings.\r\nAnd probably add a test in the CI to forbid using this in the future.'
 ""I'm happy to tackle the broader problem - will open a PR when it's ready!""
 'That would be awesome!'
 'I\'ve created a simple function that seems to do the trick:\r\n\r\n```python\r\ndef apply_encoding_on_file_open(filepath: str):\r\n    """"""Apply UTF-8 encoding for all instances where a non-binary file is opened.""""""\r\n    \r\n    with open(filepath, \'r\', encoding=\'utf-8\') as input_file:\r\n        regexp = re.compile(r""""""\r\n                            (?!.*\\b(?:encoding|rb|wb|wb+|ab|ab+)\\b)\r\n                            (open)\r\n                            \\((.*)\\)\r\n                            """""")\r\n        input_text = input_file.read()\r\n        match = regexp.search(input_text)\r\n        \r\n        if match:\r\n            print(\'Found match!\', match.group())\r\n            # append utf-8 encoding to matching groups in-place\r\n            output = regexp.sub(lambda m: m.group()[:-1]+\', encoding=""utf-8"")\', input_text)\r\n            with open(filepath, \'w\', encoding=\'utf-8\') as output_file:\r\n                output_file.write(output)\r\n        else:\r\n            print(""No match found!"")\r\n```\r\n\r\nThe regexp does a negative lookahead to avoid matching on cases where the encoding is already specified or when binary files are involved.\r\n\r\nFrom an implementation perspective:\r\n\r\n* Would it make sense to include this function in `nlp-cli` so that we can run something like\r\n```\r\nnlp-cli fix_encoding path/to/folder\r\n```\r\nand the command recursively fixes all files in the target?\r\n* What is the desired behaviour in the CI test? Here we could either have a simple script that we run as a `job` in the CI and raises an error if a missing encoding is detected. Alternatively we could incorporate this behaviour into the CLI and run that in the CI.\r\n\r\nPlease let me know what you prefer among the alternatives.\r\n'
 'I realised I was overthinking the problem, so decided to just run the regexp over the codebase and make the PR. In other words, we can ignore my comments about using the CLI 😸 ']","Hi 🤗  team!

## Description of the problem
I'm running into a `UnicodeDecodeError` while trying to load the PAN-X subset the XTREME dataset: 

```
---------------------------------------------------------------------------
UnicodeDecodeError                        Traceback (most recent call last)
<ipython-input-5-1d61f439b843> in <module>
----> 1 dataset = load_dataset(""xtreme"", ""PAN-X.en"", data_dir='./data')

/usr/local/lib/python3.6/dist-packages/nlp/load.py in load_dataset(path, name, version, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, save_infos, **config_kwargs)
    528     ignore_verifications = ignore_verifications or save_infos
    529     # Download/copy dataset processing script
--> 530     module_path, hash = prepare_module(path, download_config=download_config, dataset=True)
    531 
    532     # Get dataset builder class from the processing script

/usr/local/lib/python3.6/dist-packages/nlp/load.py in prepare_module(path, download_config, dataset, force_local_path, **download_kwargs)
    265 
    266     # Download external imports if needed
--> 267     imports = get_imports(local_path)
    268     local_imports = []
    269     library_imports = []

/usr/local/lib/python3.6/dist-packages/nlp/load.py in get_imports(file_path)
    156     lines = []
    157     with open(file_path, mode=""r"") as f:
--> 158         lines.extend(f.readlines())
    159 
    160     logger.info(""Checking %s for additional imports."", file_path)

/usr/lib/python3.6/encodings/ascii.py in decode(self, input, final)
     24 class IncrementalDecoder(codecs.IncrementalDecoder):
     25     def decode(self, input, final=False):
---> 26         return codecs.ascii_decode(input, self.errors)[0]
     27 
     28 class StreamWriter(Codec,codecs.StreamWriter):

UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 111: ordinal not in range(128)
```

## Steps to reproduce
Install from nlp's master branch
```python
pip install git+https://github.com/huggingface/nlp.git
```
then run
```python
from nlp import load_dataset
# AmazonPhotos.zip is located in data/
dataset = load_dataset(""xtreme"", ""PAN-X.en"", data_dir='./data')
```

## OS / platform details

- `nlp` version: latest from master
- Platform: Linux-4.15.0-72-generic-x86_64-with-Ubuntu-18.04-bionic
- Python version: 3.6.9
- PyTorch version (GPU?): 1.4.0 (True)
- Tensorflow version (GPU?): 2.1.0 (True)
- Using GPU in script?: True
- Using distributed or parallel set-up in script?: False

## Proposed solution
Either change [line 762](https://github.com/huggingface/nlp/blob/7ada00b1d62f94eee22a7df38c6b01e3f27194b7/datasets/xtreme/xtreme.py#L762) in `xtreme.py` to include UTF-8 encoding:

```
# old
with open(filepath) as f
# new
with open(filepath, encoding='utf-8') as f
```

or raise a warning that suggests setting the locale explicitly, e.g.
```python
import locale
locale.setlocale(locale.LC_ALL, 'C.UTF-8')
```
I have a preference for the first solution. Let me know if you agree and I'll be happy to implement the simple fix!"
https://github.com/huggingface/datasets/issues/445,DEFAULT_TOKENIZER import error in sacrebleu,['This issue was resolved by #447 '],"Latest Version 0.3.0

When loading the metric ""sacrebleu"" there is an import error due to the wrong path
![image](https://user-images.githubusercontent.com/5303103/88633063-2c5e5f00-d0bd-11ea-8ca8-4704dc975433.png)
"
https://github.com/huggingface/datasets/issues/444,Keep loading old file even I specify a new file in load_dataset,"['Same here !'
 ""This is the only fix I could come up with without touching the repo's code.\r\n```python\r\nfrom nlp.builder import FORCE_REDOWNLOAD\r\ndataset = load_dataset('csv', data_file='./a.csv', download_mode=FORCE_REDOWNLOAD, version='0.0.1')\r\n```\r\nYou'll have to change the version each time you want to load a different csv file.\r\nIf you're willing to add a ```print```, you can go to ```nlp.load``` and add ```print(builder_instance.cache_dir)``` right before the ```return ds``` in the ```load_dataset``` method. It'll print the cache folder, and you'll just have to erase it (and then you won't need the change here above).""]","I used load a file called 'a.csv' by 
```
dataset = load_dataset('csv', data_file='./a.csv')
```
And after a while, I tried to load another csv called 'b.csv'
```
dataset = load_dataset('csv', data_file='./b.csv')
```
However, the new dataset seems to remain the old 'a.csv' and not loading new csv file.

Even worse, after I load a.csv, the load_dataset function keeps loading the 'a.csv' afterward. 

Is this a cache problem?
"
https://github.com/huggingface/datasets/issues/443,Cannot unpickle saved .pt dataset with torch.save()/load(),['This seems to be fixed in a non-released version. \r\n\r\nInstalling nlp from source\r\n```\r\ngit clone https://github.com/huggingface/nlp\r\ncd nlp\r\npip install .\r\n```\r\nsolves the issue. '],"Saving a formatted torch dataset to file using `torch.save()`. Loading the same file fails during unpickling:

```python
>>> import torch
>>> import nlp

>>> squad = nlp.load_dataset(""squad.py"", split=""train"")
>>> squad
Dataset(features: {'source_text': Value(dtype='string', id=None), 'target_text': Value(dtype='string', id=None)}, num_rows: 87599)
>>> squad = squad.map(create_features, batched=True)
>>> squad.set_format(type=""torch"", columns=[""source_ids"", ""target_ids"", ""attention_mask""])
>>> torch.save(squad, ""squad.pt"")

>>> squad_pt = torch.load(""squad.pt"")
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/vegarab/.conda/envs/torch/lib/python3.7/site-packages/torch/serialization.py"", line 593, in load
    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)
  File ""/home/vegarab/.conda/envs/torch/lib/python3.7/site-packages/torch/serialization.py"", line 773, in _legacy_load
    result = unpickler.load()
  File ""/home/vegarab/.conda/envs/torch/lib/python3.7/site-packages/nlp/splits.py"", line 493, in __setitem__
    raise ValueError(""Cannot add elem. Use .add() instead."")
ValueError: Cannot add elem. Use .add() instead.
```
where `create_features` is a function that tokenizes the data using `batch_encode_plus` and returns a Dict with `input_ids`, `target_ids` and `attention_mask`. 
```python
def create_features(batch):
    source_text_encoding = tokenizer.batch_encode_plus(
        batch[""source_text""],
        max_length=max_source_length,
        pad_to_max_length=True,
        truncation=True)

    target_text_encoding = tokenizer.batch_encode_plus(
        batch[""target_text""],
        max_length=max_target_length,
        pad_to_max_length=True,
        truncation=True)

    features = {
        ""source_ids"": source_text_encoding[""input_ids""],
        ""target_ids"": target_text_encoding[""input_ids""],
        ""attention_mask"": source_text_encoding[""attention_mask""]
    }

    return features
```

I found a similar issue in [issue 5267 in the huggingface/transformers repo](https://github.com/huggingface/transformers/issues/5267) which was solved by downgrading to `nlp==0.2.0`. That did not solve this problem, however. "
https://github.com/huggingface/datasets/issues/439,Issues: Adding a FAISS or Elastic Search index to a Dataset,"['`DPRContextEncoder` and `DPRContextEncoderTokenizer` will be available in the next release of `transformers`.\r\n\r\nRight now you can experiment with it by installing `transformers` from the master branch.\r\nYou can also check the docs of DPR [here](https://huggingface.co/transformers/master/model_doc/dpr.html).\r\n\r\nMoreover all the indexing features will also be available in the next release of `nlp`.'
 '@lhoestq Thanks for the info '
 ""@lhoestq  I tried installing transformer from the master branch. Python imports for DPR again didnt' work.  Anyways, Looking forward to trying it in the next release of nlp ""
 '@nsankar have you tried with the latest version of the library?'
 '@yjernite it worked. Thanks']","It seems the DPRContextEncoder, DPRContextEncoderTokenizer cited[ in this documentation](https://huggingface.co/nlp/faiss_and_ea.html) is not implemented ? It didnot work with the standard nlp installation . Also, I couldn't find or use it with the latest nlp install from github in Colab.  Is  there any dependency on the latest PyArrow 1.0.0 ? Is it yet to be made generally available ?"
https://github.com/huggingface/datasets/issues/438,"New Datasets: IWSLT15+, ITTB","['Thanks Sam, we now have a very detailed tutorial and template on how to add a new dataset to the library. It typically take 1-2 hours to add one. Do you want to give it a try ?\r\nThe tutorial on writing a new dataset loading script is here: https://huggingface.co/nlp/add_dataset.html\r\nAnd the part on how to share a new dataset is here: https://huggingface.co/nlp/share_dataset.html'
 ""Hi @sshleifer, I'm trying to add IWSLT using the link you provided but the download urls are not working. Only `[en, de]` pair is working. For others language pairs it throws a `404` error.\r\n\r\n""]","**Links:**
[iwslt](https://pytorchnlp.readthedocs.io/en/latest/_modules/torchnlp/datasets/iwslt.html)
Don't know if that link is up to date.

[ittb](http://www.cfilt.iitb.ac.in/iitb_parallel/)
**Motivation**: replicate mbart finetuning results (table below)
![image](https://user-images.githubusercontent.com/6045025/88490093-0c1c8c00-cf67-11ea-960d-8dcaad2aa8eb.png)


For future readers, we already have the following language pairs in the wmt namespaces:

```
wmt14: ['cs-en', 'de-en', 'fr-en', 'hi-en', 'ru-en']
wmt15: ['cs-en', 'de-en', 'fi-en', 'fr-en', 'ru-en']
wmt16: ['cs-en', 'de-en', 'fi-en', 'ro-en', 'ru-en', 'tr-en']
wmt17: ['cs-en', 'de-en', 'fi-en', 'lv-en', 'ru-en', 'tr-en', 'zh-en']
wmt18: ['cs-en', 'de-en', 'et-en', 'fi-en', 'kk-en', 'ru-en', 'tr-en', 'zh-en']
wmt19: ['cs-en', 'de-en', 'fi-en', 'gu-en', 'kk-en', 'lt-en', 'ru-en', 'zh-en', 'fr-de']
```"
https://github.com/huggingface/datasets/issues/436,Google Colab - load_dataset - PyArrow exception,"['Indeed, we’ll make a new PyPi release next week to solve this. Cc @lhoestq '
 ""+1! this is the reason our tests are failing at [TextAttack](https://github.com/QData/TextAttack) \r\n\r\n(Though it's worth noting if we fixed the version number of pyarrow to 0.16.0 that would fix our problem too. But in this case we'll just wait for you all to update)""
 ""Came to raise this issue, great to see other already have and it's being fixed so soon!\r\n\r\nAs an aside, since no one wrote this already, it seems like the version check only looks at the second part of the version number making sure it is >16, but pyarrow newest version is 1.0.0 so the second past is 0!""
 '> Indeed, we’ll make a new PyPi release next week to solve this. Cc @lhoestq\r\n\r\nYes definitely'
 'please fix this on pypi! @lhoestq ' 'Is this issue fixed ?'
 'We’ll release the new version later today. Apologies for the delay.'
 'I just pushed the new version on pypi :)' 'Thanks for the update.']","With latest PyArrow 1.0.0 installed, I get the following exception   . Restarting colab has the same issue

ImportWarning: To use `nlp`, the module `pyarrow>=0.16.0` is required, and the current version of `pyarrow` doesn't match this condition. If you are running this in a Google Colab, you should probably just restart the runtime to use the right version of `pyarrow`.

The error goes only when I install version 0.16.0 
i.e.  !pip install pyarrow==0.16.0"
https://github.com/huggingface/datasets/issues/435,ImportWarning for pyarrow 1.0.0,"[""This was fixed in #434 \r\nWe'll do a release later this week to include this fix.\r\nThanks for reporting""
 'I dont know if the fix was made but the problem is still present : \r\nInstaled with pip : NLP 0.3.0 // pyarrow 1.0.0 \r\nOS : archlinux with kernel zen 5.8.5'
 'Yes it was fixed in `nlp>=0.4.0`\r\nYou can update with pip'
 ""Sorry, I didn't got the updated version, all is now working perfectly thanks""]",The following PR raised ImportWarning at `pyarrow ==1.0.0` https://github.com/huggingface/nlp/pull/265/files
https://github.com/huggingface/datasets/issues/433,How to reuse functionality of a (generic) dataset?,"['Hi @ArneBinder, we have a few ""generic"" datasets which are intended to load data files with a predefined format:\r\n- csv: https://github.com/huggingface/nlp/tree/master/datasets/csv\r\n- json: https://github.com/huggingface/nlp/tree/master/datasets/json\r\n- text: https://github.com/huggingface/nlp/tree/master/datasets/text\r\n\r\nYou can find more details about this way to load datasets here in the documentation: https://huggingface.co/nlp/loading_datasets.html#from-local-files\r\n\r\nMaybe your brat loading script could be shared in a similar fashion?'
 '> Maybe your brat loading script could be shared in a similar fashion?\r\n\r\n@thomwolf that was also my first idea and I think I will tackle that in the next days. I separated the code and created a real abstract class `AbstractBrat` to allow to inherit from that (I\'ve just seen that the dataset_loader loads the first non abstract class), now `Brat` is very similar in its functionality to https://github.com/huggingface/nlp/tree/master/datasets/text but inherits from `AbstractBrat`.\r\n\r\nHowever, it is still not clear to me how to add a specific dataset (as explained in https://huggingface.co/nlp/add_dataset.html) to your repo that uses this format/abstract class, i.e. re-using the `features` entry of the  `DatasetInfo` object and `_generate_examples()`. Again, by doing so, the only remaining entries/functions to define would be `_DESCRIPTION`, `_CITATION`, `homepage` and `_URL` (which is all copy-paste stuff) and `_split_generators()`.\r\n \r\nIn a lack of better ideas, I tried sth like below, but of course it does not work outside `nlp` (`AbstractBrat` is currently defined in [datasets/brat.py](https://github.com/ArneBinder/nlp/blob/5e81fb8710546ee7be3353a7f02a3045e9a8351e/datasets/brat/brat.py)):\r\n```python\r\nfrom __future__ import absolute_import, division, print_function\r\n\r\nimport os\r\n\r\nimport nlp\r\n\r\nfrom datasets.brat.brat import AbstractBrat\r\n\r\n_CITATION = """"""\r\n@inproceedings{lauscher2018b,\r\n  title = {An argument-annotated corpus of scientific publications},\r\n  booktitle = {Proceedings of the 5th Workshop on Mining Argumentation},\r\n  publisher = {Association for Computational Linguistics},\r\n  author = {Lauscher, Anne and Glava\\v{s}, Goran and Ponzetto, Simone Paolo},\r\n  address = {Brussels, Belgium},\r\n  year = {2018},\r\n  pages = {40–46}\r\n}\r\n""""""\r\n\r\n_DESCRIPTION = """"""\\\r\nThis dataset is an extension of the Dr. Inventor corpus (Fisas et al., 2015, 2016) with an annotation layer containing \r\nfine-grained argumentative components and relations. It is the first argument-annotated corpus of scientific \r\npublications (in English), which allows for joint analyses of argumentation and other rhetorical dimensions of \r\nscientific writing.\r\n""""""\r\n\r\n_URL = ""http://data.dws.informatik.uni-mannheim.de/sci-arg/compiled_corpus.zip""\r\n\r\n\r\nclass Sciarg(AbstractBrat):\r\n\r\n    VERSION = nlp.Version(""1.0.0"")\r\n\r\n    def _info(self):\r\n\r\n        brat_features = super()._info().features\r\n        return nlp.DatasetInfo(\r\n            # This is the description that will appear on the datasets page.\r\n            description=_DESCRIPTION,\r\n            # nlp.features.FeatureConnectors\r\n            features=brat_features,\r\n            # If there\'s a common (input, target) tuple from the features,\r\n            # specify them here. They\'ll be used if as_supervised=True in\r\n            # builder.as_dataset.\r\n            #supervised_keys=None,\r\n            # Homepage of the dataset for documentation\r\n            homepage=""https://github.com/anlausch/ArguminSci"",\r\n            citation=_CITATION,\r\n        )\r\n\r\n    def _split_generators(self, dl_manager):\r\n        """"""Returns SplitGenerators.""""""\r\n        # TODO: Downloads the data and defines the splits\r\n        # dl_manager is a nlp.download.DownloadManager that can be used to\r\n        # download and extract URLs\r\n        dl_dir = dl_manager.download_and_extract(_URL)\r\n        data_dir = os.path.join(dl_dir, ""compiled_corpus"")\r\n        print(f\'data_dir: {data_dir}\')\r\n        return [\r\n            nlp.SplitGenerator(\r\n                name=nlp.Split.TRAIN,\r\n                # These kwargs will be passed to _generate_examples\r\n                gen_kwargs={\r\n                    ""directory"": data_dir,\r\n                },\r\n            ),\r\n        ]\r\n```  \r\n\r\nNevertheless, many thanks for tackling the dataset accessibility problem with this great library!'
 ""As temporary fix I've created [ArneBinder/nlp-formats](https://github.com/ArneBinder/nlp-formats) (contributions welcome).""]","I have written a generic dataset for corpora created with the Brat annotation tool ([specification](https://brat.nlplab.org/standoff.html), [dataset code](https://github.com/ArneBinder/nlp/blob/brat/datasets/brat/brat.py)). Now I wonder how to use that to create specific dataset instances. What's the recommended way to reuse formats and loading functionality for datasets with a common format?

In my case, it took a bit of time to create the Brat dataset and I think others would appreciate to not have to think about that again. Also, I assume there are other formats (e.g. conll) that are widely used, so having this would really ease dataset onboarding and adoption of the library."
https://github.com/huggingface/datasets/issues/426,"[FEATURE REQUEST] Multiprocessing with for dataset.map, dataset.filter","[""Yes that's definitely something we plan to add ^^""
 'Yes, that would be nice. We could take a look at what tensorflow `tf.data` does under the hood for instance.'
 'So `tf.data.Dataset.map()` returns a `ParallelMapDataset` if `num_parallel_calls is not None` [link](https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/data/ops/dataset_ops.py#L1623).\r\n\r\nThere, `num_parallel_calls` is turned into a tensor and and fed to `gen_dataset_ops.parallel_map_dataset` where it looks like tensorflow takes over.\r\n\r\nWe could start with something simple like a thread or process pool that `imap`s over some shards.\r\n '
 'Multiprocessing was added in #552 . You can set the number of processes with `.map(..., num_proc=...)`. It also works for `filter`\r\n\r\nClosing this one, but feel free to reo-open if you have other questions'
 '@lhoestq Great feature implemented! Do you have plans to add it to official tutorials [Processing data in a Dataset](https://huggingface.co/docs/datasets/processing.html?highlight=save#augmenting-the-dataset)? It took me sometime to find this parallel processing api.'
 'Thanks for the heads up !\r\n\r\nI just added a paragraph about multiprocessing:\r\nhttps://huggingface.co/docs/datasets/master/processing.html#multiprocessing']",It would be nice to be able to speed up `dataset.map` or `dataset.filter`. Perhaps this is as easy as sharding the dataset sending each shard to a process/thread/dask pool and using the new `nlp.concatenate_dataset()` function to join them all together?
https://github.com/huggingface/datasets/issues/425,Correct data structure for PAN-X task in XTREME dataset?,"['Thanks for noticing ! This looks more reasonable indeed.\r\nFeel free to open a PR'
 'Hi @lhoestq \r\nI made the proposed changes to the `xtreme.py` script. I noticed that I also need to change the schema in the `dataset_infos.json` file.  More specifically the `""features""` part of the PAN-X.LANG dataset:\r\n\r\n```json\r\n""features"":{\r\n   ""word"":{\r\n      ""dtype"":""string"",\r\n      ""id"":null,\r\n      ""_type"":""Value""\r\n   },\r\n   ""ner_tag"":{\r\n      ""dtype"":""string"",\r\n      ""id"":null,\r\n      ""_type"":""Value""\r\n   },\r\n   ""lang"":{\r\n      ""dtype"":""string"",\r\n      ""id"":null,\r\n      ""_type"":""Value""\r\n   }\r\n}\r\n```\r\nTo fit the code above the fields `""word""`, `""ner_tag""`, and `""lang""` would become `""words""`, `ner_tags""` and `""langs""`. In addition the `dtype` should be changed from `""string""` to `""list""`.\r\n\r\n I made this changes but when trying to test this locally with `dataset = load_dataset(""xtreme"", ""PAN-X.en"", data_dir=\'./data\')` I face the issue that the `dataset_info.json` file is always overwritten by a downloaded version with the old settings, which then throws an error because the schema does not match. This makes it hard to test the changes locally. Do you have any suggestions on how to deal with that?\r\n'
 'Hi !\r\n\r\nYou have to point to your local script.\r\nFirst clone the repo and then:\r\n\r\n```python\r\ndataset = load_dataset(""./datasets/xtreme"", ""PAN-X.en"")\r\n```\r\nThe ""xtreme"" directory contains ""xtreme.py"".\r\n\r\nYou also have to change the features definition in the `_info` method. You could use:\r\n\r\n```python\r\nfeatures = nlp.Features({\r\n    ""words"": [nlp.Value(""string"")],\r\n    ""ner_tags"": [nlp.Value(""string"")],\r\n    ""langs"": [nlp.Value(""string"")],\r\n})\r\n```\r\n\r\nHope this helps !\r\nLet me know if you have other questions.'
 ""Thanks, I am making progress. I got a new error `NonMatchingSplitsSizesError ` (see traceback below), which I suspect is due to the fact that number of rows in the dataset changed (one row per word --> one row per sentence) as well as the number of bytes due to the slightly updated data structure. \r\n\r\n```python\r\nNonMatchingSplitsSizesError: [{'expected': SplitInfo(name='validation', num_bytes=1756492, num_examples=80536, dataset_name='xtreme'), 'recorded': SplitInfo(name='validation', num_bytes=1837109, num_examples=10000, dataset_name='xtreme')}, {'expected': SplitInfo(name='test', num_bytes=1752572, num_examples=80326, dataset_name='xtreme'), 'recorded': SplitInfo(name='test', num_bytes=1833214, num_examples=10000, dataset_name='xtreme')}, {'expected': SplitInfo(name='train', num_bytes=3496832, num_examples=160394, dataset_name='xtreme'), 'recorded': SplitInfo(name='train', num_bytes=3658428, num_examples=20000, dataset_name='xtreme')}]\r\n```\r\nI can fix the error by replacing the values in the `datasets_infos.json` file, which I tested for English. However, to update this for all 40 datasets manually is slightly painful. Is there a better way to update the expected values for all datasets?""
 'You can update the json file by calling\r\n```\r\nnlp-cli test ./datasets/xtreme --save_infos --all_configs\r\n```'
 'One more thing about features. I mentioned\r\n\r\n```python\r\nfeatures = nlp.Features({\r\n    ""words"": [nlp.Value(""string"")],\r\n    ""ner_tags"": [nlp.Value(""string"")],\r\n    ""langs"": [nlp.Value(""string"")],\r\n})\r\n```\r\n\r\nbut it\'s actually not consistent with the way we write datasets. Something like this is simpler to read and more consistent with the way we define datasets:\r\n\r\n```python\r\nfeatures = nlp.Features({\r\n    ""words"": nlp.Sequence(nlp.Value(""string"")),\r\n    ""ner_tags"": nlp.Sequence(nlp.Value(""string"")),\r\n    ""langs"": nlp.Sequence(nlp.Value(""string"")),\r\n})\r\n```\r\n\r\nSorry about that'
 'Closing this since PR #437 fixed the problem and has been merged to `master`. ']","Hi 🤗  team!

## Description of the problem
Thanks to the fix from #416 I am now able to load the NER task in the XTREME dataset as follows:

```python
from nlp import load_dataset
# AmazonPhotos.zip is located in data/
dataset = load_dataset(""xtreme"", ""PAN-X.en"", data_dir='./data')
dataset_train = dataset['train']
```

However, I am not sure that `load_dataset()` is returning the correct data structure for NER. 

Currently, every row in `dataset_train` is of the form
```python
{'word': str, 'ner_tag': str, 'lang': str}
```
but I think we actually want something like
```python
{'words': List[str], 'ner_tags': List[str], 'langs': List[str]}
```
so that each row corresponds to a _sequence_ of words associated with each example. With the current data structure I do not think it is possible to transform `dataset_train` into a form suitable for training because we do not know the boundaries between examples.

Indeed, [this line](https://github.com/google-research/xtreme/blob/522434d1aece34131d997a97ce7e9242a51a688a/third_party/utils_tag.py#L58) in the XTREME repo, processes the texts as lists of sentences, tags, and languages.

## Proposed solution
Replace
```python
with open(filepath) as f:
    data = csv.reader(f, delimiter=""\t"", quoting=csv.QUOTE_NONE)
    for id_, row in enumerate(data):
        if row:
            lang, word = row[0].split("":"")[0], row[0].split("":"")[1]
            tag = row[1]
            yield id_, {""word"": word, ""ner_tag"": tag, ""lang"": lang}
```
from  [these lines](https://github.com/huggingface/nlp/blob/ce7d3a1d630b78fe27188d1706f3ea980e8eec43/datasets/xtreme/xtreme.py#L881-L887) of the `_generate_examples()` function with something like

```python
guid_index = 1
with open(filepath, encoding=""utf-8"") as f:
    words = []
    ner_tags = []
    langs = []
    for line in f:
        if line.startswith(""-DOCSTART-"") or line == """" or line == ""\n"":
            if words:
                yield guid_index, {""words"": words, ""ner_tags"": ner_tags, ""langs"": langs}
                guid_index += 1
                words = []
                ner_tags = []
        else:
            # pan-x data is tab separated
            splits = line.split(""\t"")
            # strip out en: prefix
            langs.append(splits[0][:2])
            words.append(splits[0][3:])
            if len(splits) > 1:
                labels.append(splits[-1].replace(""\n"", """"))
            else:
                # examples have no label in test set
                labels.append(""O"")
```
If you agree, me or @lvwerra would be happy to implement this and create a PR."
https://github.com/huggingface/datasets/issues/418,Addition of google drive links to dl_manager,"['I think the problem is the way you wrote your urls. Try the following structure to see `https://drive.google.com/uc?export=download&id=your_file_id` . \r\n\r\n@lhoestq  '
 'Oh sorry, I think `_get_drive_url` is doing that. \r\n\r\nHave you tried to use `dl_manager.download_and_extract(_get_drive_url(_TRAIN_URL)`? it should work with google drive links.\r\n'
 'Yes it worked, thank you!']","Hello there, I followed the template to create a download script of my own, which works fine for me, although I had to shun the dl_manager because it was downloading nothing from the drive links and instead use gdown.

This is the script for me:

```python
class EmoConfig(nlp.BuilderConfig):
    """"""BuilderConfig for SQUAD.""""""

    def __init__(self, **kwargs):
        """"""BuilderConfig for EmoContext.
    Args:
      **kwargs: keyword arguments forwarded to super.
    """"""
        super(EmoConfig, self).__init__(**kwargs)

_TEST_URL = ""https://drive.google.com/file/d/1Hn5ytHSSoGOC4sjm3wYy0Dh0oY_oXBbb/view?usp=sharing""
_TRAIN_URL = ""https://drive.google.com/file/d/12Uz59TYg_NtxOy7SXraYeXPMRT7oaO7X/view?usp=sharing""

class EmoDataset(nlp.GeneratorBasedBuilder):
    """""" SemEval-2019 Task 3: EmoContext Contextual Emotion Detection in Text. Version 1.0.0 """"""

    VERSION = nlp.Version(""1.0.0"")
    force = False

    def _info(self):
        return nlp.DatasetInfo(
            description=_DESCRIPTION,
            features=nlp.Features(
                {
                    ""text"": nlp.Value(""string""),
                    ""label"": nlp.features.ClassLabel(names=[""others"", ""happy"", ""sad"", ""angry""]),
                }
            ),
            supervised_keys=None,
            homepage=""https://www.aclweb.org/anthology/S19-2005/"",
            citation=_CITATION,
        )
    
    def _get_drive_url(self, url):
        base_url = 'https://drive.google.com/uc?id='
        split_url = url.split('/')
        return base_url + split_url[5]
    
    def _split_generators(self, dl_manager):
        """"""Returns SplitGenerators.""""""
        if(not os.path.exists(""emo-train.json"") or self.force):
            gdown.download(self._get_drive_url(_TRAIN_URL), ""emo-train.json"", quiet = True)
        if(not os.path.exists(""emo-test.json"") or self.force):
            gdown.download(self._get_drive_url(_TEST_URL), ""emo-test.json"", quiet = True)
        return [
            nlp.SplitGenerator(
                name=nlp.Split.TRAIN,
                gen_kwargs={
                    ""filepath"": ""emo-train.json"",
                    ""split"": ""train"",
                },
            ),
            nlp.SplitGenerator(
                name=nlp.Split.TEST,
                gen_kwargs={""filepath"": ""emo-test.json"", ""split"": ""test""},
            ),
        ]

    def _generate_examples(self, filepath, split):
        """""" Yields examples. """"""
        with open(filepath, 'rb') as f:
            data = json.load(f)
            for id_, text, label in zip(data[""text""].keys(), data[""text""].values(), data[""Label""].values()):
                yield id_, {
                    ""text"": text,
                    ""label"": label,
                }
```

Can someone help me in adding gdrive links to be used with default dl_manager or adding gdown as another dl_manager, because I'd like to add this dataset to nlp's official database."
https://github.com/huggingface/datasets/issues/414,from_dict delete?,"[""`from_dict` was added in #350 that was unfortunately not included in the 0.3.0 release. It's going to be included in the next release that will be out pretty soon though.\r\nRight now if you want to use `from_dict` you have to install the package from the master branch\r\n```\r\npip install git+https://github.com/huggingface/nlp.git\r\n```""
 ""> `from_dict` was added in #350 that was unfortunately not included in the 0.3.0 release. It's going to be included in the next release that will be out pretty soon though.\r\n> Right now if you want to use `from_dict` you have to install the package from the master branch\r\n> \r\n> ```\r\n> pip install git+https://github.com/huggingface/nlp.git\r\n> ```\r\nOK, thank you.\r\n""]",AttributeError: type object 'Dataset' has no attribute 'from_dict'
https://github.com/huggingface/datasets/issues/413,Is there a way to download only NQ dev?,"[""Unfortunately it's not possible to download only the dev set of NQ.\r\n\r\nI think we could add a way to download only the test set by adding a custom configuration to the processing script though.""
 ""Ok, got it. I think this could be a valuable feature - especially for large datasets like NQ, but potentially also others. \r\nFor us, it will in this case make the difference of using the library or keeping the old downloads of the raw dev datasets.  \r\nHowever, I don't know if that fits into your plans with the library and can also understand if you don't want to support this.""
 ""I don't think we could force this behavior generally since the dataset script authors are free to organize the file download as they want (sometimes the mapping between split and files can be very much nontrivial) but we can add an additional configuration for Natural Question indeed as @lhoestq indicate.""]","Maybe I missed that in the docs, but is there a way to only download the dev set of natural questions (~1 GB)? 
As we want to benchmark QA models on different datasets, I would like to avoid downloading the 41GB of training data. 

I tried
```
dataset = nlp.load_dataset('natural_questions', split=""validation"", beam_runner=""DirectRunner"")
```
But this still triggered a big download of presumably the whole dataset. Is there any way of doing this or are splits / slicing options only available after downloading?

Thanks!"
https://github.com/huggingface/datasets/issues/412,Unable to load XTREME dataset from disk,"['Hi @lewtun, you have to provide the full path to the downloaded file for example `/home/lewtum/..`'
 'I was able to repro. Opening a PR to fix that.\r\nThanks for reporting this issue !'
 'Thanks for the rapid fix @lhoestq!']","Hi 🤗  team!

## Description of the problem
Following the [docs](https://huggingface.co/nlp/loading_datasets.html?highlight=xtreme#manually-downloading-files) I'm trying to load the `PAN-X.fr` dataset from the [XTREME](https://github.com/google-research/xtreme) benchmark.

I have manually downloaded the `AmazonPhotos.zip` file from [here](https://www.amazon.com/clouddrive/share/d3KGCRCIYwhKJF0H3eWA26hjg2ZCRhjpEQtDL70FSBN?_encoding=UTF8&%2AVersion%2A=1&%2Aentries%2A=0&mgh=1) and am running into a `FileNotFoundError` when I point to the location of the dataset.

As far as I can tell, the problem is that `AmazonPhotos.zip` decompresses to `panx_dataset` and `load_dataset()` is not looking in the correct path:

```
# path where load_dataset is looking for fr.tar.gz
/root/.cache/huggingface/datasets/9b8c4f1578e45cb2539332c79738beb3b54afbcd842b079cabfd79e3ed6704f6/
# path where it actually exists
/root/.cache/huggingface/datasets/9b8c4f1578e45cb2539332c79738beb3b54afbcd842b079cabfd79e3ed6704f6/panx_dataset/
```

## Steps to reproduce the problem

1. Manually download the XTREME benchmark from [here](https://www.amazon.com/clouddrive/share/d3KGCRCIYwhKJF0H3eWA26hjg2ZCRhjpEQtDL70FSBN?_encoding=UTF8&%2AVersion%2A=1&%2Aentries%2A=0&mgh=1)

2. Run the following code snippet
```python
from nlp import load_dataset
# AmazonPhotos.zip is in the root of the folder
dataset = load_dataset(""xtreme"", ""PAN-X.fr"", data_dir='./')
```

3. Here is the stack trace
```
---------------------------------------------------------------------------
FileNotFoundError                         Traceback (most recent call last)
<ipython-input-4-26786bb5fa93> in <module>
----> 1 dataset = load_dataset(""xtreme"", ""PAN-X.fr"", data_dir='./')

/usr/local/lib/python3.6/dist-packages/nlp/load.py in load_dataset(path, name, version, data_dir, data_files, split, cache_dir, download_config, download_mode, ignore_verifications, save_infos, **config_kwargs)
    522         download_mode=download_mode,
    523         ignore_verifications=ignore_verifications,
--> 524         save_infos=save_infos,
    525     )
    526 

/usr/local/lib/python3.6/dist-packages/nlp/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, save_infos, try_from_hf_gcs, dl_manager, **download_and_prepare_kwargs)
    430                 verify_infos = not save_infos and not ignore_verifications
    431                 self._download_and_prepare(
--> 432                     dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
    433                 )
    434                 # Sync info

/usr/local/lib/python3.6/dist-packages/nlp/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)
    464         split_dict = SplitDict(dataset_name=self.name)
    465         split_generators_kwargs = self._make_split_generators_kwargs(prepare_split_kwargs)
--> 466         split_generators = self._split_generators(dl_manager, **split_generators_kwargs)
    467         # Checksums verification
    468         if verify_infos:

/usr/local/lib/python3.6/dist-packages/nlp/datasets/xtreme/b8c2ed3583a7a7ac60b503576dfed3271ac86757628897e945bd329c43b8a746/xtreme.py in _split_generators(self, dl_manager)
    725             panx_dl_dir = dl_manager.extract(panx_path)
    726             lang = self.config.name.split(""."")[1]
--> 727             lang_folder = dl_manager.extract(os.path.join(panx_dl_dir, lang + "".tar.gz""))
    728             return [
    729                 nlp.SplitGenerator(

/usr/local/lib/python3.6/dist-packages/nlp/utils/download_manager.py in extract(self, path_or_paths)
    196         """"""
    197         return map_nested(
--> 198             lambda path: cached_path(path, extract_compressed_file=True, force_extract=False), path_or_paths,
    199         )
    200 

/usr/local/lib/python3.6/dist-packages/nlp/utils/py_utils.py in map_nested(function, data_struct, dict_only, map_tuple)
    170                 return tuple(mapped)
    171     # Singleton
--> 172     return function(data_struct)
    173 
    174 

/usr/local/lib/python3.6/dist-packages/nlp/utils/download_manager.py in <lambda>(path)
    196         """"""
    197         return map_nested(
--> 198             lambda path: cached_path(path, extract_compressed_file=True, force_extract=False), path_or_paths,
    199         )
    200 

/usr/local/lib/python3.6/dist-packages/nlp/utils/file_utils.py in cached_path(url_or_filename, download_config, **download_kwargs)
    203     elif urlparse(url_or_filename).scheme == """":
    204         # File, but it doesn't exist.
--> 205         raise FileNotFoundError(""Local file {} doesn't exist"".format(url_or_filename))
    206     else:
    207         # Something unknown

FileNotFoundError: Local file /root/.cache/huggingface/datasets/9b8c4f1578e45cb2539332c79738beb3b54afbcd842b079cabfd79e3ed6704f6/fr.tar.gz doesn't exist
```

## OS and hardware
```
- `nlp` version: 0.3.0
- Platform: Linux-4.15.0-72-generic-x86_64-with-Ubuntu-18.04-bionic
- Python version: 3.6.9
- PyTorch version (GPU?): 1.4.0 (True)
- Tensorflow version (GPU?): 2.1.0 (True)
- Using GPU in script?: <fill in>
- Using distributed or parallel set-up in script?: <fill in>
```"
https://github.com/huggingface/datasets/issues/409,train_test_split error: 'dict' object has no attribute 'deepcopy',"['It was fixed in 2ddd18d139d3047c9c3abe96e1e7d05bb360132c.\r\nCould you pull the latest changes from master @morganmcg1 ?'
 'Thanks @lhoestq, works fine now!']","`train_test_split` is giving me an error when I try and call it:

`'dict' object has no attribute 'deepcopy'`

## To reproduce

```
dataset = load_dataset('glue', 'mrpc', split='train')
dataset = dataset.train_test_split(test_size=0.2)
```

## Full Stacktrace
```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-12-feb740dbec9a> in <module>
      1 dataset = load_dataset('glue', 'mrpc', split='train')
----> 2 dataset = dataset.train_test_split(test_size=0.2)

~/anaconda3/envs/fastai2_me/lib/python3.7/site-packages/nlp/arrow_dataset.py in train_test_split(self, test_size, train_size, shuffle, seed, generator, keep_in_memory, load_from_cache_file, train_cache_file_name, test_cache_file_name, writer_batch_size)
   1032                     ""writer_batch_size"": writer_batch_size,
   1033                 }
-> 1034                 train_kwargs = cache_kwargs.deepcopy()
   1035                 train_kwargs[""split""] = ""train""
   1036                 test_kwargs = cache_kwargs.deepcopy()

AttributeError: 'dict' object has no attribute 'deepcopy'
```"
https://github.com/huggingface/datasets/issues/407,MissingBeamOptions for Wikipedia 20200501.en,"['Fixed. Could you try again @mitchellgordon95 ?\r\nIt was due a file not being updated on S3.\r\n\r\nWe need to make sure all the datasets scripts get updated properly @julien-c '
 'Works for me! Thanks.'
 'I found the same issue with almost any language other than English. (For English, it works). Will someone need to update the file on S3 again?'
 ""This is because only some languages are already preprocessed (en, de, fr, it) and stored on our google storage.\r\nWe plan to have a systematic way to preprocess more wikipedia languages in the future.\r\n\r\nFor the other languages you have to process them on your side using apache beam. That's why the lib asks for a Beam runner.""]","There may or may not be a regression for the pre-processed Wikipedia dataset. This was working fine 10 commits ago (without having Apache Beam available):

```
nlp.load_dataset('wikipedia', ""20200501.en"", split='train')
```

And now, having pulled master, I get:

```
Downloading and preparing dataset wikipedia/20200501.en (download: 16.99 GiB, generated: 17.07 GiB, total: 34.06 GiB) to /home/hltcoe/mgordon/.cache/huggingface/datasets/wikipedia/20200501.en/1.0.0/76b0b2747b679bb0ee7a1621e50e5a6378477add0c662668a324a5bc07d516dd...
Traceback (most recent call last):
  File ""scripts/download.py"", line 11, in <module>
    fire.Fire(download_pretrain)
  File ""/home/hltcoe/mgordon/.conda/envs/huggingface/lib/python3.6/site-packages/fire/core.py"", line 138, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/hltcoe/mgordon/.conda/envs/huggingface/lib/python3.6/site-packages/fire/core.py"", line 468, in _Fire
    target=component.__name__)
  File ""/home/hltcoe/mgordon/.conda/envs/huggingface/lib/python3.6/site-packages/fire/core.py"", line 672, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""scripts/download.py"", line 6, in download_pretrain
    nlp.load_dataset('wikipedia', ""20200501.en"", split='train')
  File ""/exp/mgordon/nlp/src/nlp/load.py"", line 534, in load_dataset
    save_infos=save_infos,
  File ""/exp/mgordon/nlp/src/nlp/builder.py"", line 460, in download_and_prepare
    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
  File ""/exp/mgordon/nlp/src/nlp/builder.py"", line 870, in _download_and_prepare
    ""\n\t`{}`"".format(usage_example)
nlp.builder.MissingBeamOptions: Trying to generate a dataset using Apache Beam, yet no Beam Runner or PipelineOptions() has been provided in `load_dataset` or in the builder arguments. For big datasets it has to run on large-scale data processing tools like Dataflow, S
park, etc. More information about Apache Beam runners at https://beam.apache.org/documentation/runners/capability-matrix/
If you really want to run it locally because you feel like the Dataset is small enough, you can use the local beam runner called `DirectRunner` (you may run out of memory).
Example of usage:
        `load_dataset('wikipedia', '20200501.en', beam_runner='DirectRunner')`
```"
https://github.com/huggingface/datasets/issues/406,Faster Shuffling?,"['I think the slowness here probably come from the fact that we are copying from and to python.\r\n\r\n@lhoestq for all the `select`-based methods I think we should stay in Arrow format and update the writer so that it can accept Arrow tables or batches as well. What do you think?'
 ""> @lhoestq for all the `select`-based methods I think we should stay in Arrow format and update the writer so that it can accept Arrow tables or batches as well. What do you think?\r\n\r\nI just tried with `writer.write_table` with tables of 1000 elements and it's slower that the solution in #405 \r\n\r\nOn my side (select 10 000 examples):\r\n- Original implementation: 12s\r\n- Batched solution: 100ms\r\n- solution using arrow tables: 350ms\r\n\r\nI'll try with arrays and record batches to see if we can make it work.""
 'I tried using `.take` from pyarrow recordbatches but it doesn\'t improve the speed that much:\r\n```python\r\nimport nlp\r\nimport numpy as np\r\n\r\ndset = nlp.Dataset.from_file(""dummy_test_select.arrow"")  # dummy dataset with 100000 examples like {""a"": ""h""*512}\r\nindices = np.random.randint(0, 100_000, 1000_000)\r\n```\r\n\r\n```python\r\n%%time\r\nbatch_size = 10_000\r\nwriter = ArrowWriter(schema=dset.schema, path=""dummy_path"",\r\n                     writer_batch_size=1000, disable_nullable=False)\r\nfor i in tqdm(range(0, len(indices), batch_size)):\r\n    table = pa.concat_tables(dset._data.slice(int(i), 1) for i in indices[i : min(len(indices), i + batch_size)])\r\n    batch = table.to_pydict()\r\n    writer.write_batch(batch)\r\nwriter.finalize()\r\n# 9.12s\r\n```\r\n\r\n\r\n```python\r\n%%time\r\nbatch_size = 10_000\r\nwriter = ArrowWriter(schema=dset.schema, path=""dummy_path"", \r\n                     writer_batch_size=1000, disable_nullable=False)\r\nfor i in tqdm(range(0, len(indices), batch_size)):\r\n    batch_indices = indices[i : min(len(indices), i + batch_size)]\r\n    # First, extract only the indices that we need with a mask\r\n    mask = [False] * len(dset)\r\n    for k in batch_indices:\r\n        mask[k] = True\r\n    t_batch = dset._data.filter(pa.array(mask))\r\n    # Second, build the list of indices for the filtered table, and taking care of duplicates\r\n    rev_positions = {}\r\n    duplicates = 0\r\n    for i, j in enumerate(sorted(batch_indices)):\r\n        if j in rev_positions:\r\n            duplicates += 1\r\n        else:\r\n            rev_positions[j] = i - duplicates\r\n    rev_map = [rev_positions[j] for j in batch_indices]\r\n    # Third, use `.take` from the combined recordbatch\r\n    t_combined = t_batch.combine_chunks()  # load in memory\r\n    recordbatch = t_combined.to_batches()[0]\r\n    table = pa.Table.from_arrays(\r\n        [recordbatch[c].take(pa.array(rev_map)) for c in range(len(dset._data.column_names))],\r\n        schema=writer.schema\r\n    )\r\n    writer.write_table(table)\r\nwriter.finalize()\r\n# 3.2s\r\n```\r\n'
 'Shuffling is now significantly faster thanks to #513 \r\nFeel free to play with it now :)\r\n\r\nClosing this one, but feel free to re-open if you have other questions']","Consider shuffling bookcorpus:

```
dataset = nlp.load_dataset('bookcorpus', split='train')
dataset.shuffle()
```
According to tqdm, this will take around 2.5 hours on my machine to complete (even with the faster version of select from #405). I've also tried with `keep_in_memory=True` and `writer_batch_size=1000`.

But I can also just write the lines to a text file:

```
batch_size = 100000
with open('tmp.txt', 'w+') as out_f:
    for i in tqdm(range(0, len(dataset), batch_size)):
        batch = dataset[i:i+batch_size]['text']
        print(""\n"".join(batch), file=out_f)
```

Which completes in a couple minutes, followed by `shuf tmp.txt > tmp2.txt` which completes in under a minute. And finally,

```
dataset = nlp.load_dataset('text', data_files='tmp2.txt')
```

Which completes in under 10 minutes. I read up on Apache Arrow this morning, and it seems like the columnar data format is not especially well-suited to shuffling rows, since moving items around requires a lot of book-keeping. 

Is shuffle inherently slow, or am I just using it wrong? And if it is slow, would it make sense to try converting the data to a row-based format on disk and then shuffling? (Instead of calling select with a random permutation, as is currently done.)"
https://github.com/huggingface/datasets/issues/388,"🐛 [Dataset] Cannot download wmt14, wmt15 and wmt17","[""similar slow download speed here for nlp.load_dataset('wmt14', 'fr-en')\r\n`\r\nDownloading: 100%|██████████████████████████████████████████████████████████| 658M/658M [1:00:42<00:00, 181kB/s]\r\nDownloading: 100%|██████████████████████████████████████████████████████████| 918M/918M [1:39:38<00:00, 154kB/s]\r\nDownloading:   2%|▉                                                       | 40.9M/2.37G [04:48<5:03:06, 128kB/s]\r\n`\r\nCould we just download a specific subdataset in 'wmt14', such as 'newstest14'? ""
 ""> The code runs but the download speed is extremely slow, the same behaviour is not observed on wmt16 and wmt18\r\n\r\nThe original source for the files may provide slow download speeds.\r\nWe can probably host these files ourselves.\r\n\r\n> When trying to download wmt17 zh-en, I got the following error:\r\n> ConnectionError: Couldn't reach https://storage.googleapis.com/tfdataset-data/downloadataset/uncorpus/UNv1.0.en-zh.tar.gz\r\n\r\nLooks like the file`UNv1.0.en-zh.tar.gz` is missing, or the url changed. We need to fix that\r\n\r\n> Could we just download a specific subdataset in 'wmt14', such as 'newstest14'?\r\n\r\nRight now I don't think it's possible. Maybe @patrickvonplaten knows more about it\r\n""
 'Yeah, the download speed is sadly always extremely slow :-/. \r\nI will try to check out the `wmt17 zh-en` bug :-) '
 'Maybe this can be used - https://stuncorpusprod.blob.core.windows.net/corpusfiles/UNv1.0.en-zh.tar.gz.00 ']","1. I try downloading `wmt14`, `wmt15`, `wmt17`, `wmt19` with the following code:
```
nlp.load_dataset('wmt14','de-en')
nlp.load_dataset('wmt15','de-en')
nlp.load_dataset('wmt17','de-en')
nlp.load_dataset('wmt19','de-en')
```
The code runs but the download speed is **extremely slow**, the same behaviour is not observed on `wmt16` and `wmt18`

2. When trying to download `wmt17 zh-en`, I got the following error:
> ConnectionError: Couldn't reach https://storage.googleapis.com/tfdataset-data/downloadataset/uncorpus/UNv1.0.en-zh.tar.gz"
https://github.com/huggingface/datasets/issues/387,Conversion through to_pandas output numpy arrays for lists instead of python objects,"[""To convert from arrow type we have three options: to_numpy, to_pandas and to_pydict/to_pylist.\r\n\r\n- to_numpy and to_pandas return numpy arrays instead of lists but are very fast.\r\n- to_pydict/to_pylist can be 100x slower and become the bottleneck for reading data, but at least they return lists.\r\n\r\nMaybe we can have to_pydict/to_pylist as the default and use to_numpy or to_pandas when the format (set by `set_format`) is 'numpy' or 'pandas'""]","In a related question, the conversion through to_pandas output numpy arrays for the lists instead of python objects.

Here is an example:
```python
>>> dataset._data.slice(key, 1).to_pandas().to_dict(""list"")
{'sentence1': ['Amrozi accused his brother , whom he called "" the witness "" , of deliberately distorting his evidence .'], 'sentence2': ['Referring to him as only "" the witness "" , Amrozi accused his brother of deliberately distorting his evidence .'], 'label': [1], 'idx': [0], 'input_ids': [array([  101,  7277,  2180,  5303,  4806,  1117,  1711,   117,  2292,
        1119,  1270,   107,  1103,  7737,   107,   117,  1104,  9938,
        4267, 12223, 21811,  1117,  2554,   119,   102])], 'token_type_ids': [array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0])], 'attention_mask': [array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1])]}
>>> type(dataset._data.slice(key, 1).to_pandas().to_dict(""list"")['input_ids'][0])
<class 'numpy.ndarray'>
>>> dataset._data.slice(key, 1).to_pydict()
{'sentence1': ['Amrozi accused his brother , whom he called "" the witness "" , of deliberately distorting his evidence .'], 'sentence2': ['Referring to him as only "" the witness "" , Amrozi accused his brother of deliberately distorting his evidence .'], 'label': [1], 'idx': [0], 'input_ids': [[101, 7277, 2180, 5303, 4806, 1117, 1711, 117, 2292, 1119, 1270, 107, 1103, 7737, 107, 117, 1104, 9938, 4267, 12223, 21811, 1117, 2554, 119, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}
```"
https://github.com/huggingface/datasets/issues/378,[dataset] Structure of MLQA seems unecessary nested,"['Same for the RACE dataset: https://github.com/huggingface/nlp/blob/master/datasets/race/race.py\r\n\r\nShould we scan all the datasets to remove this pattern of un-necessary nesting?'
 ""You're right, I think we don't need to use the nested dictionary. \r\n""]","The features of the MLQA dataset comprise several nested dictionaries with a single element inside (for `questions` and `ids`): https://github.com/huggingface/nlp/blob/master/datasets/mlqa/mlqa.py#L90-L97

Should we keep this @mariamabarham @patrickvonplaten? Was this added for compatibility with tfds?

```python
            features=nlp.Features(
                {
                    ""context"": nlp.Value(""string""),
                    ""questions"": nlp.features.Sequence({""question"": nlp.Value(""string"")}),
                    ""answers"": nlp.features.Sequence(
                        {""text"": nlp.Value(""string""), ""answer_start"": nlp.Value(""int32""),}
                    ),
                    ""ids"": nlp.features.Sequence({""idx"": nlp.Value(""string"")})
```"
https://github.com/huggingface/datasets/issues/376,to_pandas conversion doesn't always work,"['**Edit**: other topic previously in this message moved to a new issue: https://github.com/huggingface/nlp/issues/387'
 ""Could you try to update pyarrow to >=0.17.0 ? It should fix the `to_pandas` bug\r\n\r\nAlso I'm not sure that structures like list<struct> are fully supported in the lib (none of the datasets use that).\r\nIt can cause issues when using dataset transforms like `filter` for example""]","For some complex nested types, the conversion from Arrow to python dict through pandas doesn't seem to be possible.

Here is an example using the official SQUAD v2 JSON file.

This example was found while investigating #373.

```python
>>> squad = load_dataset('json', data_files={nlp.Split.TRAIN: [""./train-v2.0.json""]}, download_mode=nlp.GenerateMode.FORCE_REDOWNLOAD, version=""1.0.0"", field='data')
>>> squad['train']
Dataset(schema: {'title': 'string', 'paragraphs': 'list<item: struct<qas: list<item: struct<question: string, id: string, answers: list<item: struct<text: string, answer_start: int64>>, is_impossible: bool, plausible_answers: list<item: struct<text: string, answer_start: int64>>>>, context: string>>'}, num_rows: 442)
>>> squad['train'][0]
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/thomwolf/Documents/GitHub/datasets/src/nlp/arrow_dataset.py"", line 589, in __getitem__
    format_kwargs=self._format_kwargs,
  File ""/Users/thomwolf/Documents/GitHub/datasets/src/nlp/arrow_dataset.py"", line 529, in _getitem
    outputs = self._unnest(self._data.slice(key, 1).to_pandas().to_dict(""list""))
  File ""pyarrow/array.pxi"", line 559, in pyarrow.lib._PandasConvertible.to_pandas
  File ""pyarrow/table.pxi"", line 1367, in pyarrow.lib.Table._to_pandas
  File ""/Users/thomwolf/miniconda2/envs/datasets/lib/python3.7/site-packages/pyarrow/pandas_compat.py"", line 766, in table_to_blockmanager
    blocks = _table_to_blocks(options, table, categories, ext_columns_dtypes)
  File ""/Users/thomwolf/miniconda2/envs/datasets/lib/python3.7/site-packages/pyarrow/pandas_compat.py"", line 1101, in _table_to_blocks
    list(extension_columns.keys()))
  File ""pyarrow/table.pxi"", line 881, in pyarrow.lib.table_to_blocks
  File ""pyarrow/error.pxi"", line 105, in pyarrow.lib.check_status
pyarrow.lib.ArrowNotImplementedError: Not implemented type for Arrow list to pandas: struct<qas: list<item: struct<question: string, id: string, answers: list<item: struct<text: string, answer_start: int64>>, is_impossible: bool, plausible_answers: list<item: struct<text: string, answer_start: int64>>>>, context: string>
```

cc @lhoestq would we have a way to detect this from the schema maybe?

Here is the schema for this pretty complex JSON:
```python
>>> squad['train'].schema
title: string
paragraphs: list<item: struct<qas: list<item: struct<question: string, id: string, answers: list<item: struct<text: string, answer_start: int64>>, is_impossible: bool, plausible_answers: list<item: struct<text: string, answer_start: int64>>>>, context: string>>
  child 0, item: struct<qas: list<item: struct<question: string, id: string, answers: list<item: struct<text: string, answer_start: int64>>, is_impossible: bool, plausible_answers: list<item: struct<text: string, answer_start: int64>>>>, context: string>
      child 0, qas: list<item: struct<question: string, id: string, answers: list<item: struct<text: string, answer_start: int64>>, is_impossible: bool, plausible_answers: list<item: struct<text: string, answer_start: int64>>>>
          child 0, item: struct<question: string, id: string, answers: list<item: struct<text: string, answer_start: int64>>, is_impossible: bool, plausible_answers: list<item: struct<text: string, answer_start: int64>>>
              child 0, question: string
              child 1, id: string
              child 2, answers: list<item: struct<text: string, answer_start: int64>>
                  child 0, item: struct<text: string, answer_start: int64>
                      child 0, text: string
                      child 1, answer_start: int64
              child 3, is_impossible: bool
              child 4, plausible_answers: list<item: struct<text: string, answer_start: int64>>
                  child 0, item: struct<text: string, answer_start: int64>
                      child 0, text: string
                      child 1, answer_start: int64
      child 1, context: string
```"
https://github.com/huggingface/datasets/issues/375,TypeError when computing bertscore,"['I am not able to reproduce this issue on my side.\r\nCould you give us more details about the inputs you used ?\r\n\r\nI do get another error though:\r\n```\r\n~/.virtualenvs/hf-datasets/lib/python3.7/site-packages/bert_score/utils.py in bert_cos_score_idf(model, refs, hyps, tokenizer, idf_dict, verbose, batch_size, device, all_layers)\r\n    371         return sorted(list(set(l)), key=lambda x: len(x.split("" "")))\r\n    372 \r\n--> 373     sentences = dedup_and_sort(refs + hyps)\r\n    374     embs = []\r\n    375     iter_range = range(0, len(sentences), batch_size)\r\n\r\nValueError: operands could not be broadcast together with shapes (0,) (2,)\r\n```\r\nThat\'s because it gets numpy arrays as input and not lists. See #387 '
 'The other issue was fixed by #403 \r\n\r\nDo you still get this issue @willywsm1013 ?\r\n']","Hi, 

I installed nlp 0.3.0 via pip, and my python version is 3.7.
When I tried to compute bertscore with the code:
```
import nlp 
bertscore = nlp.load_metric('bertscore')  
# load hyps and refs 
...
print (bertscore.compute(hyps, refs, lang='en'))
```

I got the following error.
```
Traceback (most recent call last):
  File ""bert_score_evaluate.py"", line 16, in <module>
    print (bertscore.compute(hyps, refs, lang='en'))
  File ""/home/willywsm/anaconda3/envs/torcher/lib/python3.7/site-packages/nlp/metric.py"", line 200, in compute
    output = self._compute(predictions=predictions, references=references, **metrics_kwargs)
  File ""/home/willywsm/anaconda3/envs/torcher/lib/python3.7/site-packages/nlp/metrics/bertscore/fb176889831bf0ce995ed197edc94b2e9a83f647a869bb8c9477dbb2d04d0f08/bertscore.py"", line 105, in _compute
    hashcode = bert_score.utils.get_hash(model_type, num_layers, idf, rescale_with_baseline)
TypeError: get_hash() takes 3 positional arguments but 4 were given
```

It seems like there is something wrong with get_hash() function?"
https://github.com/huggingface/datasets/issues/373,Segmentation fault when loading local JSON dataset as of #372,"['I\'ve seen this sort of thing before -- it might help to delete the directory -- I\'ve also noticed that there is an error with the json Dataloader for any data I\'ve tried to load. I\'ve replaced it with this, which skips over the data feature population step:\r\n\r\n\r\n```python\r\nimport os\r\n\r\nimport pyarrow.json as paj\r\n\r\nimport nlp as hf_nlp\r\n\r\nfrom nlp import DatasetInfo, BuilderConfig, SplitGenerator, Split, utils\r\nfrom nlp.arrow_writer import ArrowWriter\r\n\r\n\r\nclass JSONDatasetBuilder(hf_nlp.ArrowBasedBuilder):\r\n    BUILDER_CONFIG_CLASS = BuilderConfig\r\n\r\n    def _info(self):\r\n        return DatasetInfo()\r\n\r\n    def _split_generators(self, dl_manager):\r\n        """""" We handle string, list and dicts in datafiles\r\n        """"""\r\n        if isinstance(self.config.data_files, (str, list, tuple)):\r\n            files = self.config.data_files\r\n            if isinstance(files, str):\r\n                files = [files]\r\n            return [SplitGenerator(name=Split.TRAIN, gen_kwargs={""files"": files})]\r\n        splits = []\r\n        for split_name in [Split.TRAIN, Split.VALIDATION, Split.TEST]:\r\n            if split_name in self.config.data_files:\r\n                files = self.config.data_files[split_name]\r\n                if isinstance(files, str):\r\n                    files = [files]\r\n                splits.append(SplitGenerator(name=split_name, gen_kwargs={""files"": files}))\r\n        return splits\r\n\r\n    def _prepare_split(self, split_generator):\r\n        fname = ""{}-{}.arrow"".format(self.name, split_generator.name)\r\n        fpath = os.path.join(self._cache_dir, fname)\r\n\r\n        writer = ArrowWriter(path=fpath)\r\n\r\n        generator = self._generate_tables(**split_generator.gen_kwargs)\r\n        for key, table in utils.tqdm(generator, unit="" tables"", leave=False):\r\n            writer.write_table(table)\r\n        num_examples, num_bytes = writer.finalize()\r\n\r\n        split_generator.split_info.num_examples = num_examples\r\n        split_generator.split_info.num_bytes = num_bytes\r\n\r\n    def _generate_tables(self, files):\r\n        for i, file in enumerate(files):\r\n            pa_table = paj.read_json(\r\n                file\r\n            )\r\n            yield i, pa_table\r\n\r\n```'
 ""Yes, deleting the directory solves the error whenever I try to rerun.\r\n\r\nBy replacing the json-loader, you mean the cached file in my `site-packages` directory? e.g. `/home/XXX/.cache/lib/python3.7/site-packages/nlp/datasets/json/(...)/json.py` \r\n\r\nWhen I was testing this out before the #372 PR was merged I had issues installing it properly locally. Since the `json.py` script was downloaded instead of actually using the one provided in the local install. Manually updating that file seemed to solve it, but it didn't seem like a proper solution. Especially when having to run this on a remote compute cluster with no access to that directory.""
 'I see, diving in the JSON file for SQuAD it\'s a pretty complex structure.\r\n\r\nThe best solution for you, if you have a dataset really similar to SQuAD would be to copy and modify the SQuAD data processing script. We will probably add soon an option to be able to specify file path to use instead of the automatic URL encoded in the script but in the meantime you can:\r\n- copy the [squad script](https://github.com/huggingface/nlp/blob/master/datasets/squad/squad.py)  in a new script for your dataset\r\n- in the new script replace [these `urls_to_download `](https://github.com/huggingface/nlp/blob/master/datasets/squad/squad.py#L99-L102) by `urls_to_download=self.config.data_files`\r\n- load the dataset with `dataset = load_dataset(\'path/to/your/new/script\', data_files={nlp.Split.TRAIN: ""./datasets/train-v2.0.json""})`\r\n\r\nThis way you can reuse all the processing logic of the SQuAD loading script.'
 ""This seems like a more sensible solution! Thanks, @thomwolf. It's been a little daunting to understand what these scripts actually do, due to the level of abstraction and central documentation.\r\n\r\nAm I correct in assuming that the `_generate_examples()` function is the actual procedure for how the data is loaded from file? Meaning that essentially with a file containing another format, that is the only function that requires re-implementation? I'm working with a lot of datasets that, due to licensing and privacy, cannot be published. As this library is so neatly integrated with the transformers library and gives easy access to public sets such as SQUAD and increased performance, it is very neat to be able to load my private sets as well. As of now, I have just been working on scripts for translating all my data into the SQUAD-format before using the json script, but I see that it might not be necessary after all. ""
 ""Yes `_generate_examples()` is the main entry point. If you change the shape of the returned dictionary you also need to update the `features` in the `_info`.\r\n\r\nI'm currently writing the doc so it should be easier soon to use the library and know how to add your datasets.\r\n""
 'Could you try to update pyarrow to >=0.17.0 @vegarab ?\r\nI don\'t have any segmentation fault with my version of pyarrow (0.17.1)\r\n\r\nI tested with\r\n```python\r\nimport nlp\r\ns = nlp.load_dataset(""json"", data_files=""train-v2.0.json"", field=""data"", split=""train"")\r\ns[0]\r\n# {\'title\': \'Normans\', \'paragraphs\': [{\'qas\': [{\'question\': \'In what country is Normandy located?\', \'id\':...\r\n```'
 'Also if you want to have your own dataset script, we now have a new documentation !\r\nSee here:\r\nhttps://huggingface.co/nlp/add_dataset.html'
 '@lhoestq \r\nFor some reason, I am not able to reproduce the segmentation fault, on pyarrow==0.16.0. Using the exact same environment and file.\r\n\r\nAnyhow, I discovered that pyarrow>=0.17.0 is required to read in a JSON file where the pandas structs contain lists. Otherwise, pyarrow complains when attempting to cast the struct:\r\n```py\r\nimport nlp\r\n>>> s = nlp.load_dataset(""json"", data_files=""datasets/train-v2.0.json"", field=""data"", split=""train"")\r\nUsing custom data configuration default\r\n>>> s[0]\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/vegarab/.conda/envs/torch/lib/python3.7/site-packages/nlp/arrow_dataset.py"", line 558, in __getitem__\r\n    format_kwargs=self._format_kwargs,\r\n  File ""/home/vegarab/.conda/envs/torch/lib/python3.7/site-packages/nlp/arrow_dataset.py"", line 498, in _getitem\r\n    outputs = self._unnest(self._data.slice(key, 1).to_pandas().to_dict(""list""))\r\n  File ""pyarrow/array.pxi"", line 559, in pyarrow.lib._PandasConvertible.to_pandas\r\n  File ""pyarrow/table.pxi"", line 1367, in pyarrow.lib.Table._to_pandas\r\n  File ""/home/vegarab/.conda/envs/torch/lib/python3.7/site-packages/pyarrow/pandas_compat.py"", line 766, in table_to_blockmanager\r\n    blocks = _table_to_blocks(options, table, categories, ext_columns_dtypes)\r\n  File ""/home/vegarab/.conda/envs/torch/lib/python3.7/site-packages/pyarrow/pandas_compat.py"", line 1101, in _table_to_blocks\r\n    list(extension_columns.keys()))\r\n  File ""pyarrow/table.pxi"", line 881, in pyarrow.lib.table_to_blocks\r\n  File ""pyarrow/error.pxi"", line 105, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowNotImplementedError: Not implemented type for Arrow list to pandas: struct<qas: list<item: struct<question: string, id: string, answers: list<item: struct<text: string, answer_start: int64>>, is_impossible: bool, plausible_answers: list<item: struct<text: string, answer_start: int64>>>>, context: string>\r\n>>> s\r\nDataset(schema: {\'title\': \'string\', \'paragraphs\': \'list<item: struct<qas: list<item: struct<question: string, id: string, answers: list<item: struct<text: string, answer_start: int64>>, is_impossible: bool, plausible_answers: list<item: struct<text: string, answer_start: int64>>>>, context: string>>\'}, num_rows: 35)\r\n```\r\n\r\nUpgrading to >=0.17.0 provides the same dataset structure, but accessing the records is possible without the same exception. \r\n\r\n'
 'Very happy to see some extended documentation! '
 '#376 seems to be reporting the same issue as mentioned above. '
 'This issue helped me a lot, thanks.\r\nHope this issue will be fixed soon.']","The last issue was closed (#369) once the #372 update was merged. However, I'm still not able to load a SQuAD formatted JSON file. Instead of the previously recorded pyarrow error, I now get a segmentation fault. 

```
dataset = nlp.load_dataset('json', data_files={nlp.Split.TRAIN: [""./datasets/train-v2.0.json""]}, field='data')
```
causes
```
Using custom data configuration default
Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, total: Unknown size) to /home/XXX/.cache/huggingface/datasets/json/default/0.0.0...
0 tables [00:00, ? tables/s]Segmentation fault (core dumped)
```
where `./datasets/train-v2.0.json` is downloaded directly from https://rajpurkar.github.io/SQuAD-explorer/.
This is consistent with other SQuAD-formatted JSON files.

When attempting to load the dataset again, I get the following:
```
Using custom data configuration default
Traceback (most recent call last):
  File ""dataloader.py"", line 6, in <module>
    'json', data_files={nlp.Split.TRAIN: [""./datasets/train-v2.0.json""]}, field='data')
  File ""/home/XXX/.conda/envs/torch/lib/python3.7/site-packages/nlp/load.py"", line 524, in load_dataset
    save_infos=save_infos,
  File ""/home/XXX/.conda/envs/torch/lib/python3.7/site-packages/nlp/builder.py"", line 382, in download_and_prepare
    with incomplete_dir(self._cache_dir) as tmp_data_dir:
  File ""/home/XXX/.conda/envs/torch/lib/python3.7/contextlib.py"", line 112, in __enter__
    return next(self.gen)
  File ""/home/XXX/.conda/envs/torch/lib/python3.7/site-packages/nlp/builder.py"", line 368, in incomplete_dir
    os.makedirs(tmp_dir)
  File ""/home/XXX/.conda/envs/torch/lib/python3.7/os.py"", line 223, in makedirs
    mkdir(name, mode)
FileExistsError: [Errno 17] File exists: '/home/XXX/.cache/huggingface/datasets/json/default/0.0.0.incomplete'
```

(Not sure if you wanted this in the previous issue #369 or not as it was closed.)"
https://github.com/huggingface/datasets/issues/369,can't load local dataset: pyarrow.lib.ArrowInvalid: straddling object straddles two block boundaries,"['I am able to reproduce this with the official SQuAD `train-v2.0.json` file downloaded directly from https://rajpurkar.github.io/SQuAD-explorer/'
 'I am facing this issue in transformers library 3.0.2 while reading a csv using datasets.\r\nIs this fixed in latest version? \r\nI updated the latest version 4.0.1 but still getting this error. What could cause this error?']","Trying to load a local SQuAD-formatted dataset (from a JSON file, about 60MB):
```
dataset = nlp.load_dataset(path='json', data_files={nlp.Split.TRAIN: [""./path/to/file.json""]})
```
causes
```
Traceback (most recent call last):
  File ""dataloader.py"", line 9, in <module>
    [""./path/to/file.json""]})
  File ""/home/XXX/.conda/envs/torch/lib/python3.7/site-packages/nlp/load.py"", line 524, in load_dataset
    save_infos=save_infos,
  File ""/home/XXX/.conda/envs/torch/lib/python3.7/site-packages/nlp/builder.py"", line 432, in download_and_prepare
    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
  File ""/home/XXX/.conda/envs/torch/lib/python3.7/site-packages/nlp/builder.py"", line 483, in _download_and_prepare
    self._prepare_split(split_generator, **prepare_split_kwargs)
  File ""/home/XXX/.conda/envs/torch/lib/python3.7/site-packages/nlp/builder.py"", line 719, in _prepare_split
    for key, table in utils.tqdm(generator, unit="" tables"", leave=False):
  File ""/home/XXX/.conda/envs/torch/lib/python3.7/site-packages/tqdm/std.py"", line 1129, in __iter__
    for obj in iterable:
  File ""/home/XXX/.conda/envs/torch/lib/python3.7/site-packages/nlp/datasets/json/88c1bc5c68489f7eda549ed05a5a738527c613b3e7a4ee3524d9d233353a949b/json.py"", line 53, in _generate_tables
    file, read_options=self.config.pa_read_options, parse_options=self.config.pa_parse_options,
  File ""pyarrow/_json.pyx"", line 191, in pyarrow._json.read_json
  File ""pyarrow/error.pxi"", line 85, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: straddling object straddles two block boundaries (try to increase block size?)
```

I haven't been able to find any reports of this specific pyarrow error here or elsewhere. "
https://github.com/huggingface/datasets/issues/368,load_metric can't acquire lock anymore,"[""I found that, in the same process (or the same interactive session), if I do\r\n\r\nimport nlp\r\n\r\nm1 = nlp.load_metric('glue', 'mrpc')\r\nm2 = nlp.load_metric('glue', 'sst2')\r\n\r\nI will get the same error `ValueError: Cannot acquire lock, caching file might be used by another process, you should setup a unique 'experiment_id'`.""]","I can't load metric (glue) anymore after an error in a previous run. I even removed the whole cache folder `/home/XXX/.cache/huggingface/`, and the issue persisted. What are the steps to fix this?

    Traceback (most recent call last):
      File ""/home/XXX/miniconda3/envs/ML-DL-py-3.7/lib/python3.7/site-packages/nlp/metric.py"", line 101, in __init__
        self.filelock.acquire(timeout=1)
      File ""/home/XXX/miniconda3/envs/ML-DL-py-3.7/lib/python3.7/site-packages/filelock.py"", line 278, in acquire
        raise Timeout(self._lock_file)
    filelock.Timeout: The file lock '/home/XXX/.cache/huggingface/metrics/glue/1.0.0/1-glue-0.arrow.lock' could not be acquired.

    During handling of the above exception, another exception occurred:

    Traceback (most recent call last):
      File ""examples_huggingface_nlp.py"", line 268, in <module>
        main()
      File ""examples_huggingface_nlp.py"", line 242, in main
        dataset, metric = get_dataset_metric(glue_task)
      File ""examples_huggingface_nlp.py"", line 77, in get_dataset_metric
        metric = nlp.load_metric('glue', glue_config, experiment_id=1)
      File ""/home/XXX/miniconda3/envs/ML-DL-py-3.7/lib/python3.7/site-packages/nlp/load.py"", line 440, in load_metric
        **metric_init_kwargs,
      File ""/home/XXX/miniconda3/envs/ML-DL-py-3.7/lib/python3.7/site-packages/nlp/metric.py"", line 104, in __init__
        ""Cannot acquire lock, caching file might be used by another process, ""
    ValueError: Cannot acquire lock, caching file might be used by another process, you should setup a unique 'experiment_id' for this run.
    I0709 15:54:41.008838 139854118430464 filelock.py:318] Lock 139852058030936 released on /home/XXX/.cache/huggingface/metrics/glue/1.0.0/1-glue-0.arrow.lock
"
https://github.com/huggingface/datasets/issues/365,How to augment data ?,"['Using batched map is probably the easiest way at the moment.\r\nWhat kind of augmentation would you like to do ?'
 'Some samples in the dataset are too long, I want to divide them in several samples.'
 ""Using batched map is the way to go then.\r\nWe'll make it clearer in the docs that map could be used for augmentation.\r\n\r\nLet me know if you think there should be another way to do it. Or feel free to close the issue otherwise.""
 ""It just feels awkward to use map to augment data. Also it means it's not possible to augment data in a non-batched way.\r\n\r\nBut to be honest I have no idea of a good API...""
 ""Or for non-batched samples, how about returning a tuple ?\r\n\r\n```python\r\ndef aug(sample):\r\n    # Simply copy the existing data to have x2 amount of data\r\n    return sample, sample\r\n\r\ndataset = dataset.map(aug)\r\n```\r\n\r\nIt feels really natural and easy, but :\r\n\r\n* it means the behavior with batched data is different\r\n* I don't know how doable it is backend-wise\r\n\r\n@lhoestq ""
 ""As we're working with arrow's columnar format we prefer to play with batches that are dictionaries instead of tuples.\r\nIf we have tuple it implies to re-format the data each time we want to write to arrow, which can lower the speed of map for example.\r\n\r\nIt's also a matter of coherence, as we don't want users to be confused whether they have to return dictionaries for some functions and tuples for others when they're doing batches.""]","Is there any clean way to augment data ?

For now my work-around is to use batched map, like this :

```python
def aug(samples):
    # Simply copy the existing data to have x2 amount of data
    for k, v in samples.items():
        samples[k].extend(v)
    return samples

dataset = dataset.map(aug, batched=True)
```"
https://github.com/huggingface/datasets/issues/362,[dateset subset missing]  xtreme paws-x,"[""You're right, thanks for pointing it out. We will update it ""]","I tried nlp.load_dataset('xtreme', 'PAWS-X.es') but get the value error
It turns out that the subset for Spanish is missing
https://github.com/google-research-datasets/paws/tree/master/pawsx"
https://github.com/huggingface/datasets/issues/361,🐛 [Metrics] ROUGE is non-deterministic,"['Hi, can you give a full self-contained example to reproduce this behavior?'
 '> Hi, can you give a full self-contained example to reproduce this behavior?\r\n\r\nThere is a notebook in the post ;)'
 '> If I run the ROUGE metric 2 times, with same predictions / references, the scores are slightly different.\r\n> \r\n> Refer to [this Colab notebook](https://colab.research.google.com/drive/1wRssNXgb9ldcp4ulwj-hMJn0ywhDOiDy?usp=sharing) for reproducing the problem.\r\n> \r\n> Example of F-score for ROUGE-1, ROUGE-2, ROUGE-L in 2 differents run :\r\n> \r\n> > [\'0.3350\', \'0.1470\', \'0.2329\']\r\n> > [\'0.3358\', \'0.1451\', \'0.2332\']\r\n> \r\n> Why ROUGE is not deterministic ?\r\n\r\nThis is because of rouge\'s `BootstrapAggregator` that uses sampling to get confidence intervals (low, mid, high).\r\nYou can get deterministic scores per sentence pair by using\r\n```python\r\nscore = rouge.compute(rouge_types=[""rouge1"", ""rouge2"", ""rougeL""], use_agregator=False)\r\n```\r\nOr you can set numpy\'s random seed if you still want to use the aggregator.'
 'Maybe we can set all the random seeds of numpy/torch etc. while running `metric.compute` ?'
 'We should probably indeed!'
 ""Now if you re-run the notebook, the two printed results are the same @colanim\r\n```\r\n['0.3356', '0.1466', '0.2318']\r\n['0.3356', '0.1466', '0.2318']\r\n```\r\nHowever across sessions, the results may change (as numpy's random seed can be different). You can prevent that by setting your seed:\r\n```python\r\nrouge = nlp.load_metric('rouge', seed=42)\r\n```""]","If I run the ROUGE metric 2 times, with same predictions / references, the scores are slightly different.

Refer to [this Colab notebook](https://colab.research.google.com/drive/1wRssNXgb9ldcp4ulwj-hMJn0ywhDOiDy?usp=sharing) for reproducing the problem.

Example of F-score for ROUGE-1, ROUGE-2, ROUGE-L in 2 differents run :

> ['0.3350', '0.1470', '0.2329']
['0.3358', '0.1451', '0.2332']

---

Why ROUGE is not deterministic ?"
https://github.com/huggingface/datasets/issues/360,[Feature request] Add dataset.ragged_map() function for many-to-many transformations,"[""Actually `map(batched=True)` can already change the size of the dataset.\r\nIt can accept examples of length `N` and returns a batch of length `M` (can be null or greater than `N`).\r\n\r\nI'll make that explicit in the doc that I'm currently writing.""
 'You\'re two steps ahead of me :) In my testing, it also works if `M` < `N`.\r\n\r\nA batched map of different length seems to work if you directly overwrite all of the original keys, but fails if any of the original keys are preserved.\r\n\r\nFor example,\r\n```python\r\n# Create a dummy dataset\r\ndset = load_dataset(""wikitext"", ""wikitext-2-raw-v1"")[""test""]\r\ndset = dset.map(lambda ex: {""length"": len(ex[""text""]), ""foo"": 1})\r\n\r\n# Do an allreduce on each batch, overwriting both keys\r\ndset.map(lambda batch: {""length"": [sum(batch[""length""])], ""foo"": [1]})\r\n# Dataset(schema: {\'length\': \'int64\', \'foo\': \'int64\'}, num_rows: 5)\r\n\r\n# Now attempt an allreduce without touching the `foo` key\r\ndset.map(lambda batch: {""length"": [sum(batch[""length""])]})\r\n# This fails with the error message below\r\n```\r\n\r\n```bash\r\n  File ""/path/to/nlp/src/nlp/arrow_dataset.py"", line 728, in map\r\n    arrow_schema = pa.Table.from_pydict(test_output).schema\r\n  File ""pyarrow/io.pxi"", line 1532, in pyarrow.lib.Codec.detect\r\n  File ""pyarrow/table.pxi"", line 1503, in pyarrow.lib.Table.from_arrays\r\n  File ""pyarrow/public-api.pxi"", line 390, in pyarrow.lib.pyarrow_wrap_table\r\n  File ""pyarrow/error.pxi"", line 85, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Column 1 named foo expected length 1 but got length 2\r\n```\r\n\r\nAdding the `remove_columns=[""length"", ""foo""]` argument to `map()` solves the issue. Leaving the above error for future visitors. Perfect, thank you!']","`dataset.map()` enables one-to-one transformations. Input one example and output one example. This is helpful for tokenizing and cleaning individual lines.
`dataset.filter()` enables one-to-(one-or-none) transformations. Input one example and output either zero/one example. This is helpful for removing portions from the dataset.
However, some dataset transformations are many-to-many. Consider constructing BERT training examples from a dataset of sentences, where you map `[""a"", ""b"", ""c""] -> [""a[SEP]b"", ""a[SEP]c"", ""b[SEP]c"", ""c[SEP]b"", ...]`

I propose a more general `ragged_map()` method that takes in a batch of examples of length `N` and return a batch of examples `M`. This is different from the `map(batched=True)` method, which takes examples of length `N` and returns a batch of length `N`, processing individual examples in parallel. I don't have a clear vision of how this would be implemented efficiently and lazily, but would love to hear the community's feedback on this.

My specific use case is creating an end-to-end ELECTRA data pipeline. I would like to take the raw WikiText data and generate training examples from this using the `ragged_map()` method, then export to TFRecords and train quickly. This would be a reproducible pipeline with no bash scripts. Currently I'm relying on scripts like https://github.com/google-research/electra/blob/master/build_pretraining_dataset.py, which are less general.

"
https://github.com/huggingface/datasets/issues/359,ArrowBasedBuilder _prepare_split parse_schema breaks on nested structures,"['Hi, it depends on what it is in your `dataset_builder.py` file. Can you share it?\r\n\r\nIf you are just loading `json` files, you can also directly use the `json` script (which will find the schema/features from your JSON structure):\r\n\r\n```python\r\nfrom nlp import load_dataset\r\nds = load_dataset(""json"", data_files=rel_datafiles)\r\n```'
 'The behavior I\'m seeing is from the `json` script. \r\nI hacked this together to overcome the error with the `JSON` dataloader\r\n\r\n```\r\nclass DatasetBuilder(hf_nlp.ArrowBasedBuilder):\r\n    BUILDER_CONFIG_CLASS = BuilderConfig\r\n\r\n    def _info(self):\r\n        return DatasetInfo()\r\n\r\n    def _split_generators(self, dl_manager):\r\n        """""" We handle string, list and dicts in datafiles\r\n        """"""\r\n        if isinstance(self.config.data_files, (str, list, tuple)):\r\n            files = self.config.data_files\r\n            if isinstance(files, str):\r\n                files = [files]\r\n            return [SplitGenerator(name=Split.TRAIN, gen_kwargs={""files"": files})]\r\n        splits = []\r\n        for split_name in [Split.TRAIN, Split.VALIDATION, Split.TEST]:\r\n            if split_name in self.config.data_files:\r\n                files = self.config.data_files[split_name]\r\n                if isinstance(files, str):\r\n                    files = [files]\r\n                splits.append(SplitGenerator(name=split_name, gen_kwargs={""files"": files}))\r\n        return splits\r\n\r\n    def _prepare_split(self, split_generator):\r\n        fname = ""{}-{}.arrow"".format(self.name, split_generator.name)\r\n        fpath = os.path.join(self._cache_dir, fname)\r\n\r\n        writer = ArrowWriter(path=fpath)\r\n\r\n        generator = self._generate_tables(**split_generator.gen_kwargs)\r\n        for key, table in utils.tqdm(generator, unit="" tables"", leave=False):\r\n            writer.write_table(table)\r\n        num_examples, num_bytes = writer.finalize()\r\n\r\n        split_generator.split_info.num_examples = num_examples\r\n        split_generator.split_info.num_bytes = num_bytes\r\n        # this is where the error is coming from\r\n        # def parse_schema(schema, schema_dict):\r\n        #     for field in schema:\r\n        #         if pa.types.is_struct(field.type):\r\n        #             schema_dict[field.name] = {}\r\n        #             parse_schema(field.type, schema_dict[field.name])\r\n        #         elif pa.types.is_list(field.type) and pa.types.is_struct(field.type.value_type):\r\n        #             schema_dict[field.name] = {}\r\n        #             parse_schema(field.type.value_type, schema_dict[field.name])\r\n        #         else:\r\n        #             schema_dict[field.name] = Value(str(field.type))\r\n        # \r\n        # parse_schema(writer.schema, features)\r\n        # self.info.features = Features(features)\r\n\r\n    def _generate_tables(self, files):\r\n        for i, file in enumerate(files):\r\n            pa_table = paj.read_json(\r\n                file\r\n            )\r\n            yield i, pa_table\r\n```\r\n\r\nSo I basically just don\'t populate the `self.info.features` though this doesn\'t seem to cause any problems in my downstream applications. \r\n\r\nThe other workaround I was doing was to just use pyarrow.json to build a table and then to create the Dataset with its constructor or from_table methods. `load_dataset` has nice split logic, so I\'d prefer to use that.\r\n\r\n'
 ""Also noticed that if you for example in a loader script\r\n\r\n```\r\nfrom nlp import ArrowBasedBuilder\r\n\r\nclass MyBuilder(ArrowBasedBuilder):\r\n...\r\n\r\n```\r\nand use that in the subclass, it will be on the module's __dict__ and will be selected before the `MyBuilder` subclass, and it will raise `NotImplementedError` on its `_generate_examples` method... In the code it check for abstract classes but Builder and ArrowBasedBuilder aren't abstract classes, they're regular classes with `@abstract_methods`.""
 'Indeed this is part of a more general limitation which is the fact that we should generate and update the `features` from the auto-inferred Arrow schema when they are not provided (also happen when a user change the schema using `map()`, the features should be auto-generated and guessed as much as possible to keep the `features` synced with the underlying Arrow table schema).\r\n\r\nWe will try to solve this soon.']","I tried using the Json dataloader to load some JSON lines files. but get an exception in the parse_schema function.

```
---------------------------------------------------------------------------

ValueError                                Traceback (most recent call last)

<ipython-input-23-9aecfbee53bd> in <module>
     55 from nlp import load_dataset
     56 
---> 57 ds = load_dataset(""../text2struct/model/dataset_builder.py"", data_files=rel_datafiles)
     58 
     59 

~/.virtualenvs/inv-text2struct/lib/python3.6/site-packages/nlp/load.py in load_dataset(path, name, version, data_dir, data_files, split, cache_dir, download_config, download_mode, ignore_verifications, save_infos, **config_kwargs)
    522         download_mode=download_mode,
    523         ignore_verifications=ignore_verifications,
--> 524         save_infos=save_infos,
    525     )
    526 

~/.virtualenvs/inv-text2struct/lib/python3.6/site-packages/nlp/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, save_infos, try_from_hf_gcs, dl_manager, **download_and_prepare_kwargs)
    430                 verify_infos = not save_infos and not ignore_verifications
    431                 self._download_and_prepare(
--> 432                     dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
    433                 )
    434                 # Sync info

~/.virtualenvs/inv-text2struct/lib/python3.6/site-packages/nlp/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)
    481             try:
    482                 # Prepare split will record examples associated to the split
--> 483                 self._prepare_split(split_generator, **prepare_split_kwargs)
    484             except OSError:
    485                 raise OSError(""Cannot find data file. "" + (self.manual_download_instructions or """"))

~/.virtualenvs/inv-text2struct/lib/python3.6/site-packages/nlp/builder.py in _prepare_split(self, split_generator)
    736                     schema_dict[field.name] = Value(str(field.type))
    737 
--> 738         parse_schema(writer.schema, features)
    739         self.info.features = Features(features)
    740 

~/.virtualenvs/inv-text2struct/lib/python3.6/site-packages/nlp/builder.py in parse_schema(schema, schema_dict)
    734                     parse_schema(field.type.value_type, schema_dict[field.name])
    735                 else:
--> 736                     schema_dict[field.name] = Value(str(field.type))
    737 
    738         parse_schema(writer.schema, features)

<string> in __init__(self, dtype, id, _type)

~/.virtualenvs/inv-text2struct/lib/python3.6/site-packages/nlp/features.py in __post_init__(self)
     55 
     56     def __post_init__(self):
---> 57         self.pa_type = string_to_arrow(self.dtype)
     58 
     59     def __call__(self):

~/.virtualenvs/inv-text2struct/lib/python3.6/site-packages/nlp/features.py in string_to_arrow(type_str)
     32         if str(type_str + ""_"") not in pa.__dict__:
     33             raise ValueError(
---> 34                 f""Neither {type_str} nor {type_str + '_'} seems to be a pyarrow data type. ""
     35                 f""Please make sure to use a correct data type, see: ""
     36                 f""https://arrow.apache.org/docs/python/api/datatypes.html#factory-functions""

ValueError: Neither list<item: string> nor list<item: string>_ seems to be a pyarrow data type. Please make sure to use a correct data type, see: https://arrow.apache.org/docs/python/api/datatypes.html#factory-functions
```

If I create the dataset imperatively, using a pyarrow table, the dataset is created correctly. If I override the `_prepare_split` method to avoid calling the validate schema, the dataset can load as well. "
https://github.com/huggingface/datasets/issues/355,can't load SNLI dataset,"[""I just added the processed files of `snli` on our google storage, so that when you do `load_dataset` it can download the processed files from there :)\r\n\r\nWe are thinking about having available those processed files for more datasets in the future, because sometimes files aren't available (like for `snli`), or the download speed is too slow, or sometimes the files take time to be processed.""
 'Closing this one. Feel free to re-open if you have other questions :)'
 'Thank you!']","`nlp` seems to load `snli` from some URL based on nlp.stanford.edu. This subdomain is frequently down -- including right now, when I'd like to load `snli` in a Colab notebook, but can't.

Is there a plan to move these datasets to huggingface servers for a more stable solution?

Btw, here's the stack trace:

```
File ""/content/nlp/src/nlp/builder.py"", line 432, in download_and_prepare
    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
  File ""/content/nlp/src/nlp/builder.py"", line 466, in _download_and_prepare
    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)
  File ""/content/nlp/src/nlp/datasets/snli/e417f6f2e16254938d977a17ed32f3998f5b23e4fcab0f6eb1d28784f23ea60d/snli.py"", line 76, in _split_generators
    dl_dir = dl_manager.download_and_extract(_DATA_URL)
  File ""/content/nlp/src/nlp/utils/download_manager.py"", line 217, in download_and_extract
    return self.extract(self.download(url_or_urls))
  File ""/content/nlp/src/nlp/utils/download_manager.py"", line 156, in download
    lambda url: cached_path(url, download_config=self._download_config,), url_or_urls,
  File ""/content/nlp/src/nlp/utils/py_utils.py"", line 190, in map_nested
    return function(data_struct)
  File ""/content/nlp/src/nlp/utils/download_manager.py"", line 156, in <lambda>
    lambda url: cached_path(url, download_config=self._download_config,), url_or_urls,
  File ""/content/nlp/src/nlp/utils/file_utils.py"", line 198, in cached_path
    local_files_only=download_config.local_files_only,
  File ""/content/nlp/src/nlp/utils/file_utils.py"", line 356, in get_from_cache
    raise ConnectionError(""Couldn't reach {}"".format(url))
ConnectionError: Couldn't reach https://nlp.stanford.edu/projects/snli/snli_1.0.zip
```"
https://github.com/huggingface/datasets/issues/353,[Dataset requests] New datasets for Text Classification,"['Pinging @mariamabarham as well'
 ""- `nlp` has MR! It's called `rotten_tomatoes`\r\n- SST is part of GLUE, or is that just SST-2?\r\n- `nlp` also has `ag_news`, a popular news classification dataset\r\n\r\nI'd also like to see:\r\n- the Yahoo Answers topic classification dataset\r\n- the Kaggle Fake News classification dataset""
 'Thanks @jxmorris12 for pointing this out. \r\n\r\nIn glue we only have SST-2 maybe we can add separately SST-1.\r\n'
 'This is the homepage for the Amazon dataset: https://www.kaggle.com/datafiniti/consumer-reviews-of-amazon-products\r\n\r\nIs there an easy way to download kaggle datasets programmatically? If so, I can add this one!'
 'Hi @jxmorris12 for now I think our `dl_manager` does not download from Kaggle.\r\n@thomwolf , @lhoestq'
 'Pretty sure the quora dataset is the same one I implemented here: https://github.com/huggingface/nlp/pull/366'
 ""Great list. Any idea if Amazon Reviews has been added?\r\n\r\n- ~40 GB of text (sadly no emoji)\r\n- popular MLM pre-training dataset before bigger datasets like WebText https://arxiv.org/abs/1808.01371\r\n- turns out that binarizing the 1-5 star rating leads to great Pos/Neg/Neutral dataset, T5 paper claims to get very high accuracy (98%!) on this with small amount of finetuning https://arxiv.org/abs/2004.14546\r\n\r\nApologies if it's been included (great to see where) and if not, it's one of the better medium/large NLP dataset for semi-supervised learning, albeit a bit out of date. \r\n\r\nThanks!! \r\n\r\ncc @sshleifer ""
 'On the Amazon Reviews dataset, the original UCSD website has noted these are now updated to include product reviews through 2018 -- actually quite recent compared to many other datasets. Almost certainly the largest NLP dataset out there with labels!\r\nhttps://jmcauley.ucsd.edu/data/amazon/ \r\n\r\nAny chance someone has time to onboard this dataset in a HF way?\r\n\r\ncc @sshleifer ']","We are missing a few datasets for Text Classification which is an important field.

Namely, it would be really nice to add:
- TREC-6 dataset (see here for instance: https://pytorchnlp.readthedocs.io/en/latest/source/torchnlp.datasets.html#torchnlp.datasets.trec_dataset)  **[done]**
- Yelp-5
- Movie review (Movie Review (MR) dataset [156]) **[done (same as rotten_tomatoes)]**
- SST (Stanford Sentiment Treebank) **[include in glue]**
- Multi-Perspective Question Answering (MPQA) dataset **[require authentication (indeed manual download)]**
- Amazon. This is a popular corpus of product reviews collected from the Amazon website [159]. It contains labels for both binary classification and multi-class (5-class) classification
- 20 Newsgroups. The 20 Newsgroups dataset  **[done]**
- Sogou News dataset **[done]**
- Reuters news. The Reuters-21578 dataset [165] **[done]**
- DBpedia. The DBpedia dataset [170]
- Ohsumed. The Ohsumed collection [171] is a subset of the MEDLINE database
- EUR-Lex. The EUR-Lex dataset
- WOS. The Web Of Science (WOS) dataset **[done]**
- PubMed. PubMed [173]
- TREC-QA. TREC-QA
- Quora. The Quora dataset [180]

All these datasets are cited in https://arxiv.org/abs/2004.03705"
https://github.com/huggingface/datasets/issues/347,"'cp950' codec error from load_dataset('xtreme', 'tydiqa')","[""This is probably a Windows issue, we need to specify the encoding when `load_dataset()` reads the original CSV file.\r\nTry to find the `open()` statement called by `load_dataset()` and add an `encoding='utf-8'` parameter.\r\nSee issues #242 and #307 ""
 'It should be in `xtreme.py:L755`:\r\n```python\r\n        if self.config.name == ""tydiqa"" or self.config.name.startswith(""MLQA"") or self.config.name == ""SQuAD"":\r\n            with open(filepath) as f:\r\n                data = json.load(f)\r\n```\r\n\r\nCould you try to add the encoding parameter:\r\n```python\r\nopen(filepath, encoding=\'utf-8\')\r\n```'
 'Hello @jerryIsHere :) Did it work ?\r\nIf so we may change the dataset script to force the utf-8 encoding'
 '@lhoestq sorry for being that late, I found 4 copy of xtreme.py. I did the changes as what has been told to all of them.\r\nThe problem is not solved'
 ""Could you provide a better error message so that we can make sure it comes from the opening of the `tydiqa`'s json files ?\r\n""
 '@lhoestq \r\nThe error message is same as before:\r\nException has occurred: UnicodeDecodeError\r\n\'cp950\' codec can\'t decode byte 0xe2 in position 111: illegal multibyte sequence\r\n  File ""D:\\python\\test\\test.py"", line 3, in <module>\r\n    dataset = load_dataset(\'xtreme\', \'tydiqa\')\r\n\r\n![image](https://user-images.githubusercontent.com/50871412/87748794-7c216880-c829-11ea-94f0-7caeacb4d865.png)\r\n\r\nI said that I found 4 copy of xtreme.py and add the 「, encoding=\'utf-8\'」 parameter to the open() function\r\nthese python script was found under this directory\r\nC:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python37\\Lib\\site-packages\\nlp\\datasets\\xtreme\r\n'
 ""Hi there !\r\nI encountered the same issue with the IMDB dataset on windows. It threw an error about charmap not being able to decode a symbol during the first time I tried to download it. I checked on a remote linux machine I have, and it can't be reproduced.\r\nI added ```encoding='UTF-8'``` to both lines that have ```open``` in ```imdb.py```  (108 and 114) and it worked for me.\r\nThank you !""
 ""> Hi there !\r\n> I encountered the same issue with the IMDB dataset on windows. It threw an error about charmap not being able to decode a symbol during the first time I tried to download it. I checked on a remote linux machine I have, and it can't be reproduced.\r\n> I added `encoding='UTF-8'` to both lines that have `open` in `imdb.py` (108 and 114) and it worked for me.\r\n> Thank you !\r\n\r\nHello !\r\nGlad you managed to fix this issue on your side.\r\nDo you mind opening a PR for IMDB ?""
 ""> This is probably a Windows issue, we need to specify the encoding when `load_dataset()` reads the original CSV file.\r\n> Try to find the `open()` statement called by `load_dataset()` and add an `encoding='utf-8'` parameter.\r\n> See issues #242 and #307\r\n\r\nSorry for not responding for about a month.\r\nI have just found that it is necessary to change / add the environment variable as what was told in #242.\r\nEverything works after I add the new environment variable and restart my PC.\r\n\r\nI think the encoding issue for windows isn't limited to the open() function call specific to few dataset, but actually in the entire library, depends on the machine / os you use.""
 'Since #481 we shouldn\'t have other issues with encodings as they need to be set to ""utf-8"" be default.\r\n\r\nClosing this one, but feel free to re-open if you gave other questions']","![image](https://user-images.githubusercontent.com/50871412/86744744-67481680-c06c-11ea-8612-b77eba92a392.png)

I guess the error is related to python source encoding issue that my PC is trying to decode the source code with wrong encoding-decoding tools, perhaps :
https://www.python.org/dev/peps/pep-0263/

I guess the error was triggered by the code "" module = importlib.import_module(module_path)"" at line 57 in the source code:  nlp/src/nlp/load.py / (https://github.com/huggingface/nlp/blob/911d5596f9b500e39af8642fe3d1b891758999c7/src/nlp/load.py#L51)

Any ideas?

p.s. tried the same code on colab, that runs perfectly
"
https://github.com/huggingface/datasets/issues/345,Supporting documents in ELI5,"['Hi @saverymax ! For licensing reasons, the original team was unable to release pre-processed CommonCrawl documents. Instead, they provided a script to re-create them from a CommonCrawl dump, but it unfortunately requires access to a medium-large size cluster:\r\nhttps://github.com/facebookresearch/ELI5#downloading-support-documents-from-the-commoncrawl\r\n\r\nIn order to make the task accessible to people who may not have access to this kind of infrastructure, we suggest to use Wikipedia as a knowledge source rather than the full CommonCrawl. The following blog post shows how you can create Wikipedia support documents and get a performance that is on par with a system that uses CommonCrawl pages.\r\nhttps://yjernite.github.io/lfqa.html#task_description\r\n\r\nHope that helps, using ElasticSearch to index Wiki40b and create the documents should take about 4 hours. Let us know if you have any trouble with the blog post though!'
 'Hi, thanks for the quick response. The blog post is quite an interesting working example, thanks for sharing it.\r\nTwo follow-up points/questions about my original question:\r\n\r\n1. Yes, I read that the facebook team could not share the CommonCrawl b/c of licensing reasons. They state ""No, we are not allowed to host processed Reddit or CommonCrawl data,"" which indicates they could also not share the Reddit data for licensing reasons. But it seems that HuggingFace is able to share the Reddit data, so why not a subset of CommonCrawl?\r\n\r\n2. Thanks for the suggestion about ElasticSearch and Wiki40b. This is good to know about performance. I definitely could do the indexing and querying myself. What I like about the ELI5 dataset though, at least what is suggested by the paper, is that to create the dataset they had already selected the top 100 web sources and made a single support document from those. Though it doesn\'t appear to be too sophisticated an approach, having a single support document pre-computed (without having to run the facebook code or a replacement with another dataset) is super useful for my work, especially since I\'m not working on developing the latest and greatest retrieval model. Of course, I don\'t expect HF NLP datasets to be perfectly tailored to my use-case. I know there is overhead to any project, I\'m just illustrating a use-case of ELI5 which is not possible with the data provided as-is. If it\'s for licensing reasons, that is perfectly acceptable a reason, and I appreciate your response.']","I was attempting to use the ELI5 dataset, when I realized that huggingface does not provide the supporting documents (the source documents from the common crawl). Without the supporting documents, this makes the dataset about as useful for my project as a block of cheese, or some other more apt metaphor.  According to facebook, the entire document collection is quite large. However, it would still be helpful to at least include a subset of the supporting documents i.e., having some data is better than having a block of cheese, in my case at least.

If you choose not to include them, it would be helpful to have documentation mentioning this specifically. It is especially confusing because the hf nlp ELI5 dataset has the key `'document'` but there are no documents to be found :("
https://github.com/huggingface/datasets/issues/342,Features should be updated when `map()` changes schema,"[""`dataset.column_names` are being updated but `dataset.features` aren't indeed...""]","`dataset.map()` can change the schema and column names.

We should update the features in this case (with what is possible to infer)."
https://github.com/huggingface/datasets/issues/331,Loading CNN/Daily Mail dataset produces `nlp.utils.info_utils.NonMatchingSplitsSizesError`,"[""I couldn't reproduce on my side.\r\nIt looks like you were not able to generate all the examples, and you have the problem for each split train-test-validation.\r\nCould you try to enable logging, try again and send the logs ?\r\n```python\r\nimport logging\r\nlogging.basicConfig(level=logging.INFO)\r\n```""
 'here\'s the log\r\n```\r\n>>> import nlp\r\nimport logging\r\nlogging.basicConfig(level=logging.INFO)\r\nnlp.load_dataset(\'cnn_dailymail\', \'3.0.0\')\r\n>>> import logging\r\n>>> logging.basicConfig(level=logging.INFO)\r\n>>> nlp.load_dataset(\'cnn_dailymail\', \'3.0.0\')\r\nINFO:nlp.load:Checking /u/jm8wx/.cache/huggingface/datasets/720d2e20d8dc6d98f21195a39cc934bb41dd0a40b57ea3d323661a7c5d70522c.d44c2417f4e0fe938ede0a684dcbb1fa9b4789de22e8a99c43103d4b4c374b3b.py for additional imports.\r\nINFO:filelock:Lock 140443095301136 acquired on /u/jm8wx/.cache/huggingface/datasets/720d2e20d8dc6d98f21195a39cc934bb41dd0a40b57ea3d323661a7c5d70522c.d44c2417f4e0fe938ede0a684dcbb1fa9b4789de22e8a99c43103d4b4c374b3b.py.lock\r\nINFO:nlp.load:Found main folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/cnn_dailymail/cnn_dailymail.py at /p/qdata/jm8wx/datasets/nlp/src/nlp/datasets/cnn_dailymail\r\nINFO:nlp.load:Found specific version folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/cnn_dailymail/cnn_dailymail.py at /p/qdata/jm8wx/datasets/nlp/src/nlp/datasets/cnn_dailymail/9645e0bc96f647decf46541f6f4bef6936ee82ace653ac362bab03309a46d4ad\r\nINFO:nlp.load:Found script file from https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/cnn_dailymail/cnn_dailymail.py to /p/qdata/jm8wx/datasets/nlp/src/nlp/datasets/cnn_dailymail/9645e0bc96f647decf46541f6f4bef6936ee82ace653ac362bab03309a46d4ad/cnn_dailymail.py\r\nINFO:nlp.load:Updating dataset infos file from https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/cnn_dailymail/dataset_infos.json to /p/qdata/jm8wx/datasets/nlp/src/nlp/datasets/cnn_dailymail/9645e0bc96f647decf46541f6f4bef6936ee82ace653ac362bab03309a46d4ad/dataset_infos.json\r\nINFO:nlp.load:Found metadata file for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/cnn_dailymail/cnn_dailymail.py at /p/qdata/jm8wx/datasets/nlp/src/nlp/datasets/cnn_dailymail/9645e0bc96f647decf46541f6f4bef6936ee82ace653ac362bab03309a46d4ad/cnn_dailymail.json\r\nINFO:filelock:Lock 140443095301136 released on /u/jm8wx/.cache/huggingface/datasets/720d2e20d8dc6d98f21195a39cc934bb41dd0a40b57ea3d323661a7c5d70522c.d44c2417f4e0fe938ede0a684dcbb1fa9b4789de22e8a99c43103d4b4c374b3b.py.lock\r\nINFO:nlp.info:Loading Dataset Infos from /p/qdata/jm8wx/datasets/nlp/src/nlp/datasets/cnn_dailymail/9645e0bc96f647decf46541f6f4bef6936ee82ace653ac362bab03309a46d4ad\r\nINFO:nlp.builder:Generating dataset cnn_dailymail (/u/jm8wx/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0)\r\nINFO:nlp.builder:Dataset not on Hf google storage. Downloading and preparing it from source\r\nDownloading and preparing dataset cnn_dailymail/3.0.0 (download: 558.32 MiB, generated: 1.26 GiB, total: 1.81 GiB) to /u/jm8wx/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0...\r\nINFO:nlp.utils.info_utils:All the checksums matched successfully.\r\nINFO:nlp.builder:Generating split train\r\nINFO:nlp.arrow_writer:Done writing 285161 examples in 1240618482 bytes /u/jm8wx/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0.incomplete/cnn_dailymail-train.arrow.\r\nINFO:nlp.builder:Generating split validation\r\nINFO:nlp.arrow_writer:Done writing 13255 examples in 56637485 bytes /u/jm8wx/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0.incomplete/cnn_dailymail-validation.arrow.\r\nINFO:nlp.builder:Generating split test\r\nINFO:nlp.arrow_writer:Done writing 11379 examples in 48931393 bytes /u/jm8wx/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0.incomplete/cnn_dailymail-test.arrow.\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/p/qdata/jm8wx/datasets/nlp/src/nlp/load.py"", line 520, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File ""/p/qdata/jm8wx/datasets/nlp/src/nlp/builder.py"", line 431, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File ""/p/qdata/jm8wx/datasets/nlp/src/nlp/builder.py"", line 488, in _download_and_prepare\r\n    verify_splits(self.info.splits, split_dict)\r\n  File ""/p/qdata/jm8wx/datasets/nlp/src/nlp/utils/info_utils.py"", line 70, in verify_splits\r\n    raise NonMatchingSplitsSizesError(str(bad_splits))\r\nnlp.utils.info_utils.NonMatchingSplitsSizesError: [{\'expected\': SplitInfo(name=\'test\', num_bytes=49424491, num_examples=11490, dataset_name=\'cnn_dailymail\'), \'recorded\': SplitInfo(name=\'test\', num_bytes=48931393, num_examples=11379, dataset_name=\'cnn_dailymail\')}, {\'expected\': SplitInfo(name=\'train\', num_bytes=1249178681, num_examples=287113, dataset_name=\'cnn_dailymail\'), \'recorded\': SplitInfo(name=\'train\', num_bytes=1240618482, num_examples=285161, dataset_name=\'cnn_dailymail\')}, {\'expected\': SplitInfo(name=\'validation\', num_bytes=57149241, num_examples=13368, dataset_name=\'cnn_dailymail\'), \'recorded\': SplitInfo(name=\'validation\', num_bytes=56637485, num_examples=13255, dataset_name=\'cnn_dailymail\')}]\r\n```'
 '> here\'s the log\r\n> \r\n> ```\r\n> >>> import nlp\r\n> import logging\r\n> logging.basicConfig(level=logging.INFO)\r\n> nlp.load_dataset(\'cnn_dailymail\', \'3.0.0\')\r\n> >>> import logging\r\n> >>> logging.basicConfig(level=logging.INFO)\r\n> >>> nlp.load_dataset(\'cnn_dailymail\', \'3.0.0\')\r\n> INFO:nlp.load:Checking /u/jm8wx/.cache/huggingface/datasets/720d2e20d8dc6d98f21195a39cc934bb41dd0a40b57ea3d323661a7c5d70522c.d44c2417f4e0fe938ede0a684dcbb1fa9b4789de22e8a99c43103d4b4c374b3b.py for additional imports.\r\n> INFO:filelock:Lock 140443095301136 acquired on /u/jm8wx/.cache/huggingface/datasets/720d2e20d8dc6d98f21195a39cc934bb41dd0a40b57ea3d323661a7c5d70522c.d44c2417f4e0fe938ede0a684dcbb1fa9b4789de22e8a99c43103d4b4c374b3b.py.lock\r\n> INFO:nlp.load:Found main folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/cnn_dailymail/cnn_dailymail.py at /p/qdata/jm8wx/datasets/nlp/src/nlp/datasets/cnn_dailymail\r\n> INFO:nlp.load:Found specific version folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/cnn_dailymail/cnn_dailymail.py at /p/qdata/jm8wx/datasets/nlp/src/nlp/datasets/cnn_dailymail/9645e0bc96f647decf46541f6f4bef6936ee82ace653ac362bab03309a46d4ad\r\n> INFO:nlp.load:Found script file from https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/cnn_dailymail/cnn_dailymail.py to /p/qdata/jm8wx/datasets/nlp/src/nlp/datasets/cnn_dailymail/9645e0bc96f647decf46541f6f4bef6936ee82ace653ac362bab03309a46d4ad/cnn_dailymail.py\r\n> INFO:nlp.load:Updating dataset infos file from https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/cnn_dailymail/dataset_infos.json to /p/qdata/jm8wx/datasets/nlp/src/nlp/datasets/cnn_dailymail/9645e0bc96f647decf46541f6f4bef6936ee82ace653ac362bab03309a46d4ad/dataset_infos.json\r\n> INFO:nlp.load:Found metadata file for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/cnn_dailymail/cnn_dailymail.py at /p/qdata/jm8wx/datasets/nlp/src/nlp/datasets/cnn_dailymail/9645e0bc96f647decf46541f6f4bef6936ee82ace653ac362bab03309a46d4ad/cnn_dailymail.json\r\n> INFO:filelock:Lock 140443095301136 released on /u/jm8wx/.cache/huggingface/datasets/720d2e20d8dc6d98f21195a39cc934bb41dd0a40b57ea3d323661a7c5d70522c.d44c2417f4e0fe938ede0a684dcbb1fa9b4789de22e8a99c43103d4b4c374b3b.py.lock\r\n> INFO:nlp.info:Loading Dataset Infos from /p/qdata/jm8wx/datasets/nlp/src/nlp/datasets/cnn_dailymail/9645e0bc96f647decf46541f6f4bef6936ee82ace653ac362bab03309a46d4ad\r\n> INFO:nlp.builder:Generating dataset cnn_dailymail (/u/jm8wx/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0)\r\n> INFO:nlp.builder:Dataset not on Hf google storage. Downloading and preparing it from source\r\n> Downloading and preparing dataset cnn_dailymail/3.0.0 (download: 558.32 MiB, generated: 1.26 GiB, total: 1.81 GiB) to /u/jm8wx/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0...\r\n> INFO:nlp.utils.info_utils:All the checksums matched successfully.\r\n> INFO:nlp.builder:Generating split train\r\n> INFO:nlp.arrow_writer:Done writing 285161 examples in 1240618482 bytes /u/jm8wx/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0.incomplete/cnn_dailymail-train.arrow.\r\n> INFO:nlp.builder:Generating split validation\r\n> INFO:nlp.arrow_writer:Done writing 13255 examples in 56637485 bytes /u/jm8wx/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0.incomplete/cnn_dailymail-validation.arrow.\r\n> INFO:nlp.builder:Generating split test\r\n> INFO:nlp.arrow_writer:Done writing 11379 examples in 48931393 bytes /u/jm8wx/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0.incomplete/cnn_dailymail-test.arrow.\r\n> Traceback (most recent call last):\r\n>   File ""<stdin>"", line 1, in <module>\r\n>   File ""/p/qdata/jm8wx/datasets/nlp/src/nlp/load.py"", line 520, in load_dataset\r\n>     builder_instance.download_and_prepare(\r\n>   File ""/p/qdata/jm8wx/datasets/nlp/src/nlp/builder.py"", line 431, in download_and_prepare\r\n>     self._download_and_prepare(\r\n>   File ""/p/qdata/jm8wx/datasets/nlp/src/nlp/builder.py"", line 488, in _download_and_prepare\r\n>     verify_splits(self.info.splits, split_dict)\r\n>   File ""/p/qdata/jm8wx/datasets/nlp/src/nlp/utils/info_utils.py"", line 70, in verify_splits\r\n>     raise NonMatchingSplitsSizesError(str(bad_splits))\r\n> nlp.utils.info_utils.NonMatchingSplitsSizesError: [{\'expected\': SplitInfo(name=\'test\', num_bytes=49424491, num_examples=11490, dataset_name=\'cnn_dailymail\'), \'recorded\': SplitInfo(name=\'test\', num_bytes=48931393, num_examples=11379, dataset_name=\'cnn_dailymail\')}, {\'expected\': SplitInfo(name=\'train\', num_bytes=1249178681, num_examples=287113, dataset_name=\'cnn_dailymail\'), \'recorded\': SplitInfo(name=\'train\', num_bytes=1240618482, num_examples=285161, dataset_name=\'cnn_dailymail\')}, {\'expected\': SplitInfo(name=\'validation\', num_bytes=57149241, num_examples=13368, dataset_name=\'cnn_dailymail\'), \'recorded\': SplitInfo(name=\'validation\', num_bytes=56637485, num_examples=13255, dataset_name=\'cnn_dailymail\')}]\r\n> ```\r\n\r\nWith `nlp == 0.3.0` version, I\'m not able to reproduce this error on my side.\r\nWhich version are you using for reproducing your bug?\r\n\r\n```\r\n>> nlp.load_dataset(\'cnn_dailymail\', \'3.0.0\')\r\n\r\n8.90k/8.90k [00:18<00:00, 486B/s]\r\n\r\nDownloading: 100%\r\n9.37k/9.37k [00:00<00:00, 234kB/s]\r\n\r\nDownloading and preparing dataset cnn_dailymail/3.0.0 (download: 558.32 MiB, generated: 1.26 GiB, total: 1.81 GiB) to /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0...\r\nDownloading:\r\n159M/? [00:09<00:00, 16.7MB/s]\r\n\r\nDownloading:\r\n376M/? [00:06<00:00, 62.6MB/s]\r\n\r\nDownloading:\r\n2.11M/? [00:06<00:00, 333kB/s]\r\n\r\nDownloading:\r\n46.4M/? [00:02<00:00, 18.4MB/s]\r\n\r\nDownloading:\r\n2.43M/? [00:00<00:00, 2.62MB/s]\r\n\r\nDataset cnn_dailymail downloaded and prepared to /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0. Subsequent calls will reuse this data.\r\n{\'test\': Dataset(schema: {\'article\': \'string\', \'highlights\': \'string\'}, num_rows: 11490),\r\n \'train\': Dataset(schema: {\'article\': \'string\', \'highlights\': \'string\'}, num_rows: 287113),\r\n \'validation\': Dataset(schema: {\'article\': \'string\', \'highlights\': \'string\'}, num_rows: 13368)}\r\n\r\n>> ...\r\n\r\n```'
 ""In general if some examples are missing after processing (hence causing the `NonMatchingSplitsSizesError `), it is often due to either\r\n1) corrupted cached files\r\n2) decoding errors\r\n\r\nI just checked the dataset script for code that could lead to decoding errors but I couldn't find any. Before we try to dive more into the processing of the dataset, could you try to clear your cache ? Just to make sure that it isn't 1)""
 'Yes thanks for the support! I cleared out my cache folder and everything works fine now']","```
>>> import nlp
>>> nlp.load_dataset('cnn_dailymail', '3.0.0')
Downloading and preparing dataset cnn_dailymail/3.0.0 (download: 558.32 MiB, generated: 1.26 GiB, total: 1.81 GiB) to /u/jm8wx/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0...

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/p/qdata/jm8wx/datasets/nlp/src/nlp/load.py"", line 520, in load_dataset
    builder_instance.download_and_prepare(
  File ""/p/qdata/jm8wx/datasets/nlp/src/nlp/builder.py"", line 431, in download_and_prepare
    self._download_and_prepare(
  File ""/p/qdata/jm8wx/datasets/nlp/src/nlp/builder.py"", line 488, in _download_and_prepare
    verify_splits(self.info.splits, split_dict)
  File ""/p/qdata/jm8wx/datasets/nlp/src/nlp/utils/info_utils.py"", line 70, in verify_splits
    raise NonMatchingSplitsSizesError(str(bad_splits))
nlp.utils.info_utils.NonMatchingSplitsSizesError: [{'expected': SplitInfo(name='test', num_bytes=49424491, num_examples=11490, dataset_name='cnn_dailymail'), 'recorded': SplitInfo(name='test', num_bytes=48931393, num_examples=11379, dataset_name='cnn_dailymail')}, {'expected': SplitInfo(name='train', num_bytes=1249178681, num_examples=287113, dataset_name='cnn_dailymail'), 'recorded': SplitInfo(name='train', num_bytes=1240618482, num_examples=285161, dataset_name='cnn_dailymail')}, {'expected': SplitInfo(name='validation', num_bytes=57149241, num_examples=13368, dataset_name='cnn_dailymail'), 'recorded': SplitInfo(name='validation', num_bytes=56637485, num_examples=13255, dataset_name='cnn_dailymail')}]
```"
https://github.com/huggingface/datasets/issues/329,[Bug] FileLock dependency incompatible with filesystem,"['Hi, can you give details on your environment/os/packages versions/etc?'
 'Environment is Ubuntu 18.04, Python 3.7.5, nlp==0.3.0, filelock=3.0.12.\r\n\r\nThe external volume is Amazon FSx for Lustre, and it by default creates files with limited permissions. My working theory is that FileLock creates a lockfile that isn\'t writable, and thus there\'s no way to acquire it by removing the .lock file. But Python is able to create new files and write to them outside of the FileLock package.\r\n\r\nWhen I attempt to use FileLock within a Docker container by writing to `/root/.cache/hello.txt`, it succeeds. So there\'s some permissions issue. But it\'s not a Docker configuration issue; I\'ve replicated it without Docker.\r\n```bash\r\necho ""hello world"" >> hello.txt\r\nls -l\r\n\r\n-rw-rw-r-- 1 ubuntu ubuntu 10 Jun 30 19:52 hello.txt\r\n```'
 'Looks like the `flock` syscall does not work on Lustre filesystems by default: https://github.com/benediktschmitt/py-filelock/issues/67.\r\n\r\nI added the `-o flock` option when mounting the filesystem, as [described here](https://docs.aws.amazon.com/fsx/latest/LustreGuide/getting-started-step2.html), which fixed the issue.'
 'Awesome, thanks a lot for sharing your fix!']","I'm downloading a dataset successfully with
`load_dataset(""wikitext"", ""wikitext-2-raw-v1"")`

But when I attempt to cache it on an external volume, it hangs indefinitely:
`load_dataset(""wikitext"", ""wikitext-2-raw-v1"", cache_dir=""/fsx"") # /fsx is an external volume mount`

The filesystem when hanging looks like this:
```bash
/fsx
----downloads
       ----94be...73.lock
----wikitext
       ----wikitext-2-raw
             ----wikitext-2-raw-1.0.0.incomplete
```

It appears that on this filesystem, the FileLock object is forever stuck in its ""acquire"" stage. I have verified that the issue lies specifically with the `filelock` dependency:
```python
open(""/fsx/hello.txt"").write(""hello"") # succeeds

from filelock import FileLock
with FileLock(""/fsx/hello.lock""):
    open(""/fsx/hello.txt"").write(""hello"") # hangs indefinitely
```

Has anyone else run into this issue? I'd raise it directly on the FileLock repo, but that project appears abandoned with the last update over a year ago. Or if there's a solution that would remove the FileLock dependency from the project, I would appreciate that."
https://github.com/huggingface/datasets/issues/328,Fork dataset,"['To be able to generate the Arrow dataset you need to either use our csv or json utilities `load_dataset(""json"", data_files=my_json_files)` OR write your own custom dataset script (you can find some inspiration from the [squad](https://github.com/huggingface/nlp/blob/master/datasets/squad/squad.py) script for example). Custom dataset scripts can be called locally with `nlp.load_dataset(path_to_my_script_directory)`.\r\n\r\nThis should help you get what you call ""Dataset1"".\r\n\r\nThen using some dataset transforms like `.map` for example you can get to ""DatasetNER"" and ""DatasetREL"".\r\n'
 ""Thanks for the helpful advice, @lhoestq  -- I wasn't quite able to get the json recipe working - \r\n\r\n```\r\n~/.virtualenvs/inv-text2struct/lib/python3.6/site-packages/pyarrow/ipc.py in __init__(self, source)\r\n     60 \r\n     61     def __init__(self, source):\r\n---> 62         self._open(source)\r\n     63 \r\n     64 \r\n~/.virtualenvs/inv-text2struct/lib/python3.6/site-packages/pyarrow/ipc.pxi in pyarrow.lib._RecordBatchStreamReader._open()\r\n~/.virtualenvs/inv-text2struct/lib/python3.6/site-packages/pyarrow/error.pxi in pyarrow.lib.pyarrow_internal_check_status()\r\n~/.virtualenvs/inv-text2struct/lib/python3.6/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()\r\nArrowInvalid: Tried reading schema message, was null or length 0\r\n```\r\n\r\nBut I'm going to give the generator_dataset_builder a try.\r\n\r\n1 more quick question -- can .map be used to output different length mappings -- could I skip one, or yield 2, can you map_batch  ""
 'You can use `.map(my_func, batched=True)` and return less examples, or more examples if you want'
 'Thanks this answers my question. I think the issue I was having using the json loader were due to using gzipped jsonl files.\r\n\r\nThe error I get now is :\r\n\r\n```\r\n\r\nUsing custom data configuration test\r\n---------------------------------------------------------------------------\r\n\r\nValueError                                Traceback (most recent call last)\r\n\r\n<ipython-input-38-29082a31e5b2> in <module>\r\n      5 print(ner_datafiles)\r\n      6 \r\n----> 7 ds = nlp.load_dataset(""json"", ""test"",  data_files=ner_datafiles[0])\r\n      8 \r\n\r\n~/.virtualenvs/inv-text2struct/lib/python3.6/site-packages/nlp/load.py in load_dataset(path, name, version, data_dir, data_files, split, cache_dir, download_config, download_mode, ignore_verifications, save_infos, **config_kwargs)\r\n    522         download_mode=download_mode,\r\n    523         ignore_verifications=ignore_verifications,\r\n--> 524         save_infos=save_infos,\r\n    525     )\r\n    526 \r\n\r\n~/.virtualenvs/inv-text2struct/lib/python3.6/site-packages/nlp/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, save_infos, try_from_hf_gcs, dl_manager, **download_and_prepare_kwargs)\r\n    430                 verify_infos = not save_infos and not ignore_verifications\r\n    431                 self._download_and_prepare(\r\n--> 432                     dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    433                 )\r\n    434                 # Sync info\r\n\r\n~/.virtualenvs/inv-text2struct/lib/python3.6/site-packages/nlp/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    481             try:\r\n    482                 # Prepare split will record examples associated to the split\r\n--> 483                 self._prepare_split(split_generator, **prepare_split_kwargs)\r\n    484             except OSError:\r\n    485                 raise OSError(""Cannot find data file. "" + (self.manual_download_instructions or """"))\r\n\r\n~/.virtualenvs/inv-text2struct/lib/python3.6/site-packages/nlp/builder.py in _prepare_split(self, split_generator)\r\n    736                     schema_dict[field.name] = Value(str(field.type))\r\n    737 \r\n--> 738         parse_schema(writer.schema, features)\r\n    739         self.info.features = Features(features)\r\n    740 \r\n\r\n~/.virtualenvs/inv-text2struct/lib/python3.6/site-packages/nlp/builder.py in parse_schema(schema, schema_dict)\r\n    734                     parse_schema(field.type.value_type, schema_dict[field.name])\r\n    735                 else:\r\n--> 736                     schema_dict[field.name] = Value(str(field.type))\r\n    737 \r\n    738         parse_schema(writer.schema, features)\r\n\r\n<string> in __init__(self, dtype, id, _type)\r\n\r\n~/.virtualenvs/inv-text2struct/lib/python3.6/site-packages/nlp/features.py in __post_init__(self)\r\n     55 \r\n     56     def __post_init__(self):\r\n---> 57         self.pa_type = string_to_arrow(self.dtype)\r\n     58 \r\n     59     def __call__(self):\r\n\r\n~/.virtualenvs/inv-text2struct/lib/python3.6/site-packages/nlp/features.py in string_to_arrow(type_str)\r\n     32         if str(type_str + ""_"") not in pa.__dict__:\r\n     33             raise ValueError(\r\n---> 34                 f""Neither {type_str} nor {type_str + \'_\'} seems to be a pyarrow data type. ""\r\n     35                 f""Please make sure to use a correct data type, see: ""\r\n     36                 f""https://arrow.apache.org/docs/python/api/datatypes.html#factory-functions""\r\n\r\nValueError: Neither list<item: int64> nor list<item: int64>_ seems to be a pyarrow data type. Please make sure to use a correct data type, see: https://arrow.apache.org/docs/python/api/datatypes.html#factory-functions.\r\n```\r\n\r\nIf I just create a pa- table manually like is done in the jsonloader -- it seems to work fine. Ths JSON I\'m trying to load isn\'t overly complex -  1 integer field, the rest text fields with a nested list of objects with text fields .'
 ""I'll close this -- It's still unclear how to go about troubleshooting the json example as I mentioned above. If I decide it's worth the trouble, I'll create another issue, or wait for a better support for using nlp for making custom data-loaders.""]","We have a multi-task learning model training I'm trying to convert to using the Arrow-based nlp dataset. 

We're currently training a custom TensorFlow model but the nlp paradigm should be a bridge for us to be able to use the wealth of pre-trained models in Transformers.

Our preprocessing flow parses raw text and json with Entity and Relations annotations and creates 2 datasets for training a NER and Relations prediction heads.

Is there some good way to ""fork"" dataset-

EG

1. text + json -> Dataset1
1. Dataset1 -> DatasetNER
1. Dataset1 -> DatasetREL

or 

1. text + json -> Dataset1
1. Dataset1 -> DatasetNER
1. Dataset1 + DatasetNER -> DatasetREL

"
https://github.com/huggingface/datasets/issues/326,Large dataset in Squad2-format,"[""I'm pretty sure you can get some inspiration from the squad_v2 script. It looks like the dataset is quite big so it will take some time for the users to generate it, but it should be reasonable.\r\n\r\nAlso you are saying that you are still making the dataset grow in size right ?\r\nIt's probably good practice to let the users do their training/evaluations with the exact same version of the dataset.\r\nWe allow for each dataset to specify a version (ex: 1.0.0) and increment this number every time there are new samples in the dataset for example. Does it look like a good solution for you ? Or would you rather have one final version with the full dataset ?""
 'It would also be good if there is any possibility for versioning, I think this way is much better than the dynamic way.\nIf you mean that part to put the tiles into one is the generation it would take up to 15-20 minutes on home computer hardware.\nAre there any compression or optimization algorithms while generating the dataset ?\nOtherwise the hardware limit is around 32 GB ram at the moment.\nIf everything works well we will add some more gigabytes of data in future what would make it pretty memory costly.'
 ""15-20 minutes is fine !\r\nAlso there's no RAM limitations as we save to disk every 1000 elements while generating the dataset by default.\r\nAfter generation, the dataset is ready to use with (again) no RAM limitations as we do memory-mapping.""
 ""Wow, that sounds pretty cool.\nActually I have the problem of running out of memory while tokenization on our local machine.\nThat wouldn't happen again, would it ?""
 'You can do the tokenization step using `my_tokenized_dataset = my_dataset.map(my_tokenize_function)` that writes the tokenized texts on disk as well. And then `my_tokenized_dataset` will be a memory-mapped dataset too, so you should be fine :)'
 'Does it have an affect to the trainings speed ?'
 ""In your training loop, loading the tokenized texts is going to be fast and pretty much negligible compared to a forward pass. You shouldn't expect any slow down.""
 'Closing this one. Feel free to re-open if you have other questions']","At the moment we are building an large question answering dataset and think about sharing it with the huggingface community.
Caused the computing power we splitted it into multiple tiles, but they are all in the same format.
Right now the most important facts about are this:
- Contexts: 1.047.671
- questions: 1.677.732
- Answers: 6.742.406
- unanswerable: 377.398

It is already cleaned

<pre><code>
train_data = [
    {
        'context': ""this is the context"",
        'qas': [
            {
                'id': ""00002"",
                'is_impossible': False,
                'question': ""whats is this"",
                'answers': [
                    {
                        'text': ""answer"",
                        'answer_start': 0
                    }
                ]
            },
            {
                'id': ""00003"",
                'is_impossible': False,
                'question': ""question2"",
                'answers': [
                    {
                        'text': ""answer2"",
                        'answer_start': 1
                    }
                ]
            }
        ]
    }
]
</code></pre>

Cause it is growing every day we are thinking about an structure like this:
We host an Json file, containing all the download links and the script can load it dynamically.
At the moment it is around ~20GB

Any advice how to handle this, or an ready to use template ?"
https://github.com/huggingface/datasets/issues/324,Error when calculating glue score,"['The glue metric for cola is a metric for classification. It expects label ids as integers as inputs.'
 'I want to evaluate a sentence pair whether they are semantically equivalent, so I used MRPC and it gives the same error, does that mean we have to encode the sentences and parse as input?\r\n\r\nusing BertTokenizer;\r\n```\r\nencoded_reference=tokenizer.encode(reference, add_special_tokens=False)\r\nencoded_prediction=tokenizer.encode(prediction, add_special_tokens=False)\r\n```\r\n\r\n`glue_score = glue_metric.compute(encoded_prediction, encoded_reference)`\r\n```\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-9-4c3a3ce7b583> in <module>()\r\n----> 1 glue_score = glue_metric.compute(encoded_prediction, encoded_reference)\r\n\r\n6 frames\r\n/usr/local/lib/python3.6/dist-packages/nlp/metric.py in compute(self, predictions, references, timeout, **metrics_kwargs)\r\n    198         predictions = self.data[""predictions""]\r\n    199         references = self.data[""references""]\r\n--> 200         output = self._compute(predictions=predictions, references=references, **metrics_kwargs)\r\n    201         return output\r\n    202 \r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/metrics/glue/27b1bc63e520833054bd0d7a8d0bc7f6aab84cc9eed1b576e98c806f9466d302/glue.py in _compute(self, predictions, references)\r\n    101             return pearson_and_spearman(predictions, references)\r\n    102         elif self.config_name in [""mrpc"", ""qqp""]:\r\n--> 103             return acc_and_f1(predictions, references)\r\n    104         elif self.config_name in [""sst2"", ""mnli"", ""mnli_mismatched"", ""mnli_matched"", ""qnli"", ""rte"", ""wnli"", ""hans""]:\r\n    105             return {""accuracy"": simple_accuracy(predictions, references)}\r\n\r\n/usr/local/lib/python3.6/dist-packages/nlp/metrics/glue/27b1bc63e520833054bd0d7a8d0bc7f6aab84cc9eed1b576e98c806f9466d302/glue.py in acc_and_f1(preds, labels)\r\n     60 def acc_and_f1(preds, labels):\r\n     61     acc = simple_accuracy(preds, labels)\r\n---> 62     f1 = f1_score(y_true=labels, y_pred=preds)\r\n     63     return {\r\n     64         ""accuracy"": acc,\r\n\r\n/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py in f1_score(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\r\n   1097                        pos_label=pos_label, average=average,\r\n   1098                        sample_weight=sample_weight,\r\n-> 1099                        zero_division=zero_division)\r\n   1100 \r\n   1101 \r\n\r\n/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py in fbeta_score(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\r\n   1224                                                  warn_for=(\'f-score\',),\r\n   1225                                                  sample_weight=sample_weight,\r\n-> 1226                                                  zero_division=zero_division)\r\n   1227     return f\r\n   1228 \r\n\r\n/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py in precision_recall_fscore_support(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\r\n   1482         raise ValueError(""beta should be >=0 in the F-beta score"")\r\n   1483     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\r\n-> 1484                                     pos_label)\r\n   1485 \r\n   1486     # Calculate tp_sum, pred_sum, true_sum ###\r\n\r\n/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py in _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\r\n   1314             raise ValueError(""Target is %s but average=\'binary\'. Please ""\r\n   1315                              ""choose another average setting, one of %r.""\r\n-> 1316                              % (y_type, average_options))\r\n   1317     elif pos_label not in (None, 1):\r\n   1318         warnings.warn(""Note that pos_label (set to %r) is ignored when ""\r\n\r\nValueError: Target is multiclass but average=\'binary\'. Please choose another average setting, one of [None, \'micro\', \'macro\', \'weighted\'].\r\n\r\n```'
 'MRPC is also a binary classification task, so its metric is a binary classification metric.\r\n\r\nTo evaluate if pairs of sentences are semantically equivalent, maybe you could take a look at models that compute if one sentence entails the other or not (typically the kinds of model that could work well on the MRPC task).'
 'Closing this one. Feel free to re-open if you have other questions :)']","I was trying glue score along with other metrics here. But glue gives me this error;

```
import nlp
glue_metric = nlp.load_metric('glue',name=""cola"")

glue_score = glue_metric.compute(predictions, references)
```

```
---------------------------------------------------------------------------
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-8-b9210a524504> in <module>()
----> 1 glue_score = glue_metric.compute(predictions, references)

6 frames
/usr/local/lib/python3.6/dist-packages/nlp/metric.py in compute(self, predictions, references, timeout, **metrics_kwargs)
    191         """"""
    192         if predictions is not None:
--> 193             self.add_batch(predictions=predictions, references=references)
    194         self.finalize(timeout=timeout)
    195 

/usr/local/lib/python3.6/dist-packages/nlp/metric.py in add_batch(self, predictions, references, **kwargs)
    207         if self.writer is None:
    208             self._init_writer()
--> 209         self.writer.write_batch(batch)
    210 
    211     def add(self, prediction=None, reference=None, **kwargs):

/usr/local/lib/python3.6/dist-packages/nlp/arrow_writer.py in write_batch(self, batch_examples, writer_batch_size)
    155         if self.pa_writer is None:
    156             self._build_writer(pa_table=pa.Table.from_pydict(batch_examples))
--> 157         pa_table: pa.Table = pa.Table.from_pydict(batch_examples, schema=self._schema)
    158         if writer_batch_size is None:
    159             writer_batch_size = self.writer_batch_size

/usr/local/lib/python3.6/dist-packages/pyarrow/types.pxi in __iter__()

/usr/local/lib/python3.6/dist-packages/pyarrow/array.pxi in pyarrow.lib.asarray()

/usr/local/lib/python3.6/dist-packages/pyarrow/array.pxi in pyarrow.lib.array()

/usr/local/lib/python3.6/dist-packages/pyarrow/array.pxi in pyarrow.lib._sequence_to_array()

TypeError: an integer is required (got type str)
```
I'm not sure whether I'm doing this wrong or whether it's an issue. I would like to know a workaround. Thank you."
https://github.com/huggingface/datasets/issues/321,ERROR:root:mwparserfromhell,"['It looks like it comes from `mwparserfromhell`.\r\n\r\nWould it be possible to get the bad `section` that causes this issue ? The `section` string is from `datasets/wikipedia.py:L548` ? You could just add a `try` statement and print the section if the line `section_text.append(section.strip_code().strip())` crashes.\r\n\r\nIt will help us know if we have to fix it on our side or if it is a `mwparserfromhell` issue.'
 ""Hi, \r\n\r\nThank you for you answer.\r\nI have try to print the bad section using `try` and `except`, but it is a bit weird as the error seems to appear 3 times for instance, but the two first error does not print anything (as if the function did not go in the `except` part).\r\nFor the third one, I got that (I haven't display the entire text) :\r\n\r\n> error : ==== Parque nacional Cajas ====\r\n> {{AP|Parque nacional Cajas}}\r\n> [[Archivo:Ecuador cajas national park.jpg|thumb|left|300px|Laguna del Cajas]]\r\n> El parque nacional Cajas está situado en los [[Cordillera de los Andes|Andes]], al sur del [[Ecuador]], en la provincia de [[Provincia de Azuay|Azuay]], a 33\r\n>  [[km]] al noroccidente de la ciudad de [[Cuenca (Ecuador)|Cuenca]]. Los accesos más comunes al parque inician todos en Cuenca: Desde allí, la vía Cuenca-Mol\r\n> leturo atraviesa en Control de [[Surocucho]] en poco más de 30 minutos de viaje; más adelante, esta misma carretera pasa a orillas de la laguna La Toreadora donde están el Centro Administrativo y de Información del parque. Siguiendo de largo hacia [[Molleturo]], por esta vía se conoce el sector norte del Cajas y se serpentea entre varias lagunas mayores y menores.\r\n> Para acceder al parque desde la costa, la vía Molleturo-Cuenca es también la mejor opción.\r\n\r\nHow can I display the link instead of the text ? I suppose it will help you more ""
 'The error appears several times as Apache Beam retries to process examples up to 4 times irc.\r\n\r\nI just tried to run this text into `mwparserfromhell` but it worked without the issue.\r\n\r\nI used this code (from the `wikipedia.py` script):\r\n```python\r\nimport mwparserfromhell as parser\r\nimport re\r\nimport six\r\n\r\nraw_content = r""""""==== Parque nacional Cajas ====\r\n{{AP|Parque nacional Cajas}}\r\n[[Archivo:Ecuador cajas national park.jpg|thumb|left|300px|Laguna del Cajas]]\r\nEl parque nacional Cajas está situado en los [[Cordillera de los Andes|Andes]], al sur del [[Ecuador]], en la provincia de [[Provincia de Azuay|Azuay]], a 33\r\n[[km]] al noroccidente de la ciudad de [[Cuenca (Ecuador)|Cuenca]]. Los accesos más comunes al parque inician todos en Cuenca: Desde allí, la vía Cuenca-Mol\r\nleturo atraviesa en Control de [[Surocucho]] en poco más de 30 minutos de viaje; más adelante, esta misma carretera pasa a orillas de la laguna La Toreadora donde están el Centro Administrativo y de Información del parque. Siguiendo de largo hacia [[Molleturo]], por esta vía se conoce el sector norte del Cajas y se serpentea entre varias lagunas mayores y menores.\r\n""""""\r\n\r\nwikicode = parser.parse(raw_content)\r\n\r\n# Filters for references, tables, and file/image links.\r\nre_rm_wikilink = re.compile(""^(?:File|Image|Media):"", flags=re.IGNORECASE | re.UNICODE)\r\n\r\ndef rm_wikilink(obj):\r\n    return bool(re_rm_wikilink.match(six.text_type(obj.title)))\r\n\r\ndef rm_tag(obj):\r\n    return six.text_type(obj.tag) in {""ref"", ""table""}\r\n\r\ndef rm_template(obj):\r\n    return obj.name.lower() in {""reflist"", ""notelist"", ""notelist-ua"", ""notelist-lr"", ""notelist-ur"", ""notelist-lg""}\r\n\r\ndef try_remove_obj(obj, section):\r\n    try:\r\n        section.remove(obj)\r\n    except ValueError:\r\n        # For unknown reasons, objects are sometimes not found.\r\n        pass\r\n\r\nsection_text = []\r\nfor section in wikicode.get_sections(flat=True, include_lead=True, include_headings=True):\r\n    for obj in section.ifilter_wikilinks(matches=rm_wikilink, recursive=True):\r\n        try_remove_obj(obj, section)\r\n    for obj in section.ifilter_templates(matches=rm_template, recursive=True):\r\n        try_remove_obj(obj, section)\r\n    for obj in section.ifilter_tags(matches=rm_tag, recursive=True):\r\n        try_remove_obj(obj, section)\r\n\r\n    section_text.append(section.strip_code().strip())\r\n```'
 ""Not sure why we're having this issue. Maybe could you get also the file that's causing that ?""
 'thanks for your answer.\r\nHow can I know which file is causing the issue ? \r\nI am trying to load the spanish wikipedia data. '
 ""Because of the way Apache Beam works we indeed don't have access to the file name at this point in the code.\r\nWe'll have to use some tricks I think :p \r\n\r\nYou can append `filepath` to `title` in `wikipedia.py:L512` for example. [[EDIT: it's L494 my bad]]\r\nThen just do `try:...except:` on the  call of `_parse_and_clean_wikicode` L500 I guess.\r\n\r\nThanks for diving into this ! I tried it myself but I run out of memory on my laptop\r\nAs soon as we have the name of the file it should be easier to find what's wrong.""
 'Thanks for your help.\r\n\r\nI tried to print the ""title"" of the document inside the` except (mwparserfromhell.parser.ParserError) as e`,the title displayed was : ""Campeonato Mundial de futsal de la AMF 2015"". (Wikipedia ES) Is it what you were looking for ?'
 'Thanks a lot @Shiro-LK !\r\n\r\nI was able to reproduce the issue. It comes from [this table on wikipedia](https://es.wikipedia.org/wiki/Campeonato_Mundial_de_futsal_de_la_AMF_2015#Clasificados) that can\'t be parsed.\r\n\r\nThe file in which the problem occurs comes from the wikipedia dumps, and it can be downloaded [here](https://dumps.wikimedia.org/eswiki/20200501/eswiki-20200501-pages-articles-multistream6.xml-p6424816p7924815.bz2)\r\n\r\nParsing the file this way raises the parsing issue:\r\n\r\n```python\r\nimport mwparserfromhell as parser\r\nfrom tqdm.auto import tqdm\r\nimport bz2\r\nimport six\r\nimport logging\r\nimport codecs\r\nimport xml.etree.cElementTree as etree\r\n\r\nfilepath = ""path/to/eswiki-20200501-pages-articles-multistream6.xml-p6424816p7924815.bz2""\r\n\r\ndef _extract_content(filepath):\r\n    """"""Extracts article content from a single WikiMedia XML file.""""""\r\n    logging.info(""generating examples from = %s"", filepath)\r\n    with open(filepath, ""rb"") as f:\r\n        f = bz2.BZ2File(filename=f)\r\n        if six.PY3:\r\n            # Workaround due to:\r\n            # https://github.com/tensorflow/tensorflow/issues/33563\r\n            utf_f = codecs.getreader(""utf-8"")(f)\r\n        else:\r\n            utf_f = f\r\n        # To clear root, to free-up more memory than just `elem.clear()`.\r\n        context = etree.iterparse(utf_f, events=(""end"",))\r\n        context = iter(context)\r\n        unused_event, root = next(context)\r\n        for unused_event, elem in tqdm(context, total=949087):\r\n            if not elem.tag.endswith(""page""):\r\n                continue\r\n            namespace = elem.tag[:-4]\r\n            title = elem.find(""./{0}title"".format(namespace)).text\r\n            ns = elem.find(""./{0}ns"".format(namespace)).text\r\n            id_ = elem.find(""./{0}id"".format(namespace)).text\r\n            # Filter pages that are not in the ""main"" namespace.\r\n            if ns != ""0"":\r\n                root.clear()\r\n                continue\r\n            raw_content = elem.find(""./{0}revision/{0}text"".format(namespace)).text\r\n            root.clear()\r\n\r\n            if ""Campeonato Mundial de futsal de la AMF 2015"" in title:\r\n                yield (id_, title, raw_content)\r\n\r\nfor id_, title, raw_content in _extract_content(filepath):\r\n    wikicode = parser.parse(raw_content)\r\n```\r\n\r\nThe copied the raw content that can\'t be parsed [here](https://pastebin.com/raw/ZbmevLyH).\r\n\r\nThe minimal code to reproduce is:\r\n```python\r\nimport mwparserfromhell as parser\r\nimport requests\r\n\r\nraw_content = requests.get(""https://pastebin.com/raw/ZbmevLyH"").content.decode(""utf-8"")\r\nwikicode = parser.parse(raw_content)\r\n\r\n```\r\n\r\nI will create an issue on mwparserfromhell\'s repo to see if we can fix that\r\n'
 'This going to be fixed in the next `mwparserfromhell` release :)']","Hi,

I am trying to download some wikipedia data but I got this error for spanish ""es"" (but there are maybe some others languages which have the same error I haven't tried all of them ).

`ERROR:root:mwparserfromhell ParseError: This is a bug and should be reported. Info: C tokenizer exited with non-empty token stack.`

The code I have use was : 
`dataset = load_dataset('wikipedia', '20200501.es', beam_runner='DirectRunner')`

"
https://github.com/huggingface/datasets/issues/320,"Blog Authorship Corpus, Non Matching Splits Sizes Error, nlp viewer","['I wonder if this means downloading failed? That corpus has a really slow server.'
 ""This dataset seems to have a decoding problem that results in inconsistencies in the number of generated examples.\r\nSee #215.\r\nThat's why we end up with a `NonMatchingSplitsSizesError `.""]","Selecting `blog_authorship_corpus` in the nlp viewer throws the following error: 

```
NonMatchingSplitsSizesError: [{'expected': SplitInfo(name='train', num_bytes=610252351, num_examples=532812, dataset_name='blog_authorship_corpus'), 'recorded': SplitInfo(name='train', num_bytes=614706451, num_examples=535568, dataset_name='blog_authorship_corpus')}, {'expected': SplitInfo(name='validation', num_bytes=37500394, num_examples=31277, dataset_name='blog_authorship_corpus'), 'recorded': SplitInfo(name='validation', num_bytes=32553710, num_examples=28521, dataset_name='blog_authorship_corpus')}]
Traceback:
File ""/home/sasha/streamlit/lib/streamlit/ScriptRunner.py"", line 322, in _run_script
    exec(code, module.__dict__)
File ""/home/sasha/nlp-viewer/run.py"", line 172, in <module>
    dts, fail = get(str(option.id), str(conf_option.name) if conf_option else None)
File ""/home/sasha/streamlit/lib/streamlit/caching.py"", line 591, in wrapped_func
    return get_or_create_cached_value()
File ""/home/sasha/streamlit/lib/streamlit/caching.py"", line 575, in get_or_create_cached_value
    return_value = func(*args, **kwargs)
File ""/home/sasha/nlp-viewer/run.py"", line 132, in get
    builder_instance.download_and_prepare()
File ""/home/sasha/.local/share/virtualenvs/lib-ogGKnCK_/lib/python3.7/site-packages/nlp/builder.py"", line 432, in download_and_prepare
    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
File ""/home/sasha/.local/share/virtualenvs/lib-ogGKnCK_/lib/python3.7/site-packages/nlp/builder.py"", line 488, in _download_and_prepare
    verify_splits(self.info.splits, split_dict)
File ""/home/sasha/.local/share/virtualenvs/lib-ogGKnCK_/lib/python3.7/site-packages/nlp/utils/info_utils.py"", line 70, in verify_splits
    raise NonMatchingSplitsSizesError(str(bad_splits))
```
@srush @lhoestq "
https://github.com/huggingface/datasets/issues/319,Nested sequences with dicts,"['Oh yes, this is a backward compatibility feature with tensorflow_dataset in which a `Sequence` or `dict` is converted in a `dict` of `lists`, unfortunately it is not very intuitive, see here: https://github.com/huggingface/nlp/blob/master/src/nlp/features.py#L409\r\n\r\nTo avoid this behavior, you can just define the list in the feature with a simple list or a tuple (which is also simpler to write).\r\nIn your case, the features could be as follow:\r\n``` python\r\n...\r\nfeatures=nlp.Features({\r\n    ""title"": nlp.Value(""string""),\r\n    ""vertexSet"": [[{\r\n        ""name"": nlp.Value(""string""),\r\n        ""sent_id"": nlp.Value(""int32""),\r\n        ""pos"": nlp.features.Sequence(nlp.Value(""int32"")),\r\n        ""type"": nlp.Value(""string""),\r\n    }]],\r\n    ...\r\n    }),\r\n...\r\n```']","Am pretty much finished [adding a dataset](https://github.com/ghomasHudson/nlp/blob/DocRED/datasets/docred/docred.py) for [DocRED](https://github.com/thunlp/DocRED), but am getting an error when trying to add a nested `nlp.features.sequence(nlp.features.sequence({key:value,...}))`. 

The original data is in this format:
```python
{
  'title': ""Title of wiki page"",
  'vertexSet': [
                  [
                    { 'name': ""mention_name"", 
                      'sent_id': ""mention in which sentence"", 
                      'pos': [""postion of mention in a sentence""], 
                      'type': ""NER_type""},
                    {another mention}
                  ], 
                  [another entity]
                ]
    ...
}
```
So to represent this I've attempted to write:
```
...
features=nlp.Features({
    ""title"": nlp.Value(""string""),
    ""vertexSet"": nlp.features.Sequence(nlp.features.Sequence({
        ""name"": nlp.Value(""string""),
        ""sent_id"": nlp.Value(""int32""),
        ""pos"": nlp.features.Sequence(nlp.Value(""int32"")),
        ""type"": nlp.Value(""string""),
    })),
    ...
    }),
...
```
This is giving me the error:
```
pyarrow.lib.ArrowTypeError: Could not convert [{'pos': [[0,2], [2,4], [3,5]], ""type"": [""ORG"", ""ORG"", ""ORG""], ""name"": [""Lark Force"", ""Lark Force"", ""Lark Force"", ""sent_id"": [0, 3, 4]}..... with type list: was not a dict, tuple, or recognized null value for conversion to struct type
```
Do we expect the pyarrow stuff to break when doing this deeper nesting? I've checked that it still works when you do `nlp.features.Sequence(nlp.features.Sequence(nlp.Value(""string""))` or `nlp.features.Sequence({key:value,...})` just not nested sequences with a dict.

If it's not possible, I can always convert it to a shallower structure. I'd rather not change the DocRED authors' structure if I don't have to though."
https://github.com/huggingface/datasets/issues/317,Adding a dataset with multiple subtasks,"['For one dataset you can have different configurations that each have their own `nlp.Features`.\r\nWe imagine having one configuration per subtask for example.\r\nThey are loaded with `nlp.load_dataset(""my_dataset"", ""my_config"")`.\r\n\r\nFor example the `glue` dataset has many configurations. It is a bit different from your case though because each configuration is a dataset by itself (sst2, mnli).\r\nAnother example is `wikipedia` that has one configuration per language.']","I intent to add the datasets of the MT Quality Estimation shared tasks to `nlp`. However, they have different subtasks -- such as word-level, sentence-level and document-level quality estimation, each of which having different language pairs, and some of the data reused in different subtasks.

For example, in [QE 2019,](http://www.statmt.org/wmt19/qe-task.html) we had the same English-Russian and English-German data for word-level and sentence-level QE. 

I suppose these datasets could have both their word and sentence-level labels inside `nlp.Features`; but what about other subtasks? Should they be considered a different dataset altogether?

I read the discussion on #217 but the case of QE seems a lot simpler."
https://github.com/huggingface/datasets/issues/315,[Question] Best way to batch a large dataset?,"['Update: I think I\'ve found a solution.\r\n\r\n```python\r\noutput_types = {""input_ids"": tf.int64, ""token_type_ids"": tf.int64, ""attention_mask"": tf.int64}\r\ndef train_dataset_gen():\r\n    for i in range(len(train_dataset)):\r\n        yield train_dataset[i]\r\ntf_dataset = tf.data.Dataset.from_generator(train_dataset_gen, output_types=output_types)\r\n```\r\n\r\nloads WikiText-2 in 20 ms, and WikiText-103 in 20 ms. It appears to be lazily loading via indexing train_dataset.'
 ""Yes this is the current best solution. We should probably show it in the tutorial notebook.\r\n\r\nNote that this solution unfortunately doesn't allow to train on TPUs (yet). See #193 ""
 'This approach still seems quite slow. When using TFRecords with a similar training loop, I get ~3.0-3.5 it/s on multi-node, multi-GPU training. I notice a pretty severe performance regression when scaling, with observed performance numbers. Since the allreduce step takes less than 100ms/it and I\'ve achieved 80% scaling efficiency up to 64 GPUs, it must be the data pipeline.\r\n\r\n| Nodes | GPUs | Iterations/Second |\r\n| --- | --- | --- |\r\n| 1 | 2 | 2.01 |\r\n| 1 | 8 | 0.81 |\r\n| 2 | 16 | 0.37 |\r\n\r\nHere are performance metrics over 10k steps. The iteration speed appears to follow some sort of caching pattern. I would love to use `nlp` in my project, but a slowdown from 3.0 it/s to 0.3 it/s is too great to stomach.\r\n\r\n<img width=""1361"" alt=""Screen Shot 2020-07-02 at 8 29 22 AM"" src=""https://user-images.githubusercontent.com/4564897/86378156-2f8d3900-bc3e-11ea-918b-c395c3df5377.png"">\r\n'
 'An interesting alternative to investigate here would be to use the tf.io library which has some support for Arrow to TF conversion: https://www.tensorflow.org/io/api_docs/python/tfio/arrow/ArrowDataset\r\n\r\nThere are quite a few types supported, including lists so if the unsupported columns are dropped then we could maybe have a zero-copy mapping from Arrow to TensorFlow, including tokenized inputs and 1D tensors like the ones we mostly use in NLP: https://github.com/tensorflow/io/blob/322b3170c43ecac5c6af9e39dbd18fd747913e5a/tensorflow_io/arrow/python/ops/arrow_dataset_ops.py#L44-L72\r\n\r\nHere is an introduction on Arrow to TF using tf.io: https://medium.com/tensorflow/tensorflow-with-apache-arrow-datasets-cdbcfe80a59f'
 'Interesting. There\'s no support for strings, but it does enable int and floats so that would work for tokenized inputs. \r\n\r\nArrowStreamDataset requires loading from a ""record batch iterator"", which can be instantiated from in-memory arrays as described here: https://arrow.apache.org/docs/python/ipc.html. \r\n\r\nBut the nlp.Dataset stores its data as a `pyarrow.lib.Table`, and the underlying features are `pyarrow.lib.ChunkedArray`. I can\'t find any documentation about lazily creating a record batch iterator from a ChunkedArray or a Table. Have you had any success?\r\n\r\nI can\'t find [any uses](https://grep.app/search?q=ArrowDataset&filter[lang][0]=Python) of tfio.arrow.ArrowDataset on GitHub.'
 'You can use `to_batches` maybe?\r\nhttps://arrow.apache.org/docs/python/generated/pyarrow.Table.html#pyarrow.Table.to_batches'
 ""Also note that since #322 it is now possible to do\r\n```python\r\nids = [1, 10, 42, 100]\r\nbatch = dataset[ids]\r\n```\r\nFrom my experience it is quite fast but it can take lots of memory for large batches (haven't played that much with it).\r\nLet me know if you think there could be a better way to implement it. (current code is [here](https://github.com/huggingface/nlp/blob/78628649962671b4aaa31a6b24e7275533416845/src/nlp/arrow_dataset.py#L463))""
 'Thanks @lhoestq! That format is much better to work with.\r\n\r\nI put together a benchmarking script. This doesn\'t measure the CPU-to-GPU efficiency, nor how it scales with multi-GPU multi-node training where many processes are making the same demands on the same dataset. But it does show some interesting results:\r\n\r\n```python\r\nimport nlp\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport time\r\n\r\ndset = nlp.load_dataset(""wikitext"", ""wikitext-2-raw-v1"", split=""train"")\r\ndset = dset.filter(lambda ex: len(ex[""text""]) > 0)\r\nbsz = 1024\r\nn_batches = 100\r\n\r\ndef single_item_gen():\r\n    for i in range(len(dset)):\r\n        yield dset[i]\r\n\r\ndef sequential_batch_gen():\r\n    for i in range(0, len(dset), bsz):\r\n        yield dset[i:i+bsz]\r\n\r\ndef random_batch_gen():\r\n    for i in range(len(dset)):\r\n        indices = list(np.random.randint(len(dset), size=(bsz,)))\r\n        yield dset[indices]\r\n\r\noutput_types = {""text"": tf.string}\r\nsingle_item = tf.data.Dataset.from_generator(single_item_gen, output_types=output_types).batch(bsz)\r\ninterleaved = tf.data.Dataset.range(10).interleave(\r\n    lambda idx: tf.data.Dataset.from_generator(single_item_gen, output_types=output_types),\r\n    cycle_length=10,\r\n)\r\nsequential_batch = tf.data.Dataset.from_generator(sequential_batch_gen, output_types=output_types)\r\nrandom_batch = tf.data.Dataset.from_generator(random_batch_gen, output_types=output_types)\r\n\r\ndef iterate(tf_dset):\r\n    start = time.perf_counter()\r\n    for i, batch in enumerate(tf_dset.take(n_batches)):\r\n        pass\r\n    elapsed = time.perf_counter() - start\r\n    print(f""{tf_dset} took {elapsed:.3f} secs"")\r\n\r\niterate(single_item)\r\niterate(interleaved)\r\niterate(sequential_batch)\r\niterate(random_batch)\r\n```\r\n\r\nResults:\r\n```\r\n<BatchDataset shapes: {text: <unknown>}, types: {text: tf.string}> took 23.005 secs\r\n<InterleaveDataset shapes: {text: <unknown>}, types: {text: tf.string}> took 0.135 secs\r\n<FlatMapDataset shapes: {text: <unknown>}, types: {text: tf.string}> took 0.074 secs\r\n<FlatMapDataset shapes: {text: <unknown>}, types: {text: tf.string}> took 0.550 secs\r\n```\r\n\r\n- Batching a generator which fetches a single item is terrible.\r\n- Interleaving performs well on a single process, but doesn\'t scale well to multi-GPU training. I believe the bottleneck here is in Arrow dataset locking or something similar. The numbers from the table above are with interleaving.\r\n- The sequential access dominates the random access (7x faster). Is there any way to bring random access times closer to sequential access? Maybe re-indexing the dataset after shuffling each pass over the data.'
 'Hey @jarednielsen \r\n\r\nThanks for this very interesting analysis!! IMHO to read text data one should use `tf.data.TextLineDataset`. It would be interesting to compare what you have done with simply load with a `TextLineDataset` and see if there is a difference.\r\n\r\nA good example can be found here https://www.tensorflow.org/tutorials/load_data/text'
 ""Thanks! I'm not actually loading in raw text data, that was just the synthetic data I created for this benchmark. A more realistic use case would be a dataset of tokenized examples, which would be a dict of lists of integers. TensorFlow's TextLineDataset greedily loads the dataset into the graph itself, which can lead to out-of-memory errors - one of the main reason I'm so drawn to the `nlp` library is its zero-copy no-RAM approach to dataset loading and mapping. \r\n\r\nIt's quite helpful for running a preprocessing pipeline - a sample ELECTRA pipeline I've built is here: https://github.com/jarednielsen/deep-learning-models/blob/nlp/models/nlp/common/preprocess.py.""
 'Sorry, I think I badly expressed myself, my bad. What I suggested is to compare with the usual loading textual data in pure TF with `TextLineDataset` with `nlp`. I know it is not recommended with very large datasets to use it, but I was curious to see how it behaves compared to a processing with `nlp` on smaller datasets.\r\n\r\nBTW your script looks very interesting, thanks for sharing!!']","I'm training on large datasets such as Wikipedia and BookCorpus. Following the instructions in [the tutorial notebook](https://colab.research.google.com/github/huggingface/nlp/blob/master/notebooks/Overview.ipynb), I see the following recommended for TensorFlow:

```python
train_tf_dataset = train_tf_dataset.filter(remove_none_values, load_from_cache_file=False)
columns = ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions']
train_tf_dataset.set_format(type='tensorflow', columns=columns)
features = {x: train_tf_dataset[x].to_tensor(default_value=0, shape=[None, tokenizer.max_len]) for x in columns[:3]} 
labels = {""output_1"": train_tf_dataset[""start_positions""].to_tensor(default_value=0, shape=[None, 1])}
labels[""output_2""] = train_tf_dataset[""end_positions""].to_tensor(default_value=0, shape=[None, 1])
### Question about this last line ###
tfdataset = tf.data.Dataset.from_tensor_slices((features, labels)).batch(8)
```

This code works for something like WikiText-2. However, scaling up to WikiText-103, the last line takes 5-10 minutes to run. I assume it is because tf.data.Dataset.from_tensor_slices() is pulling everything into memory, not lazily loading. This approach won't scale up to datasets 25x larger such as Wikipedia.

So I tried manual batching using `dataset.select()`:

```python
idxs = np.random.randint(len(dataset), size=bsz)
batch = dataset.select(idxs).map(lambda example: {""input_ids"": tokenizer(example[""text""])})
tf_batch = tf.constant(batch[""ids""], dtype=tf.int64)
```

This appears to create a new Apache Arrow dataset with every batch I grab, and then tries to cache it. The runtime of `dataset.select([0, 1])` appears to be much worse than `dataset[:2]`. So using `select()` doesn't seem to be performant enough for a training loop.

Is there a performant scalable way to lazily load batches of nlp Datasets?"
https://github.com/huggingface/datasets/issues/312,[Feature request] Add `shard()` method to dataset,"['Hi Jared,\r\nInteresting, thanks for raising this question. You can also do that after loading with `dataset.select()` or `dataset.filter()` which let you keep only a specific subset of rows in a dataset.\r\nWhat is your use-case for sharding?'
 ""Thanks for the pointer to those functions! It's still a little more verbose since you have to manually calculate which ids each rank would keep, but definitely works.\r\n\r\nMy use case is multi-node, multi-GPU training and avoiding global batches of duplicate elements. I'm using horovod. You can shuffle indices, or set random seeds, but explicitly sharding the dataset up front is the safest and clearest way I've found to do so.""]","Currently, to shard a dataset into 10 pieces on different ranks, you can run

```python
rank = 3 # for example
size = 10
dataset = nlp.load_dataset('wikitext', 'wikitext-2-raw-v1', split=f""train[{rank*10}%:{(rank+1)*10}%]"")
```

However, this breaks down if you have a number of ranks that doesn't divide cleanly into 100, such as 64 ranks. Is there interest in adding a method shard() that looks like this?

```python
rank = 3
size = 64
dataset = nlp.load_dataset(""wikitext"", ""wikitext-2-raw-v1"", split=""train"").shard(rank=rank, size=size)
```

TensorFlow has a similar API: https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shard. I'd be happy to contribute this code."
https://github.com/huggingface/datasets/issues/302,Question - Sign Language Datasets,"['Even more complicating - \r\n\r\nAs I see it, datasets can have ""addons"".\r\nFor example, the WebNLG dataset is a dataset for data-to-text. However, a work of mine and other works enriched this dataset with text plans / underlying text structures. In that case, I see a need to load the dataset ""WebNLG"" with ""plans"" addon.\r\n\r\nSame for sign language - if there is a dataset of videos, one addon can be to run OpenPose, another to run ARKit4 pose estimation, and another to run PoseNet, or even just a video embedding addon. (which are expensive to run individually for everyone who wants to use these data)\r\n\r\nThis is something I dabbled with my own implementation to a [research datasets library](https://github.com/AmitMY/meta-scholar/) and I love to get the discussion going on these topics.'
 ""This is a really cool idea !\r\nThe example for data objects you gave for the RWTH-PHOENIX-Weather 2014 T dataset can totally fit inside the library.\r\n\r\nFor your point about formats like `ilex`, `eaf`, or `srt`, it is possible to use any library in your dataset script.\r\nHowever most user probably won't need these libraries, as most datasets don't need them, and therefore it's unlikely that we will have them in the minimum requirements to use `nlp` (we want to keep it as light-weight as possible). If a user wants to load your dataset and doesn't have the libraries you need, an error is raised asking the user to install them.\r\n\r\nMore generally, we plan to have something like a `requirements.txt` per dataset. This could also be a place for addons as you said. What do you think ?""
 ""Thanks, Quentin, I think a `requirements.txt` per dataset will be a good thing.\r\nI will work on adding this dataset next week, and once we sort all of the kinks, I'll add more.""]","An emerging field in NLP is SLP - sign language processing.

I was wondering about adding datasets here, specifically because it's shaping up to be large and easily usable.
The metrics for sign language to text translation are the same.

So, what do you think about (me, or others) adding datasets here?


An example dataset would be [RWTH-PHOENIX-Weather 2014 T](https://www-i6.informatik.rwth-aachen.de/~koller/RWTH-PHOENIX-2014-T/)
For every item in the dataset, the data object includes:
1. video_path - path to mp4 file
2. pose_path - a path to `.pose` file with human pose landmarks
3. openpose_path - a path to a `.json` file with human pose landmarks
4. gloss - string
5. text - string
6. video_metadata - height, width, frames, framerate


------

To make it a tad more complicated - what if sign language libraries add requirements to `nlp`? for example, sign language is commonly annotated using `ilex`, `eaf`, or `srt` files, which are all loadable as text, but there is no reason for the dataset to parse that file by itself, if libraries exist to do so."
https://github.com/huggingface/datasets/issues/301,Setting cache_dir gives error on wikipedia download,"[""Whoops didn't mean to close this one.\r\nI did some changes, could you try to run it from the master branch ?""
 'Now it works, thanks!']","First of all thank you for a super handy library! I'd like to download large files to a specific drive so I set `cache_dir=my_path`. This works fine with e.g. imdb and squad. But on wikipedia I get an error:
```
nlp.load_dataset('wikipedia', '20200501.de', split = 'train', cache_dir=my_path)
```
```
OSError                                   Traceback (most recent call last)
<ipython-input-2-23551344d7bc> in <module>
      1 import nlp
----> 2 nlp.load_dataset('wikipedia', '20200501.de', split = 'train', cache_dir=path)

~/anaconda3/envs/fastai2/lib/python3.7/site-packages/nlp/load.py in load_dataset(path, name, version, data_dir, data_files, split, cache_dir, download_config, download_mode, ignore_verifications, save_infos, **config_kwargs)
    522         download_mode=download_mode,
    523         ignore_verifications=ignore_verifications,
--> 524         save_infos=save_infos,
    525     )
    526 

~/anaconda3/envs/fastai2/lib/python3.7/site-packages/nlp/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, save_infos, try_from_hf_gcs, dl_manager, **download_and_prepare_kwargs)
    385                     with utils.temporary_assignment(self, ""_cache_dir"", tmp_data_dir):
    386                         reader = ArrowReader(self._cache_dir, self.info)
--> 387                         reader.download_from_hf_gcs(self._cache_dir, self._relative_data_dir(with_version=True))
    388                         downloaded_info = DatasetInfo.from_directory(self._cache_dir)
    389                         self.info.update(downloaded_info)

~/anaconda3/envs/fastai2/lib/python3.7/site-packages/nlp/arrow_reader.py in download_from_hf_gcs(self, cache_dir, relative_data_dir)
    231             remote_dataset_info = os.path.join(remote_cache_dir, ""dataset_info.json"")
    232             downloaded_dataset_info = cached_path(remote_dataset_info)
--> 233             os.rename(downloaded_dataset_info, os.path.join(cache_dir, ""dataset_info.json""))
    234             if self._info is not None:
    235                 self._info.update(self._info.from_directory(cache_dir))

OSError: [Errno 18] Invalid cross-device link: '/home/local/NTU/nn/.cache/huggingface/datasets/025fa4fd4f04aaafc9e939260fbc8f0bb190ce14c61310c8ae1ddd1dcb31f88c.9637f367b6711a79ca478be55fe6989b8aea4941b7ef7adc67b89ff403020947' -> '/data/nn/nlp/wikipedia/20200501.de/1.0.0.incomplete/dataset_info.json'
```"
https://github.com/huggingface/datasets/issues/297,Error in Demo for Specific Datasets,"[""Thanks for reporting these errors :)\r\n\r\nI can actually see two issues here.\r\n\r\nFirst, datasets like `natural_questions` require apache_beam to be processed. Right now the import is not at the right place so we have this error message. However, even the imports are fixed, the nlp viewer doesn't actually have the resources to process NQ right now so we'll have to wait until we have a version that we've already processed on our google storage (that's what we've done for wikipedia for example).\r\n\r\nSecond, datasets like `newsroom` require manual downloads as we're not allowed to redistribute the data ourselves (if I'm not wrong). An error message should be displayed saying that we're not allowed to show the dataset.\r\n\r\nI can fix the first issue with the imports but for the second one I think we'll have to see with @srush to show a message for datasets that require manual downloads (it can be checked whether a dataset requires manual downloads if `dataset_builder_instance.manual_download_instructions is not None`).\r\n\r\n""
 'I added apache-beam to the viewer. We can think about how to add newsroom. '
 ""We don't plan to host the source files of newsroom ourselves for now.\r\nYou can still get the dataset if you follow the download instructions given by `dataset = load_dataset('newsroom')` though.\r\nThe viewer also shows the instructions now.\r\n\r\nClosing this one. If you have other questions, feel free to re-open :)""]","Selecting `natural_questions` or `newsroom` dataset in the online demo results in an error similar to the following.

![image](https://user-images.githubusercontent.com/60150701/85347842-ac861900-b4ae-11ea-98c4-a53a00934783.png)
"
https://github.com/huggingface/datasets/issues/296,snli -1 labels,"['@jxmorris12  , we use `-1` to label examples for which `gold label`  is missing (`gold label = -` in the original dataset). '
 'Thanks @mariamabarham! so the original dataset is missing some labels? That is weird. Is standard practice just to discard those examples training/eval?'
 ""Yes the original dataset is missing some labels maybe  @sleepinyourhat , @gangeli can correct me if I'm wrong \r\nFor my personal opinion at least if you want your model to learn to predict no answer (-1) you can leave it their but otherwise you can discard them. ""
 'thanks @mariamabarham  :)']","I'm trying to train a model on the SNLI dataset. Why does it have so many -1 labels?
```
import nlp
from collections import Counter
data = nlp.load_dataset('snli')['train']
print(Counter(data['label']))
Counter({0: 183416, 2: 183187, 1: 182764, -1: 785})
```
"
https://github.com/huggingface/datasets/issues/294,Cannot load arxiv dataset on MacOS?,"[""I couldn't replicate this issue on my macbook :/\r\nCould you try to play with different encodings in `with open(path, encoding=...) as f` in scientific_papers.py:L108 ?""
 'I was able to track down the file causing the problem by adding the following to `scientific_papers.py` (starting at line 116):\r\n\r\n```python\r\n                from json import JSONDecodeError\r\n                try:\r\n                    d = json.loads(line)\r\n                    summary = ""\\n"".join(d[""abstract_text""])\r\n                except JSONDecodeError:\r\n                    print(path, line)\r\n```\r\n\r\n\r\n\r\nFor me it was at: `/Users/johngiorgi/.cache/huggingface/datasets/f87fd498c5003cbe253a2af422caa1e58f87a4fd74cb3e67350c635c8903b259/arxiv-dataset/train.txt` with `""article_id"": ""1407.3051""`.\r\n\r\nNot really 100% sure at the moment, but it looks like this specific substring from `""article_text""` may be causing the problem?\r\n\r\n```\r\n""after the missing - mass scale adjustment , the validity of the corrections was tested in the @xmath85 productions at 1.69 gev/@xmath1 . in fig .  ["", ""fig : calibrations ] ( a ) , we show the missing - mass spectrum in the @xmath86 region in the @xmath87 reaction at 1.69 gev/@xmath1 . a fitting result with a lorentzian function for the @xmath86  ( dashed line ) and the three - body phas\r\n```\r\n\r\nperhaps because it appears to be truncated. I (think) I can recreate the problem by doing the following:\r\n\r\n```python\r\nimport json\r\n\r\n# A minimal example of the json file that causes the error\r\ninvalid_json = \'{""article_id"": ""1407.3051"", ""article_text"": [""the missing - mass resolution was obtained to be 2.8 @xmath3 0.1  mev/@xmath4  ( fwhm ) , which corresponds to the missing - mass resolution of 3.2  @xmath3  0.2  mev/@xmath4  ( fwhm ) at the @xmath6 cusp region in the @xmath0 reaction ."", ""this resolution is at least by a factor of 2 better than the previous measurement with the same reaction ( 3.2@xmath595.5 mev/@xmath4 in @xmath84 )  @xcite ."", ""after the missing - mass scale adjustment , the validity of the corrections was tested in the @xmath85 productions at 1.69 gev/@xmath1 . in fig .  ["", ""fig : calibrations ] ( a ) , we show the missing - mass spectrum in the @xmath86 region in the @xmath87 reaction at 1.69 gev/@xmath1 . a fitting result with a lorentzian function for the @xmath86  ( dashed line ) and the three - body phas\'   \r\n# The line of code from `scientific_papers.py` which appears to cause the error\r\njson.loads(invalid_json)\r\n```\r\n\r\nThis is as far as I get before I am stumped.'
 ""I just checked inside `train.txt` and this line isn't truncated for me (line 163577).\r\nCould you try to clear your cache and re-download the dataset ?""
 'Ah the turn-it-off-turn-it-on again solution! That did it, thanks a lot :) ']","I am having trouble loading the `""arxiv""` config from the `""scientific_papers""` dataset on MacOS. When I try loading the dataset with:

```python
arxiv = nlp.load_dataset(""scientific_papers"", ""arxiv"")
```

I get the following stack trace:

```bash
JSONDecodeError                           Traceback (most recent call last)
<ipython-input-2-8e00c55d5a59> in <module>
----> 1 arxiv = nlp.load_dataset(""scientific_papers"", ""arxiv"")

~/miniconda3/envs/t2t/lib/python3.7/site-packages/nlp/load.py in load_dataset(path, name, version, data_dir, data_files, split, cache_dir, download_config, download_mode, ignore_verifications, save_infos, **config_kwargs)
    522         download_mode=download_mode,
    523         ignore_verifications=ignore_verifications,
--> 524         save_infos=save_infos,
    525     )
    526 

~/miniconda3/envs/t2t/lib/python3.7/site-packages/nlp/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, save_infos, try_from_hf_gcs, dl_manager, **download_and_prepare_kwargs)
    430                 verify_infos = not save_infos and not ignore_verifications
    431                 self._download_and_prepare(
--> 432                     dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
    433                 )
    434                 # Sync info

~/miniconda3/envs/t2t/lib/python3.7/site-packages/nlp/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)
    481             try:
    482                 # Prepare split will record examples associated to the split
--> 483                 self._prepare_split(split_generator, **prepare_split_kwargs)
    484             except OSError:
    485                 raise OSError(""Cannot find data file. "" + (self.manual_download_instructions or """"))

~/miniconda3/envs/t2t/lib/python3.7/site-packages/nlp/builder.py in _prepare_split(self, split_generator)
    662 
    663         generator = self._generate_examples(**split_generator.gen_kwargs)
--> 664         for key, record in utils.tqdm(generator, unit="" examples"", total=split_info.num_examples, leave=False):
    665             example = self.info.features.encode_example(record)
    666             writer.write(example)

~/miniconda3/envs/t2t/lib/python3.7/site-packages/tqdm/std.py in __iter__(self)
   1106                 fp_write=getattr(self.fp, 'write', sys.stderr.write))
   1107 
-> 1108         for obj in iterable:
   1109             yield obj
   1110             # Update and possibly print the progressbar.

~/miniconda3/envs/t2t/lib/python3.7/site-packages/nlp/datasets/scientific_papers/107a416c0e1958cb846f5934b5aae292f7884a5b27e86af3f3ef1a093e058bbc/scientific_papers.py in _generate_examples(self, path)
    114                 # ""section_names"": list[str], list of section names.
    115                 # ""sections"": list[list[str]], list of sections (list of paragraphs)
--> 116                 d = json.loads(line)
    117                 summary = ""\n"".join(d[""abstract_text""])
    118                 # In original paper, <S> and </S> are not used in vocab during training

~/miniconda3/envs/t2t/lib/python3.7/json/__init__.py in loads(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)
    346             parse_int is None and parse_float is None and
    347             parse_constant is None and object_pairs_hook is None and not kw):
--> 348         return _default_decoder.decode(s)
    349     if cls is None:
    350         cls = JSONDecoder

~/miniconda3/envs/t2t/lib/python3.7/json/decoder.py in decode(self, s, _w)
    335 
    336         """"""
--> 337         obj, end = self.raw_decode(s, idx=_w(s, 0).end())
    338         end = _w(s, end).end()
    339         if end != len(s):

~/miniconda3/envs/t2t/lib/python3.7/json/decoder.py in raw_decode(self, s, idx)
    351         """"""
    352         try:
--> 353             obj, end = self.scan_once(s, idx)
    354         except StopIteration as err:
    355             raise JSONDecodeError(""Expecting value"", s, err.value) from None

JSONDecodeError: Unterminated string starting at: line 1 column 46983 (char 46982)

163502 examples [02:10, 2710.68 examples/s]   
```

I am not sure how to trace back to the specific JSON file that has the ""Unterminated string"". Also, I do not get this error on colab so I suspect it may be MacOS specific. Copy pasting the relevant lines from `transformers-cli env` below:

- Platform: Darwin-19.5.0-x86_64-i386-64bit
- Python version: 3.7.5
- PyTorch version (GPU?): 1.5.0 (False)
- Tensorflow version (GPU?): 2.2.0 (False)

Any ideas?"
https://github.com/huggingface/datasets/issues/290,ConnectionError - Eli5 dataset download,"[""It should ne fixed now, thanks for reporting this one :)\r\nIt was an issue on our google storage.\r\n\r\nLet me now if you're still facing this issue.""
 'It works now, thanks for prompt help!']","Hi, I have a problem with downloading Eli5 dataset. When typing `nlp.load_dataset('eli5')`, I get ConnectionError: Couldn't reach https://storage.googleapis.com/huggingface-nlp/cache/datasets/eli5/LFQA_reddit/1.0.0/explain_like_im_five-train_eli5.arrow

I would appreciate if you could help me with this issue."
https://github.com/huggingface/datasets/issues/288,Error at the first example in README: AttributeError: module 'dill' has no attribute '_dill',"['It looks like the bug comes from `dill`. Which version of `dill` are you using ?'
 'Thank you. It is version 0.2.6, which version is better?'
 '0.2.6 is three years old now, maybe try a more recent one, e.g. the current 0.3.2 if you can?'
 'Thanks guys! I upgraded dill and it works.' 'Awesome']","/Users/parasol_tree/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:469: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/Users/parasol_tree/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:470: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/Users/parasol_tree/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/Users/parasol_tree/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/Users/parasol_tree/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/Users/parasol_tree/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:476: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
/Users/parasol_tree/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6
  return f(*args, **kwds)
/Users/parasol_tree/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Traceback (most recent call last):
  File ""/Users/parasol_tree/Resource/019 - Github/AcademicEnglishToolkit /test.py"", line 7, in <module>
    import nlp
  File ""/Users/parasol_tree/anaconda3/lib/python3.6/site-packages/nlp/__init__.py"", line 27, in <module>
    from .arrow_dataset import Dataset
  File ""/Users/parasol_tree/anaconda3/lib/python3.6/site-packages/nlp/arrow_dataset.py"", line 31, in <module>
    from nlp.utils.py_utils import dumps
  File ""/Users/parasol_tree/anaconda3/lib/python3.6/site-packages/nlp/utils/__init__.py"", line 20, in <module>
    from .download_manager import DownloadManager, GenerateMode
  File ""/Users/parasol_tree/anaconda3/lib/python3.6/site-packages/nlp/utils/download_manager.py"", line 25, in <module>
    from .py_utils import flatten_nested, map_nested, size_str
  File ""/Users/parasol_tree/anaconda3/lib/python3.6/site-packages/nlp/utils/py_utils.py"", line 244, in <module>
    class Pickler(dill.Pickler):
  File ""/Users/parasol_tree/anaconda3/lib/python3.6/site-packages/nlp/utils/py_utils.py"", line 247, in Pickler
    dispatch = dill._dill.MetaCatchingDict(dill.Pickler.dispatch.copy())
AttributeError: module 'dill' has no attribute '_dill'"
https://github.com/huggingface/datasets/issues/281,Private/sensitive data,"[""Hi @MFreidank, you should already be able to load a dataset from local sources, indeed. (ping @lhoestq and @jplu)\r\n\r\nWe're also thinking about the ability to host private datasets on a hosted bucket with permission management, but that's further down the road.""
 'Hi @MFreidank, it is possible to load a dataset from your local storage, but only CSV/TSV and JSON are supported. To load a dataset in JSON format:\r\n\r\n```\r\nnlp.load_dataset(path=""json"", data_files={nlp.Split.TRAIN: [""path/to/train.json""], nlp.Split.TEST: [""path/to/test.json""]})\r\n```\r\n\r\nFor CSV/TSV datasets, you have to replace `json` by `csv`.'
 'Hi @julien-c @jplu,\r\nThanks for sharing this solution with me, it helps, this is what I was looking for. \r\nIf not already there and only missed by me, this could be a great addition in the docs.\r\n\r\nClosing my issue as resolved, thanks again.']","Hi all,
Thanks for this fantastic library, it makes it very easy to do prototyping for NLP projects interchangeably between TF/Pytorch. 

Unfortunately, there is data that cannot easily be shared publicly as it may contain sensitive information. 
Is there support/a plan to support such data with NLP, e.g. by reading it from local sources?

Use case flow could look like this: use NLP to prototype an approach on similar, public data and apply the resulting prototype on sensitive/private data without the need to rethink data processing pipelines. 

Many thanks for your responses ahead of time and kind regards,
MFreidank"
https://github.com/huggingface/datasets/issues/279,Dataset Preprocessing Cache with .map() function not working as expected,"[""When you're processing a dataset with `.map`, it checks whether it has already done this computation using a hash based on the function and the input (using some fancy serialization with `dill`). If you found that it doesn't work as expected in some cases, let us know !\r\n\r\nGiven that, you can still force to re-process using `.map(my_func, load_from_cache_file=False)` if you want to.\r\n\r\nI am curious about the problem you have with splits. It makes me think about #160 that was an issue of version 0.1.0. What version of `nlp` are you running ? Could you give me more details ?""
 ""Thanks, that's helpful! I was running 0.1.0, but since upgraded to 0.2.1. I can't reproduce the issue anymore as I've cleared the cache & everything now seems to be running fine since the upgrade. I've added some checks to my code, so if I do encounter it again I will reopen this issue.""
 'Just checking in, the cache sometimes still does not work when I make changes in my processing function in version `1.2.1`. The changes made to my data processing function only propagate to the dataset when I use `load_from_cache_file=False` or clear the cache. Is this a system-specific issue?'
 ""Hi @sarahwie \r\nThe data are reloaded from the cache if the hash of the function you provide is the same as a computation you've done before. The hash is computed by recursively looking at the python objects of the function you provide.\r\n\r\nIf you think there's an issue, can you share the function you used or a google colab please ?""
 ""I can't reproduce it, so I'll close for now.""]","I've been having issues with reproducibility when loading and processing datasets with the `.map` function. I was only able to resolve them by clearing all of the cache files on my system. 

Is there a way to disable using the cache when processing a dataset? As I make minor processing changes on the same dataset, I want to be able to be certain the data is being re-processed rather than loaded from a cached file. 

Could you also help me understand a bit more about how the caching functionality is used for pre-processing? E.g. how is it determined when to load from a cache vs. reprocess. 
I was particularly having an issue where the correct dataset splits were loaded, but as soon as I applied the `.map()` function to each split independently, they somehow all exited this process having been converted to the test set.
Thanks!"
https://github.com/huggingface/datasets/issues/278,MemoryError when loading German Wikipedia,"['Hi !\r\n\r\nAs you noticed, ""big"" datasets like Wikipedia require apache beam to be processed.\r\nHowever users usually don\'t have an apache beam runtime available (spark, dataflow, etc.) so our goal for this library is to also make available processed versions of these datasets, so that users can just download and use them right away.\r\n\r\nThis is the case for english and french wikipedia right now: we\'ve processed them ourselves and now they are available from our google storage. However we\'ve not processed the german one (yet).'
 'Hi @lhoestq \r\n\r\nThank you for your quick reply. I thought this might be the case, that the processing was done for some languages and not for others. Is there any set timeline for when other languages (German, Italian) will be processed?\r\n\r\nGiven enough memory, is it possible to process the data ourselves by specifying the `beam_runner`?'
 ""Adding them is definitely in our short term objectives. I'll be working on this early next week :)\r\n\r\nAlthough if you have an apache beam runtime feel free to specify the beam runner. You can find more info [here](https://github.com/huggingface/nlp/blob/master/docs/beam_dataset.md) on how to make it work on Dataflow but you can adapt it for Spark or any other beam runtime (by changing the `runner`).\r\n\r\nHowever if you don't have a beam runtime and even if you have enough memory, I discourage you to use the `DirectRunner` on the german or italian wikipedia. According to Apache Beam documentation it was made for testing purposes and therefore it is memory-inefficient.""
 'German is [almost] done @gregburman'
 'I added the German and the Italian Wikipedia to our google cloud storage:\r\nFirst update the `nlp` package to 0.3.0:\r\n```bash\r\npip install nlp --upgrade\r\n```\r\nand then\r\n```python\r\nfrom nlp import load_dataset\r\nwiki_de = load_dataset(""wikipedia"", ""20200501.de"")\r\nwiki_it = load_dataset(""wikipedia"", ""20200501.it"")\r\n```\r\nThe datasets are downloaded and directly ready to use (no processing).'
 ""Hi @lhoestq \r\n\r\nWow, thanks so much, that's **really** incredible! I was considering looking at creating my own Beam Dataset, as per the doc you linked, but instead opted to process the data myself using `wikiextractor`. However, now that this is available, I'll definitely switch across and use it.\r\n\r\nThanks so much for the incredible work, this really helps out our team considerably!\r\n\r\nHave a great (and well-deserved ;) weekend ahead!\r\n\r\nP.S. I'm not sure if I should close the issue here - if so I'm happy to do so.""
 'Thanks for your message, glad I could help :)\r\nClosing this one.']","Hi, first off let me say thank you for all the awesome work you're doing at Hugging Face across all your projects (NLP, Transformers, Tokenizers) - they're all amazing contributions to us working with NLP models :)

I'm trying to download the German Wikipedia dataset as follows:

```
wiki = nlp.load_dataset(""wikipedia"", ""20200501.de"", split=""train"")
```

However, when I do so, I get the following error:

```
Downloading and preparing dataset wikipedia/20200501.de (download: Unknown size, generated: Unknown size, total: Unknown size) to /home/ubuntu/.cache/huggingface/datasets/wikipedia/20200501.de/1.0.0...
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/ubuntu/anaconda3/envs/albert/lib/python3.7/site-packages/nlp/load.py"", line 520, in load_dataset
    save_infos=save_infos,
  File ""/home/ubuntu/anaconda3/envs/albert/lib/python3.7/site-packages/nlp/builder.py"", line 433, in download_and_prepare
    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
  File ""/home/ubuntu/anaconda3/envs/albert/lib/python3.7/site-packages/nlp/builder.py"", line 824, in _download_and_prepare
    ""\n\t`{}`"".format(usage_example)
nlp.builder.MissingBeamOptions: Trying to generate a dataset using Apache Beam, yet no Beam Runner or PipelineOptions() has been provided in `load_dataset` or in the builder arguments. For big datasets it has to run on large-scale data processing tools like Dataflow, Spark, etc. More information about Apache Beam runners at https://beam.apache.org/documentation/runners/capability-matrix/
If you really want to run it locally because you feel like the Dataset is small enough, you can use the local beam runner called `DirectRunner` (you may run out of memory). 
Example of usage: 
	`load_dataset('wikipedia', '20200501.de', beam_runner='DirectRunner')`
```

So, following on from the example usage at the bottom, I tried specifying `beam_runner='DirectRunner`, however when I do this after about 20 min after the data has all downloaded, I get a `MemoryError` as warned.

This isn't an issue for the English or French Wikipedia datasets (I've tried both), as neither seem to require that `beam_runner` be specified. Can you please clarify why this is an issue for the German dataset?

My nlp version is 0.2.1.

Thank you!"
https://github.com/huggingface/datasets/issues/277,Empty samples in glue/qqp,"['We are only wrapping the original dataset.\r\n\r\nMaybe try to ask on the GLUE mailing list or reach out to the original authors?'
 ""Tanks for the suggestion, I'll try to ask GLUE benchmark.\r\nI'll first close the issue,  post the following up here afterwards, and reopen the issue if needed. ""]","```
qqp = nlp.load_dataset('glue', 'qqp')
print(qqp['train'][310121])
print(qqp['train'][362225])
```
```
{'question1': 'How can I create an Android app?', 'question2': '', 'label': 0, 'idx': 310137}
{'question1': 'How can I develop android app?', 'question2': '', 'label': 0, 'idx': 362246}
```
Notice that question 2 is empty string. 
BTW, I have checked and these two are the only naughty ones in all splits of qqp."
https://github.com/huggingface/datasets/issues/275,NonMatchingChecksumError when loading pubmed dataset,"['For some reason the files are not available for unauthenticated users right now (like the download service of this package). Instead of downloading the right files, it downloads the html of the error.\r\nAccording to the error it should be back again in 24h.\r\n\r\n![image](https://user-images.githubusercontent.com/42851186/84751599-096c6580-afbd-11ea-97f3-ee4aef791711.png)\r\n']","I get this error when i run `nlp.load_dataset('scientific_papers', 'pubmed', split = 'train[:50%]')`.
The error is:

```
---------------------------------------------------------------------------
NonMatchingChecksumError                  Traceback (most recent call last)
<ipython-input-2-7742dea167d0> in <module>()
----> 1 df = nlp.load_dataset('scientific_papers', 'pubmed', split = 'train[:50%]')
      2 df = pd.DataFrame(df)
      3 gc.collect()

3 frames
/usr/local/lib/python3.6/dist-packages/nlp/load.py in load_dataset(path, name, version, data_dir, data_files, split, cache_dir, download_config, download_mode, ignore_verifications, save_infos, **config_kwargs)
    518         download_mode=download_mode,
    519         ignore_verifications=ignore_verifications,
--> 520         save_infos=save_infos,
    521     )
    522 

/usr/local/lib/python3.6/dist-packages/nlp/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, save_infos, try_from_hf_gcs, dl_manager, **download_and_prepare_kwargs)
    431                 verify_infos = not save_infos and not ignore_verifications
    432                 self._download_and_prepare(
--> 433                     dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
    434                 )
    435                 # Sync info

/usr/local/lib/python3.6/dist-packages/nlp/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)
    468         # Checksums verification
    469         if verify_infos:
--> 470             verify_checksums(self.info.download_checksums, dl_manager.get_recorded_sizes_checksums())
    471         for split_generator in split_generators:
    472             if str(split_generator.split_info.name).lower() == ""all"":

/usr/local/lib/python3.6/dist-packages/nlp/utils/info_utils.py in verify_checksums(expected_checksums, recorded_checksums)
     34     bad_urls = [url for url in expected_checksums if expected_checksums[url] != recorded_checksums[url]]
     35     if len(bad_urls) > 0:
---> 36         raise NonMatchingChecksumError(str(bad_urls))
     37     logger.info(""All the checksums matched successfully."")
     38 

NonMatchingChecksumError: ['https://drive.google.com/uc?id=1b3rmCSIoh6VhD4HKWjI4HOW-cSwcwbeC&export=download', 'https://drive.google.com/uc?id=1lvsqvsFi3W-pE1SqNZI0s8NR9rC1tsja&export=download']
```
I'm currently working on google colab.

That is quite strange because yesterday it was fine.
"
https://github.com/huggingface/datasets/issues/274,PG-19,"['Sounds good! Do you want to give it a try?'
 ""Ok, I'll see if I can figure it out tomorrow!""
 ""Got around to this today, and so far so good, I'm able to download and load pg19 locally. However, I think there may be an issue with the dummy data, and testing in general.\r\n\r\nThe problem lies in the fact that each book from pg19 actually resides as its own text file in a google cloud folder that denotes the split, where the book id is the name of the text file. https://console.cloud.google.com/storage/browser/deepmind-gutenberg/train/ I don't believe there's anywhere else (even in the supplied metadata), where the mapping of id -> split can be found.\r\n\r\nTherefore I end up making a network call `tf.io.gfile.listdir` to get all the files within each of the split directories. https://github.com/lucidrains/nlp/commit/adbacbd85decc80db2347d0882e7dab4faa6fd03#diff-cece8f166a85dd927caf574ba303d39bR78\r\n\r\nDoes this network call need to be eventually stubbed out for testing?""
 ""Ohh nevermind, I think I can use `download_custom` here with `listdir` as the custom function. Ok, I'll keep trying to make the dummy data work!""]","Hi, and thanks for all your open-sourced work, as always!

I was wondering if you would be open to adding PG-19 to your collection of datasets. https://github.com/deepmind/pg19 It is often used for benchmarking long-range language modeling."
https://github.com/huggingface/datasets/issues/270,c4 dataset is not viewable in nlpviewer demo,['C4 is too large to be shown in the viewer'],"I get the following error when I try to view the c4 dataset in [nlpviewer](https://huggingface.co/nlp/viewer/)

```python
ModuleNotFoundError: No module named 'langdetect'
Traceback:
File ""/home/sasha/.local/lib/python3.7/site-packages/streamlit/ScriptRunner.py"", line 322, in _run_script
    exec(code, module.__dict__)
File ""/home/sasha/nlp_viewer/run.py"", line 54, in <module>
    configs = get_confs(option.id)
File ""/home/sasha/.local/lib/python3.7/site-packages/streamlit/caching.py"", line 591, in wrapped_func
    return get_or_create_cached_value()
File ""/home/sasha/.local/lib/python3.7/site-packages/streamlit/caching.py"", line 575, in get_or_create_cached_value
    return_value = func(*args, **kwargs)
File ""/home/sasha/nlp_viewer/run.py"", line 48, in get_confs
    builder_cls = nlp.load.import_main_class(module_path, dataset=True)
File ""/home/sasha/.local/lib/python3.7/site-packages/nlp/load.py"", line 57, in import_main_class
    module = importlib.import_module(module_path)
File ""/usr/lib/python3.7/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
File ""<frozen importlib._bootstrap_external>"", line 728, in exec_module
File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
File ""/home/sasha/.local/lib/python3.7/site-packages/nlp/datasets/c4/88bb1b1435edad3fb772325710c4a43327cbf4a23b9030094556e6f01e14ec19/c4.py"", line 29, in <module>
    from .c4_utils import (
File ""/home/sasha/.local/lib/python3.7/site-packages/nlp/datasets/c4/88bb1b1435edad3fb772325710c4a43327cbf4a23b9030094556e6f01e14ec19/c4_utils.py"", line 29, in <module>
    import langdetect
```"
https://github.com/huggingface/datasets/issues/267,How can I load/find WMT en-romanian?,['I will take a look :-) '],"I believe it is from `wmt16`

When I run

```python
wmt = nlp.load_dataset('wmt16')
```
I get:
```python
AssertionError: The dataset wmt16 with config cs-en requires manual data. 
 Please follow the manual download instructions:   Some of the wmt configs here, require a manual download.
  Please look into wmt.py to see the exact path (and file name) that has to
  be downloaded.
  . 
 Manual data can be loaded with `nlp.load(wmt16, data_dir='<path/to/manual/data>')
```
There is no wmt.py,as the error message suggests, and wmt16.py doesn't have manual download instructions.

Any idea how to do this?

Thanks in advance!


"
https://github.com/huggingface/datasets/issues/263,[Feature request] Support for external modality for language datasets,"['Thanks a lot, @aleSuglia for the very detailed and introductive feature request.\r\nIt seems like we could build something pretty useful here indeed.\r\n\r\nOne of the questions here is that Arrow doesn\'t have built-in support for generic ""tensors"" in records but there might be ways to do that in a clean way. We\'ll probably try to tackle this during the summer.'
 'I was looking into Facebook MMF and apparently they decided to use LMDB to store additional features associated with every example: https://github.com/facebookresearch/mmf/blob/master/mmf/datasets/databases/features_database.py\r\n\r\n'
 ""I saw the Mozilla common_voice dataset in model hub, which has mp3 audio recordings as part it. It's use predominantly maybe in ASR and TTS, but dataset is a Language + Voice Dataset similar to @aleSuglia's point about Language + Vision. \r\n\r\nhttps://huggingface.co/datasets/common_voice""]","# Background

In recent years many researchers have advocated that learning meanings from text-based only datasets is just like asking a human to ""learn to speak by listening to the radio"" [[E. Bender and A. Koller,2020](https://openreview.net/forum?id=GKTvAcb12b), [Y. Bisk et. al, 2020](https://arxiv.org/abs/2004.10151)]. Therefore, the importance of multi-modal datasets for the NLP community is of paramount importance for next-generation models. For this reason, I raised a [concern](https://github.com/huggingface/nlp/pull/236#issuecomment-639832029) related to the best way to integrate external features in NLP datasets (e.g., visual features associated with an image, audio features associated with a recording, etc.). This would be of great importance for a more systematic way of representing data for ML models that are learning from multi-modal data. 

# Language + Vision

## Use case
Typically, people working on Language+Vision tasks, have a reference dataset (either in JSON or JSONL format) and for each example, they have an identifier that specifies the reference image. For a practical example, you can refer to the [GQA](https://cs.stanford.edu/people/dorarad/gqa/download.html#seconddown) dataset.

Currently, images are represented by either pooling-based features (average pooling of ResNet or VGGNet features, see [DeVries et.al, 2017](https://arxiv.org/abs/1611.08481), [Shekhar et.al, 2019](https://www.aclweb.org/anthology/N19-1265.pdf)) where you have a single vector for every image. Another option is to use a set of feature maps for every image extracted from a specific layer of a CNN (see [Xu et.al, 2015](https://arxiv.org/abs/1502.03044)). A more recent option, especially with large-scale multi-modal transformers [Li et. al, 2019](https://arxiv.org/abs/1908.03557), is to use FastRCNN features. 

For all these types of features, people use one of the following formats:
1. [HD5F](https://pypi.org/project/h5py/)
2. [NumPy](https://numpy.org/doc/stable/reference/generated/numpy.savez.html)
3. [LMDB](https://lmdb.readthedocs.io/en/release/)

## Implementation considerations

I was thinking about possible ways of implementing this feature. As mentioned above, depending on the model, different visual features can be used. This step usually relies on another model (say ResNet-101) that is used to generate the visual features for each image used in the dataset. Typically, this step is done in a separate script that completes the feature generation procedure. The usual processing steps for these datasets are the following:

1. Download dataset
2. Download images associated with the dataset
3. Write a script that generates the visual features for every image and store them in a specific file
4. Create a DataLoader that maps the visual features to the corresponding language example

In my personal projects, I've decided to ignore HD5F because it doesn't have out-of-the-box support for multi-processing (see this PyTorch [issue](https://github.com/pytorch/pytorch/issues/11929)). I've been successfully using a NumPy compressed file for each image so that I can store any sort of information in it.

For ease of use of all these Language+Vision datasets, it would be really handy to have a way to associate the visual features with the text and store them in an efficient way. That's why I immediately thought about the HuggingFace NLP backend based on Apache Arrow. The assumption here is that the external modality will be mapped to a N-dimensional tensor so easily represented by a NumPy array. 

Looking forward to hearing your thoughts about it!"
https://github.com/huggingface/datasets/issues/261,Downloading dataset error with pyarrow.lib.RecordBatch,"[""When you install `nlp` for the first time on a Colab runtime, it updates the `pyarrow` library that was already on colab. This update shows this message on colab:\r\n```\r\nWARNING: The following packages were previously imported in this runtime:\r\n  [pyarrow]\r\nYou must restart the runtime in order to use newly installed versions.\r\n```\r\nYou just have to restart the runtime and it should be fine.\r\nIf you don't restart, then it breaks like in your message.""
 'Yeah, that worked! Thanks :) ']","I am trying to download `sentiment140` and I have the following error

```
/usr/local/lib/python3.6/dist-packages/nlp/load.py in load_dataset(path, name, version, data_dir, data_files, split, cache_dir, download_config, download_mode, ignore_verifications, save_infos, **config_kwargs)
    518         download_mode=download_mode,
    519         ignore_verifications=ignore_verifications,
--> 520         save_infos=save_infos,
    521     )
    522 

/usr/local/lib/python3.6/dist-packages/nlp/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, save_infos, try_from_hf_gcs, dl_manager, **download_and_prepare_kwargs)
    418                 verify_infos = not save_infos and not ignore_verifications
    419                 self._download_and_prepare(
--> 420                     dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
    421                 )
    422                 # Sync info

/usr/local/lib/python3.6/dist-packages/nlp/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)
    472             try:
    473                 # Prepare split will record examples associated to the split
--> 474                 self._prepare_split(split_generator, **prepare_split_kwargs)
    475             except OSError:
    476                 raise OSError(""Cannot find data file. "" + (self.MANUAL_DOWNLOAD_INSTRUCTIONS or """"))

/usr/local/lib/python3.6/dist-packages/nlp/builder.py in _prepare_split(self, split_generator)
    652         for key, record in utils.tqdm(generator, unit="" examples"", total=split_info.num_examples, leave=False):
    653             example = self.info.features.encode_example(record)
--> 654             writer.write(example)
    655         num_examples, num_bytes = writer.finalize()
    656 

/usr/local/lib/python3.6/dist-packages/nlp/arrow_writer.py in write(self, example, writer_batch_size)
    143             self._build_writer(pa_table=pa.Table.from_pydict(example))
    144         if writer_batch_size is not None and len(self.current_rows) >= writer_batch_size:
--> 145             self.write_on_file()
    146 
    147     def write_batch(

/usr/local/lib/python3.6/dist-packages/nlp/arrow_writer.py in write_on_file(self)
    127             else:
    128                 # All good
--> 129                 self._write_array_on_file(pa_array)
    130             self.current_rows = []
    131 

/usr/local/lib/python3.6/dist-packages/nlp/arrow_writer.py in _write_array_on_file(self, pa_array)
     96     def _write_array_on_file(self, pa_array):
     97         """"""Write a PyArrow Array""""""
---> 98         pa_batch = pa.RecordBatch.from_struct_array(pa_array)
     99         self._num_bytes += pa_array.nbytes
    100         self.pa_writer.write_batch(pa_batch)

AttributeError: type object 'pyarrow.lib.RecordBatch' has no attribute 'from_struct_array'
```

I installed the last version and ran the following command:

```python
import nlp
sentiment140 = nlp.load_dataset('sentiment140', cache_dir='/content')
```"
https://github.com/huggingface/datasets/issues/259,documentation missing how to split a dataset,"[""this seems to work for my specific problem:\r\n\r\n`self.train_ds, self.test_ds, self.val_ds = map(_prepare_ds, ('train', 'test[:25%]+test[50%:75%]', 'test[75%:]'))`""
 ""Currently you can indeed split a dataset using `ds_test = nlp.load_dataset('imdb, split='test[:5000]')` (works also with percentages).\r\n\r\nHowever right now we don't have a way to shuffle a dataset but we are thinking about it in the discussion in #166. Feel free to share your thoughts about it.\r\n\r\nOne trick that you can do until we have a better solution is to shuffle and split the indices of your dataset:\r\n```python\r\nimport nlp\r\nfrom sklearn.model_selection import train_test_split\r\n\r\nimdb = nlp.load_dataset('imbd', split='test')\r\ntest_indices, val_indices = train_test_split(range(len(imdb)))\r\n```\r\n\r\nand then to iterate each split:\r\n```python\r\nfor i in test_indices:\r\n    example = imdb[i]\r\n   ...\r\n```\r\n""
 'I added a small guide [here](https://github.com/huggingface/nlp/tree/master/docs/splits.md) that explains how to split a dataset. It is very similar to the tensorflow datasets guide, as we kept the same logic.'
 ""Thanks a lot, the new explanation is very helpful!\r\n\r\nAbout using train_test_split from sklearn: I stumbled across the [same error message as this user ](https://github.com/huggingface/nlp/issues/147 )and thought it can't be used at the moment in this context. Will check it out again.\r\n\r\nOne of the problems is how to shuffle very large datasets, which don't fit into the memory. Well, one strategy could be shuffling data in sections. But in a case where the data is sorted by the labels you have to swap larger sections first. \r\n""
 'We added a way to shuffle datasets (shuffle the indices and then reorder to make a new dataset).\r\nYou can do `shuffled_dset = dataset.shuffle(seed=my_seed)`. It shuffles the whole dataset.\r\nThere is also `dataset.train_test_split()` which if very handy (with the same signature as sklearn).\r\n\r\nClosing this issue as we added the docs for splits and tools to split datasets. Thanks again for your feedback !']","I am trying to understand how to split a dataset ( as arrow_dataset). 
I know I can do something like this to access a split which is already in the original dataset : 

`ds_test = nlp.load_dataset('imdb, split='test') `

But how can I split ds_test into a test and a validation set (without reading the data into memory and keeping the arrow_dataset as container)?
I guess it has something to do with the module split :-) but there is no real documentation in the code but only a reference to a longer description: 

> See the  [guide on splits](https://github.com/huggingface/nlp/tree/master/docs/splits.md)  for more information.

But the guide seems to be missing.

To clarify: I know that this has been modelled after the dataset of tensorflow and that some of the documentation there can be used [like this one](https://www.tensorflow.org/datasets/splits). But to come back to the example above: I cannot simply split the testset doing this: 
`ds_test = nlp.load_dataset('imdb, split='test'[:5000]) `
`ds_val = nlp.load_dataset('imdb, split='test'[5000:])`

because the imdb test data is sorted by class (probably not a good idea anyway)
"
https://github.com/huggingface/datasets/issues/258,Why is dataset after tokenization far more larger than the orginal one ?,"['Hi ! This is because `.map` added the new column `input_ids` to the dataset, and so all the other columns were kept. Therefore the dataset size increased a lot.\r\n If you want to only keep the `input_ids` column, you can stash the other ones by specifying `remove_columns=[""title"", ""text""]` in the arguments of `.map`'
 'Hi ! Thanks for your reply.\r\n\r\nBut since size of `input_ids` < size of `text`, I am wondering why\r\nsize of `input_ids` + `text` > 2x the size of `text`  🤔'
 'Hard to tell... This is probably related to the way apache arrow compresses lists of integers, that may be different from the compression of strings.'
 ""Thanks for your point. 😀, It might be answer.\r\nSince this is hard to know, I'll close this issue.\r\nBut if somebody knows more details, please comment below ~ 😁""]","I tokenize wiki dataset by `map` and cache the results.
```
def tokenize_tfm(example):
    example['input_ids'] = hf_fast_tokenizer.convert_tokens_to_ids(hf_fast_tokenizer.tokenize(example['text']))
    return example
wiki = nlp.load_dataset('wikipedia', '20200501.en', cache_dir=cache_dir)['train']
wiki.map(tokenize_tfm, cache_file_name=cache_dir/""wikipedia/20200501.en/1.0.0/tokenized_wiki.arrow"")
```
and when I see their size
```
ls -l --block-size=M
17460M  wikipedia-train.arrow
47511M  tokenized_wiki.arrow
```
The tokenized one is over 2x size of original one.
Is there something I did wrong ?"
https://github.com/huggingface/datasets/issues/257,Tokenizer pickling issue fix not landed in `nlp` yet?,"['Yes, the new release of tokenizers solves this and should be out soon.\r\nIn the meantime, you can install it with `pip install tokenizers==0.8.0-dev2`'
 'If others run into this issue, a quick fix is to use python 3.6 instead of 3.7+. Serialization differences between the 3rd party `dataclasses` package for 3.6 and the built in `dataclasses` in 3.7+ cause the issue.\r\n\r\nProbably a dumb fix, but it works for me.']","Unless I recreate an arrow_dataset from my loaded nlp dataset myself (which I think does not use the cache by default), I get the following error when applying the map function:

```
dataset = nlp.load_dataset('cos_e')
tokenizer = GPT2TokenizerFast.from_pretrained('gpt2', cache_dir=cache_dir)

for split in dataset.keys():
    dataset[split].map(lambda x: some_function(x, tokenizer))
```
```
06/09/2020 10:09:19 - INFO - nlp.builder -   Constructing Dataset for split train[:10], from /home/sarahw/.cache/huggingface/datasets/cos_e/default/0.0.1
Traceback (most recent call last):
  File ""generation/input_to_label_and_rationale.py"", line 390, in <module>
    main()
  File ""generation/input_to_label_and_rationale.py"", line 263, in main
    dataset[split] = dataset[split].map(lambda x: input_to_explanation_plus_label(x, tokenizer, max_length, datasource=data_args.task_name, wt5=(model_class=='t5'), expl_only=model_args.rationale_only), batched=False)
  File ""/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/site-packages/nlp/arrow_dataset.py"", line 522, in map
    cache_file_name = self._get_cache_file_path(function, cache_kwargs)
  File ""/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/site-packages/nlp/arrow_dataset.py"", line 381, in _get_cache_file_path
    function_bytes = dumps(function)
  File ""/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/site-packages/nlp/utils/py_utils.py"", line 257, in dumps
    dump(obj, file)
  File ""/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/site-packages/nlp/utils/py_utils.py"", line 250, in dump
    Pickler(file).dump(obj)
  File ""/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/site-packages/dill/_dill.py"", line 445, in dump
    StockPickler.dump(self, obj)
  File ""/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py"", line 485, in dump
    self.save(obj)
  File ""/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py"", line 558, in save
    f(self, obj)  # Call unbound method with explicit self
  File ""/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/site-packages/dill/_dill.py"", line 1410, in save_function
    pickler.save_reduce(_create_function, (obj.__code__,
  File ""/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py"", line 690, in save_reduce
    save(args)
  File ""/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py"", line 558, in save
    f(self, obj)  # Call unbound method with explicit self
  File ""/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py"", line 899, in save_tuple
    save(element)
  File ""/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py"", line 558, in save
    f(self, obj)  # Call unbound method with explicit self
  File ""/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py"", line 899, in save_tuple
    save(element)
  File ""/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py"", line 558, in save
    f(self, obj)  # Call unbound method with explicit self
  File ""/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/site-packages/dill/_dill.py"", line 1147, in save_cell
    pickler.save_reduce(_create_cell, (f,), obj=obj)
  File ""/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py"", line 690, in save_reduce
    save(args)
  File ""/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py"", line 558, in save
    f(self, obj)  # Call unbound method with explicit self
  File ""/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py"", line 884, in save_tuple
    save(element)
  File ""/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py"", line 601, in save
    self.save_reduce(obj=obj, *rv)
  File ""/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py"", line 715, in save_reduce
    save(state)
  File ""/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py"", line 558, in save
    f(self, obj)  # Call unbound method with explicit self
  File ""/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/site-packages/dill/_dill.py"", line 912, in save_module_dict
    StockPickler.save_dict(pickler, obj)
  File ""/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py"", line 969, in save_dict
    self._batch_setitems(obj.items())
  File ""/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py"", line 995, in _batch_setitems
    save(v)
  File ""/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py"", line 601, in save
    self.save_reduce(obj=obj, *rv)
  File ""/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py"", line 715, in save_reduce
    save(state)
  File ""/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py"", line 558, in save
    f(self, obj)  # Call unbound method with explicit self
  File ""/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/site-packages/dill/_dill.py"", line 912, in save_module_dict
    StockPickler.save_dict(pickler, obj)
  File ""/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py"", line 969, in save_dict
    self._batch_setitems(obj.items())
  File ""/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py"", line 995, in _batch_setitems
    save(v)
  File ""/home/sarahw/miniconda3/envs/project_huggingface/lib/python3.8/pickle.py"", line 576, in save
    rv = reduce(self.proto)
TypeError: cannot pickle 'Tokenizer' object
```
Fix seems to be in the tokenizers [`0.8.0.dev1 pre-release`](https://github.com/huggingface/tokenizers/issues/87), which I can't install with any package managers. "
https://github.com/huggingface/datasets/issues/256,[Feature request] Add a feature to dataset,"['Do you have an example of what you would like to do? (you can just add a field in the output of the unction you give to map and this will add this field in the output table)'
 ""Given another source of data loaded in, I want to pre-add it to the dataset so that it aligns with the indices of the arrow dataset prior to performing map.\r\n\r\nE.g. \r\n```\r\nnew_info = list of length dataset['train']\r\n\r\ndataset['train'] = dataset['train'].map(lambda x: some_function(x, new_info[index of x]))\r\n\r\ndef some_function(x, new_info_x):\r\n    # adds new_info[index of x] as a field to x\r\n    x['new_info'] = new_info_x\r\n    return x\r\n```\r\nI was thinking to instead create a new field in the arrow dataset so that instance x contains all the necessary information when map function is applied (since I don't have index information to pass to map function).""
 ""This is what I have so far: \r\n\r\n```\r\nimport pyarrow as pa\r\nfrom nlp.arrow_dataset import Dataset\r\n\r\naug_dataset = dataset['train'][:]\r\naug_dataset['new_info'] = new_info\r\n\r\n#reformat as arrow-table\r\nschema = dataset['train'].schema\r\n\r\n# this line doesn't work:\r\nschema.append(pa.field('new_info', pa.int32()))\r\n\r\ntable = pa.Table.from_pydict(\r\n    aug_dataset,\r\n    schema=schema\r\n)\r\ndataset['train'] = Dataset(table) \r\n```""
 ""Maybe you can use `with_indices`?\r\n\r\n```python\r\nnew_info = list of length dataset['train']\r\n\r\ndef some_function(indice, x):\r\n    # adds new_info[index of x] as a field to x\r\n    x['new_info'] = new_info_x[indice]\r\n    return x\r\n\r\ndataset['train'] = dataset['train'].map(some_function, with_indices=True)\r\n```""
 'Oh great. That should work. I missed that in the documentation- thanks :) ']","Is there a straightforward way to add a field to the arrow_dataset, prior to performing map?"
https://github.com/huggingface/datasets/issues/254,[Feature request] Be able to remove a specific sample of the dataset,['Oh yes you can now do that with the `dataset.filter()` method that was added in #214 '],"As mentioned in #117, it's currently not possible to remove a sample of the dataset.

But it is a important use case : After applying some preprocessing, some samples might be empty for example. We should be able to remove these samples from the dataset, or at least mark them as `removed` so when iterating the dataset, we don't iterate these samples.

I think it should be a feature. What do you think ?

---

Any work-around in the meantime ?"
https://github.com/huggingface/datasets/issues/252,NonMatchingSplitsSizesError error when reading the IMDB dataset,"[""I just tried on my side and I didn't encounter your problem.\r\nApparently the script doesn't generate all the examples on your side.\r\n\r\nCan you provide the version of `nlp` you're using ?\r\nCan you try to clear your cache and re-run the code ?""
 'I updated it, that was it, thanks!'
 'Hello, I am facing the same problem... how do you clear the huggingface cache?'
 'Hi ! The cache is at ~/.cache/huggingface\r\nYou can just delete this folder if needed :)']","Hi!

I am trying to load the `imdb` dataset with this line:

`dataset = nlp.load_dataset('imdb', data_dir='/A/PATH', cache_dir='/A/PATH')`

but I am getting the following error:

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/mounts/Users/cisintern/antmarakis/anaconda3/lib/python3.7/site-packages/nlp/load.py"", line 517, in load_dataset
    save_infos=save_infos,
  File ""/mounts/Users/cisintern/antmarakis/anaconda3/lib/python3.7/site-packages/nlp/builder.py"", line 363, in download_and_prepare
    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
  File ""/mounts/Users/cisintern/antmarakis/anaconda3/lib/python3.7/site-packages/nlp/builder.py"", line 421, in _download_and_prepare
    verify_splits(self.info.splits, split_dict)
  File ""/mounts/Users/cisintern/antmarakis/anaconda3/lib/python3.7/site-packages/nlp/utils/info_utils.py"", line 70, in verify_splits
    raise NonMatchingSplitsSizesError(str(bad_splits))
nlp.utils.info_utils.NonMatchingSplitsSizesError: [{'expected': SplitInfo(name='train', num_bytes=33442202, num_examples=25000, dataset_name='imdb'), 'recorded': SplitInfo(name='train', num_bytes=5929447, num_examples=4537, dataset_name='imdb')}, {'expected': SplitInfo(name='unsupervised', num_bytes=67125548, num_examples=50000, dataset_name='imdb'), 'recorded': SplitInfo(name='unsupervised', num_bytes=0, num_examples=0, dataset_name='imdb')}]
```

Am I overlooking something? Thanks!"
https://github.com/huggingface/datasets/issues/249,[Dataset created] some critical small issues when I was creating a dataset,"['Thanks for noticing all these :) They should be easy to fix indeed'
 'Alright I think I fixed all the problems you mentioned. Thanks again, that will be useful for many people.\r\nThere is still more work needed for point 7. but we plan to have some nice docs soon.']","Hi, I successfully created a dataset and has made a pr #248.
But I have encountered several problems when I was creating it, and those should be easy to fix.

1. Not found dataset_info.json
should be fixed by #241 , eager to wait it be merged.

2. Forced to install `apach_beam`
If we should install it, then it might be better to include it in the pakcage dependency or specified in `CONTRIBUTING.md`
```
Traceback (most recent call last):
  File ""nlp-cli"", line 10, in <module>
    from nlp.commands.run_beam import RunBeamCommand
  File ""/home/yisiang/nlp/src/nlp/commands/run_beam.py"", line 6, in <module>
    import apache_beam as beam
ModuleNotFoundError: No module named 'apache_beam'
```

3.  `cached_dir` is `None`
```
File ""/home/yisiang/nlp/src/nlp/datasets/bookscorpus/aea0bd5142d26df645a8fce23d6110bb95ecb81772bb2a1f29012e329191962c/bookscorpus.py"", line 88, in _split_generators
    downloaded_path_or_paths = dl_manager.download_custom(_GDRIVE_FILE_ID, download_file_from_google_drive)
  File ""/home/yisiang/nlp/src/nlp/utils/download_manager.py"", line 128, in download_custom
    downloaded_path_or_paths = map_nested(url_to_downloaded_path, url_or_urls)
  File ""/home/yisiang/nlp/src/nlp/utils/py_utils.py"", line 172, in map_nested
    return function(data_struct)
  File ""/home/yisiang/nlp/src/nlp/utils/download_manager.py"", line 126, in url_to_downloaded_path
    return os.path.join(self._download_config.cache_dir, hash_url_to_filename(url))
  File ""/home/yisiang/miniconda3/envs/nlppr/lib/python3.7/posixpath.py"", line 80, in join
    a = os.fspath(a)
```
This is because this line
https://github.com/huggingface/nlp/blob/2e0a8639a79b1abc848cff5c669094d40bba0f63/src/nlp/commands/test.py#L30-L32
And I add `--cache_dir=""....""` to `python nlp-cli test datasets/<your-dataset-folder> --save_infos --all_configs`  in the doc, finally I could pass this error.
But it seems to ignore my arg and use `/home/yisiang/.cache/huggingface/datasets/bookscorpus/plain_text/1.0.0` as cahe_dir

4. There is no `pytest`
So maybe in the doc we should specify a step to install pytest

5. Not enough capacity in my `/tmp`
When run test for dummy data, I don't know why it ask me for 5.6g to download something, 
```
def download_and_prepare
...
if not utils.has_sufficient_disk_space(self.info.size_in_bytes or 0, directory=self._cache_dir_root):
                raise IOError(
                    ""Not enough disk space. Needed: {} (download: {}, generated: {})"".format(
                        utils.size_str(self.info.size_in_bytes or 0),
                        utils.size_str(self.info.download_size or 0),
>                       utils.size_str(self.info.dataset_size or 0),
                    )
                )
E               OSError: Not enough disk space. Needed: 5.62 GiB (download: 1.10 GiB, generated: 4.52 GiB)
```
I add a `processed_temp_dir=""some/dir""; raw_temp_dir=""another/dir""` to 71, and the test passed
https://github.com/huggingface/nlp/blob/a67a6c422dece904b65d18af65f0e024e839dbe8/tests/test_dataset_common.py#L70-L72

I suggest we can create tmp dir under the `/home/user/tmp` but not `/tmp`, because take our lab server for example, everyone use `/tmp` thus it has not much capacity. Or at least we can improve error message, so the user know is what directory has no space and how many has it lefted. Or we could do both.

6. name of datasets
I was surprised by the dataset name `books_corpus`, and didn't know it is from `class BooksCorpus(nlp.GeneratorBasedBuilder)` . I change it to `Bookscorpus` afterwards. I think this point shold be also on the doc.

7. More thorough doc to how to create `dataset.py`
I believe there will be.

**Feel free to close this issue** if you think these are solved."
https://github.com/huggingface/datasets/issues/246,What is the best way to cache a dataset? ,"['Everything is already cached by default in 🤗nlp (in particular dataset\nloading and all the “map()” operations) so I don’t think you need to do any\nspecific caching in streamlit.\n\nTell us if you feel like it’s not the case.\n\nOn Sat, 6 Jun 2020 at 13:02, Fabrizio Milo <notifications@github.com> wrote:\n\n> For example if I want to use streamlit with a nlp dataset:\n>\n> @st.cache\n> def load_data():\n>     return nlp.load_dataset(\'squad\')\n>\n> This code raises the error ""uncachable object""\n>\n> Right now I just fixed with a constant for my specific case:\n>\n>     @st.cache(hash_funcs={pyarrow.lib.Buffer: lambda b: 0})\n>\n> But I was curious to know what is the best way in general\n>\n> —\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/huggingface/nlp/issues/246>, or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ABYDIHKAKO7CWGX2QY55UXLRVIO3ZANCNFSM4NV333RQ>\n> .\n>\n'
 'Closing this one. Feel free to re-open if you have other questions !']","For example if I want to use streamlit with a nlp dataset:

```
@st.cache
def load_data():
    return nlp.load_dataset('squad')
```
This code raises the error ""uncachable object""

Right now I just fixed with a constant for my specific case:
```
    @st.cache(hash_funcs={pyarrow.lib.Buffer: lambda b: 0})
```
But I was curious to know what is the best way in general

"
https://github.com/huggingface/datasets/issues/245,SST-2 test labels are all -1,"[""this also happened to me with `nlp.load_dataset('glue', 'mnli')`""
 ""Yes, this is because the test sets for glue are hidden so the labels are\nnot publicly available. You can read the glue paper for more details.\n\nOn Sat, 6 Jun 2020 at 18:16, Jack Morris <notifications@github.com> wrote:\n\n> this also happened to me with nlp.load_datasets('glue', 'mnli')\n>\n> —\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/huggingface/nlp/issues/245#issuecomment-640083980>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ABYDIHMVQD2EDX2HTZUXG5DRVJTWRANCNFSM4NVG3AKQ>\n> .\n>\n""
 'Thanks @thomwolf!'
 ""@thomwolf shouldn't this be visible in the .info and/or in the .features?""
 'It should be in the datasets card (the README.md and on the hub) in my opinion. What do you think @yjernite?'
 'I checked both before I got to looking at issues, so that would be fine as well.\r\n\r\nSome additional thoughts on this: Is there a specific reason why the ""test"" split even has a ""label"" column if it isn\'t tagged. Shouldn\'t there just not be any. Seems more transparent'
 ""I'm a little confused with the data size.\r\n`sst2` dataset is referenced to `Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank` and the link of the dataset in the paper is https://nlp.stanford.edu/sentiment/index.html which is often shown in GLUE/SST2 reference.\r\nFrom the original data, the standard train/dev/test splits split is 6920/872/1821 for binary classification. \r\nWhy in GLUE/SST2 the train/dev/test split is 67,349/872/1,821 ? \r\n\r\n""
 ""> I'm a little confused with the data size.\r\n> `sst2` dataset is referenced to `Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank` and the link of the dataset in the paper is https://nlp.stanford.edu/sentiment/index.html which is often shown in GLUE/SST2 reference.\r\n> From the original data, the standard train/dev/test splits split is 6920/872/1821 for binary classification.\r\n> Why in GLUE/SST2 the train/dev/test split is 67,349/872/1,821 ?\r\n\r\nHave you figured out this problem? AFAIK, the original sst-2 dataset is totally different from the GLUE/sst-2. Do you think so?""
 ""@yc1999 Sorry, I didn't solve this conflict. In the end, I just use a local data file provided by the previous work I followed(for consistent comparison), not use `datasets` package.\r\n\r\nRelated information: https://github.com/thunlp/OpenAttack/issues/146#issuecomment-766323571""]","I'm trying to test a model on the SST-2 task, but all the labels I see in the test set are -1.
```
>>> import nlp
>>> glue = nlp.load_dataset('glue', 'sst2')
>>> glue
{'train': Dataset(schema: {'sentence': 'string', 'label': 'int64', 'idx': 'int32'}, num_rows: 67349), 'validation': Dataset(schema: {'sentence': 'string', 'label': 'int64', 'idx': 'int32'}, num_rows: 872), 'test': Dataset(schema: {'sentence': 'string', 'label': 'int64', 'idx': 'int32'}, num_rows: 1821)}
>>> list(l['label'] for l in glue['test'])
[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]
```"
https://github.com/huggingface/datasets/issues/242,UnicodeDecodeError when downloading GLUE-MNLI,"['It should be good now, thanks for noticing and fixing it ! I would say that it was because you are on windows but not 100% sure'
 ""On Windows Python supports Unicode almost everywhere, but one of the notable exceptions is open() where it uses the locale encoding schema. So platform independent python scripts would always set the encoding='utf-8' in calls to open explicitly. \r\nIn the meantime: since Python 3.7 Windows users can set the default encoding for everything including open() to Unicode by setting this environment variable: set PYTHONUTF8=1 (details can be found in [PEP 540](https://www.python.org/dev/peps/pep-0540/))\r\n\r\nFor me this fixed the problem described by the OP.""]","When I run
```python
dataset = nlp.load_dataset('glue', 'mnli')
```
I get an encoding error (could it be because I'm using Windows?) :
```python
# Lots of error log lines later...
~\Miniconda3\envs\nlp\lib\site-packages\tqdm\std.py in __iter__(self)
   1128         try:
-> 1129             for obj in iterable:
   1130                 yield obj

~\Miniconda3\envs\nlp\lib\site-packages\nlp\datasets\glue\5256cc2368cf84497abef1f1a5f66648522d5854b225162148cb8fc78a5a91cc\glue.py in _generate_examples(self, data_file, split, mrpc_files)
    529 
--> 530                 for n, row in enumerate(reader):
    531                     if is_cola_non_test:

~\Miniconda3\envs\nlp\lib\csv.py in __next__(self)
    110             self.fieldnames
--> 111         row = next(self.reader)
    112         self.line_num = self.reader.line_num

~\Miniconda3\envs\nlp\lib\encodings\cp1252.py in decode(self, input, final)
     22     def decode(self, input, final=False):
---> 23         return codecs.charmap_decode(input,self.errors,decoding_table)[0]
     24 

UnicodeDecodeError: 'charmap' codec can't decode byte 0x9d in position 6744: character maps to <undefined>
```
Anyway this can be solved by specifying to decode in UTF when reading the csv file. I am proposing a PR if that's okay."
https://github.com/huggingface/datasets/issues/240,Deterministic dataset loading,"['Yes good point !'
 'I think using `sorted(glob.glob())` would actually solve this problem. Can you think of other reasons why dataset loading might not be deterministic? @mariamabarham @yjernite @lhoestq @thomwolf . \r\n\r\nI can do a sweep through the dataset scripts and fix the glob.glob() if you guys are ok with it'
 ""I'm pretty sure it would solve the problem too.\r\n\r\nThe only other dataset that is not deterministic right now is `blog_authorship_corpus` (see #215) but this is a problem related to string encodings.""
 'I think we should do the same also for `os.list_dir`']","When calling:
```python 
import nlp
dataset = nlp.load_dataset(""trivia_qa"", split=""validation[:1%]"")
```

the resulting dataset is not deterministic over different google colabs. 
After talking to @thomwolf, I suspect the reason to be the use of `glob.glob` in line:

https://github.com/huggingface/nlp/blob/2e0a8639a79b1abc848cff5c669094d40bba0f63/datasets/trivia_qa/trivia_qa.py#L180

which seems to return an ordering of files that depends on the filesystem:
https://stackoverflow.com/questions/6773584/how-is-pythons-glob-glob-ordered

I think we should go through all the dataset scripts and make sure to have deterministic behavior.

A simple solution for `glob.glob()` would be to just replace it with `sorted(glob.glob())` to have everything sorted by name. 

What do you think @lhoestq?"
https://github.com/huggingface/datasets/issues/239,[Creating new dataset] Not found dataset_info.json,"['I think you can just `rm` this directory and it should be good :)'
 '@lhoestq - this seems to happen quite often (already the 2nd issue). Can we maybe delete this automatically?'
 ""Yes I have an idea of what's going on. I'm sure I can fix that""
 'Hi, I rebase my local copy to `fix-empty-cache-dir`, and try to run again `python nlp-cli test datasets/bookcorpus --save_infos --all_configs`.\r\n\r\nI got this,  \r\n```\r\nTraceback (most recent call last):\r\n  File ""nlp-cli"", line 10, in <module>\r\n    from nlp.commands.run_beam import RunBeamCommand\r\n  File ""/home/yisiang/nlp/src/nlp/commands/run_beam.py"", line 6, in <module>\r\n    import apache_beam as beam\r\nModuleNotFoundError: No module named \'apache_beam\'\r\n```\r\nAnd after I installed it. I got this\r\n```\r\nFile ""/home/yisiang/nlp/src/nlp/datasets/bookcorpus/aea0bd5142d26df645a8fce23d6110bb95ecb81772bb2a1f29012e329191962c/bookcorpus.py"", line 88, in _split_generators\r\n    downloaded_path_or_paths = dl_manager.download_custom(_GDRIVE_FILE_ID, download_file_from_google_drive)\r\n  File ""/home/yisiang/nlp/src/nlp/utils/download_manager.py"", line 128, in download_custom\r\n    downloaded_path_or_paths = map_nested(url_to_downloaded_path, url_or_urls)\r\n  File ""/home/yisiang/nlp/src/nlp/utils/py_utils.py"", line 172, in map_nested\r\n    return function(data_struct)\r\n  File ""/home/yisiang/nlp/src/nlp/utils/download_manager.py"", line 126, in url_to_downloaded_path\r\n    return os.path.join(self._download_config.cache_dir, hash_url_to_filename(url))\r\n  File ""/home/yisiang/miniconda3/envs/nlppr/lib/python3.7/posixpath.py"", line 80, in join\r\n    a = os.fspath(a)\r\n```\r\nThe problem is when I print `self._download_config.cache_dir` using pdb, it is `None`.\r\n\r\nDid I miss something ?  Or can you provide a workaround first so I can keep testing my script ?'
 ""I'll close this issue because I brings more reports in another issue #249 .""]","Hi, I am trying to create Toronto Book Corpus. #131 

I ran
`~/nlp % python nlp-cli test datasets/bookcorpus --save_infos --all_configs`
but this doesn't create `dataset_info.json` and try to use it
```
INFO:nlp.load:Checking datasets/bookcorpus/bookcorpus.py for additional imports.
INFO:filelock:Lock 139795325778640 acquired on datasets/bookcorpus/bookcorpus.py.lock
INFO:nlp.load:Found main folder for dataset datasets/bookcorpus/bookcorpus.py at /home/yisiang/miniconda3/envs/ml/lib/python3.7/site-packages/nlp/datasets/bookcorpus
INFO:nlp.load:Found specific version folder for dataset datasets/bookcorpus/bookcorpus.py at /home/yisiang/miniconda3/envs/ml/lib/python3.7/site-packages/nlp/datasets/bookcorpus/8e84759446cf68d0b0deb3417e60cc331f30a3bbe58843de18a0f48e87d1efd9
INFO:nlp.load:Found script file from datasets/bookcorpus/bookcorpus.py to /home/yisiang/miniconda3/envs/ml/lib/python3.7/site-packages/nlp/datasets/bookcorpus/8e84759446cf68d0b0deb3417e60cc331f30a3bbe58843de18a0f48e87d1efd9/bookcorpus.py
INFO:nlp.load:Couldn't find dataset infos file at datasets/bookcorpus/dataset_infos.json
INFO:nlp.load:Found metadata file for dataset datasets/bookcorpus/bookcorpus.py at /home/yisiang/miniconda3/envs/ml/lib/python3.7/site-packages/nlp/datasets/bookcorpus/8e84759446cf68d0b0deb3417e60cc331f30a3bbe58843de18a0f48e87d1efd9/bookcorpus.json
INFO:filelock:Lock 139795325778640 released on datasets/bookcorpus/bookcorpus.py.lock
INFO:nlp.builder:Overwrite dataset info from restored data version.
INFO:nlp.info:Loading Dataset info from /home/yisiang/.cache/huggingface/datasets/book_corpus/plain_text/1.0.0
Traceback (most recent call last):
  File ""nlp-cli"", line 37, in <module>
    service.run()
  File ""/home/yisiang/miniconda3/envs/ml/lib/python3.7/site-packages/nlp/commands/test.py"", line 78, in run
    builders.append(builder_cls(name=config.name, data_dir=self._data_dir))
  File ""/home/yisiang/miniconda3/envs/ml/lib/python3.7/site-packages/nlp/builder.py"", line 610, in __init__
    super(GeneratorBasedBuilder, self).__init__(*args, **kwargs)
  File ""/home/yisiang/miniconda3/envs/ml/lib/python3.7/site-packages/nlp/builder.py"", line 152, in __init__
    self.info = DatasetInfo.from_directory(self._cache_dir)
  File ""/home/yisiang/miniconda3/envs/ml/lib/python3.7/site-packages/nlp/info.py"", line 157, in from_directory
    with open(os.path.join(dataset_info_dir, DATASET_INFO_FILENAME), ""r"") as f:
FileNotFoundError: [Errno 2] No such file or directory: '/home/yisiang/.cache/huggingface/datasets/book_corpus/plain_text/1.0.0/dataset_info.json'
```
btw, `ls /home/yisiang/.cache/huggingface/datasets/book_corpus/plain_text/1.0.0/` show me nothing is in the directory.

I have also pushed the script to my fork [bookcorpus.py](https://github.com/richardyy1188/nlp/blob/bookcorpusdev/datasets/bookcorpus/bookcorpus.py).
"
https://github.com/huggingface/datasets/issues/238,[Metric] Bertscore : Warning : Empty candidate sentence; Setting recall to be 0.,"[""This print statement comes from the official implementation of bert_score (see [here](https://github.com/Tiiiger/bert_score/blob/master/bert_score/utils.py#L343)). The warning shows up only if the attention mask outputs no candidate.\r\nRight now we want to only use official code for metrics to have fair evaluations, so I'm not sure we can do anything about it. Maybe you can try to create an issue on their [repo](https://github.com/Tiiiger/bert_score) ?""]","When running BERT-Score, I'm meeting this warning :

> Warning: Empty candidate sentence; Setting recall to be 0.

Code :

```
import nlp
metric = nlp.load_metric(""bertscore"")
scores = metric.compute([""swag"", ""swags""], [""swags"", ""totally something different""], lang=""en"", device=0)
```

---

**What am I doing wrong / How can I hide this warning ?**"
https://github.com/huggingface/datasets/issues/237,Can't download MultiNLI,"[""You should use `load_dataset('glue', 'mnli')`""
 ""Thanks! I thought I had to use the same code displayed in the live viewer:\r\n```python\r\n!pip install nlp\r\nfrom nlp import load_dataset\r\ndataset = load_dataset('multi_nli', 'plain_text')\r\n```\r\nYour suggestion works, even if then I got a different issue (#242).  ""
 'Glad it helps !\nThough I am not one of hf team, but maybe you should close this issue first.']","When I try to download MultiNLI with 
```python
dataset = load_dataset('multi_nli')
```

I get this long error:
```python
---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
<ipython-input-13-3b11f6be4cb9> in <module>
      1 # Load a dataset and print the first examples in the training set
      2 # nli_dataset = nlp.load_dataset('multi_nli')
----> 3 dataset = load_dataset('multi_nli')
      4 # nli_dataset = nlp.load_dataset('multi_nli', split='validation_matched[:10%]')
      5 # print(nli_dataset['train'][0])

~\Miniconda3\envs\nlp\lib\site-packages\nlp\load.py in load_dataset(path, name, version, data_dir, data_files, split, cache_dir, download_config, download_mode, ignore_verifications, save_infos, **config_kwargs)
    514 
    515     # Download and prepare data
--> 516     builder_instance.download_and_prepare(
    517         download_config=download_config,
    518         download_mode=download_mode,

~\Miniconda3\envs\nlp\lib\site-packages\nlp\builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, save_infos, try_from_hf_gcs, dl_manager, **download_and_prepare_kwargs)
    417             with utils.temporary_assignment(self, ""_cache_dir"", tmp_data_dir):
    418                 verify_infos = not save_infos and not ignore_verifications
--> 419                 self._download_and_prepare(
    420                     dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
    421                 )

~\Miniconda3\envs\nlp\lib\site-packages\nlp\builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)
    455         split_dict = SplitDict(dataset_name=self.name)
    456         split_generators_kwargs = self._make_split_generators_kwargs(prepare_split_kwargs)
--> 457         split_generators = self._split_generators(dl_manager, **split_generators_kwargs)
    458         # Checksums verification
    459         if verify_infos:

~\Miniconda3\envs\nlp\lib\site-packages\nlp\datasets\multi_nli\60774175381b9f3f1e6ae1028229e3cdb270d50379f45b9f2c01008f50f09e6b\multi_nli.py in _split_generators(self, dl_manager)
     99     def _split_generators(self, dl_manager):
    100 
--> 101         downloaded_dir = dl_manager.download_and_extract(
    102             ""http://storage.googleapis.com/tfds-data/downloads/multi_nli/multinli_1.0.zip""
    103         )

~\Miniconda3\envs\nlp\lib\site-packages\nlp\utils\download_manager.py in download_and_extract(self, url_or_urls)
    214             extracted_path(s): `str`, extracted paths of given URL(s).
    215         """"""
--> 216         return self.extract(self.download(url_or_urls))
    217 
    218     def get_recorded_sizes_checksums(self):

~\Miniconda3\envs\nlp\lib\site-packages\nlp\utils\download_manager.py in extract(self, path_or_paths)
    194                 path_or_paths.
    195         """"""
--> 196         return map_nested(
    197             lambda path: cached_path(path, extract_compressed_file=True, force_extract=False), path_or_paths,
    198         )

~\Miniconda3\envs\nlp\lib\site-packages\nlp\utils\py_utils.py in map_nested(function, data_struct, dict_only, map_tuple)
    168                 return tuple(mapped)
    169     # Singleton
--> 170     return function(data_struct)
    171 
    172 

~\Miniconda3\envs\nlp\lib\site-packages\nlp\utils\download_manager.py in <lambda>(path)
    195         """"""
    196         return map_nested(
--> 197             lambda path: cached_path(path, extract_compressed_file=True, force_extract=False), path_or_paths,
    198         )
    199 

~\Miniconda3\envs\nlp\lib\site-packages\nlp\utils\file_utils.py in cached_path(url_or_filename, download_config, **download_kwargs)
    231             if is_zipfile(output_path):
    232                 with ZipFile(output_path, ""r"") as zip_file:
--> 233                     zip_file.extractall(output_path_extracted)
    234                     zip_file.close()
    235             elif tarfile.is_tarfile(output_path):

~\Miniconda3\envs\nlp\lib\zipfile.py in extractall(self, path, members, pwd)
   1644 
   1645         for zipinfo in members:
-> 1646             self._extract_member(zipinfo, path, pwd)
   1647 
   1648     @classmethod

~\Miniconda3\envs\nlp\lib\zipfile.py in _extract_member(self, member, targetpath, pwd)
   1698 
   1699         with self.open(member, pwd=pwd) as source, \
-> 1700              open(targetpath, ""wb"") as target:
   1701             shutil.copyfileobj(source, target)
   1702 

OSError: [Errno 22] Invalid argument: 'C:\\Users\\Python\\.cache\\huggingface\\datasets\\3e12413b8ec69f22dfcfd54a79d1ba9e7aac2e18e334bbb6b81cca64fd16bffc\\multinli_1.0\\Icon\r'
```
"
https://github.com/huggingface/datasets/issues/234,"Huggingface NLP, Uploading custom dataset","[""What do you mean 'custom' ? You may want to elaborate on it when ask a question.\r\n\r\nAnyway, there are two things you may interested\r\n`nlp.Dataset.from_file` and `load_dataset(..., cache_dir=)`""
 'To load a dataset you need to have a script that defines the format of the examples, the splits and the way to generate examples. As your dataset has the same format of squad, you can just copy the squad script (see the [datasets](https://github.com/huggingface/nlp/tree/master/datasets) forlder) and just replace the url to load the data to your local or remote path.\r\n\r\nThen what you can do is `load_dataset(<path/to/your/script>)`'
 'Also if you want to upload your script, you should be able to use the `nlp-cli`.\r\n\r\nUnfortunately the upload feature was not shipped in the latest version 0.2.0. so right now you can either clone the repo to use it or wait for the next release. We will add some docs to explain how to upload datasets.\r\n'
 'Since the latest release 0.2.1 you can use \r\n```bash\r\nnlp-cli upload_dataset <path/to/dataset>\r\n```\r\nwhere `<path/to/dataset>` is a path to a folder containing your script (ex: `squad.py`).\r\nThis will upload the script under your namespace on our S3.\r\n\r\nOptionally the folder can also contain `dataset_infos.json` generated using\r\n```bash\r\nnlp-cli test <path/to/dataset> --all_configs --save_infos\r\n```\r\n\r\nThen you should be able to do\r\n```python\r\nnlp.load_dataset(""my_namespace/dataset_name"")\r\n```']","Hello,

Does anyone know how we can call our custom dataset using the nlp.load command? Let's say that I have a dataset based on the same format as that of squad-v1.1, how am I supposed to load it using huggingface nlp.

Thank you!"
https://github.com/huggingface/datasets/issues/233,Fail to download c4 english corpus,"['Hello ! Thanks for noticing this bug, let me fix that.\r\n\r\nAlso for information, as specified in the changelog of the latest release, C4 currently needs to have a runtime for apache beam to work on. Apache beam is used to process this very big dataset and it can work on dataflow, spark, flink, apex, etc. You can find more info on beam datasets [here](https://github.com/huggingface/nlp/blob/master/docs/beam_dataset.md).\r\n\r\nOur goal in the future is to make available an already-processed version of C4 (as we do for wikipedia for example) so that users without apache beam runtimes can load it.'
 '@lhoestq I am facing `IsADirectoryError` while downloading with this command.\r\nCan you pls look into it & help me.\r\nI\'m using version 0.4.0 of `nlp`.\r\n\r\n```\r\ndataset = load_dataset(""c4"", \'en\', data_dir=\'.\', beam_runner=\'DirectRunner\')\r\n```\r\n\r\nHere\'s the complete stack trace.\r\n\r\n```\r\nDownloading and preparing dataset c4/en (download: Unknown size, generated: Unknown size, post-processed: Unknown sizetotal: Unknown size) to /home/devops/.cache/huggingface/datasets/c4/en/2.3.0/096df5a27756d51957c959a2499453e60a08154971fceb017bbb29f54b11bef7...\r\n\r\n---------------------------------------------------------------------------\r\nIsADirectoryError                         Traceback (most recent call last)\r\n<ipython-input-11-f622e6705e03> in <module>\r\n----> 1 dataset = load_dataset(""c4"", \'en\', data_dir=\'.\', beam_runner=\'DirectRunner\')\r\n\r\n/data/anaconda/envs/hf/lib/python3.6/site-packages/nlp/load.py in load_dataset(path, name, version, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, save_infos, **config_kwargs)\r\n    547     # Download and prepare data\r\n    548     builder_instance.download_and_prepare(\r\n--> 549         download_config=download_config, download_mode=download_mode, ignore_verifications=ignore_verifications,\r\n    550     )\r\n    551 \r\n\r\n/data/anaconda/envs/hf/lib/python3.6/site-packages/nlp/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, **download_and_prepare_kwargs)\r\n    461                 if not downloaded_from_gcs:\r\n    462                     self._download_and_prepare(\r\n--> 463                         dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    464                     )\r\n    465                 # Sync info\r\n\r\n/data/anaconda/envs/hf/lib/python3.6/site-packages/nlp/builder.py in _download_and_prepare(self, dl_manager, verify_infos)\r\n    964         pipeline = beam_utils.BeamPipeline(runner=beam_runner, options=beam_options,)\r\n    965         super(BeamBasedBuilder, self)._download_and_prepare(\r\n--> 966             dl_manager, verify_infos=False, pipeline=pipeline,\r\n    967         )  # TODO handle verify_infos in beam datasets\r\n    968         # Run pipeline\r\n\r\n/data/anaconda/envs/hf/lib/python3.6/site-packages/nlp/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    516         split_dict = SplitDict(dataset_name=self.name)\r\n    517         split_generators_kwargs = self._make_split_generators_kwargs(prepare_split_kwargs)\r\n--> 518         split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n    519         # Checksums verification\r\n    520         if verify_infos:\r\n\r\n/data/anaconda/envs/hf/lib/python3.6/site-packages/nlp/datasets/c4/096df5a27756d51957c959a2499453e60a08154971fceb017bbb29f54b11bef7/c4.py in _split_generators(self, dl_manager, pipeline)\r\n    187         if self.config.realnewslike:\r\n    188             files_to_download[""realnews_domains""] = _REALNEWS_DOMAINS_URL\r\n--> 189         file_paths = dl_manager.download_and_extract(files_to_download)\r\n    190 \r\n    191         if self.config.webtextlike:\r\n\r\n/data/anaconda/envs/hf/lib/python3.6/site-packages/nlp/utils/download_manager.py in download_and_extract(self, url_or_urls)\r\n    218             extracted_path(s): `str`, extracted paths of given URL(s).\r\n    219         """"""\r\n--> 220         return self.extract(self.download(url_or_urls))\r\n    221 \r\n    222     def get_recorded_sizes_checksums(self):\r\n\r\n/data/anaconda/envs/hf/lib/python3.6/site-packages/nlp/utils/download_manager.py in download(self, url_or_urls)\r\n    156             lambda url: cached_path(url, download_config=self._download_config,), url_or_urls,\r\n    157         )\r\n--> 158         self._record_sizes_checksums(url_or_urls, downloaded_path_or_paths)\r\n    159         return downloaded_path_or_paths\r\n    160 \r\n\r\n/data/anaconda/envs/hf/lib/python3.6/site-packages/nlp/utils/download_manager.py in _record_sizes_checksums(self, url_or_urls, downloaded_path_or_paths)\r\n    106         flattened_downloaded_path_or_paths = flatten_nested(downloaded_path_or_paths)\r\n    107         for url, path in zip(flattened_urls_or_urls, flattened_downloaded_path_or_paths):\r\n--> 108             self._recorded_sizes_checksums[url] = get_size_checksum_dict(path)\r\n    109 \r\n    110     def download_custom(self, url_or_urls, custom_download):\r\n\r\n/data/anaconda/envs/hf/lib/python3.6/site-packages/nlp/utils/info_utils.py in get_size_checksum_dict(path)\r\n     77     """"""Compute the file size and the sha256 checksum of a file""""""\r\n     78     m = sha256()\r\n---> 79     with open(path, ""rb"") as f:\r\n     80         for chunk in iter(lambda: f.read(1 << 20), b""""):\r\n     81             m.update(chunk)\r\n\r\nIsADirectoryError: [Errno 21] Is a directory: \'/\'\r\n\r\n```\r\n\r\nCan anyone please try to see what I am doing wrong or is this a bug?'
 'I have the same problem as @prashant-kikani'
 'Looks like a bug in the dataset script, can you open an issue ?'
 ""I see the same issue as @prashant-kikani. I'm using `datasets` version 1.2.0 to download C4.""]","i run following code to download c4 English corpus.

```
dataset = nlp.load_dataset('c4', 'en', beam_runner='DirectRunner'
, data_dir='/mypath')
```

and i met failure as follows

```
Downloading and preparing dataset c4/en (download: Unknown size, generated: Unknown size, total: Unknown size) to /home/adam/.cache/huggingface/datasets/c4/en/2.3.0...
Traceback (most recent call last):
  File ""download_corpus.py"", line 38, in <module>
    , data_dir='/home/adam/data/corpus/en/c4')
  File ""/home/adam/anaconda3/envs/adam/lib/python3.7/site-packages/nlp/load.py"", line 520, in load_dataset
    save_infos=save_infos,
  File ""/home/adam/anaconda3/envs/adam/lib/python3.7/site-packages/nlp/builder.py"", line 420, in download_and_prepare
    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
  File ""/home/adam/anaconda3/envs/adam/lib/python3.7/site-packages/nlp/builder.py"", line 816, in _download_and_prepare
    dl_manager, verify_infos=False, pipeline=pipeline,
  File ""/home/adam/anaconda3/envs/adam/lib/python3.7/site-packages/nlp/builder.py"", line 457, in _download_and_prepare
    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)
  File ""/home/adam/anaconda3/envs/adam/lib/python3.7/site-packages/nlp/datasets/c4/f545de9f63300d8d02a6795e2eb34e140c47e62a803f572ac5599e170ee66ecc/c4.py"", line 175, in _split_generators
    dl_manager.download_checksums(_CHECKSUMS_URL)
AttributeError: 'DownloadManager' object has no attribute 'download_checksums

```
can i get any advice?"
https://github.com/huggingface/datasets/issues/228,Not able to access the XNLI dataset,"['Added pull request to change the name of the file from dataset_infos.json to dataset_info.json'
 'Thanks for reporting this bug !\r\nAs it seems to be just a cache problem, I closed your PR.\r\nI think we might just need to clear and reload the `xnli` cache @srush ? '
 ""Update: The dataset_info.json error is gone, but we have a new one instead:\r\n```\r\nConnectionError: Couldn't reach https://www.nyu.edu/projects/bowman/xnli/XNLI-1.0.zip\r\n```\r\nI am not able to reproduce on my side unfortunately. Any idea @srush ?""
 'xnli is now properly shown in the viewer.\r\nClosing this one.']","When I try to access the XNLI dataset, I get the following error. The option of plain_text get selected automatically and then I get the following error.

```
FileNotFoundError: [Errno 2] No such file or directory: '/home/sasha/.cache/huggingface/datasets/xnli/plain_text/1.0.0/dataset_info.json'
Traceback:
File ""/home/sasha/.local/lib/python3.7/site-packages/streamlit/ScriptRunner.py"", line 322, in _run_script
    exec(code, module.__dict__)
File ""/home/sasha/nlp_viewer/run.py"", line 86, in <module>
    dts, fail = get(str(option.id), str(conf_option.name) if conf_option else None)
File ""/home/sasha/.local/lib/python3.7/site-packages/streamlit/caching.py"", line 591, in wrapped_func
    return get_or_create_cached_value()
File ""/home/sasha/.local/lib/python3.7/site-packages/streamlit/caching.py"", line 575, in get_or_create_cached_value
    return_value = func(*args, **kwargs)
File ""/home/sasha/nlp_viewer/run.py"", line 72, in get
    builder_instance = builder_cls(name=conf)
File ""/home/sasha/.local/lib/python3.7/site-packages/nlp/builder.py"", line 610, in __init__
    super(GeneratorBasedBuilder, self).__init__(*args, **kwargs)
File ""/home/sasha/.local/lib/python3.7/site-packages/nlp/builder.py"", line 152, in __init__
    self.info = DatasetInfo.from_directory(self._cache_dir)
File ""/home/sasha/.local/lib/python3.7/site-packages/nlp/info.py"", line 157, in from_directory
    with open(os.path.join(dataset_info_dir, DATASET_INFO_FILENAME), ""r"") as f:
```

Is it possible to see if the dataset_info.json is correctly placed?"
https://github.com/huggingface/datasets/issues/227,Should we still have to force to install apache_beam to download wikipedia ?,"[""Thanks for your message 😊 \r\nIndeed users shouldn't have to install those dependencies""
 'Got it, feel free to close this issue when you think it’s resolved.'
 'It should be good now :)']","Hi, first thanks to @lhoestq 's revolutionary work, I successfully downloaded processed wikipedia according to the doc. 😍😍😍

But at the first try, it tell me to install `apache_beam` and `mwparserfromhell`, which I thought wouldn't be used according to #204 , it was kind of confusing me at that time.

Maybe we should not force users to install these ? Or we just add them to`nlp`'s dependency ?"
https://github.com/huggingface/datasets/issues/225,[ROUGE] Different scores with `files2rouge`,"[""@Colanim unfortunately there are different implementations of the ROUGE metric floating around online which yield different results, and we had to chose one for the package :) We ended up including the one from the google-research repository, which does minimal post-processing before computing the P/R/F scores. If I recall correctly, files2rouge relies on the Perl, script, which among other things normalizes all numbers to a special token: in the case you presented, this should account for a good chunk of the difference.\r\n\r\nWe may end up adding in more versions of the metric, but probably not for a while (@lhoestq correct me if I'm wrong). However, feel free to take a stab at adding it in yourself and submitting a PR if you're interested!""
 ""Thank you for your kind answer.\r\n\r\nAs a side question : Isn't it better to have a package that normalize more ?\r\n\r\nI understand to idea of having a package that does minimal post-processing for transparency.\r\n\r\nBut it means that people using different architecture (with different tokenizers for example) will have difference in ROUGE scores even if their predictions are actually similar.  \r\nThe goal of `nlp` is to have _one package to rule them all_, right ?\r\n\r\nI will look into it but I'm not sure I have the required skill for this ^^ ""
 ""You're right, there's a pretty interesting trade-off here between robustness and sensitivity :) The flip side of your argument is that we also still want the metric to be sensitive to model mistakes. How we think about number normalization for example has evolved a fair bit since the Perl script was written: at the time, ROUGE was used mostly to evaluate short-medium text summarization systems, where there were only a few numbers in the input and it was assumed that the most popular methods in use at the time would get those right. However, as your example showcases, that assumption does not hold any more, and we do want to be able to penalize a model that generates a wrong numerical value.\r\n\r\nAlso, we think that abstracting away tokenization differences is the role of the model/tokenizer: if you use the 🤗Tokenizers library for example, it will handle that for you ;)\r\n\r\nFinally, there is a lot of active research on developing model-powered metrics that are both more sensitive and more robust than ROUGE. Check out for example BERTscore, which is implemented in this library!""]","It seems that the ROUGE score of `nlp` is lower than the one of `files2rouge`.

Here is a self-contained notebook to reproduce both scores : https://colab.research.google.com/drive/14EyAXValB6UzKY9x4rs_T3pyL7alpw_F?usp=sharing

---

`nlp` : (Only mid F-scores)

>rouge1 0.33508031962733364
rouge2 0.14574333776191592
rougeL 0.2321187823256159

`files2rouge` :

>Running ROUGE...
===========================
1 ROUGE-1 Average_R: 0.48873 (95%-conf.int. 0.41192 - 0.56339)
1 ROUGE-1 Average_P: 0.29010 (95%-conf.int. 0.23605 - 0.34445)
1 ROUGE-1 Average_F: 0.34761 (95%-conf.int. 0.29479 - 0.39871)
===========================
1 ROUGE-2 Average_R: 0.20280 (95%-conf.int. 0.14969 - 0.26244)
1 ROUGE-2 Average_P: 0.12772 (95%-conf.int. 0.08603 - 0.17752)
1 ROUGE-2 Average_F: 0.14798 (95%-conf.int. 0.10517 - 0.19240)
===========================
1 ROUGE-L Average_R: 0.32960 (95%-conf.int. 0.26501 - 0.39676)
1 ROUGE-L Average_P: 0.19880 (95%-conf.int. 0.15257 - 0.25136)
1 ROUGE-L Average_F: 0.23619 (95%-conf.int. 0.19073 - 0.28663)

---

When using longer predictions/gold, the difference is bigger.  
**How can I reproduce same score as `files2rouge` ?**

@lhoestq 
"
https://github.com/huggingface/datasets/issues/224,[Feature Request/Help] BLEURT model -> PyTorch,"['Is there any update on this? \r\n\r\nThanks!'
 ""Hitting this error when using bleurt with PyTorch ...\r\n\r\n```\r\nUnrecognizedFlagError: Unknown command line flag 'f'\r\n```\r\n... and I'm assuming because it was built for TF specifically.  Is there a way to use this metric in PyTorch?""
 ""We currently provide a wrapper on the TensorFlow implementation: https://huggingface.co/metrics/bleurt\r\n\r\nWe have long term plans to better handle model-based metrics, but they probably won't be implemented right away\r\n\r\n@adamwlev it would still be cool to add the BLEURT checkpoints to the transformers repo if you're interested, but that would best be discussed there :) \r\n\r\nclosing for now""
 ""Hi there. We ran into the same problem this year (converting BLEURT to PyTorch) and thanks to @adamwlev found his colab notebook which didn't work but served as a good starting point. Finally, we **made it work** by doing just two simple conceptual fixes: \r\n\r\n1. Transposing 'kernel' layers instead of 'dense' ones when copying params from the original model;\r\n2. Taking pooler_output as a cls_state in forward function of the BleurtModel class.\r\n\r\nPlus few minor syntactical fixes for the outdated parts. The result is still not exactly the same, but is very close to the expected one (1.0483 vs 1.0474).\r\n\r\nFind the fixed version here (fixes are commented): https://colab.research.google.com/drive/1KsCUkFW45d5_ROSv2aHtXgeBa2Z98r03?usp=sharing  \r\n""]","Hi, I am interested in porting google research's new BLEURT learned metric to PyTorch (because I wish to do something experimental with language generation and backpropping through BLEURT). I noticed that you guys don't have it yet so I am partly just asking if you plan to add it (@thomwolf said you want to do so on Twitter).

I had a go of just like manually using the checkpoint that they publish which includes the weights. It seems like the architecture is exactly aligned with the out-of-the-box BertModel in transformers just with a single linear layer on top of the CLS embedding. I loaded all the weights to the PyTorch model but I am not able to get the same numbers as the BLEURT package's python api. Here is my colab notebook where I tried  https://colab.research.google.com/drive/1Bfced531EvQP_CpFvxwxNl25Pj6ptylY?usp=sharing . If you have any pointers on what might be going wrong that would be much appreciated!

Thank you muchly!"
https://github.com/huggingface/datasets/issues/223,[Feature request] Add FLUE dataset ,"['Hi @lbourdois, yes please share it with us'
 '@mariamabarham \r\nI put all the datasets on this drive: https://1drv.ms/u/s!Ao2Rcpiny7RFinDypq7w-LbXcsx9?e=iVsEDh\r\n\r\n\r\nSome information : \r\n• For FLUE, the quote used is\r\n\r\n> @misc{le2019flaubert,\r\n>     title={FlauBERT: Unsupervised Language Model Pre-training for French},\r\n>     author={Hang Le and Loïc Vial and Jibril Frej and Vincent Segonne and Maximin Coavoux and Benjamin Lecouteux and Alexandre Allauzen and Benoît Crabbé and Laurent Besacier and Didier Schwab},\r\n>     year={2019},\r\n>     eprint={1912.05372},\r\n>     archivePrefix={arXiv},\r\n>     primaryClass={cs.CL}\r\n> }\r\n\r\n• The Github repo of FLUE is avaible here : https://github.com/getalp/Flaubert/tree/master/flue\r\n\r\n\r\n\r\nInformation related to the different tasks of FLUE : \r\n\r\n**1. Classification**\r\nThree dataframes are available: \r\n- Book\r\n- DVD\r\n- Music\r\nFor each of these dataframes is available a set of training and test data, and a third one containing unlabelled data.\r\n\r\nCitation  : \r\n>@dataset{prettenhofer_peter_2010_3251672,\r\n  author       = {Prettenhofer, Peter and\r\n                  Stein, Benno},\r\n  title        = {{Webis Cross-Lingual Sentiment Dataset 2010 (Webis- \r\n                   CLS-10)}},\r\n  month        = jul,\r\n  year         = 2010,\r\n  publisher    = {Zenodo},\r\n  doi          = {10.5281/zenodo.3251672},\r\n  url          = {https://doi.org/10.5281/zenodo.3251672}\r\n}\r\n\r\n\r\n**2. Paraphrasing** \r\nFrench part of the PAWS-X dataset (https://github.com/google-research-datasets/paws).\r\nThree dataframes are available: \r\n- train\r\n- dev\r\n- test \r\n\r\nCitation : \r\n> @InProceedings{pawsx2019emnlp,\r\n>   title = {{PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification}},\r\n>   author = {Yang, Yinfei and Zhang, Yuan and Tar, Chris and Baldridge, Jason},\r\n>   booktitle = {Proc. of EMNLP},\r\n>   year = {2019}\r\n> }\r\n\r\n\r\n\r\n**3. Natural Language Inference**\r\nFrench part of the XNLI dataset (https://github.com/facebookresearch/XNLI).\r\nThree dataframes are available: \r\n- train\r\n- dev\r\n- test \r\n\r\nFor the dev and test datasets, extra columns compared to the train dataset were available so I left them in the dataframe (I didn\'t know if these columns could be useful for other tasks or not). \r\nIn the context of the FLUE benchmark, only the columns gold_label, sentence1 and sentence2 are useful.\r\n\r\n\r\nCitation : \r\n\r\n> @InProceedings{conneau2018xnli,\r\n>   author = ""Conneau, Alexis\r\n>         and Rinott, Ruty\r\n>         and Lample, Guillaume\r\n>         and Williams, Adina\r\n>         and Bowman, Samuel R.\r\n>         and Schwenk, Holger\r\n>         and Stoyanov, Veselin"",\r\n>   title = ""XNLI: Evaluating Cross-lingual Sentence Representations"",\r\n>   booktitle = ""Proceedings of the 2018 Conference on Empirical Methods\r\n>                in Natural Language Processing"",\r\n>   year = ""2018"",\r\n>   publisher = ""Association for Computational Linguistics"",\r\n>   location = ""Brussels, Belgium"",\r\n\r\n\r\n**4. Parsing**\r\nThe dataset used by the FLUE authors for this task is not freely available.\r\nUsers of your library will therefore not be able to access it.\r\nNevertheless, I think maybe it is useful to add a link to the site where to request this dataframe: http://ftb.linguist.univ-paris-diderot.fr/telecharger.php?langue=en \r\n(personally it was sent to me less than 48 hours after I requested it).\r\n\r\n\r\n**5. Word Sense Disambiguation Tasks**\r\n5.1 Verb Sense Disambiguation\r\n\r\nTwo dataframes are available: train and test\r\nFor both dataframes, 4 columns are available: document, sentence, lemma and word.\r\nI created the document column thinking that there were several documents in the dataset but afterwards it turns out that there were not: several sentences but only one document. It\'s up to you to keep it or not when importing these two dataframes.\r\n\r\nThe sentence column is used to determine to which sentence the word in the word column belongs. It is in the form of a dictionary {\'id\': \'d000.s001\', \'idx\': \'1\'}. I thought for a while to keep only the idx because the id doesn\'t matter any more information. Nevertheless for the test dataset, the dictionary has an extra value indicating the source of the sentence. I don\'t know if it\'s useful or not, that\'s why I left the dictionary just in case. The user is free to do what he wants with it.\r\n\r\nCitation : \r\n\r\n> Segonne, V., Candito, M., and Crabb ́e, B. (2019). Usingwiktionary as a resource for wsd: the case of frenchverbs. InProceedings of the 13th International Confer-ence on Computational Semantics-Long Papers, pages259–270\r\n\r\n5.2 Noun Sense Disambiguation\r\nTwo dataframes are available:  2 train and  1 test\r\n\r\nI confess I didn\'t fully understand the procedure for this task.\r\n\r\nCitation : \r\n\r\n> @dataset{loic_vial_2019_3549806,\r\n>   author       = {Loïc Vial},\r\n>   title        = {{French Word Sense Disambiguation with Princeton \r\n>                    WordNet Identifiers}},\r\n>   month        = nov,\r\n>   year         = 2019,\r\n>   publisher    = {Zenodo},\r\n>   version      = {1.0},\r\n>   doi          = {10.5281/zenodo.3549806},\r\n>   url          = {https://doi.org/10.5281/zenodo.3549806}\r\n> }\r\n\r\nFinally, additional information about FLUE is available in the FlauBERT publication : \r\nhttps://arxiv.org/abs/1912.05372 (p. 4).\r\n\r\n\r\nHoping to have provided you with everything you need to add this benchmark  :) \r\n'
 'https://github.com/huggingface/datasets/pull/943']","Hi,

I think it would be interesting to add the FLUE dataset for francophones or anyone wishing to work on French.

In other requests, I read that you are already working on some datasets, and I was wondering if FLUE was planned.

If it is not the case, I can provide each of the cleaned FLUE datasets (in the form of a directly exploitable dataset rather than in the original xml formats which require additional processing, with the French part for cases where the dataset is based on a multilingual dataframe, etc.)."
https://github.com/huggingface/datasets/issues/222,Colab Notebook breaks when downloading the squad dataset,"[""The notebook forces version 0.1.0. If I use the latest, things work, I'll run the whole notebook and create a PR.\r\n\r\nBut in the meantime, this issue gets fixed by changing:\r\n`!pip install nlp==0.1.0`\r\nto\r\n`!pip install nlp`""
 'It still breaks very near the end\r\n\r\n![image](https://user-images.githubusercontent.com/338917/83312264-aa96a600-a1df-11ea-987f-2f4a0474247e.png)\r\n'
 ""When you install `nlp` for the first time on a Colab runtime, it updates the `pyarrow` library that was already on colab. This update shows this message on colab:\r\n```\r\nWARNING: The following packages were previously imported in this runtime:\r\n  [pyarrow]\r\nYou must restart the runtime in order to use newly installed versions.\r\n```\r\nYou just have to restart the runtime and it should be fine.\r\nIf you don't restart, then it breaks like in your first message ""
 ""Thanks for reporting the second one ! We'll update the notebook to fix this one :)""
 ""This trick from @thomwolf seems to be the most reliable solution to fix this colab notebook issue:\r\n\r\n```python\r\n# install nlp\r\n!pip install -qq nlp==0.2.0\r\n\r\n# Make sure that we have a recent version of pyarrow in the session before we continue - otherwise reboot Colab to activate it\r\nimport pyarrow\r\nif int(pyarrow.__version__.split('.')[1]) < 16:\r\n    import os\r\n    os.kill(os.getpid(), 9)\r\n```""
 ""The second part got fixed here: 2cbc656d6fc4b18ce57eac070baec05b31180d39\r\n\r\nThanks! I'm then closing this issue.""]","When I run the notebook in Colab
https://colab.research.google.com/github/huggingface/nlp/blob/master/notebooks/Overview.ipynb
breaks when running this cell:
![image](https://user-images.githubusercontent.com/338917/83311709-ffd1b800-a1dd-11ea-8394-3a87df0d7f8b.png)
"
https://github.com/huggingface/datasets/issues/217,Multi-task dataset mixing,"['I like this feature! I think the first question we should decide on is how to convert all datasets into the same format. In T5, the authors decided to format every dataset into a text-to-text format. If the dataset had ""multiple"" inputs like MNLI, the inputs were concatenated. So in MNLI the input:\r\n\r\n> - **Hypothesis**: The St. Louis Cardinals have always won.\r\n> \r\n> - **Premise**: yeah well losing is i mean i’m i’m originally from Saint Louis and Saint Louis Cardinals when they were there were uh a mostly a losing team but \r\n\r\nwas flattened to a single input:\r\n\r\n> mnli hypothesis: The St. Louis Cardinals have always won. premise:\r\n> yeah well losing is i mean i’m i’m originally from Saint Louis and Saint Louis Cardinals\r\n> when they were there were uh a mostly a losing team but.\r\n\r\nThis flattening is actually a very simple operation in `nlp` already. You would just need to do the following:\r\n\r\n```python \r\ndef flatten_inputs(example):\r\n    return {""input"": ""mnli hypothesis: "" + example[\'hypothesis\'] + "" premise: "" + example[\'premise\']}\r\n\r\nt5_ready_mnli_ds = mnli_ds.map(flatten_inputs, remove_columns=[<all columns except output>])\r\n```\r\n\r\nSo I guess converting the datasets into the same format can be left to the user for now. \r\nThen the question is how we can merge the datasets. I would probably be in favor of a simple \r\n\r\n```python \r\ndataset.add()\r\n```\r\n\r\nfunction that checks if the dataset is of the same format and if yes merges the two datasets. Finally, how should the sampling be implemented? **Examples-proportional mixing** corresponds to just merging the datasets and shuffling. For the other two sampling approaches we would need some higher-level features, maybe even a `dataset.sample()` function for merged datasets. \r\n\r\nWhat are your thoughts on this @thomwolf @lhoestq  @ghomasHudson @enzoampil ?'
 'I agree that we should leave the flattening of the dataset to the user for now. Especially because although the T5 framing seems obvious, there are slight variations on how the T5 authors do it in comparison to other approaches such as gpt-3 and decaNLP.\r\n\r\nIn terms of sampling, Examples-proportional mixing does seem the simplest to implement so would probably be a good starting point.\r\n\r\nTemperature-scaled mixing would probably most useful, offering flexibility as it can simulate the other 2 methods by setting the temperature parameter. There is a [relevant part of the T5 repo](https://github.com/google-research/text-to-text-transfer-transformer/blob/03c94165a7d52e4f7230e5944a0541d8c5710788/t5/data/utils.py#L889-L1118) which should help with implementation.\r\n\r\nAccording to the T5 authors, equal-mixing performs worst. Among the other two methods, tuning the K value (the artificial dataset size limit) has a large impact.\r\n'
 ""I agree with going with temperature-scaled mixing for its flexibility!\r\n\r\nFor the function that combines the datasets, I also find `dataset.add()` okay while also considering that users may want it to be easy to combine a list of say 10 data sources in one go.\r\n\r\n`dataset.sample()` should also be good. By the looks of it, we're planning to have as main parameters: `temperature`, and `K`.\r\n\r\nOn converting the datasets to the same format, I agree that we can leave these to the users for now. But, I do imagine it'd be an awesome feature for the future to have this automatically handled, based on a chosen *approach* to formatting :smile: \r\n\r\nE.g. T5, GPT-3, decaNLP, original raw formatting, or a contributed way of formatting in text-to-text. ""
 'This is an interesting discussion indeed and it would be nice to make multi-task easier.\r\n\r\nProbably the best would be to have a new type of dataset especially designed for that in order to easily combine and sample from the multiple datasets.\r\n\r\nThis way we could probably handle the combination of datasets with differing schemas as well (unlike T5).'
 ""@thomwolf Are you suggesting making a wrapper class which can take existing datasets as arguments and do all the required sampling/combining, to present the same interface as a normal dataset?\r\n\r\nThat doesn't seem too complicated to implement.\r\n""
 'I guess we\'re looking at the end user writing something like:\r\n``` python\r\nds = nlp.load_dataset(\'multitask-t5\',datasets=[""squad"",""cnn_dm"",...], k=1000, t=2.0)\r\n```\r\nUsing the t5 method of combining here (or this could be a function passed in as an arg) \r\n\r\nPassing kwargs to each \'sub-dataset\' might become tricky.'
 'From thinking upon @thomwolf \'s suggestion, I\'ve started experimenting:\r\n```python\r\nclass MultitaskDataset(DatasetBuilder):\r\n    def __init__(self, *args, **kwargs):\r\n        super(MultitaskDataset, self).__init__(*args, **kwargs)\r\n        self._datasets = kwargs.get(""datasets"")\r\n\r\n    def _info(self):\r\n        return nlp.DatasetInfo(\r\n            description=_DESCRIPTION,\r\n            features=nlp.Features({\r\n                    ""source"": nlp.Value(""string""),\r\n                    ""target"": nlp.Sequence(nlp.Value(""string""))\r\n                })\r\n        )\r\n\r\n    def _get_common_splits(self):\r\n        \'\'\'Finds the common splits present in all self._datasets\'\'\'\r\n        min_set = None\r\n        for dataset in self._datasets:\r\n            if min_set != None:\r\n                min_set.intersection(set(dataset.keys()))\r\n            else:\r\n                min_set = set(dataset.keys())\r\n        return min_set\r\n\r\n....\r\n\r\n# Maybe this?:\r\nsquad = nlp.load_dataset(""squad"")\r\ncnn_dm = nlp.load_dataset(""cnn_dailymail"",""3.0.0"")\r\nmultitask_dataset = nlp.load_dataset(\r\n    \'multitask_dataset\',\r\n    datasets=[squad,cnn_dailymail], \r\n    k=1000, \r\n    t=2.0\r\n)\r\n\r\n```\r\n\r\nDoes anyone know what methods of `MultitaskDataset` I would need to implement? Maybe `as_dataset` and `download_and_prepare`? Most of these should be just calling the methods of the sub-datasets. \r\n\r\nI\'m assuming DatasetBuilder is better than the more specific `GeneratorBasedBuilder`, `BeamBasedBuilder`, etc....\r\n\r\nOne of the other problems is that the dataset size is unknown till you construct it (as you can pick the sub-datasets). Am hoping not to need to make changes to `nlp.load_dataset` just for this class.\r\n\r\nI\'d appreciate it if anyone more familiar with nlp\'s internal workings could tell me if I\'m on the right track!'
 'I think I would probably go for a `MultiDataset` wrapper around a list of `Dataset`.\r\n\r\nI\'m not sure we need to give it `k` and `t` parameters at creation, it can maybe be something along the lines of:\r\n```python\r\nsquad = nlp.load_dataset(""squad"")\r\ncnn_dm = nlp.load_dataset(""cnn_dailymail"",""3.0.0"")\r\n\r\nmultitask_dataset = nlp.MultiDataset(squad, cnn_dm)\r\n\r\nbatch = multitask_dataset.sample(10, temperature=2.0, k=1000)\r\n```\r\n\r\nThe first proof-of-concept for multi-task datasets could definitely require that the provided datasets have the same name/type for columns (if needed you easily rename/cast a column prior to instantiating the `MultiDataset`).\r\n\r\nIt\'s good to think about it for some time though and don\'t overfit too much on the T5 examples (in particular for the ways/kwargs for sampling among datasets).'
 ""The problem with changing `k` and `t` per sampling is that you'd have to somehow remember which examples you'd already returned while re-weighting the remaining examples based on the new `k` and `t`values. It seems possible but complicated (I can't really see a reason why you'd want to change the weighting of datasets after you constructed the multidataset).\r\n\r\nWouldn't it be convenient if it implemented the dataset interface? Then if someone has code using a single nlp dataset, they can replace it with a multitask combination of more datasets without having to change other code. We would at least need to be able to pass it into a `DataLoader`.\r\n\r\n""
 'A very janky (but working) implementation of  `multitask_dataset.sample()` could be something like this:\r\n```python\r\nimport nlp\r\nimport torch\r\n\r\nclass MultiDataset():\r\n    def __init__(self, *args, temperature=2.0, k=1000, maximum=None, scale=1):\r\n        self.datasets = args\r\n        self._dataloaders = {}\r\n        for split in self._get_common_splits():\r\n            split_datasets = [ds[split] for ds in self.datasets]\r\n            mixing_rates = self._calc_mixing_rates(split_datasets,temperature, k, maximum, scale)\r\n            weights = []\r\n            for i in range(len(self.datasets)):\r\n                weights += [mixing_rates[i]]*len(self.datasets[i][split])\r\n            self._dataloaders[split] = torch.utils.data.DataLoader(torch.utils.data.ConcatDataset(split_datasets),\r\n                                                        sampler=torch.utils.data.sampler.WeightedRandomSampler(\r\n                                                            num_samples=len(weights),\r\n                                                            weights = weights,\r\n                                                            replacement=True),\r\n                                                        shuffle=False)\r\n\r\n    def _get_common_splits(self):\r\n        \'\'\'Finds the common splits present in all self.datasets\'\'\'\r\n        min_set = None\r\n        for dataset in self.datasets:\r\n            if min_set != None:\r\n                min_set.intersection(set(dataset.keys()))\r\n            else:\r\n                min_set = set(dataset.keys())\r\n        return min_set\r\n\r\n\r\n    def _calc_mixing_rates(self,datasets, temperature=2.0, k=1000, maximum=None, scale=1):\r\n       \'\'\'Work out the weighting of each dataset based on t and k\'\'\'\r\n        mixing_rates = []\r\n        for dataset in datasets:\r\n            rate = len(dataset)\r\n            rate *= scale\r\n            if maximum:\r\n                rate = min(rate, maximum)\r\n            if temperature != 1.0:\r\n                rate = rate ** (1.0/temperature)\r\n            mixing_rates.append(rate)\r\n        return mixing_rates\r\n\r\n    def sample(self,n,split):\r\n        batch = []\r\n        for example in self._dataloaders[split]:\r\n            batch.append(example)\r\n            n -= 1\r\n            if n == 0:\r\n                return batch\r\n\r\n\r\ndef flatten(dataset,flatten_fn):\r\n    for k in dataset.keys():\r\n        if isinstance(dataset[k],nlp.Dataset):\r\n            dataset[k] = dataset[k].map(flatten_fn,remove_columns=dataset[k].column_names)\r\n\r\n# Squad\r\ndef flatten_squad(example):\r\n    return {""source"": ""squad context: "" + example[\'context\'] + "" question: "" + example[\'question\'],""target"":example[""answers""][""text""]}\r\nsquad = nlp.load_dataset(""squad"")\r\nflatten(squad,flatten_squad)\r\n\r\n# CNN_DM\r\ndef flatten_cnn_dm(example):\r\n    return {""source"": ""cnn_dm: "" + example[\'article\'],""target"":[example[""highlights""]]}\r\ncnn_dm = nlp.load_dataset(""cnn_dailymail"", ""3.0.0"")\r\nflatten(cnn_dm,flatten_cnn_dm)\r\n\r\nmultitask_dataset = MultiDataset(squad, cnn_dm)\r\nbatch = multitask_dataset.sample(100,""train"")\r\n```\r\n\r\nThere\'s definitely a more sensible way than embedding `DataLoader`s inside. '
 'There is an interesting related investigation by @zphang here https://colab.research.google.com/github/zphang/zphang.github.io/blob/master/files/notebooks/Multi_task_Training_with_Transformers_NLP.ipynb'
 'Good spot! Here are my thoughts:\r\n\r\n- Aside: Adding `MultitaskModel` to transformers might be a thing to raise - even though having task-specific heads has become unfashionable in recent times in favour of text-to-text type models.\r\n- Adding the task name as an extra field also seems useful for these kind of models which have task-specific heads\r\n- There is some validation of our approach that the user should be expected to `map` datasets into a common form.\r\n- The size-proportional sampling (also called ""Examples-proportional mixing"") used here doesn\'t perform too badly in the T5 paper (it\'s comparable to temperature-scaled mixing in many cases but less flexible. This is only reasonable with a `K` maximum size parameter to prevent very large datasets dominating). This might be good for a first prototype using:\r\n    ```python\r\n    def __iter__(self):\r\n        """"""\r\n        For each batch, sample a task, and yield a batch from the respective\r\n        task Dataloader.\r\n\r\n        We use size-proportional sampling, but you could easily modify this\r\n        to sample from some-other distribution.\r\n        """"""\r\n        task_choice_list = []\r\n        for i, task_name in enumerate(self.task_name_list):\r\n            task_choice_list += [i] * self.num_batches_dict[task_name]\r\n        task_choice_list = np.array(task_choice_list)\r\n        np.random.shuffle(task_choice_list)\r\n\r\n        dataloader_iter_dict = {\r\n            task_name: iter(dataloader) \r\n            for task_name, dataloader in self.dataloader_dict.items()\r\n        }\r\n        for task_choice in task_choice_list:\r\n            task_name = self.task_name_list[task_choice]\r\n            yield next(dataloader_iter_dict[task_name])    \r\n    ```\r\n    We\'d just need to pull samples from the raw datasets and not from `DataLoader`s for each task. We can assume the user has done `dataset.shuffle()` if they want to.\r\n\r\n    Other sampling methods can later be implemented by changing how the `task_choice_list` is generated. This should allow more flexibility and not tie us to specific methods for sampling among datasets.\r\n'
 'Another thought: Multitasking over benchmarks (represented as Meta-datasets in nlp) is probably a common use case. Would be nice to pass an entire benchmark to our `MultiDataset` wrapper rather than having to pass individual components.'
 'Here\'s a fully working implementation based on the `__iter__` function of @zphang.\r\n\r\n- I\'ve generated the task choice list in the constructor as it allows us to index into the MultiDataset just like a normal dataset. I\'m changing `task_choice_list` into a list of `(dataset_idx, example_idx)` so each entry references a unique dataset example. The shuffling has to be done before this as we don\'t want to shuffle within each task (we assume this is done by the user if this is what they intend).\r\n-  I\'m slightly concerned this list could become very large if many large datasets were used. Can\'t see a way round it at the moment though.\r\n- I\'ve used `task.info.builder_name` as the dataset name. Not sure if this is correct.\r\n- I\'d love to add some of the other `Dataset` methods (map, slicing by column, etc...). Would be great to implement the whole interface so a single dataset can be simply replaced by this.\r\n- This does everything on the individual example-level. If some application required batches all from a single task in turn we can\'t really do that.\r\n\r\n```python\r\nimport nlp\r\nimport numpy as np\r\n\r\nclass MultiDataset:\r\n    def __init__(self,tasks):\r\n        self.tasks = tasks\r\n\r\n        # Create random order of tasks\r\n        # Using size-proportional sampling\r\n        task_choice_list = []\r\n        for i, task in enumerate(self.tasks):\r\n            task_choice_list += [i] * len(task)\r\n        task_choice_list = np.array(task_choice_list)\r\n        np.random.shuffle(task_choice_list)\r\n\r\n        # Add index into each dataset\r\n        # - We don\'t want to shuffle within each task\r\n        counters = {}\r\n        self.task_choice_list = []\r\n        for i in range(len(task_choice_list)):\r\n            idx = counters.get(task_choice_list[i],0)\r\n            self.task_choice_list.append((task_choice_list[i],idx))\r\n            counters[task_choice_list[i]] = idx + 1\r\n\r\n\r\n    def __len__(self):\r\n        return np.sum([len(t) for t in self.tasks])\r\n\r\n    def __repr__(self):\r\n        task_str = "", "".join([str(t) for t in self.tasks])\r\n        return f""MultiDataset(tasks: {task_str})""\r\n\r\n    def __getitem__(self,key):\r\n        if isinstance(key, int):\r\n            task_idx, example_idx = self.task_choice_list[key]\r\n            task = self.tasks[task_idx]\r\n            example = task[example_idx]\r\n            example[""task_name""] = task.info.builder_name\r\n            return example\r\n        elif isinstance(key, slice):\r\n            raise NotImplementedError()\r\n\r\n    def __iter__(self):\r\n        for i in range(len(self)):\r\n            yield self[i]\r\n\r\n\r\ndef load_multitask(*datasets):\r\n    \'\'\'Create multitask datasets per split\'\'\'\r\n\r\n    def _get_common_splits(datasets):\r\n        \'\'\'Finds the common splits present in all self.datasets\'\'\'\r\n        min_set = None\r\n        for dataset in datasets:\r\n            if min_set != None:\r\n                min_set.intersection(set(dataset.keys()))\r\n            else:\r\n                min_set = set(dataset.keys())\r\n        return min_set\r\n\r\n    common_splits = _get_common_splits(datasets)\r\n    out = {}\r\n    for split in common_splits:\r\n        out[split] = MultiDataset([d[split] for d in datasets])\r\n    return out\r\n\r\n\r\n##########################################\r\n# Dataset Flattening\r\n\r\ndef flatten(dataset,flatten_fn):\r\n    for k in dataset.keys():\r\n        if isinstance(dataset[k],nlp.Dataset):\r\n            dataset[k] = dataset[k].map(flatten_fn,remove_columns=dataset[k].column_names)\r\n\r\n# Squad\r\ndef flatten_squad(example):\r\n    return {""source"": ""squad context: "" + example[\'context\'] + "" question: "" + example[\'question\'],\r\n          ""target"":example[""answers""][""text""]}\r\nsquad = nlp.load_dataset(""squad"")\r\nflatten(squad,flatten_squad)\r\n\r\n# CNN_DM\r\ndef flatten_cnn_dm(example):\r\n    return {""source"": ""cnn_dm: "" + example[\'article\'],""target"":[example[""highlights""]]}\r\ncnn_dm = nlp.load_dataset(""cnn_dailymail"", ""3.0.0"")\r\nflatten(cnn_dm,flatten_cnn_dm)\r\n\r\n#############################################\r\n\r\nmtds = load_multitask(squad,cnn_dm)\r\n\r\nfor example in mtds[""train""]:\r\n    print(example[""task_name""],example[""target""])\r\n```\r\nLet me know if you have any thoughts. I\'ve started using this in some of my projects and it seems to work. If people are happy with the general approach for a first version, I can make a pull request.'
 ""Hey! Happy to jump into the discussion here. I'm still getting familiar with bits of this code, but the reasons I sampled over data loaders rather than datasets is 1) ensuring that each sampled batch corresponds to only 1 task (in case of different inputs formats/downstream models) and 2) potentially having different batch sizes per task (e.g. some tasks have very long/short inputs). How are you currently dealing with these in your PR?""
 ""The short answer is - I'm not! Everything is currently on a per-example basis. It would be fairly simple to add a `batch_size` argument which would ensure that every `batch_size` examples come from the same task. That should suit most use-cases (unless you wanted to ensure batches all came from the same task and apply something like `SortishSampler` on each task first)\r\n\r\nYour notebook was really inspiring by the way - thanks!""
 ""@zphang is having different batch sizes per task actually helpful? Would be interesting to know as it's not something I've come across as a technique used by any MTL papers.""
 ""mt-dnn's [batcher.py](https://github.com/namisan/mt-dnn/blob/master/mt_dnn/batcher.py) might be worth looking at.""
 ""> @zphang is having different batch sizes per task actually helpful? Would be interesting to know as it's not something I've come across as a technique used by any MTL papers.\r\n\r\nI think having different batch sizes per task is particularly helpful in some scenarios where each task has different amount of data. For example, the problem I'm currently facing is one task has tens of thousands of samples while one task has a couple hundreds. I think in this case different batch size could help. But if using the same batch size is a lot simpler to implement, I guess it makes sense to go with that.""
 ""I think that instead of proportional to size sampling you should specify weights or probabilities for drawing a batch from each dataset. We should also ensure that the smaller datasets are repeated so that the encoder layer doesn't overtrain on the largest dataset.""
 ""Are there any references for people doing different batch sizes per task in the literature? I've only seen constant batch sizes with differing numbers of batches for each task which seems sufficient to prevent the impact of large datasets (Read 3.5.3 of the [T5 paper](https://arxiv.org/pdf/1910.10683.pdf) for example).\r\n\r\n""
 'Hi,\r\nregarding building T5 dataset , I think we can use datasets https://github.com/huggingface/datasets and then need something similar to tf.data.experimental.sample_from_datasets, do you know if similar functionality exist in pytorch? Which can sample multiple datasets with the given rates. thanks. ']","It seems like many of the best performing models on the GLUE benchmark make some use of multitask learning (simultaneous training on multiple tasks).

The [T5 paper](https://arxiv.org/pdf/1910.10683.pdf) highlights multiple ways of mixing the tasks together during finetuning:
- **Examples-proportional mixing** - sample from tasks proportionally to their dataset size
- **Equal mixing** - sample uniformly from each task
- **Temperature-scaled mixing** - The generalized approach used by multilingual BERT which uses a temperature T, where the mixing rate of each task is raised to the power 1/T and renormalized. When T=1 this is equivalent to equal mixing, and becomes closer to equal mixing with increasing T.

Following this discussion https://github.com/huggingface/transformers/issues/4340 in [transformers](https://github.com/huggingface/transformers), @enzoampil suggested that the `nlp` library might be a better place for this functionality.

Some method for combining datasets could be implemented ,e.g.
```
dataset = nlp.load_multitask(['squad','imdb','cnn_dm'], temperature=2.0, ...)
```

We would need a few additions:
- Method of identifying the tasks - how can we support adding a string to each task as an identifier: e.g. 'summarisation: '?
- Method of combining the metrics - a standard approach is to use the specific metric for each task and add them together for a combined score.

It would be great to support common use cases such as pretraining on the GLUE benchmark before fine-tuning on each GLUE task in turn. 

I'm willing to write bits/most of this I just need some guidance on the interface and other library details so I can integrate it properly.

"
https://github.com/huggingface/datasets/issues/216,❓ How to get ROUGE-2 with the ROUGE metric ?,"[""ROUGE-1 and ROUGE-L shouldn't return the same thing. This is weird""
 'For the rouge2 metric you can do\r\n\r\n```python\r\nrouge = nlp.load_metric(\'rouge\')\r\nwith open(""pred.txt"") as p, open(""ref.txt"") as g:\r\n    for lp, lg in zip(p, g):\r\n        rouge.add(lp, lg)\r\nscore = rouge.compute(rouge_types=[""rouge2""])\r\n```\r\n\r\nNote that I just did a PR to have both `.add` and `.add_batch` for metrics, that\'s why now this is `rouge.add(lp, lg)` and not `rouge.add([lp], [lg])`'
 'Well I just tested with the official script and both rouge1 and rougeL return exactly the same thing for the input you gave, so this is actually fine ^^\r\n\r\nI hope it helped :)']","I'm trying to use ROUGE metric, but I don't know how to get the ROUGE-2 metric.

---

I compute scores with :

```python
import nlp

rouge = nlp.load_metric('rouge')
with open(""pred.txt"") as p, open(""ref.txt"") as g:
    for lp, lg in zip(p, g):
        rouge.add([lp], [lg])
score = rouge.compute()
```

then : _(print only the F-score for readability)_

```python
for k, s in score.items():
    print(k, s.mid.fmeasure)
```

It gives :

>rouge1 0.7915168355671788
rougeL 0.7915168355671788

---

**How can I get the ROUGE-2 score ?**

Also, it's seems weird that ROUGE-1 and ROUGE-L scores are the same. Did I made a mistake ?

@lhoestq "
https://github.com/huggingface/datasets/issues/215,NonMatchingSplitsSizesError when loading blog_authorship_corpus,"[""I just ran it on colab and got this\r\n```\r\n[{'expected': SplitInfo(name='train', num_bytes=610252351, num_examples=532812,\r\ndataset_name='blog_authorship_corpus'), 'recorded': SplitInfo(name='train',\r\nnum_bytes=611607465, num_examples=533285, dataset_name='blog_authorship_corpus')},\r\n{'expected': SplitInfo(name='validation', num_bytes=37500394, num_examples=31277,\r\ndataset_name='blog_authorship_corpus'), 'recorded': SplitInfo(name='validation',\r\nnum_bytes=35652716, num_examples=30804, dataset_name='blog_authorship_corpus')}]\r\n```\r\nwhich is different from the `dataset_infos.json` and also different from yours.\r\n\r\nIt looks like the script for generating examples is not consistent""
 ""The files provided by the authors are corrupted and the script seems to ignore the xml files that can't be decoded (it does `try:... except UnicodeDecodeError`). Maybe depending of the environment some files can be opened and some others don't but not sure why""
 'Feel free to do `ignore_verifications=True` for now... The verifications only include a check on the checksums of the downloaded files, and a check on the number of examples in each splits.'
 'I\'m getting this same issue when loading the `imdb` corpus via `dataset = load_dataset(""imdb"")`. When I try `ignore_verifications=True`, no examples are read into the `train` portion of the dataset. '
 '> I\'m getting this same issue when loading the `imdb` corpus via `dataset = load_dataset(""imdb"")`. When I try `ignore_verifications=True`, no examples are read into the `train` portion of the dataset.\r\n\r\nWhen the checksums don\'t match, it may mean that the file you downloaded is corrupted. In this case you can try to load the dataset again `load_dataset(""imdb"", download_mode=""force_redownload"")`\r\n\r\nAlso I just checked on my side and it worked fine:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(""imdb"")\r\nprint(len(dataset[""train""]))\r\n# 25000\r\n```\r\n\r\nLet me know if redownloading fixes your issue @EmilyAlsentzer .\r\nIf not, feel free to open a separate issue.'
 ""It doesn't seem to fix the problem. I'll open a separate issue. Thanks. ""
 'I wasn\'t aware of the ""force_redownload"" option and manually removed the \'/home/me/.cache/huggingface/datasets/\' dir, this worked for me (dataset \'cnn_dailymail\')'
 'Yes I think this might not be documented well enough. Let’s add it to the doc @lhoestq @SBrandeis.\r\nAnd everything on how to control the cache behavior better (removing, overriding, changing the path, etc)']","Getting this error when i run `nlp.load_dataset('blog_authorship_corpus')`. 

```
raise NonMatchingSplitsSizesError(str(bad_splits))
nlp.utils.info_utils.NonMatchingSplitsSizesError: [{'expected': SplitInfo(name='train', 
num_bytes=610252351, num_examples=532812, dataset_name='blog_authorship_corpus'), 
'recorded': SplitInfo(name='train', num_bytes=616473500, num_examples=536323, 
dataset_name='blog_authorship_corpus')}, {'expected': SplitInfo(name='validation', 
num_bytes=37500394, num_examples=31277, dataset_name='blog_authorship_corpus'), 
'recorded': SplitInfo(name='validation', num_bytes=30786661, num_examples=27766, 
dataset_name='blog_authorship_corpus')}]
```

Upon checking it seems like there is a disparity between the information in `datasets/blog_authorship_corpus/dataset_infos.json` and what was downloaded. Although I can get away with this by passing `ignore_verifications=True` in `load_dataset`, I'm thinking doing so might give problems later on."
https://github.com/huggingface/datasets/issues/211,"[Arrow writer, Trivia_qa] Could not convert TagMe with type str: converting to null type","['Here the full error trace:\r\n\r\n```\r\nArrowInvalid                              Traceback (most recent call last)\r\n<ipython-input-1-7aaf3f011358> in <module>\r\n      1 import nlp\r\n      2 ds = nlp.load_dataset(""trivia_qa"", ""rc"", split=""validation[:1%]"")  # this might take 2.3 min to download but it\'s cached afterwards...\r\n----> 3 ds.map(lambda x: x, load_from_cache_file=False)\r\n\r\n~/python_bin/nlp/arrow_dataset.py in map(self, function, with_indices, batched, batch_size, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, arrow_schema, disable_nullable)\r\n    549\r\n    550         if update_data:\r\n--> 551             writer.finalize()  # close_stream=bool(buf_writer is None))  # We only close if we are writing in a file\r\n    552\r\n    553             # Create new Dataset from buffer or file\r\n\r\n~/python_bin/nlp/arrow_writer.py in finalize(self, close_stream)\r\n    182     def finalize(self, close_stream=True):\r\n    183         if self.pa_writer is not None:\r\n--> 184             self.write_on_file()\r\n    185             self.pa_writer.close()\r\n    186         if close_stream:\r\n\r\n~/python_bin/nlp/arrow_writer.py in write_on_file(self)\r\n    104         """"""\r\n    105         if self.current_rows:\r\n--> 106             pa_array = pa.array(self.current_rows, type=self._type)\r\n    107             first_example = pa.array(self.current_rows[0:1], type=self._type)[0]\r\n    108             # Sanity check\r\n\r\n~/hugging_face/venv_3.7/lib/python3.7/site-packages/pyarrow/array.pxi in pyarrow.lib.array()\r\n\r\n~/hugging_face/venv_3.7/lib/python3.7/site-packages/pyarrow/array.pxi in pyarrow.lib._sequence_to_array()\r\n\r\n~/hugging_face/venv_3.7/lib/python3.7/site-packages/pyarrow/error.pxi in pyarrow.lib.check_status()\r\n\r\nArrowInvalid: Could not convert TagMe with type str: converting to null type\r\n```'
 'Actually thinking a bit more about it, it\'s probably a data sample that is not correct in `trivia_qa`. But I\'m a bit surprised though that we managed to write it in .arrow format and now cannot write it anymore after an ""identity"" mapping.'
 ""I don't have this error :x""
 'Interesting, maybe I have a very old cache of trivia_qa...thanks for checking'
 ""I'm running it right now on colab to double check""
 'Actually, I know what the problem is...I\'m quite sure it\'s a bug. Here we take some test inputs: https://github.com/huggingface/nlp/blob/0e0ef12c14d2175e0b0bd7d8aa814b09e2cd7e1f/src/nlp/arrow_dataset.py#L472\r\n\r\nIt might be that in the test inputs, a `Sequence` type value is an emtpy list. So in my case I have `ds[0][""entity_pages\'][""wiki_context""] = []`. => this leads to an `arrow_schema` equal to `null`  for `[""entity_pages\'][""wiki_context""]` => see line: https://github.com/huggingface/nlp/blob/0e0ef12c14d2175e0b0bd7d8aa814b09e2cd7e1f/src/nlp/arrow_dataset.py#L501  instead of list of string which it should for other examples. \r\n\r\nGuess it\'s an edge case, but it can happen.'
 ""Good point, I think the schema should be infered at the writing stage where we have a `writer_batch_size` number of examples (typically 10k) so it's even less likely to run into this scenario.""]","Running the following code 

```
import nlp
ds = nlp.load_dataset(""trivia_qa"", ""rc"", split=""validation[:1%]"")  # this might take 2.3 min to download but it's cached afterwards...
ds.map(lambda x: x, load_from_cache_file=False)
```

triggers a `ArrowInvalid: Could not convert TagMe with type str: converting to null type` error.

On the other hand if we remove a certain column of `trivia_qa` which seems responsible for the bug, it works:

```
import nlp
ds = nlp.load_dataset(""trivia_qa"", ""rc"", split=""validation[:1%]"")  # this might take 2.3 min to download but it's cached afterwards...
ds.map(lambda x: x, remove_columns=[""entity_pages""], load_from_cache_file=False)
```

. Seems quite hard to debug what's going on here... @lhoestq @thomwolf - do you have a good first guess what the problem could be?

**Note** BTW: I think this could be a good test to check that the datasets work correctly: Take a tiny portion of the dataset and check that it can be written correctly."
https://github.com/huggingface/datasets/issues/207,Remove test set from NLP viewer,"['~is the viewer also open source?~\r\n[is a streamlit app!](https://docs.streamlit.io/en/latest/getting_started.html)'
 'Appears that [two thirds of those polled on Twitter](https://twitter.com/srush_nlp/status/1265734497632477185) are in favor of _some_ mechanism for averting eyeballs from the test data.']","While the new [NLP viewer](https://huggingface.co/nlp/viewer/) is a great tool, I think it would be best to outright remove the option of looking at the test sets. At the very least, a warning should be displayed to users before showing the test set. Newcomers to the field might not be aware of best practices, and small things like this can help increase awareness."
https://github.com/huggingface/datasets/issues/206,[Question] Combine 2 datasets which have the same columns,"['We are thinking about ways to combine datasets for T5 in #217, feel free to share your thoughts about this.'
 'Ok great! I will look at it. Thanks']","Hi,

I am using ``nlp`` to load personal datasets. I created summarization datasets in multi-languages based on wikinews. I have one dataset for english and one for german (french is getting to be ready as well). I want to keep these datasets independent because they need different pre-processing (add different task-specific prefixes for T5 : *summarize:* for english and *zusammenfassen:* for german)

My issue is that I want to train T5 on the combined english and german datasets to see if it improves results. So I would like to combine 2 datasets (which have the same columns) to make one and train T5 on it. I was wondering if there is a proper way to do it? I assume that it can be done by combining all examples of each dataset but maybe you have a better solution.

Hoping this is clear enough,

Thanks a lot 😊
Best"
https://github.com/huggingface/datasets/issues/202,Mistaken `_KWARGS_DESCRIPTION` for XNLI metric,"['Indeed, good catch ! thanks\r\nFixing it right now']","Hi!

The [`_KWARGS_DESCRIPTION`](https://github.com/huggingface/nlp/blob/7d0fa58641f3f462fb2861dcdd6ce7f0da3f6a56/metrics/xnli/xnli.py#L45) for the XNLI metric uses `Args` and `Returns` text from [BLEU](https://github.com/huggingface/nlp/blob/7d0fa58641f3f462fb2861dcdd6ce7f0da3f6a56/metrics/bleu/bleu.py#L58) metric:

```
_KWARGS_DESCRIPTION = """"""
Computes XNLI score which is just simple accuracy.
Args:
    predictions: list of translations to score.
        Each translation should be tokenized into a list of tokens.
    references: list of lists of references for each translation.
        Each reference should be tokenized into a list of tokens.
    max_order: Maximum n-gram order to use when computing BLEU score.
    smooth: Whether or not to apply Lin et al. 2004 smoothing.
Returns:
    'bleu': bleu score,
    'precisions': geometric mean of n-gram precisions,
    'brevity_penalty': brevity penalty,
    'length_ratio': ratio of lengths,
    'translation_length': translation_length,
    'reference_length': reference_length
""""""
```

But it should be something like:

```
_KWARGS_DESCRIPTION = """"""
Computes XNLI score which is just simple accuracy.
Args:
    predictions: Predicted labels.
    references: Ground truth labels.
Returns:
    'accuracy': accuracy
```"
https://github.com/huggingface/datasets/issues/198,Index outside of table length,['Sounds like something related to the nlp viewer @srush ' 'Fixed. '],"The offset input box warns of numbers larger than a limit (like 2000) but then the errors start at a smaller value than that limit (like 1955).

> ValueError: Index (2000) outside of table length (2000).
> Traceback:
> File ""/home/sasha/.local/lib/python3.7/site-packages/streamlit/ScriptRunner.py"", line 322, in _run_script
>     exec(code, module.__dict__)
> File ""/home/sasha/nlp_viewer/run.py"", line 116, in <module>
>     v = d[item][k]
> File ""/home/sasha/.local/lib/python3.7/site-packages/nlp/arrow_dataset.py"", line 338, in __getitem__
>     output_all_columns=self._output_all_columns,
> File ""/home/sasha/.local/lib/python3.7/site-packages/nlp/arrow_dataset.py"", line 290, in _getitem
>     raise ValueError(f""Index ({key}) outside of table length ({self._data.num_rows})."")"
https://github.com/huggingface/datasets/issues/197,Scientific Papers only downloading Pubmed,"[""Hi so there are indeed two configurations in the datasets as you can see [here](https://github.com/huggingface/nlp/blob/master/datasets/scientific_papers/scientific_papers.py#L81-L82).\r\n\r\nYou can load either one with:\r\n```python\r\ndataset = nlp.load_dataset('scientific_papers', 'pubmed')\r\ndataset = nlp.load_dataset('scientific_papers', 'arxiv')\r\n```\r\n\r\nThis issues is actually related to a similar user-experience issue with GLUE. When several configurations are available and the first configuration is loaded by default (see issue #152 and #130), it seems to be unexpected for users.\r\n\r\nI think we should maybe raise a (very explicit) error when there are several configurations available and the user doesn't specify one.\r\n\r\nWhat do you think @lhoestq @patrickvonplaten @mariamabarham ?""
 'Yes, it looks like the right thing to do '
 ""Now if you don't specify which part you want, it raises an error:\r\n```\r\nValueError: Config name is missing.\r\nPlease pick one among the available configs: ['pubmed', 'arxiv']\r\nExample of usage:\r\n\t`load_dataset('scientific_papers', 'pubmed')`\r\n```""]","Hi!

I have been playing around with this module, and I am a bit confused about the `scientific_papers` dataset. I thought that it would download two separate datasets, arxiv and pubmed. But when I run the following:

```
dataset = nlp.load_dataset('scientific_papers', data_dir='.', cache_dir='.')
Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5.05k/5.05k [00:00<00:00, 2.66MB/s]
Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4.90k/4.90k [00:00<00:00, 2.42MB/s]
Downloading and preparing dataset scientific_papers/pubmed (download: 4.20 GiB, generated: 2.33 GiB, total: 6.53 GiB) to ./scientific_papers/pubmed/1.1.1...
Downloading: 3.62GB [00:40, 90.5MB/s]
Downloading: 880MB [00:08, 101MB/s]
Dataset scientific_papers downloaded and prepared to ./scientific_papers/pubmed/1.1.1. Subsequent calls will reuse this data.
```

only a pubmed folder is created. There doesn't seem to be something for arxiv. Are these two datasets merged? Or have I misunderstood something?

Thanks!"
https://github.com/huggingface/datasets/issues/193,[Tensorflow] Use something else than `from_tensor_slices()`,"[""I guess we can use `tf.data.Dataset.from_generator` instead. I'll give it a try.""
 'Is `tf.data.Dataset.from_generator` working on TPU ?'
 '`from_generator` is not working on TPU, I met the following error :\r\n\r\n```\r\nFile ""/usr/local/lib/python3.6/contextlib.py"", line 88, in __exit__\r\n    next(self.gen)\r\n  File ""/home/usr/.venv/bart/lib/python3.6/site-packages/tensorflow_core/python/eager/context.py"", line 1900, in execution_mode\r\n    executor_new.wait()\r\n  File ""/home/usr/.venv/bart/lib/python3.6/site-packages/tensorflow_core/python/eager/executor.py"", line 67, in wait\r\n    pywrap_tensorflow.TFE_ExecutorWaitForAllPendingNodes(self._handle)\r\ntensorflow.python.framework.errors_impl.NotFoundError: No registered \'PyFunc\' OpKernel for \'CPU\' devices compatible with node {{node PyFunc}}\r\n        .  Registered:  <no registered kernels>\r\n\r\n         [[PyFunc]]\r\n```\r\n\r\n---\r\n\r\n@lhoestq It seems you merged some changes that allow lazy-loading. **Can you give an example of how to use ?** Maybe the Colab notebook should be updated with this method as well.'
 'Could you send me the code you used to run create the dataset using `.from_generator` ? What version of tensorflow are you using ?'
 'I\'m using TF2.2\r\n\r\nHere is my code :\r\n```\r\nimport nlp\r\nfrom transformers import BartTokenizer\r\n\r\ntokenizer = BartTokenizer.from_pretrained(\'bart-large\')\r\n\r\ndef encode(sample):\r\n    article_inputs = tokenizer.encode_plus(sample[""article""], max_length=tokenizer.model_max_length, pad_to_max_length=True)\r\n    summary_inputs = tokenizer.encode_plus(sample[""highlights""], max_length=tokenizer.model_max_length, pad_to_max_length=True)\r\n\r\n    article_inputs.update({""lm_labels"": summary_inputs[\'input_ids\']})\r\n    return article_inputs\r\n\r\ncnn_dm = nlp.load_dataset(\'cnn_dailymail\', \'3.0.0\', split=\'test\')\r\ncnn_dm = cnn_dm.map(encode)\r\n\r\ndef gen():\r\n    for sample in cnn_dm:\r\n        s = {}\r\n        s[\'input_ids\'] = sample[\'input_ids\']\r\n        s[\'attention_mask\'] = sample[\'attention_mask\']\r\n        s[\'lm_labels\'] = sample[\'lm_labels\']\r\n        yield s\r\n\r\ndataset = tf.data.Dataset.from_generator(gen, output_types={k: tf.int32 for k in [\'input_ids\', \'attention_mask\', \'lm_labels\']}, output_shapes={k: tf.TensorShape([tokenizer.model_max_length]) for k in [\'input_ids\', \'attention_mask\', \'lm_labels\']}\r\n```'
 ""Apparently we'll have to wait for the next tensorflow release to use `.from_generator` and TPU. See https://github.com/tensorflow/tensorflow/issues/34346#issuecomment-598262489""
 'Fixed by https://github.com/huggingface/datasets/pull/339']","In the example notebook, the TF Dataset is built using `from_tensor_slices()` :

```python
columns = ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions']
train_tf_dataset.set_format(type='tensorflow', columns=columns)
features = {x: train_tf_dataset[x] for x in columns[:3]} 
labels = {""output_1"": train_tf_dataset[""start_positions""]}
labels[""output_2""] = train_tf_dataset[""end_positions""]
tfdataset = tf.data.Dataset.from_tensor_slices((features, labels)).batch(8)
```

But according to [official tensorflow documentation](https://www.tensorflow.org/guide/data#consuming_numpy_arrays), this will load the entire dataset to memory.

**This defeats one purpose of this library, which is lazy loading.**

Is there any other way to load the `nlp` dataset into TF dataset lazily ?

---

For example, is it possible to use [Arrow dataset](https://www.tensorflow.org/io/api_docs/python/tfio/arrow/ArrowDataset) ? If yes, is there any code example ?"
https://github.com/huggingface/datasets/issues/192,[Question] Create Apache Arrow dataset from raw text file,"[""We store every dataset in the Arrow format. This is convenient as it supports nested types and memory mapping. If you are curious feel free to check the [pyarrow documentation](https://arrow.apache.org/docs/python/)\r\n\r\nYou can use this library to load your covid papers by creating a dataset script. You can find inspiration from the ones we've already written in `/datasets`. Here is a link to the steps to [add a dataset](https://github.com/huggingface/nlp/blob/master/CONTRIBUTING.md#how-to-add-a-dataset)""
 'Hello @mrm8488 and @lhoestq \r\n\r\nIs there a way to convert a dataset to Apache arrow format (locally/personal use) & use it before sending it to hugging face?\r\n\r\nThanks :)'
 '> Is there a way to convert a dataset to Apache arrow format (locally/personal use) & use it before sending it to hugging face?\r\n\r\nSure, to get a dataset in arrow format you can either:\r\n- [load from local files (txt, json, csv)](https://huggingface.co/nlp/loading_datasets.html?highlight=csv#from-local-files)\r\n- OR [load from python data (dict, pandas)](https://huggingface.co/nlp/loading_datasets.html?highlight=csv#from-in-memory-data)\r\n- OR [create your own dataset script](https://huggingface.co/nlp/loading_datasets.html?highlight=csv#using-a-custom-dataset-loading-script)\r\n']","Hi guys, I have gathered and preprocessed about 2GB of COVID papers from CORD dataset @ Kggle. I have seen you have a text dataset as ""Crime and punishment"" in Apache arrow format. Do you have any script to do it from a raw txt file (preprocessed as for BERT like) or any guide?
Is the worth of send it to you and add it to the NLP library?
Thanks, Manu
"
https://github.com/huggingface/datasets/issues/189,[Question] BERT-style multiple choice formatting,"['Hi @sarahwie, can you details this a little more?\r\n\r\nI\'m not sure I understand what you refer to and what you mean when you say ""Previously, this was done by passing a list of InputFeatures to the dataloader instead of a list of InputFeature""'
 'I think I\'ve resolved it. For others\' reference: to convert from using the [`MultipleChoiceDataset` class](https://github.com/huggingface/transformers/blob/a34a9896ac2a4a33ff9cd805c76eed914c8d8965/examples/multiple-choice/utils_multiple_choice.py#L82)/[`run_multiple_choice.py`](https://github.com/huggingface/transformers/blob/a34a9896ac2a4a33ff9cd805c76eed914c8d8965/examples/multiple-choice/run_multiple_choice.py) script in Huggingface Transformers, I\'ve done the following for hellaswag:\r\n\r\n1. converted the `convert_examples_to_features()` function to only take one input and return a dictionary rather than a list:\r\n```\r\ndef convert_examples_to_features(example, tokenizer, max_length):\r\n\r\n    choices_inputs = defaultdict(list)\r\n    for ending_idx, ending in enumerate(example[\'endings\'][\'ending\']):\r\n        text_a = example[\'ctx\']\r\n        text_b = ending\r\n\r\n        inputs = tokenizer.encode_plus(\r\n            text_a,\r\n            text_b,\r\n            add_special_tokens=True,\r\n            max_length=max_length,\r\n            pad_to_max_length=True,\r\n            return_overflowing_tokens=True,\r\n        )\r\n        if ""num_truncated_tokens"" in inputs and inputs[""num_truncated_tokens""] > 0:\r\n            logger.info(\r\n                ""Attention! you are cropping tokens (swag task is ok). ""\r\n                ""If you are training ARC and RACE and you are poping question + options,""\r\n                ""you need to try to use a bigger max seq length!""\r\n            )\r\n\r\n        for key in inputs:\r\n            choices_inputs[key].append(inputs[key])\r\n            \r\n        choices_inputs[\'label\'] = int(example[\'label\'])\r\n\r\n    return choices_inputs\r\n```\r\n2. apply this directly (instance-wise) to dataset, convert dataset to torch tensors. Dataset is then ready to be passed to `Trainer` instance.\r\n\r\n```\r\ndataset[\'train\'] = dataset[\'train\'].map(lambda x: convert_examples_to_features(x, tokenizer, max_length), batched=False)\r\ncolumns = [\'input_ids\', \'token_type_ids\', \'attention_mask\', \'label\']\r\ndataset[\'train\'].set_format(type=\'torch\', columns=columns)\r\n```']","Hello, I am wondering what the equivalent formatting of a dataset should be to allow for multiple-choice answering prediction, BERT-style. Previously, this was done by passing a list of `InputFeatures` to the dataloader instead of a list of `InputFeature`, where `InputFeatures` contained lists of length equal to the number of answer choices in the MCQ instead of single items. I'm a bit confused on what the output of my feature conversion function should be when using `dataset.map()` to ensure similar behavior.

Thanks!"
https://github.com/huggingface/datasets/issues/188,When will the remaining math_dataset modules be added as dataset objects,"['On a similar note it would be nice to differentiate between train-easy, train-medium, and train-hard'
 ""Hi @tylerroost, we don't have a timeline for this at the moment.\r\nIf you want to give it a look we would be happy to review a PR on it.\r\nAlso, the library is one week old so everything is quite barebones, in particular the doc.\r\nYou should expect some bumps on the road.\r\n\r\nTo get you started, you can check the datasets scripts in the `./datasets` folder on the repo and find the one on math_datasets that will need to be modified. Then you should check the original repository on the math_dataset to see where the other files to download are located and what is the expected format for the various parts of the dataset.\r\n\r\nTo get a general overview on how datasets scripts are written and used, you can read the nice tutorial on how to add a new dataset for TensorFlow Dataset [here](https://www.tensorflow.org/datasets/add_dataset), our API is not exactly identical but it can give you a high-level overview.""
 ""Thanks I'll give it a look""]","Currently only the algebra_linear_1d is supported. Is there a timeline for making the other modules supported. If no timeline is established, how can I help?"
https://github.com/huggingface/datasets/issues/187,[Question] How to load wikipedia ? Beam runner ?,"['I have seen that somebody is hard working on easierly loadable wikipedia. #129 \r\nMaybe I should wait a few days for that version ?'
 'Yes we (well @lhoestq) are very actively working on this.']","When `nlp.load_dataset('wikipedia')`, I got
* `WARNING:nlp.builder:Trying to generate a dataset using Apache Beam, yet no Beam Runner or PipelineOptions() has been provided. Please pass a nlp.DownloadConfig(beam_runner=...) object to the builder.download_and_prepare(download_config=...) method. Default values will be used.`
* `AttributeError: 'NoneType' object has no attribute 'size'`

Could somebody tell me what should I do ? 

# Env
On Colab,
```
git clone https://github.com/huggingface/nlp
cd nlp
pip install -q .
```
```
%pip install -q apache_beam mwparserfromhell
-> ERROR: pydrive 1.3.1 has requirement oauth2client>=4.0.0, but you'll have oauth2client 3.0.0 which is incompatible.
ERROR: google-api-python-client 1.7.12 has requirement httplib2<1dev,>=0.17.0, but you'll have httplib2 0.12.0 which is incompatible.
ERROR: chainer 6.5.0 has requirement typing-extensions<=3.6.6, but you'll have typing-extensions 3.7.4.2 which is incompatible.
```
```
pip install -q apache-beam[interactive]
ERROR: google-colab 1.0.0 has requirement ipython~=5.5.0, but you'll have ipython 5.10.0 which is incompatible.
```

# The whole message
```
WARNING:nlp.builder:Trying to generate a dataset using Apache Beam, yet no Beam Runner or PipelineOptions() has been provided. Please pass a nlp.DownloadConfig(beam_runner=...) object to the builder.download_and_prepare(download_config=...) method. Default values will be used.

Downloading and preparing dataset wikipedia/20200501.aa (download: Unknown size, generated: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/wikipedia/20200501.aa/1.0.0...

---------------------------------------------------------------------------

AttributeError                            Traceback (most recent call last)

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/common.cpython-36m-x86_64-linux-gnu.so in apache_beam.runners.common.DoFnRunner.process()

44 frames

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/common.cpython-36m-x86_64-linux-gnu.so in apache_beam.runners.common.PerWindowInvoker.invoke_process()

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/common.cpython-36m-x86_64-linux-gnu.so in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window()

/usr/local/lib/python3.6/dist-packages/apache_beam/io/iobase.py in process(self, element, init_result)
   1081       writer.write(e)
-> 1082     return [window.TimestampedValue(writer.close(), timestamp.MAX_TIMESTAMP)]
   1083 

/usr/local/lib/python3.6/dist-packages/apache_beam/io/filebasedsink.py in close(self)
    422   def close(self):
--> 423     self.sink.close(self.temp_handle)
    424     return self.temp_shard_path

/usr/local/lib/python3.6/dist-packages/apache_beam/io/parquetio.py in close(self, writer)
    537     if len(self._buffer[0]) > 0:
--> 538       self._flush_buffer()
    539     if self._record_batches_byte_size > 0:

/usr/local/lib/python3.6/dist-packages/apache_beam/io/parquetio.py in _flush_buffer(self)
    569       for b in x.buffers():
--> 570         size = size + b.size
    571     self._record_batches_byte_size = self._record_batches_byte_size + size

AttributeError: 'NoneType' object has no attribute 'size'

During handling of the above exception, another exception occurred:

AttributeError                            Traceback (most recent call last)

<ipython-input-9-340aabccefff> in <module>()
----> 1 dset = nlp.load_dataset('wikipedia')

/usr/local/lib/python3.6/dist-packages/nlp/load.py in load_dataset(path, name, version, data_dir, data_files, split, cache_dir, download_config, download_mode, ignore_verifications, save_infos, **config_kwargs)
    518         download_mode=download_mode,
    519         ignore_verifications=ignore_verifications,
--> 520         save_infos=save_infos,
    521     )
    522 

/usr/local/lib/python3.6/dist-packages/nlp/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, save_infos, dl_manager, **download_and_prepare_kwargs)
    370                 verify_infos = not save_infos and not ignore_verifications
    371                 self._download_and_prepare(
--> 372                     dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
    373                 )
    374                 # Sync info

/usr/local/lib/python3.6/dist-packages/nlp/builder.py in _download_and_prepare(self, dl_manager, verify_infos)
    770         with beam.Pipeline(runner=beam_runner, options=beam_options,) as pipeline:
    771             super(BeamBasedBuilder, self)._download_and_prepare(
--> 772                 dl_manager, pipeline=pipeline, verify_infos=False
    773             )  # TODO{beam} verify infos
    774 

/usr/local/lib/python3.6/dist-packages/apache_beam/pipeline.py in __exit__(self, exc_type, exc_val, exc_tb)
    501   def __exit__(self, exc_type, exc_val, exc_tb):
    502     if not exc_type:
--> 503       self.run().wait_until_finish()
    504 
    505   def visit(self, visitor):

/usr/local/lib/python3.6/dist-packages/apache_beam/pipeline.py in run(self, test_runner_api)
    481       return Pipeline.from_runner_api(
    482           self.to_runner_api(use_fake_coders=True), self.runner,
--> 483           self._options).run(False)
    484 
    485     if self._options.view_as(TypeOptions).runtime_type_check:

/usr/local/lib/python3.6/dist-packages/apache_beam/pipeline.py in run(self, test_runner_api)
    494       finally:
    495         shutil.rmtree(tmpdir)
--> 496     return self.runner.run_pipeline(self, self._options)
    497 
    498   def __enter__(self):

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/direct/direct_runner.py in run_pipeline(self, pipeline, options)
    128       runner = BundleBasedDirectRunner()
    129 
--> 130     return runner.run_pipeline(pipeline, options)
    131 
    132 

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/portability/fn_api_runner.py in run_pipeline(self, pipeline, options)
    553 
    554     self._latest_run_result = self.run_via_runner_api(
--> 555         pipeline.to_runner_api(default_environment=self._default_environment))
    556     return self._latest_run_result
    557 

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/portability/fn_api_runner.py in run_via_runner_api(self, pipeline_proto)
    563     # TODO(pabloem, BEAM-7514): Create a watermark manager (that has access to
    564     #   the teststream (if any), and all the stages).
--> 565     return self.run_stages(stage_context, stages)
    566 
    567   @contextlib.contextmanager

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/portability/fn_api_runner.py in run_stages(self, stage_context, stages)
    704               stage,
    705               pcoll_buffers,
--> 706               stage_context.safe_coders)
    707           metrics_by_stage[stage.name] = stage_results.process_bundle.metrics
    708           monitoring_infos_by_stage[stage.name] = (

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/portability/fn_api_runner.py in _run_stage(self, worker_handler_factory, pipeline_components, stage, pcoll_buffers, safe_coders)
   1071         cache_token_generator=cache_token_generator)
   1072 
-> 1073     result, splits = bundle_manager.process_bundle(data_input, data_output)
   1074 
   1075     def input_for(transform_id, input_id):

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/portability/fn_api_runner.py in process_bundle(self, inputs, expected_outputs)
   2332 
   2333     with UnboundedThreadPoolExecutor() as executor:
-> 2334       for result, split_result in executor.map(execute, part_inputs):
   2335 
   2336         split_result_list += split_result

/usr/lib/python3.6/concurrent/futures/_base.py in result_iterator()
    584                     # Careful not to keep a reference to the popped future
    585                     if timeout is None:
--> 586                         yield fs.pop().result()
    587                     else:
    588                         yield fs.pop().result(end_time - time.monotonic())

/usr/lib/python3.6/concurrent/futures/_base.py in result(self, timeout)
    430                 raise CancelledError()
    431             elif self._state == FINISHED:
--> 432                 return self.__get_result()
    433             else:
    434                 raise TimeoutError()

/usr/lib/python3.6/concurrent/futures/_base.py in __get_result(self)
    382     def __get_result(self):
    383         if self._exception:
--> 384             raise self._exception
    385         else:
    386             return self._result

/usr/local/lib/python3.6/dist-packages/apache_beam/utils/thread_pool_executor.py in run(self)
     42       # If the future wasn't cancelled, then attempt to execute it.
     43       try:
---> 44         self._future.set_result(self._fn(*self._fn_args, **self._fn_kwargs))
     45       except BaseException as exc:
     46         # Even though Python 2 futures library has #set_exection(),

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/portability/fn_api_runner.py in execute(part_map)
   2329           self._registered,
   2330           cache_token_generator=self._cache_token_generator)
-> 2331       return bundle_manager.process_bundle(part_map, expected_outputs)
   2332 
   2333     with UnboundedThreadPoolExecutor() as executor:

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/portability/fn_api_runner.py in process_bundle(self, inputs, expected_outputs)
   2243             process_bundle_descriptor_id=self._bundle_descriptor.id,
   2244             cache_tokens=[next(self._cache_token_generator)]))
-> 2245     result_future = self._worker_handler.control_conn.push(process_bundle_req)
   2246 
   2247     split_results = []  # type: List[beam_fn_api_pb2.ProcessBundleSplitResponse]

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/portability/fn_api_runner.py in push(self, request)
   1557       self._uid_counter += 1
   1558       request.instruction_id = 'control_%s' % self._uid_counter
-> 1559     response = self.worker.do_instruction(request)
   1560     return ControlFuture(request.instruction_id, response)
   1561 

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/worker/sdk_worker.py in do_instruction(self, request)
    413       # E.g. if register is set, this will call self.register(request.register))
    414       return getattr(self, request_type)(
--> 415           getattr(request, request_type), request.instruction_id)
    416     else:
    417       raise NotImplementedError

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/worker/sdk_worker.py in process_bundle(self, request, instruction_id)
    448         with self.maybe_profile(instruction_id):
    449           delayed_applications, requests_finalization = (
--> 450               bundle_processor.process_bundle(instruction_id))
    451           monitoring_infos = bundle_processor.monitoring_infos()
    452           monitoring_infos.extend(self.state_cache_metrics_fn())

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/worker/bundle_processor.py in process_bundle(self, instruction_id)
    837         for data in data_channel.input_elements(instruction_id,
    838                                                 expected_transforms):
--> 839           input_op_by_transform_id[data.transform_id].process_encoded(data.data)
    840 
    841       # Finish all operations.

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/worker/bundle_processor.py in process_encoded(self, encoded_windowed_values)
    214       decoded_value = self.windowed_coder_impl.decode_from_stream(
    215           input_stream, True)
--> 216       self.output(decoded_value)
    217 
    218   def try_split(self, fraction_of_remainder, total_buffer_size):

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/worker/operations.cpython-36m-x86_64-linux-gnu.so in apache_beam.runners.worker.operations.Operation.output()

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/worker/operations.cpython-36m-x86_64-linux-gnu.so in apache_beam.runners.worker.operations.Operation.output()

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/worker/operations.cpython-36m-x86_64-linux-gnu.so in apache_beam.runners.worker.operations.SingletonConsumerSet.receive()

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/worker/operations.cpython-36m-x86_64-linux-gnu.so in apache_beam.runners.worker.operations.DoOperation.process()

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/worker/operations.cpython-36m-x86_64-linux-gnu.so in apache_beam.runners.worker.operations.DoOperation.process()

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/common.cpython-36m-x86_64-linux-gnu.so in apache_beam.runners.common.DoFnRunner.process()

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/common.cpython-36m-x86_64-linux-gnu.so in apache_beam.runners.common.DoFnRunner._reraise_augmented()

/usr/local/lib/python3.6/dist-packages/future/utils/__init__.py in raise_with_traceback(exc, traceback)
    417         if traceback == Ellipsis:
    418             _, _, traceback = sys.exc_info()
--> 419         raise exc.with_traceback(traceback)
    420 
    421 else:

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/common.cpython-36m-x86_64-linux-gnu.so in apache_beam.runners.common.DoFnRunner.process()

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/common.cpython-36m-x86_64-linux-gnu.so in apache_beam.runners.common.PerWindowInvoker.invoke_process()

/usr/local/lib/python3.6/dist-packages/apache_beam/runners/common.cpython-36m-x86_64-linux-gnu.so in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window()

/usr/local/lib/python3.6/dist-packages/apache_beam/io/iobase.py in process(self, element, init_result)
   1080     for e in bundle[1]:  # values
   1081       writer.write(e)
-> 1082     return [window.TimestampedValue(writer.close(), timestamp.MAX_TIMESTAMP)]
   1083 
   1084 

/usr/local/lib/python3.6/dist-packages/apache_beam/io/filebasedsink.py in close(self)
    421 
    422   def close(self):
--> 423     self.sink.close(self.temp_handle)
    424     return self.temp_shard_path

/usr/local/lib/python3.6/dist-packages/apache_beam/io/parquetio.py in close(self, writer)
    536   def close(self, writer):
    537     if len(self._buffer[0]) > 0:
--> 538       self._flush_buffer()
    539     if self._record_batches_byte_size > 0:
    540       self._write_batches(writer)

/usr/local/lib/python3.6/dist-packages/apache_beam/io/parquetio.py in _flush_buffer(self)
    568     for x in arrays:
    569       for b in x.buffers():
--> 570         size = size + b.size
    571     self._record_batches_byte_size = self._record_batches_byte_size + size

AttributeError: 'NoneType' object has no attribute 'size' [while running 'train/Save to parquet/Write/WriteImpl/WriteBundles']
```"
https://github.com/huggingface/datasets/issues/186,Weird-ish: Not creating unique caches for different phases,"[""Looks like a duplicate of #120.\r\nThis is already fixed on master. We'll do a new release on pypi soon""
 'Good catch, it looks fixed.\r\n']","Sample code:

```python
import nlp
dataset = nlp.load_dataset('boolq')

def func1(x):
    return x

def func2(x):
    return None

train_output = dataset[""train""].map(func1)
valid_output = dataset[""validation""].map(func1)
print()
print(len(train_output), len(valid_output))
# Output: 9427 9427
```

The map method in both cases seem to be pointing to the same cache, so the latter call based on the validation data will return the processed train data cache.

What's weird is that the following doesn't seem to be an issue:

```python
train_output = dataset[""train""].map(func2)
valid_output = dataset[""validation""].map(func2)
print()
print(len(train_output), len(valid_output))
# 9427 3270
```"
https://github.com/huggingface/datasets/issues/183,[Bug] labels of glue/ax are all -1 ,"['This is the test set given by the Glue benchmark. The labels are not provided, and therefore set to -1.'
 'Ah, yeah. Why it didn’t occur to me. 😂\nThank you for your comment.']","```
ax = nlp.load_dataset('glue', 'ax')
for i in range(30): print(ax['test'][i]['label'], end=', ')
```
```
-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 
```"
https://github.com/huggingface/datasets/issues/181,Cannot upload my own dataset,"[""It's my misunderstanding. I cannot just upload a csv. I need to write a dataset loading script too.""
 'I now try with the sample `datasets/csv` folder. \r\n\r\n    nlp-cli upload csv\r\n\r\nThe error is still the same\r\n\r\n```\r\n2020-05-21 17:20:56.394659: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\nAbout to upload file /content/csv/csv.py to S3 under filename csv/csv.py and namespace korakot\r\nAbout to upload file /content/csv/dummy/0.0.0/dummy_data.zip to S3 under filename csv/dummy/0.0.0/dummy_data.zip and namespace korakot\r\nProceed? [Y/n] y\r\nUploading... This might take a while if files are large\r\nTraceback (most recent call last):\r\n  File ""/usr/local/bin/nlp-cli"", line 33, in <module>\r\n    service.run()\r\n  File ""/usr/local/lib/python3.6/dist-packages/nlp/commands/user.py"", line 234, in run\r\n    token=token, filename=filename, filepath=filepath, organization=self.args.organization\r\n  File ""/usr/local/lib/python3.6/dist-packages/nlp/hf_api.py"", line 141, in presign_and_upload\r\n    urls = self.presign(token, filename=filename, organization=organization)\r\n  File ""/usr/local/lib/python3.6/dist-packages/nlp/hf_api.py"", line 132, in presign\r\n    return PresignedUrl(**d)\r\nTypeError: __init__() got an unexpected keyword argument \'cdn\'\r\n```\r\n'
 ""We haven't tested the dataset upload feature yet cc @julien-c \r\nThis is on our short/mid-term roadmap though""
 ""Even if I fix the `TypeError: __init__() got an unexpected keyword argument 'cdn'` error, it looks like it still uploads to                                                                                                                               `https://s3.amazonaws.com/models.huggingface.co/bert/<namespace>/<dataset_name>` instead of `https://s3.amazonaws.com/datasets.huggingface.co/nlp/<namespace>/<dataset_name>`""
 ""@lhoestq The endpoints in https://github.com/huggingface/nlp/blob/master/src/nlp/hf_api.py should be (depending on the type of file):\r\n```\r\nPOST   /api/datasets/presign\r\nGET    /api/datasets/listObjs\r\nDELETE /api/datasets/deleteObj\r\nPOST   /api/metrics/presign  \r\nGET    /api/metrics/listObjs\r\nDELETE /api/metrics/deleteObj\r\n```\r\n\r\nIn addition to this, @thomwolf cleaned up the objects with dataclasses but you should revert this and re-align to the hf_api that's in this branch of transformers: https://github.com/huggingface/transformers/pull/4632 (so that potential new JSON attributes in the API output don't break existing versions of any library)""
 'New commands are\r\n```\r\nnlp-cli upload_dataset <path/to/dataset>\r\nnlp-cli upload_metric <path/to/metric>\r\nnlp-cli s3_datasets {rm, ls}\r\nnlp-cli s3_metrics {rm, ls}\r\n```\r\nClosing this issue.']","I look into `nlp-cli` and `user.py` to learn how to upload my own data.

It is supposed to work like this
- Register to get username, password at huggingface.co
- `nlp-cli login` and type username, passworld
- I have a single file to upload at `./ttc/ttc_freq_extra.csv`
- `nlp-cli upload ttc/ttc_freq_extra.csv`

But I got this error.

```
2020-05-21 16:33:52.722464: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
About to upload file /content/ttc/ttc_freq_extra.csv to S3 under filename ttc/ttc_freq_extra.csv and namespace korakot
Proceed? [Y/n] y
Uploading... This might take a while if files are large
Traceback (most recent call last):
  File ""/usr/local/bin/nlp-cli"", line 33, in <module>
    service.run()
  File ""/usr/local/lib/python3.6/dist-packages/nlp/commands/user.py"", line 234, in run
    token=token, filename=filename, filepath=filepath, organization=self.args.organization
  File ""/usr/local/lib/python3.6/dist-packages/nlp/hf_api.py"", line 141, in presign_and_upload
    urls = self.presign(token, filename=filename, organization=organization)
  File ""/usr/local/lib/python3.6/dist-packages/nlp/hf_api.py"", line 132, in presign
    return PresignedUrl(**d)
TypeError: __init__() got an unexpected keyword argument 'cdn'
```"
https://github.com/huggingface/datasets/issues/179,[Feature request] separate split name and split instructions,"['If your dataset is a collection of sub-datasets, you should probably consider having one config per sub-dataset. For example for Glue, we have sst2, mnli etc.\r\nIf you want to have multiple train sets (for example one per stage). The easiest solution would be to name them `nlp.Split(""train_stage1"")`, `nlp.Split(""train_stage2"")`, etc. or something like that.'
 'Thanks for the tip! I ended up setting up three different versions of the dataset with their own configs.\r\n\r\nfor the named splits, I was trying with `nlp.Split(""train-stage1"")`, which fails. Changing to `nlp.Split(""train_stage1"")` works :) I looked for examples of what works in the code comments, it may be worth adding some examples of valid/invalid names in there?']","Currently, the name of an nlp.NamedSplit is parsed in arrow_reader.py and used as the instruction.

This makes it impossible to have several training sets, which can occur when:
- A dataset corresponds to a collection of sub-datasets
- A dataset was built in stages, adding new examples at each stage

Would it be possible to have two separate fields in the Split class, a name /instruction and a unique ID that is used as the key in the builder's split_dict ?"
https://github.com/huggingface/datasets/issues/172,Clone not working on Windows environment,"['Should be fixed on master now :)'
 'Thanks @lhoestq 👍  Now I can uninstall WSL and get back to work with windows.🙂']","Cloning in a windows environment is not working because of use of special character '?' in folder name ..
Please consider changing the folder name ....
Reference to folder -
nlp/datasets/cnn_dailymail/dummy/3.0.0/3.0.0/dummy_data-zip-extracted/dummy_data/uc?export=download&id=0BwmD_VLjROrfM1BxdkxVaTY2bWs/dailymail/stories/

error log:
fatal: cannot create directory at 'datasets/cnn_dailymail/dummy/3.0.0/3.0.0/dummy_data-zip-extracted/dummy_data/uc?export=download&id=0BwmD_VLjROrfM1BxdkxVaTY2bWs': Invalid argument

"
https://github.com/huggingface/datasets/issues/168,Loading 'wikitext' dataset fails,"['Hi, make sure you have a recent version of pyarrow.\r\n\r\nAre you using it in Google Colab? In this case, this error is probably the same as #128'
 ""Thanks!\r\n\r\nYes I'm using Google Colab, it seems like a duplicate then.""
 'Closing as it is a duplicate'
 ""Hi,\r\nThe squad bug seems to be fixed, but the loading of the 'wikitext' still suffers from this problem (on Colab with pyarrow=0.17.1).""
 'When you install `nlp` for the first time on a Colab runtime, it updates the `pyarrow` library that was already on colab. This update shows this message on colab:\r\n```\r\nWARNING: The following packages were previously imported in this runtime:\r\n  [pyarrow]\r\nYou must restart the runtime in order to use newly installed versions.\r\n```\r\nYou just have to restart the runtime and it should be fine.'
 'That was it, thanks!']","Loading the 'wikitext' dataset fails with Attribute error:

Code to reproduce (From example notebook):

import nlp
wikitext_dataset = nlp.load_dataset('wikitext')


Error:
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-17-d5d9df94b13c> in <module>()
     11 
     12 # Load a dataset and print the first examples in the training set
---> 13 wikitext_dataset = nlp.load_dataset('wikitext')
     14 print(wikitext_dataset['train'][0])

6 frames
/usr/local/lib/python3.6/dist-packages/nlp/load.py in load_dataset(path, name, version, data_dir, data_files, split, cache_dir, download_config, download_mode, ignore_verifications, save_infos, **config_kwargs)
    518         download_mode=download_mode,
    519         ignore_verifications=ignore_verifications,
--> 520         save_infos=save_infos,
    521     )
    522 

/usr/local/lib/python3.6/dist-packages/nlp/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, save_infos, dl_manager, **download_and_prepare_kwargs)
    363                 verify_infos = not save_infos and not ignore_verifications
    364                 self._download_and_prepare(
--> 365                     dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
    366                 )
    367                 # Sync info

/usr/local/lib/python3.6/dist-packages/nlp/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)
    416             try:
    417                 # Prepare split will record examples associated to the split
--> 418                 self._prepare_split(split_generator, **prepare_split_kwargs)
    419             except OSError:
    420                 raise OSError(""Cannot find data file. "" + (self.MANUAL_DOWNLOAD_INSTRUCTIONS or """"))

/usr/local/lib/python3.6/dist-packages/nlp/builder.py in _prepare_split(self, split_generator)
    594             example = self.info.features.encode_example(record)
    595             writer.write(example)
--> 596         num_examples, num_bytes = writer.finalize()
    597 
    598         assert num_examples == num_examples, f""Expected to write {split_info.num_examples} but wrote {num_examples}""

/usr/local/lib/python3.6/dist-packages/nlp/arrow_writer.py in finalize(self, close_stream)
    173     def finalize(self, close_stream=True):
    174         if self.pa_writer is not None:
--> 175             self.write_on_file()
    176             self.pa_writer.close()
    177         if close_stream:

/usr/local/lib/python3.6/dist-packages/nlp/arrow_writer.py in write_on_file(self)
    124             else:
    125                 # All good
--> 126                 self._write_array_on_file(pa_array)
    127             self.current_rows = []
    128 

/usr/local/lib/python3.6/dist-packages/nlp/arrow_writer.py in _write_array_on_file(self, pa_array)
     93     def _write_array_on_file(self, pa_array):
     94         """"""Write a PyArrow Array""""""
---> 95         pa_batch = pa.RecordBatch.from_struct_array(pa_array)
     96         self._num_bytes += pa_array.nbytes
     97         self.pa_writer.write_batch(pa_batch)

AttributeError: type object 'pyarrow.lib.RecordBatch' has no attribute 'from_struct_array'"
https://github.com/huggingface/datasets/issues/166,Add a method to shuffle a dataset,"['+1 for the naming convention\r\n\r\nAbout the `shuffle` method, from my understanding it should be done in `Dataloader` (better separation between dataset processing - usage)'
 '+1 for shuffle in `Dataloader`. \r\nSome `Dataloader` just store idxs of dataset and just shuffle those idxs, which might(?) be faster than do shuffle in dataset, especially when doing shuffle every epoch.\r\n\r\nAlso +1 for the naming convention.'
 'As you might already know the issue of dataset shuffling came up in the nlp code [walkthrough](https://youtu.be/G3pOvrKkFuk?t=3204) by Yannic Kilcher\r\n'
 'We added the `.shuffle` method :)\r\n\r\nClosing this one.']","Could maybe be a `dataset.shuffle(generator=None, seed=None)` signature method.

Also, we could maybe have a clear indication of which method modify in-place and which methods return/cache a modified dataset. I kinda like torch conversion of having an underscore suffix for all the methods which modify a dataset in-place. What do you think?"
https://github.com/huggingface/datasets/issues/164,Add Spanish POR and NER Datasets,"['Hello @mrm8488, are these datasets official datasets published in an NLP/CL/ML venue?'
 'What about this one: https://github.com/ccasimiro88/TranslateAlignRetrieve?']","Hi guys,
In order to cover multilingual support a little step could be adding standard Datasets used for Spanish NER and POS tasks.
I can provide it in raw and preprocessed formats."
https://github.com/huggingface/datasets/issues/163,[Feature request] Add cos-e v1.0,"['Sounds good, @mariamabarham do you want to give a look?\r\nI think we should have two configurations so we can allow either version of the dataset to be loaded with the `1.0` version being the default maybe.\r\n\r\nCc some authors of the great cos-e: @nazneenrajani @bmccann'
 'cos_e v1.0 is related to CQA v1.0 but only CQA v1.11 dataset is available on their website. Indeed their is lots of ids in cos_e v1, which are  not in CQA v1.11  or the other way around.\r\n@sarahwie, @thomwolf, @nazneenrajani,  @bmccann  do you know where I can  find CQA v1.0\r\n'
 ""@mariamabarham I'm also not sure where to find CQA 1.0. Perhaps it's not possible to include this version of the dataset. I'll close the issue if that's the case.""
 'I do have a copy of the dataset. I can upload it to our repo.'
 'Great @nazneenrajani. let me know once done.\r\nThanks'
 '@mariamabarham @sarahwie I added them to the cos-e repo https://github.com/salesforce/cos-e/tree/master/data/v1.0'
 'You can now do\r\n```python\r\nfrom nlp import load_dataset\r\ncos_e = load_dataset(""cos_e"", ""v1.0"")\r\n```\r\nThanks @mariamabarham !'
 'Thanks!'
 '@mariamabarham Just wanted to note that default behavior `cos_e = load_dataset(""cos_e"")` now loads `v1.0`. Not sure if this is intentional (but the flag specification does work as intended). '
 '> @mariamabarham Just wanted to note that default behavior `cos_e = load_dataset(""cos_e"")` now loads `v1.0`. Not sure if this is intentional (but the flag specification does work as intended).\r\n\r\nIn the new version of `nlp`, if you try `cos_e = load_dataset(""cos_e"")` it throws this error:\r\n```\r\nValueError: Config name is missing.\r\nPlease pick one among the available configs: [\'v1.0\', \'v1.11\']\r\nExample of usage:\r\n\t`load_dataset(\'cos_e\', \'v1.0\')`\r\n```\r\nFor datasets with at least two configurations, we now force the user to pick one (no default)']","I noticed the second release of cos-e (v1.11) is included in this repo. I wanted to request inclusion of v1.0, since this is the version on which results are reported on in [the paper](https://www.aclweb.org/anthology/P19-1487/), and v1.11 has noted [annotation](https://github.com/salesforce/cos-e/issues/2) [issues](https://arxiv.org/pdf/2004.14546.pdf)."
https://github.com/huggingface/datasets/issues/161,Discussion on version identifier & MockDataLoaderManager for test data,"['usually you can replace `download` in your dataset script with `download_and_prepare()` - could you share the code for your dataset here? :-) '
 ""I have an initial version here: https://github.com/EntilZha/nlp/tree/master/datasets/qanta Thats pretty close to what I'll do as a PR, but still want to do some more sanity checks/tests (just got tests passing).\r\n\r\nI figured out how to get all tests passing by adding a download command and some finagling with the data zip https://github.com/EntilZha/nlp/blob/master/tests/utils.py#L127\r\n\r\n""
 ""I'm quite positive that you can just replace the `dl_manager.download()` statements here: https://github.com/EntilZha/nlp/blob/4d46443b65f1f756921db8275594e6af008a1de7/datasets/qanta/qanta.py#L194 with `dl_manager.download_and_extract()` even though you don't extract anything. I would prefer to avoid adding more functions to the MockDataLoadManager and keep it as simple as possible (It's already to complex now IMO). \r\n\r\nCould you check if you can replace the `download()` function? ""
 ""I might be doing something wrong, but swapping those two gives this error:\r\n```\r\n>       with open(path) as f:\r\nE       IsADirectoryError: [Errno 21] Is a directory: 'datasets/qanta/dummy/mode=first,char_skip=25/2018.4.18/dummy_data-zip-extracted/dummy_data'\r\n\r\nsrc/nlp/datasets/qanta/3d965403133687b819905ead4b69af7bcee365865279b2f797c79f809b4490c3/qanta.py:280: IsADirectoryError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n```\r\n\r\nSo it seems like the directory name is getting passed. Is this not functioning as expected, or is there some caching happening maybe? I deleted the dummy files and re-ran the import script with no changes. I'm digging a bit in with a debugger, but no clear reason yet""
 ""From what I can tell here: https://github.com/huggingface/nlp/blob/master/tests/utils.py#L115\r\n\r\n1. `data_url` is the correct http link\r\n2. `path_to_dummy_data` is a directory, which is causing the issue\r\n\r\nThat path comes from `download_dummy_data`, which I think assumes that the data comes from the zip file, but isn't aware of individual files. So it seems like it data manager needs to be aware if the url its getting is for a file or a zip/directory, and pass this information along. This might happen in `download_dummy_data`, but probably better to happen in `download_and_extract`? Maybe a simple check to see if `os.path.basename` returns the dummy data zip filename, if not then join paths with the basename of the url?""
 'I think the dataset script works correctly. Just the dummy data structure seems to be wrong. I will soon add more commands that should make the create of the dummy data easier.\r\n\r\nI\'d recommend that you won\'t concentrate too much on the dummy data.\r\nIf you manage to load the dataset correctly via:\r\n\r\n```python \r\n# use local path to qanta\r\nnlp.load_dataset(""./datasets/qanta"")\r\n```\r\n\r\nthen feel free to open a PR and we will look into the dummy data problem together :-) \r\n\r\nAlso please make sure that the Version is in the format 1.0.0 (three numbers separated by two points) - not a date. '
 ""The script loading seems to work fine so I'll work on getting a PR open after a few sanity checks on the data.\r\n\r\nOn version, we currently have it versioned with YYYY.MM.DD scheme so it would be nice to not change that, but will it cause issues?""
 ""> The script loading seems to work fine so I'll work on getting a PR open after a few sanity checks on the data.\r\n> \r\n> On version, we currently have it versioned with YYYY.MM.DD scheme so it would be nice to not change that, but will it cause issues?\r\n\r\nIt would cause issues for sure for the tests....not sure if it would also cause issues otherwise.\r\n\r\nI would prefer to keep the same version style as we have for other models. You could for example simply add version 1.0.0 and add a comment with the date you currently use for the versioning.\r\n\r\n What is your opinion regarding the version here @lhoestq @mariamabarham @thomwolf ? ""
 ""Maybe use the YYYY.MM.DD as the config name ? That's what we are doing for wikipedia""
 ""> Maybe use the YYYY.MM.DD as the config name ? That's what we are doing for wikipedia\r\n\r\nI'm not sure if this will work because the name should be unique and it seems that he has multiple config name in his data with the same version.\r\nAs @patrickvonplaten  suggested, I think  you can add a comment about the version in the data description.""
 'Actually maybe our versioning format (inherited from tfds) is too strong for what we use it for?\r\nWe could allow any string maybe?\r\n\r\nI see it more and more like an identifier for the user that we will back with a serious hashing/versioning system.- so we could let the user quite free on it.'
 ""I'm good with either putting it in description, adding it to the config, or loosening version formatting. I mostly don't have a full conceptual grasp of what each identifier ends up meaning in the datasets code so hard to evaluate the best approach.\r\n\r\nFor background, the multiple formats is a consequence of:\r\n\r\n1. Each example is one multi-sentence trivia question\r\n2. For training, its better to treat each sentence as an example\r\n3. For evaluation, should test on: (1) first sentence, (2) full question, and (3) partial questions (does the model get the question right having seen the first half)\r\n\r\nWe use the date format for version since: (1) we expect some degree of updates since new questions come in every year and (2) the timestamp itself matches the Wikipedia dump that it is dependent on (so similar to the Wikipedia dataset).\r\n\r\nperhaps this is better discussed in https://github.com/huggingface/nlp/pull/169 or update title?""]","Hi, I'm working on adding a dataset and ran into an error due to `download` not being defined on `MockDataLoaderManager`, but being defined in `nlp/utils/download_manager.py`. The readme step running this: `RUN_SLOW=1 pytest tests/test_dataset_common.py::DatasetTest::test_load_real_dataset_localmydatasetname` triggers the error. If I can get something to work, I can include it in my data PR once I'm done."
https://github.com/huggingface/datasets/issues/160,"caching in map causes same result to be returned for train, validation and test","[""Hi @dpressel, \r\n\r\nthanks for posting your issue! Can you maybe add a complete code snippet that we can copy paste to reproduce the error? For example, I'm not sure where the variable `train_set` comes from in your code and it seems like you are loading multiple datasets at once?  ""
 'Hi, the full example was listed in the PR above, but here is the exact link:\r\n\r\nhttps://github.com/dpressel/mead-baseline/blob/3c1aa3ca062cb23f303ca98ac40b6652b37ee971/api-examples/layers-classify-hf-datasets.py\r\n\r\nThe problem is coming from\r\n```\r\n            if cache_file_name is None:\r\n                # we create a unique hash from the function, current dataset file and the mapping args\r\n                cache_kwargs = {\r\n                    ""with_indices"": with_indices,\r\n                    ""batched"": batched,\r\n                    ""batch_size"": batch_size,\r\n                    ""remove_columns"": remove_columns,\r\n                    ""keep_in_memory"": keep_in_memory,\r\n                    ""load_from_cache_file"": load_from_cache_file,\r\n                    ""cache_file_name"": cache_file_name,\r\n                    ""writer_batch_size"": writer_batch_size,\r\n                    ""arrow_schema"": arrow_schema,\r\n                    ""disable_nullable"": disable_nullable,\r\n                }\r\n                cache_file_name = self._get_cache_file_path(function, cache_kwargs)\r\n```\r\nThe cached value is always the same, but I was able to change that by just renaming the function each time which seems to fix the issue.'
 'Ok, I think @lhoestq has already found a solution :-) Maybe you can chime in @lhoestq '
 'This fixed my issue (I think)\r\n\r\nhttps://github.com/dpressel/mead-baseline/commit/48aa8ecde4b307bd3e7dde5fe71e43a1d4769ee1'
 '> Ok, I think @lhoestq has already found a solution :-) Maybe you can chime in @lhoestq\r\n\r\nOh, awesome!  I see the PR, Ill check it out'
 'The PR should prevent the cache from losing track of the of the dataset type (based on the location of its data). Not sure about your second problem though (cache off).'
 'Yes, with caching on, it seems to work without the function renaming hack, I mentioned this also in the PR. Thanks!']","hello,

I am working on a program that uses the `nlp` library with the `SST2` dataset.

The rough outline of the program is:

```
import nlp as nlp_datasets
...
parser.add_argument('--dataset', help='HuggingFace Datasets id', default=['glue', 'sst2'], nargs='+')
...
dataset = nlp_datasets.load_dataset(*args.dataset)
...
# Create feature vocabs
vocabs = create_vocabs(dataset.values(), vectorizers)
...
# Create a function to vectorize based on vectorizers and vocabs:

print('TS', train_set.num_rows)
print('VS', valid_set.num_rows)
print('ES', test_set.num_rows)

# factory method to create a `convert_to_features` function based on vocabs
convert_to_features = create_featurizer(vectorizers, vocabs)
train_set = train_set.map(convert_to_features, batched=True)
train_set.set_format(type='torch', columns=list(vectorizers.keys()) + ['y', 'lengths'])
train_loader = torch.utils.data.DataLoader(train_set, batch_size=args.batchsz)

valid_set = valid_set.map(convert_to_features, batched=True)
valid_set.set_format(type='torch', columns=list(vectorizers.keys()) + ['y', 'lengths'])
valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=args.batchsz)

test_set = test_set.map(convert_to_features, batched=True)
test_set.set_format(type='torch', columns=list(vectorizers.keys()) + ['y', 'lengths'])
test_loader = torch.utils.data.DataLoader(test_set, batch_size=args.batchsz)

print('TS', train_set.num_rows)
print('VS', valid_set.num_rows)
print('ES', test_set.num_rows)

```
Im not sure if Im using it incorrectly, but the results are not what I expect.  Namely, the `.map()`  seems to grab the datset from the cache and then loses track of what the specific dataset is, instead using my training data for all datasets:

```
TS 67349
VS 872
ES 1821
TS 67349
VS 67349
ES 67349
```

The behavior changes if I turn off the caching but then the results fail:

```
train_set = train_set.map(convert_to_features, batched=True, load_from_cache_file=False)
...
valid_set = valid_set.map(convert_to_features, batched=True, load_from_cache_file=False)
...
test_set = test_set.map(convert_to_features, batched=True, load_from_cache_file=False)
```

Now I get the right set of features back...
```
TS 67349
VS 872
ES 1821
100%|██████████| 68/68 [00:00<00:00, 92.78it/s]
100%|██████████| 1/1 [00:00<00:00, 75.47it/s]
  0%|          | 0/2 [00:00<?, ?it/s]TS 67349
VS 872
ES 1821
100%|██████████| 2/2 [00:00<00:00, 77.19it/s]
```
but I think its losing track of the original training set:

```
Traceback (most recent call last):
  File ""/home/dpressel/dev/work/baseline/api-examples/layers-classify-hf-datasets.py"", line 148, in <module>
    for x in train_loader:
  File ""/home/dpressel/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 345, in __next__
    data = self._next_data()
  File ""/home/dpressel/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py"", line 385, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File ""/home/dpressel/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py"", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File ""/home/dpressel/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py"", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File ""/home/dpressel/anaconda3/lib/python3.7/site-packages/nlp/arrow_dataset.py"", line 338, in __getitem__
    output_all_columns=self._output_all_columns,
  File ""/home/dpressel/anaconda3/lib/python3.7/site-packages/nlp/arrow_dataset.py"", line 294, in _getitem
    outputs = self._unnest(self._data.slice(key, 1).to_pydict())
  File ""pyarrow/table.pxi"", line 1211, in pyarrow.lib.Table.slice
  File ""pyarrow/public-api.pxi"", line 390, in pyarrow.lib.pyarrow_wrap_table
  File ""pyarrow/error.pxi"", line 85, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: Column 3: In chunk 0: Invalid: Length spanned by list offsets (15859698) larger than values array (length 100000)

Process finished with exit code 1
```

The full-example program (minus the print stmts) is here:
https://github.com/dpressel/mead-baseline/pull/620/files

"
https://github.com/huggingface/datasets/issues/159,How can we add more datasets to nlp library?,['Found it. https://github.com/huggingface/nlp/tree/master/datasets'],
https://github.com/huggingface/datasets/issues/157,"nlp.load_dataset() gives ""TypeError: list_() takes exactly one argument (2 given)""","['You can just run: \r\n`val = nlp.load_dataset(\'squad\')` \r\n\r\nif you want to have just the validation script you can also do:\r\n\r\n`val = nlp.load_dataset(\'squad\', split=""validation"")`'
 'If you want to load a local dataset, make sure you include a `./` before the folder name. '
 'This happens by just doing run all cells on colab ... I assumed the colab example is broken. '
 'Oh I see you might have a wrong version of pyarrow install on the colab -> could you try the following. Add these lines to the beginning of your notebook, restart the runtime and run it again:\r\n```\r\n!pip uninstall -y -qq pyarrow\r\n!pip uninstall -y -qq nlp\r\n!pip install -qq git+https://github.com/huggingface/nlp.git\r\n```'
 '> Oh I see you might have a wrong version of pyarrow install on the colab -> could you try the following. Add these lines to the beginning of your notebook, restart the runtime and run it again:\r\n> \r\n> ```\r\n> !pip uninstall -y -qq pyarrow\r\n> !pip uninstall -y -qq nlp\r\n> !pip install -qq git+https://github.com/huggingface/nlp.git\r\n> ```\r\n\r\nTried, having the same error.'
 ""Can you post a link here of your colab? I'll make a copy of it and see what's wrong""
 'This should be fixed in the current version of the notebook. You can try it again'
 'Also see: https://github.com/huggingface/nlp/issues/222'
 'I am getting this error when running this command\r\n```\r\nval = nlp.load_dataset(\'squad\', split=""validation"")\r\n```\r\n\r\nFileNotFoundError: [Errno 2] No such file or directory: \'/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/dataset_info.json\'\r\n\r\nCan anybody help?'
 'It seems like your download was corrupted :-/ Can you run the following command: \r\n\r\n```\r\nrm -r /root/.cache/huggingface/datasets\r\n```\r\n\r\nto delete the cache completely and rerun the download? '
 'I tried the notebook again today and it worked without barfing. 👌  ']","I'm trying to load datasets from nlp but there seems to have error saying 
""TypeError: list_() takes exactly one argument (2 given)""

gist can be found here
https://gist.github.com/saahiluppal/c4b878f330b10b9ab9762bc0776c0a6a"
https://github.com/huggingface/datasets/issues/156,SyntaxError with WMT datasets,"[""Jeez - don't know what happened there :D Should be fixed now! \r\n\r\nThanks a lot for reporting this @tomhosking !""
 ""Hi @patrickvonplaten!\r\n\r\nI'm now getting the below error:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-28-3206959998b9> in <module>\r\n      1 import nlp\r\n      2 \r\n----> 3 dataset = nlp.load_dataset('wmt14')\r\n      4 print(dataset['train'][0])\r\n\r\n~/.local/lib/python3.6/site-packages/nlp/load.py in load_dataset(path, name, version, data_dir, data_files, split, cache_dir, download_config, download_mode, ignore_verifications, save_infos, **config_kwargs)\r\n    507     # Instantiate the dataset builder\r\n    508     builder_instance = builder_cls(\r\n--> 509         cache_dir=cache_dir, name=name, version=version, data_dir=data_dir, data_files=data_files, **config_kwargs,\r\n    510     )\r\n    511 \r\n\r\nTypeError: Can't instantiate abstract class Wmt with abstract methods _subsets\r\n```\r\n\r\n""
 'To correct this error I think you need the master branch of `nlp`. Can you try to install `nlp` with. `WMT` was not included at the beta release of the library. \r\n\r\nCan you try:\r\n`pip install git+https://github.com/huggingface/nlp.git`\r\n\r\nand check again? '
 'That works, thanks :)\r\n\r\nThe WMT datasets are listed in by `list_datasets()` in the beta release on pypi - it would be good to only show datasets that are actually supported by that version?'
 'Usually, the idea is that a dataset can be added without releasing a new version. The problem in the case of `WMT` was that some ""core"" code of the library had to be changed as well. \r\n\r\n@thomwolf @lhoestq @julien-c - How should we go about this. If we add a dataset that also requires ""core"" code changes, how do we handle the versioning? The moment a dataset is on AWS it will actually be listed with `list_datasets()` in all earlier versions...\r\n\r\nIs there a way to somehow insert the `pip version` to the HfApi() and get only the datasets that were available for this version (at the date of the release of the version) @julien-c ? '
 'We plan to have something like a `requirements.txt` per dataset to prevent user from loading dataset with old version of `nlp` or any other libraries. Right now the solution is just to keep `nlp` up to date when you want to load a dataset that leverages the latests features of `nlp`.\r\n\r\nFor datasets that are on AWS but that use features that are not released yet we should be able to filter those from the `list_dataset` as soon as we have the `requirements.txt` feature on (filter datasets that need a future version of `nlp`).\r\n\r\nShall we rename this issue to be more explicit about the problem ?\r\nSomething like `Specify the minimum version of the nlp library required for each dataset` ?'
 'Closing this one.\r\nFeel free to re-open if you have other questions :)']","The following snippet produces a syntax error:

```
import nlp

dataset = nlp.load_dataset('wmt14')
print(dataset['train'][0])
```

```
Traceback (most recent call last):

  File ""/home/tom/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3326, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)

  File ""<ipython-input-8-3206959998b9>"", line 3, in <module>
    dataset = nlp.load_dataset('wmt14')

  File ""/home/tom/.local/lib/python3.6/site-packages/nlp/load.py"", line 505, in load_dataset
    builder_cls = import_main_class(module_path, dataset=True)

  File ""/home/tom/.local/lib/python3.6/site-packages/nlp/load.py"", line 56, in import_main_class
    module = importlib.import_module(module_path)

  File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)

  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import

  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load

  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked

  File ""<frozen importlib._bootstrap>"", line 665, in _load_unlocked

  File ""<frozen importlib._bootstrap_external>"", line 678, in exec_module

  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed

  File ""/home/tom/.local/lib/python3.6/site-packages/nlp/datasets/wmt14/c258d646f4f5870b0245f783b7aa0af85c7117e06aacf1e0340bd81935094de2/wmt14.py"", line 21, in <module>
    from .wmt_utils import Wmt, WmtConfig

  File ""/home/tom/.local/lib/python3.6/site-packages/nlp/datasets/wmt14/c258d646f4f5870b0245f783b7aa0af85c7117e06aacf1e0340bd81935094de2/wmt_utils.py"", line 659
    <<<<<<< HEAD
     ^
SyntaxError: invalid syntax
```

Python version:
`3.6.9 (default, Apr 18 2020, 01:56:04)  [GCC 8.4.0]`
Running on Ubuntu 18.04, via a Jupyter notebook"
https://github.com/huggingface/datasets/issues/153,Meta-datasets (GLUE/XTREME/...) – Special care to attributions and citations,"['As @yoavgo suggested, there should be the possibility to call a function like  nlp.bib that outputs all bibtex ref from the datasets and models actually used and eventually nlp.bib.forreadme that would output the same info + versions numbers so they can be included in a readme.md file.'
 ""Actually, double checking with @mariamabarham, we already have this feature I think.\r\n\r\nIt's like this currently:\r\n```python\r\n>>> from nlp import load_dataset\r\n>>> \r\n>>> dataset = load_dataset('glue', 'cola', split='train')\r\n>>> print(dataset.info.citation)\r\n@article{warstadt2018neural,\r\n  title={Neural Network Acceptability Judgments},\r\n  author={Warstadt, Alex and Singh, Amanpreet and Bowman, Samuel R},\r\n  journal={arXiv preprint arXiv:1805.12471},\r\n  year={2018}\r\n}\r\n@inproceedings{wang2019glue,\r\n  title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\r\n  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},\r\n  note={In the Proceedings of ICLR.},\r\n  year={2019}\r\n}\r\n\r\nNote that each GLUE dataset has its own citation. Please see the source to see\r\nthe correct citation for each contained dataset.\r\n```\r\n\r\nWhat do you think @dseddah?""
 'Looks good but why would there be a difference between the ref in the source and the one to be printed? '
 ""Yes, I think we should remove this warning @mariamabarham.\r\n\r\nIt's probably a relic of tfds which didn't have the same way to access citations. ""]","Meta-datasets are interesting in terms of standardized benchmarks but they also have specific behaviors, in particular in terms of attribution and authorship. It's very important that each specific dataset inside a meta dataset is properly referenced and the citation/specific homepage/etc are very visible and accessible and not only the generic citation of the meta-dataset itself.

Let's take GLUE as an example:

The configuration has the citation for each dataset included (e.g. [here](https://github.com/huggingface/nlp/blob/master/datasets/glue/glue.py#L154-L161)) but it should be copied inside the dataset info so that, when people access `dataset.info.citation` they get both the citation for GLUE and the citation for the specific datasets inside GLUE that they have loaded."
https://github.com/huggingface/datasets/issues/149,[Feature request] Add Ubuntu Dialogue Corpus dataset,['@AlphaMycelium the Ubuntu Dialogue Corpus [version 2]( https://github.com/rkadlec/ubuntu-ranking-dataset-creator) is added. Note that it requires a manual download by following the download instructions in the [repos]( https://github.com/rkadlec/ubuntu-ranking-dataset-creator).\r\nMaybe we can close this issue for now?'],https://github.com/rkadlec/ubuntu-ranking-dataset-creator or http://dataset.cs.mcgill.ca/ubuntu-corpus-1.0/
https://github.com/huggingface/datasets/issues/148,_download_and_prepare() got an unexpected keyword argument 'verify_infos',"[""Same error for dataset 'wiki40b'"" 'Should be fixed on master :)']","# Reproduce
In Colab,
```
%pip install -q  nlp
%pip install -q apache_beam mwparserfromhell

dataset = nlp.load_dataset('wikipedia')
```
get
```
Downloading and preparing dataset wikipedia/20200501.aa (download: Unknown size, generated: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/wikipedia/20200501.aa/1.0.0...

---------------------------------------------------------------------------

TypeError                                 Traceback (most recent call last)

<ipython-input-6-52471d2a0088> in <module>()
----> 1 dataset = nlp.load_dataset('wikipedia')

1 frames

/usr/local/lib/python3.6/dist-packages/nlp/load.py in load_dataset(path, name, version, data_dir, data_files, split, cache_dir, download_config, download_mode, ignore_verifications, save_infos, **config_kwargs)
    515         download_mode=download_mode,
    516         ignore_verifications=ignore_verifications,
--> 517         save_infos=save_infos,
    518     )
    519 

/usr/local/lib/python3.6/dist-packages/nlp/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, save_infos, dl_manager, **download_and_prepare_kwargs)
    361                 verify_infos = not save_infos and not ignore_verifications
    362                 self._download_and_prepare(
--> 363                     dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
    364                 )
    365                 # Sync info

TypeError: _download_and_prepare() got an unexpected keyword argument 'verify_infos'
```"
https://github.com/huggingface/datasets/issues/147,Error with sklearn train_test_split,"['Indeed. Probably we will want to have a similar method directly in the library'
 'Related: #166 ']","It would be nice if we could use sklearn `train_test_split` to quickly generate subsets from the dataset objects returned by `nlp.load_dataset`. At the moment the code:

```python
data = nlp.load_dataset('imdb', cache_dir=data_cache)
f_half, s_half = train_test_split(data['train'], test_size=0.5, random_state=seed)
```
throws:
```
ValueError: Can only get row(s) (int or slice) or columns (string).
```
It's not a big deal, since there are other ways to split the data, but it would be a cool thing to have."
https://github.com/huggingface/datasets/issues/143,ArrowTypeError in squad metrics,"['There was an issue in the format, thanks.\r\nNow you can do\r\n```python3\r\nsquad_dset = nlp.load_dataset(""squad"")\r\nsquad_metric = nlp.load_metric(""/Users/quentinlhoest/Desktop/hf/nlp-bis/metrics/squad"")\r\npredictions = [\r\n    {""id"": v[""id""], ""prediction_text"": v[""answers""][""text""][0]}  # take first possible answer\r\n    for v in squad_dset[""validation""]\r\n]\r\nsquad_metric.compute(predictions, squad_dset[""validation""])\r\n```\r\n\r\nand the expected format is \r\n```\r\nArgs:\r\n    predictions: List of question-answers dictionaries with the following key-values:\r\n        - \'id\': id of the question-answer pair as given in the references (see below)\r\n        - \'prediction_text\': the text of the answer\r\n    references: List of question-answers dictionaries with the following key-values:\r\n        - \'id\': id of the question-answer pair (see above),\r\n        - \'answers\': a Dict {\'text\': list of possible texts for the answer, as a list of strings}\r\n```']","`squad_metric.compute` is giving following error
```
ArrowTypeError: Could not convert [{'text': 'Denver Broncos'}, {'text': 'Denver Broncos'}, {'text': 'Denver Broncos'}] with type list: was not a dict, tuple, or recognized null value for conversion to struct type
```

This is how my predictions and references look like
```
predictions[0]
# {'id': '56be4db0acb8001400a502ec', 'prediction_text': 'Denver Broncos'}
```

```
references[0]
# {'answers': [{'text': 'Denver Broncos'},
  {'text': 'Denver Broncos'},
  {'text': 'Denver Broncos'}],
 'id': '56be4db0acb8001400a502ec'}
```

These are structured as per the `squad_metric.compute` help string."
https://github.com/huggingface/datasets/issues/138,Consider renaming to nld,"['I would suggest `nlds`. NLP is a very general, broad and ambiguous term, the library is not about NLP (as in processing) per se, it is about accessing Natural Language related datasets. So the name should reflect its purpose.\r\n'
 'Chiming in to second everything @honnibal said, and to add that I think the current name is going to impact the discoverability of this library. People who are looking for ""NLP Datasets"" through a search engine are going to see a library called `nlp` and think it\'s too broad. People who are looking to do NLP in python are going to search ""Python NLP"" and end up here, confused that this is a collection of datasets.\r\n\r\nThe names of the other huggingface libraries work because they\'re the only game in town: there are not very many robust, distinct libraries for `tokenizers` or `transformers` in python, for example. But there are several options for NLP in python, and adding this as a possible search result for ""python nlp"" when datasets are likely not what someone is searching for adds noise and frustrates potential users.'
 'I\'m also not sure whether the naming of `nlp` is the problem itself, as long as it comes with the appropriate identifier, so maybe something like `huggingface_nlp`? This is analogous to what @honnibal and spacy are doing for `spacy-transformers`. Of course, this is a ""step back"" from the recent changes/renaming of transformers, but may be some middle ground between a complete rebranding, and keeping it identifiable.'
 'Interesting, thanks for sharing your thoughts.\r\n\r\nAs we’ll move toward a first non-beta release, we will pool the community of contributors/users of the library for their opinions on a good final name (like when we renamed the beautifully (?) named `pytorch-pretrained-bert`)\r\n\r\nIn the meantime, using `from nlp import load_dataset, load_metric` should work 😉'
 ""I feel like we are conflating two distinct subjects here:\r\n\r\n1. @honnibal's point is that using `nlp` as a package name might break existing code and bring developer usability issues in the future\r\n2. @pmbaumgartner's point is that the `nlp` package name is too broad and shouldn't be used by a package that exposes only datasets and metrics\r\n\r\n(let me know if I mischaracterize your point)\r\n\r\nI'll chime in to say that the first point is a bit silly IMO. As Python developers due to the limitations of the import system we already have to share:\r\n- a single flat namespace for packages\r\n- which also conflicts with local modules i.e. local files\r\n\r\nIf we add the constraint that this flat namespace also be shared with variable names this gets untractable pretty fast :)\r\n\r\nI also think all Python software developers/ML engineers/scientists are capable of at least a subset of:\r\n- importing only the methods that they need like @thomwolf suggested\r\n- aliasing their import\r\n- renaming a local variable""
 ""By the way, `nlp` will very likely not be only about datasets, and not even just about datasets and metrics.\r\n\r\nI see it as a laboratory for testing several long-term ideas about how we could do NLP in terms of research as well as open-source and community sharing, most of these ideas being too experimental/big to fit in `transformers`.\r\n\r\nSome of the directions we would like to explore are about sharing, traceability and more experimental models, as well as seeing a model as the community-based process of creating a composite entity from data, optimization, and code.\r\n\r\nWe'll see how these ideas end up being implemented and we'll better know how we should define the library when we start to dive into these topics. I'll try to get the `nlp` team to draft a roadmap on these topics at some point.""
 '> If we add the constraint that this flat namespace also be shared with variable names this gets untractable pretty fast :)\r\n\r\nI\'m sort of confused by your point here. The namespace *is* shared by variable names. You should not use local variables that are named the same as modules, because then you cannot use the module within the scope of your function.\r\n\r\nFor instance,\r\n\r\n```python\r\n\r\nimport nlp\r\nimport transformers\r\n\r\nnlp = transformers.pipeline(""sentiment-analysis"")\r\n```\r\n\r\nThis is a bug: you\'ve just overwritten the module, so now you can\'t use it. Or instead:\r\n\r\n```python\r\n\r\nimport transformers\r\n\r\nnlp = transformers.pipeline(""sentiment-analysis"")\r\n# (Later, e.g. in a notebook)\r\nimport nlp\r\n```\r\n\r\nThis is also a bug: you\'ve overwritten your variable with an import.\r\n\r\nIf you have a module named `nlp`, you should avoid using `nlp` as a variable, or you\'ll have bugs in some contexts and inconsistencies in other contexts. You\'ll have situations where you need to import differently in one module vs another, or name variables differently in one context vs another, which is bad.\r\n\r\n> importing only the methods that they need like @thomwolf suggested\r\n\r\nOkay but the same logic applies to naming the module *literally anything else*. There\'s absolutely no point in having a module name that\'s 3 letters if you always plan to do `import from`! It would be entirely better to name it `nlp_datasets` if you don\'t want people to do `import nlp`.\r\n\r\nAnd finally:\r\n\r\n> By the way, nlp will very likely not be only about datasets, and not even just about datasets and metrics.\r\n\r\nSo...it isn\'t a datasets library? https://twitter.com/Thom_Wolf/status/1261282491622731781\r\n\r\nI\'m confused 😕 '
 'Dropping by as I noticed that the library has been renamed `datasets` so I wonder if the conversation above is settled (`nlp` not used anymore) :) '
 'I guess indeed'
 ""I'd argue that `datasets` is worse than `nlp`. Datasets should be a user specific decision and not encapsulate all of python (`pip install datasets`). If this package contained every dataset in the world (NLP / vision / etc) then it would make sense =/""
 ""I can't speak for the HF team @jramapuram, but as member of the community it looks to me that HF wanted to avoid the past path of changing names as scope broadened over time:\r\n\r\nRemember\r\nhttps://github.com/huggingface/pytorch-openai-transformer-lm\r\nhttps://github.com/huggingface/pytorch-pretrained-BERT\r\nhttps://github.com/huggingface/pytorch-transformers\r\nand now\r\nhttps://github.com/huggingface/transformers\r\n\r\n;) \r\n\r\nJokes aside, seems that the library is growing in a multi-modal direction (https://github.com/huggingface/datasets/pull/363) so the current name is not that implausible. Possibly HF ambition is really to grow its community and bring here a large chunk of datasets of the world (including tabular / vision / audio?).""
 ""Yea I see your point. However, wouldn't scoping solve the entire problem? \r\n\r\n```python\r\nimport huggingface.datasets as D\r\nimport huggingface.transformers as T\r\n```\r\n\r\nCalling something `datasets` is akin to saying I'm going to name my package `python` --> `import python` ""]","Hey :)

Just making a thread here recording what I said on Twitter, as it's impossible to follow discussion there. It's also just really not a good way to talk about this sort of thing.

The issue is that modules go into the global namespace, so you shouldn't use variable names that conflict with module names. This means the package makes `nlp` a bad variable name everywhere in the codebase. I've always used `nlp` as the canonical variable name of spaCy's `Language` objects, and this is a convention that a lot of other code has followed (Stanza, flair, etc). And actually, your `transformers` library uses `nlp` as the name for its `Pipeline` instance in your readme.

If you stick with the `nlp` name for this package, if anyone uses it then they should rewrite all of that code. If `nlp` is a bad choice of variable anywhere, it's a bad choice of variable everywhere --- because you shouldn't have to notice whether some other function uses a module when you're naming variables within a function. You want to have one convention that you can stick to everywhere.

If people use your `nlp` package and continue to use the `nlp` variable name, they'll find themselves with confusing bugs. There will be many many bits of code cut-and-paste from tutorials that give confusing results when combined with the data loading from the `nlp` library. The problem will be especially bad for shadowed modules (people might reasonably have a module named `nlp.py` within their codebase) and notebooks, as people might run notebook cells for data loading out-of-order.

I don't think it's an exaggeration to say that if your library becomes popular, we'll all be answering issues around this about once a week for the next few years. That seems pretty unideal, so I do hope you'll reconsider.

I suggest `nld` as a better name. It more accurately represents what the package actually does. It's pretty unideal to have a package named `nlp` that doesn't do any processing, and contains data about natural language generation or other non-NLP tasks. The name is equally short, and is sort of a visual pun on `nlp`, since a d is a rotated p."
https://github.com/huggingface/datasets/issues/137,Tokenized BLEU considered harmful - Discussion on community-based process,"['I second this request. The bottom line is that **scores produced with different reference tokenizations are not comparable**. To discourage (even inadvertent) cheating, the user should never touch the reference. The `v13a` tokenization standard is not ideal, but at least it has been consistently used at matrix.statmt.org, facilitating comparisons.\r\n\r\nSacrebleu exposes [all its data sources](https://github.com/mjpost/sacrebleu/blob/master/sacrebleu/dataset.py) and additionally provides [an API](https://github.com/mjpost/sacrebleu/blob/master/sacrebleu/__init__.py) to accessing the references, which seem to fit within the spirit of your codebase.'
 ""Didn't we have a slide and discussion at WMT admitting that, for production-quality models, BLEU doesn't correlate with human eval anyway?\r\n""
 ""Yes, there are slides like that at WMT every year :) BLEU correlates with human judgment only at coarse levels, and it seems to be getting worse when people try to use it to do model selection among high-performing neural systems.\r\n\r\nHowever, the point isn't whether BLEU is a good metric, but whether your BLEU score can be compared to other BLEU scores. They only can be compared if you use the same reference tokenization (similar to how you [can't compare LM perplexities across different segmentations](https://sjmielke.com/comparing-perplexities.htm)). sacrebleu was an attempt to get everyone to use WMT's reference tokenization (meaning, your system has to first remove its own tokenization) so that you could just compare across papers. This also prevents scores from being gamed.""
 ""I do not consider as a sufficient solution switching this library's default metric from BLEU to the wrapper around SacreBLEU. \r\n\r\nAs currently implemented, the wrapper allows end users to toggle SacreBLEU options, but doesn't pass along the SacreBLEU signature. As @mjpost showed in [Post18](https://www.aclweb.org/anthology/W18-6319.pdf), it's simply not credible to assume that people will stick to the defaults, therefore, the signature is necessary to be explicit about what options were used. \r\n\r\nIn addition to the `v13a` or `intl` options for the SacreBLEU `tokenize` argument, which was pointed out earlier, papers frequently differ on whether they lowercase text before scoring (`lowercase`) and the smoothing method used (`smooth_method`). BLEU scores can differ substantially (over 1 BLEU) just by changing these options. \r\n\r\nLosing the SacreBLEU signature is a regression in reproducibility and clarity.\r\n\r\n(Perhaps this should belong in a separate issue?)""
 'Thanks for sharing your thoughts. This is a very important discussion.\r\n\r\nAlso one of the first items on our mid-term roadmap (we will try to clean it and share it soon) is to introduce mechanisms to get high-quality traceability and reproducibility for all the processes related to the library.\r\n\r\nSo having the signature for `sacrebleu` is really important!\r\n\r\nRegarding BLEU, I guess we can just remove it from the canonical metrics included in the repo itself (it won\'t prevent people to add it as ""user-metrics"" but at least we won\'t be promoting it).\r\n\r\nOn a more general note (definitely too large for the scope of this issue) we are wondering, with @srush in particular, how we could handle the selection of metrics/datasets with the most community-based and bottom-up approach possible. If you have opinions on this, please share!'
 'Yeah, I would love to have discussions about ways this project can have an community-based, transparent process to arrive at strong default metrics. @kpu / @mjpost do you have any suggestions of how that might work or pointers to places where this is done right? Perhaps this question can be template for what is likely to be repeated for other datasets.'
 ""I think @bittlingmayer is referring to Figure 6 in http://statmt.org/wmt19/pdf/53/WMT02.pdf .  When you look at Appendix A there are some cases where metrics fall apart at the high end and some where they correlate well.  en-zh is arguably production-quality.  \r\n\r\nThis could evolve into a metrics Bazaar where the value add is really the packaging and consistency: it installs/compiles the metrics for me, gives a reproducible name to use in publication (involve the authors; you don't want a different sacrebleu hash system), a version number, and evaluation of the metrics like http://ufallab.ms.mff.cuni.cz/~bojar/wmt19-metrics-task-package.tgz but run when code changes rather than once a year.  ""
 'While a Bazaar setup works for models / datasets, I am not sure it is ideal for metrics ? Ideal from my perspective would be to have tasks with metrics moderated by experts who document, cite, and codify known pitchfalls (as above^) and make it non-trivial for beginners to mess it up. '
 '@srush @thomwolf \r\n\r\nModelFront could provide (automated, ""QE-based"") evaluation for all the pretrained translation models you host.  Not bottom-up and not valid for claiming SoTA, but independent, practical for builders and not top-down.\r\n\r\nFor that I would also suggest some diverse benchmarks (so split it out into datasets with only user-generated data, or only constants, or only UI strings, or only READMEs) which tease out known trade-offs.  Even hypothetical magic eval is limited if we always reduce it to a single number.\r\n\r\nRealistically people want to know how a model compares to an API like Google Translate, Microsoft Translator, DeepL or Yandex (especially for a language pair like EN:RU, or for the many languages that only Yandex supports), and that could be done too.\r\n'
 'Very important discussion.\r\nI am trying to understand the effects of tokenization.\r\nI wanted to ask which is a good practice.\r\nSacrebleu should be used on top of the tokenized output, or detokenized(raw) text?'
 'Use sacrebleu on detokenized output and raw unmodified references.  ']","https://github.com/huggingface/nlp/blob/7d1526dfeeb29248d832f1073192dbf03ad642da/metrics/bleu/bleu.py#L76 assumes the inputs are tokenized by the user.  This is bad practice because the user's tokenizer is usually not the same as the one used by `mteval-v13a.pl`, the closest thing we have to a standard.  Moreover, tokenizers are like window managers: they can be endlessly customized and nobody has quite the same options.  

As @mjpost reported in https://www.aclweb.org/anthology/W18-6319.pdf BLEU configurations can vary by 1.8.  Yet people are incorrectly putting non-comparable BLEU scores in the same table, such as Table 1 in https://arxiv.org/abs/2004.04902 .  

There are a few use cases for tokenized BLEU like Thai.  For Chinese, people seem to use character BLEU for better or worse.

The default easy option should be the one that's correct more often.  And that is sacrebleu.  Please don't make it easy for people to run what is usually the wrong option; it definitely shouldn't be `bleu`.  

Also, I know this is inherited from TensorFlow and, paging @lmthang, they should discourage it too.  "
https://github.com/huggingface/datasets/issues/133,[Question] Using/adding a local dataset,"[""Hi @zphang,\r\n\r\nSo you can just give the local path to a dataset script file and it should work.\r\n\r\nHere is an example:\r\n- you can download one of the scripts in the `datasets` folder of the present repo (or clone the repo)\r\n- then you can load it with `load_dataset('PATH/TO/YOUR/LOCAL/SCRIPT.py')`\r\n\r\nDoes it make sense?""
 'Could you give a more concrete example, please? \r\n\r\nI looked up wikitext dataset script from the repo. Should I just overwrite the `data_file` on line 98 to point to the local dataset directory? Would it work for different configurations of wikitext (wikitext2, wikitext103 etc.)?\r\n\r\nOr maybe we can use DownloadManager to specify local dataset location? In that case, where do we use DownloadManager instance?\r\n\r\nThanks'
 'Hi @MaveriQ , although what I am doing is to commit a new dataset, but I think looking at imdb script might help.\r\nYou may want to use `dl_manager.download_custom`, give it a url(arbitrary string), a custom_download(arbitrary function) and return a path, and finally use _get sample to fetch a sample.'
 'The download manager supports local directories. You can specify a local directory instead of a url and it should work.'
 'Closing this one.\r\nFeel free to re-open if you have other questions :)']","Users may want to either create/modify a local copy of a dataset, or use a custom-built dataset with the same `Dataset` API as externally downloaded datasets.

It appears to be possible to point to a local dataset path rather than downloading the external ones, but I'm not exactly sure how to go about doing this.

A notebook/example script demonstrating this would be very helpful."
https://github.com/huggingface/datasets/issues/132,[Feature Request] Add the OpenWebText dataset,"[""We're experimenting with hosting the OpenWebText corpus on Zenodo for easier downloading. https://zenodo.org/record/3834942#.Xs1w8i-z2J8""
 ""Closing since it's been added in #660 ""]","The OpenWebText dataset is an open clone of OpenAI's WebText dataset. It can be used to train ELECTRA as is specified in the [README](https://www.github.com/google-research/electra).

More information and the download link are available [here](https://skylion007.github.io/OpenWebTextCorpus/)."
https://github.com/huggingface/datasets/issues/131,[Feature request] Add Toronto BookCorpus dataset,"['As far as I understand, `wikitext` is refer to `WikiText-103` and `WikiText-2` that created by researchers in Salesforce, and mostly used in traditional language modeling.\r\n\r\nYou might want to say `wikipedia`, a dump from wikimedia foundation.\r\n\r\nAlso I would like to have Toronto BookCorpus too ! Though it involves copyright problem...'
 'Hi, @lhoestq, just a reminder that this is solved by #248 .😉 ']","I know the copyright/distribution of this one is complex, but it would be great to have! That, combined with the existing `wikitext`, would provide a complete dataset for pretraining models like BERT."
https://github.com/huggingface/datasets/issues/130,Loading GLUE dataset loads CoLA by default,"[""As a follow-up to this: It looks like the actual GLUE task name is supplied as the `name` argument. Is there a way to check what `name`s/sub-datasets are available under a grouping like GLUE? That information doesn't seem to be readily available in info from `nlp.list_datasets()`.\r\n\r\nEdit: I found the info under `Glue.BUILDER_CONFIGS`""
 'Yes so the first config is loaded by default when no `name` is supplied but for GLUE this should probably throw an error indeed.\r\n\r\nWe can probably just add an `__init__` at the top of the `class Glue(nlp.GeneratorBasedBuilder)` in the `glue.py` script which does this check:\r\n```\r\nclass Glue(nlp.GeneratorBasedBuilder):\r\n    def __init__(self, *args, **kwargs):\r\n        assert \'name\' in kwargs and kwargs[name] is not None, ""Glue has to be called with a configuration name""\r\n        super(Glue, self).__init__(*args, **kwargs)\r\n```'
 ""An error is raised if the sub-dataset is not specified :)\r\n```\r\nValueError: Config name is missing.\r\nPlease pick one among the available configs: ['cola', 'sst2', 'mrpc', 'qqp', 'stsb', 'mnli', 'mnli_mismatched', 'mnli_matched', 'qnli', 'rte', 'wnli', 'ax']\r\nExample of usage:\r\n\t`load_dataset('glue', 'cola')`\r\n```""]","If I run:

```python
dataset = nlp.load_dataset('glue')
```
The resultant dataset seems to be CoLA be default, without throwing any error. This is in contrast to calling:

```python
metric = nlp.load_metric(""glue"")
```
which throws an error telling the user that they need to specify a task in GLUE. Should the same apply for loading datasets?"
https://github.com/huggingface/datasets/issues/129,[Feature request] Add Google Natural Question dataset,"['Indeed, I think this one is almost ready cc @lhoestq '
 ""I'm doing the latest adjustments to make the processing of the dataset run on Dataflow""
 'Is there an update to this? It will be very beneficial for the QA community!'
 ""Still work in progress :)\r\nThe idea is to have the dataset already processed somewhere so that the user only have to download the processed files. I'm also doing it for wikipedia.""
 ""Super appreciate your hard work !!\r\nI'll cross my fingers and hope easily loadable wikipedia dataset will come soon. ""
 ""Quick update on NQ: due to some limitations I met using apache beam + parquet I was not able to use the dataset in a nested parquet structure in python to convert it to our Apache Arrow format yet.\r\nHowever we had planned to change this conversion step anyways so we'll make just sure that it enables to process and convert the NQ dataset to arrow.""
 'NQ was added in #427 🎉']",Would be great to have https://github.com/google-research-datasets/natural-questions as an alternative to SQuAD.
https://github.com/huggingface/datasets/issues/128,Some error inside nlp.load_dataset(),"['Google colab has an old version of Apache Arrow built-in.\r\nBe sure you execute the ""pip install"" cell and restart the notebook environment if the colab asks for it.'
 'Thanks for reply, worked fine!\r\n']","First of all, nice work!

I am going through [this overview notebook](https://colab.research.google.com/github/huggingface/nlp/blob/master/notebooks/Overview.ipynb)

In simple step `dataset = nlp.load_dataset('squad', split='validation[:10%]')`

I get an error, which is connected with some inner code, I think:
```
---------------------------------------------------------------------------

TypeError                                 Traceback (most recent call last)

<ipython-input-8-d848d3a99b8c> in <module>()
      1 # Downloading and loading a dataset
      2 
----> 3 dataset = nlp.load_dataset('squad', split='validation[:10%]')

8 frames

/usr/local/lib/python3.6/dist-packages/nlp/load.py in load_dataset(path, name, version, data_dir, data_files, split, cache_dir, download_config, download_mode, ignore_verifications, save_infos, **config_kwargs)
    515         download_mode=download_mode,
    516         ignore_verifications=ignore_verifications,
--> 517         save_infos=save_infos,
    518     )
    519 

/usr/local/lib/python3.6/dist-packages/nlp/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, save_infos, dl_manager, **download_and_prepare_kwargs)
    361                 verify_infos = not save_infos and not ignore_verifications
    362                 self._download_and_prepare(
--> 363                     dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs
    364                 )
    365                 # Sync info

/usr/local/lib/python3.6/dist-packages/nlp/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)
    414             try:
    415                 # Prepare split will record examples associated to the split
--> 416                 self._prepare_split(split_generator, **prepare_split_kwargs)
    417             except OSError:
    418                 raise OSError(""Cannot find data file. "" + (self.MANUAL_DOWNLOAD_INSTRUCTIONS or """"))

/usr/local/lib/python3.6/dist-packages/nlp/builder.py in _prepare_split(self, split_generator)
    585         fname = ""{}-{}.arrow"".format(self.name, split_generator.name)
    586         fpath = os.path.join(self._cache_dir, fname)
--> 587         examples_type = self.info.features.type
    588         writer = ArrowWriter(data_type=examples_type, path=fpath, writer_batch_size=self._writer_batch_size)
    589 

/usr/local/lib/python3.6/dist-packages/nlp/features.py in type(self)
    460     @property
    461     def type(self):
--> 462         return get_nested_type(self)
    463 
    464     @classmethod

/usr/local/lib/python3.6/dist-packages/nlp/features.py in get_nested_type(schema)
    370     # Nested structures: we allow dict, list/tuples, sequences
    371     if isinstance(schema, dict):
--> 372         return pa.struct({key: get_nested_type(value) for key, value in schema.items()})
    373     elif isinstance(schema, (list, tuple)):
    374         assert len(schema) == 1, ""We defining list feature, you should just provide one example of the inner type""

/usr/local/lib/python3.6/dist-packages/nlp/features.py in <dictcomp>(.0)
    370     # Nested structures: we allow dict, list/tuples, sequences
    371     if isinstance(schema, dict):
--> 372         return pa.struct({key: get_nested_type(value) for key, value in schema.items()})
    373     elif isinstance(schema, (list, tuple)):
    374         assert len(schema) == 1, ""We defining list feature, you should just provide one example of the inner type""

/usr/local/lib/python3.6/dist-packages/nlp/features.py in get_nested_type(schema)
    379         # We allow to reverse list of dict => dict of list for compatiblity with tfds
    380         if isinstance(inner_type, pa.StructType):
--> 381             return pa.struct(dict((f.name, pa.list_(f.type, schema.length)) for f in inner_type))
    382         return pa.list_(inner_type, schema.length)
    383 

/usr/local/lib/python3.6/dist-packages/nlp/features.py in <genexpr>(.0)
    379         # We allow to reverse list of dict => dict of list for compatiblity with tfds
    380         if isinstance(inner_type, pa.StructType):
--> 381             return pa.struct(dict((f.name, pa.list_(f.type, schema.length)) for f in inner_type))
    382         return pa.list_(inner_type, schema.length)
    383 

TypeError: list_() takes exactly one argument (2 given)
```"
https://github.com/huggingface/datasets/issues/120,🐛 `map` not working,"[""I didn't assign the output 🤦\u200d♂️\r\n\r\n```python\r\ndataset.map(test)\r\n```\r\n\r\nshould be :\r\n\r\n```python\r\ndataset = dataset.map(test)\r\n```""]","I'm trying to run a basic example (mapping function to add a prefix).  
[Here is the colab notebook I'm using.](https://colab.research.google.com/drive/1YH4JCAy0R1MMSc-k_Vlik_s1LEzP_t1h?usp=sharing)

```python
import nlp

dataset = nlp.load_dataset('squad', split='validation[:10%]')

def test(sample):
    sample['title'] = ""test prefix @@@ "" + sample[""title""]
    return sample

print(dataset[0]['title'])
dataset.map(test)
print(dataset[0]['title'])
```
Output :
> Super_Bowl_50
Super_Bowl_50

Expected output :
> Super_Bowl_50
test prefix @@@ Super_Bowl_50"
https://github.com/huggingface/datasets/issues/119,🐛 Colab : type object 'pyarrow.lib.RecordBatch' has no attribute 'from_struct_array',"['It\'s strange, after installing `nlp` on Colab, the `pyarrow` version seems fine from `pip` but not from python :\r\n\r\n```python\r\nimport pyarrow\r\n\r\n!pip show pyarrow\r\nprint(""version = {}"".format(pyarrow.__version__))\r\n```\r\n\r\n> Name: pyarrow\r\nVersion: 0.17.0\r\nSummary: Python library for Apache Arrow\r\nHome-page: https://arrow.apache.org/\r\nAuthor: None\r\nAuthor-email: None\r\nLicense: Apache License, Version 2.0\r\nLocation: /usr/local/lib/python3.6/dist-packages\r\nRequires: numpy\r\nRequired-by: nlp, feather-format\r\n> \r\n> version = 0.14.1'
 'Ok I just had to restart the runtime after installing `nlp`. After restarting, the version of `pyarrow` is fine.']","I'm trying to load CNN/DM dataset on Colab.

[Colab notebook](https://colab.research.google.com/drive/11Mf7iNhIyt6GpgA1dBEtg3cyMHmMhtZS?usp=sharing)

But I meet this error :

> AttributeError: type object 'pyarrow.lib.RecordBatch' has no attribute 'from_struct_array'
"
https://github.com/huggingface/datasets/issues/118,❓ How to apply a map to all subsets ?,"[""That's the way!""]","I'm working with CNN/DM dataset, where I have 3 subsets : `train`, `test`, `validation`.

Should I apply my map function on the subsets one by one ?

```python
import nlp

cnn_dm = nlp.load_dataset('cnn_dailymail')
for corpus in ['train', 'test', 'validation']:
         cnn_dm[corpus] = cnn_dm[corpus].map(my_func)
```

Or is there a better way to do this ?"
https://github.com/huggingface/datasets/issues/117,❓ How to remove specific rows of a dataset ?,"[""Hi, you can't do that at the moment.""]","I saw on the [example notebook](https://colab.research.google.com/github/huggingface/nlp/blob/master/notebooks/Overview.ipynb#scrollTo=efFhDWhlvSVC) how to remove a specific column :

```python
dataset.drop('id')
```

But I didn't find how to remove a specific row. 

**For example, how can I remove all sample with `id` < 10 ?**"
https://github.com/huggingface/datasets/issues/116,🐛 Trying to use ROUGE metric : pyarrow.lib.ArrowInvalid: Column 1 named references expected length 534 but got length 323,"['Can you share your data files or a minimally reproducible example?'
 'Sure, [here is a Colab notebook](https://colab.research.google.com/drive/1uiS89fnHMG7HV_cYxp3r-_LqJQvNNKs9?usp=sharing) reproducing the error.\r\n\r\n> ArrowInvalid: Column 1 named references expected length 36 but got length 56'
 'This is because `add` takes as input a batch of elements and you provided only one. I think we should have `add` for one prediction/reference and `add_batch` for a batch of predictions/references. This would make it more coherent with the way we use Arrow.\r\n\r\nLet me do this change'
 'Thanks for noticing though. I was mainly used to do `.compute` directly ^^'
 'Thanks @lhoestq it works :)']","I'm trying to use rouge metric.

I have to files : `test.pred.tokenized` and `test.gold.tokenized` with each line containing a sentence.  
I tried :

```python
import nlp

rouge = nlp.load_metric('rouge')
with open(""test.pred.tokenized"") as p, open(""test.gold.tokenized"") as g:
    for lp, lg in zip(p, g):
            rouge.add(lp, lg)
```

But I meet following error :

> pyarrow.lib.ArrowInvalid: Column 1 named references expected length 534 but got length 323

---

Full stack-trace :

```
Traceback (most recent call last):
  File ""<stdin>"", line 3, in <module>
  File ""/home/me/.venv/transformers/lib/python3.6/site-packages/nlp/metric.py"", line 224, in add
    self.writer.write_batch(batch)
  File ""/home/me/.venv/transformers/lib/python3.6/site-packages/nlp/arrow_writer.py"", line 148, in write_batch
    pa_table: pa.Table = pa.Table.from_pydict(batch_examples, schema=self._schema)
  File ""pyarrow/table.pxi"", line 1550, in pyarrow.lib.Table.from_pydict
  File ""pyarrow/table.pxi"", line 1503, in pyarrow.lib.Table.from_arrays
  File ""pyarrow/public-api.pxi"", line 390, in pyarrow.lib.pyarrow_wrap_table
  File ""pyarrow/error.pxi"", line 85, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: Column 1 named references expected length 534 but got length 323
```

(`nlp` installed from source)"
https://github.com/huggingface/datasets/issues/115,AttributeError: 'dict' object has no attribute 'info',"['I could access the info by first accessing the different splits :\r\n\r\n```python\r\nimport nlp\r\n\r\ncnn_dm = nlp.load_dataset(\'cnn_dailymail\')\r\nprint(cnn_dm[\'train\'].info)\r\n```\r\n\r\nInformation seems to be duplicated between the subsets :\r\n\r\n```python\r\nprint(cnn_dm[""train""].info == cnn_dm[""test""].info == cnn_dm[""validation""].info)\r\n# True\r\n```\r\n\r\nIs it expected ?'
 'Good point @Colanim ! What happens under the hood when running:\r\n\r\n```python\r\nimport nlp\r\n\r\ncnn_dm = nlp.load_dataset(\'cnn_dailymail\')\r\n```\r\n\r\nis that for every split in `cnn_dailymail`, a different dataset object (which all holds the same info) is created. This has the advantages that the datasets are easily separable in a training setup. \r\nAlso note that you can load e.g. only the `train` split of the dataset via:\r\n\r\n```python\r\ncnn_dm_train = nlp.load_dataset(\'cnn_dailymail\', split=""train"")\r\nprint(cnn_dm_train.info)\r\n```\r\n\r\nI think we should make the `info` object slightly different when creating the dataset for each split - at the moment it contains for example the variable `splits` which should maybe be renamed to `split` and contain only one `SplitInfo` object ...\r\n']","I'm trying to access the information of CNN/DM dataset :

```python
cnn_dm = nlp.load_dataset('cnn_dailymail')
print(cnn_dm.info)
```

returns :

> AttributeError: 'dict' object has no attribute 'info'"
https://github.com/huggingface/datasets/issues/114,Couldn't reach CNN/DM dataset,['Installing from source (instead of Pypi package) solved the problem.'],"I can't get CNN / DailyMail dataset.

```python
import nlp

assert ""cnn_dailymail"" in [dataset.id for dataset in nlp.list_datasets()]
cnn_dm = nlp.load_dataset('cnn_dailymail')
```

[Colab notebook](https://colab.research.google.com/drive/1zQ3bYAVzm1h0mw0yWPqKAg_4EUlSx5Ex?usp=sharing)

gives following error :

```
ConnectionError: Couldn't reach https://s3.amazonaws.com/datasets.huggingface.co/nlp/cnn_dailymail/cnn_dailymail.py
```"
https://github.com/huggingface/datasets/issues/38,[Checksums] Error for some datasets,"[""@lhoestq - could you take a look? It's not very urgent though!""
 'Fixed with 06882b4\r\n\r\nNow your command works :)\r\nNote that you can also do\r\n```\r\nnlp-cli test datasets/nlp/xnli --save_checksums\r\n```\r\nSo that it will save the checksums directly in the right directory.'
 'Awesome!']","The checksums command works very nicely for `squad`. But for `crime_and_punish` and `xnli`, 
the same bug happens:

When running: 
```
python nlp-cli nlp-cli test xnli --save_checksums
```

leads to:

```
  File ""nlp-cli"", line 33, in <module>
    service.run()
  File ""/home/patrick/python_bin/nlp/commands/test.py"", line 61, in run
    ignore_checksums=self._ignore_checksums,
  File ""/home/patrick/python_bin/nlp/builder.py"", line 383, in download_and_prepare
    self._download_and_prepare(dl_manager=dl_manager, download_config=download_config)
  File ""/home/patrick/python_bin/nlp/builder.py"", line 627, in _download_and_prepare
    dl_manager=dl_manager, max_examples_per_split=download_config.max_examples_per_split,
  File ""/home/patrick/python_bin/nlp/builder.py"", line 431, in _download_and_prepare
    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)
  File ""/home/patrick/python_bin/nlp/datasets/xnli/8bf4185a2da1ef2a523186dd660d9adcf0946189e7fa5942ea31c63c07b68a7f/xnli.py"", line 95, in _split_generators
    dl_dir = dl_manager.download_and_extract(_DATA_URL)
  File ""/home/patrick/python_bin/nlp/utils/download_manager.py"", line 246, in download_and_extract
    return self.extract(self.download(url_or_urls))
  File ""/home/patrick/python_bin/nlp/utils/download_manager.py"", line 186, in download
    self._record_sizes_checksums(url_or_urls, downloaded_path_or_paths)
  File ""/home/patrick/python_bin/nlp/utils/download_manager.py"", line 166, in _record_sizes_checksums
    self._recorded_sizes_checksums[url] = get_size_checksum(path)
  File ""/home/patrick/python_bin/nlp/utils/checksums_utils.py"", line 81, in get_size_checksum
    with open(path, ""rb"") as f:
TypeError: expected str, bytes or os.PathLike object, not tuple
```
"
https://github.com/huggingface/datasets/issues/6,Error when citation is not given in the DatasetInfo,"[""Yes looks good to me.\r\nNote that we may refactor quite strongly the `info.py` to make it a lot simpler (it's very complicated for basically a dictionary of info I think)""
 'No, problem ^^ It might just be a temporary fix :)' 'Fixed.']","The following error is raised when the `citation` parameter is missing when we instantiate a `DatasetInfo`:
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/jplu/dev/jplu/datasets/src/nlp/info.py"", line 338, in __repr__
    citation_pprint = _indent('""""""{}""""""'.format(self.citation.strip()))
AttributeError: 'NoneType' object has no attribute 'strip'
```

I propose to do the following change in the `info.py` file. The method:
```python
def __repr__(self):
        splits_pprint = _indent(""\n"".join([""{""] + [
                ""    '{}': {},"".format(k, split.num_examples)
                for k, split in sorted(self.splits.items())
        ] + [""}""]))
        features_pprint = _indent(repr(self.features))
        citation_pprint = _indent('""""""{}""""""'.format(self.citation.strip()))
        return INFO_STR.format(
                name=self.name,
                version=self.version,
                description=self.description,
                total_num_examples=self.splits.total_num_examples,
                features=features_pprint,
                splits=splits_pprint,
                citation=citation_pprint,
                homepage=self.homepage,
                supervised_keys=self.supervised_keys,
                # Proto add a \n that we strip.
                license=str(self.license).strip())
```
Becomes:
```python
def __repr__(self):
        splits_pprint = _indent(""\n"".join([""{""] + [
                ""    '{}': {},"".format(k, split.num_examples)
                for k, split in sorted(self.splits.items())
        ] + [""}""]))
        features_pprint = _indent(repr(self.features))
        ## the strip is done only is the citation is given
        citation_pprint = self.citation

        if self.citation:
            citation_pprint = _indent('""""""{}""""""'.format(self.citation.strip()))
        return INFO_STR.format(
                name=self.name,
                version=self.version,
                description=self.description,
                total_num_examples=self.splits.total_num_examples,
                features=features_pprint,
                splits=splits_pprint,
                citation=citation_pprint,
                homepage=self.homepage,
                supervised_keys=self.supervised_keys,
                # Proto add a \n that we strip.
                license=str(self.license).strip())
```
And now it is ok. @thomwolf are you ok with this fix?"
https://github.com/huggingface/datasets/issues/5,ValueError when a split is empty,"['To fix this I propose to modify only the file `arrow_reader.py` with few updates. First update, the following method:\r\n```python\r\ndef _make_file_instructions_from_absolutes(\r\n        name,\r\n        name2len,\r\n        absolute_instructions,\r\n):\r\n    """"""Returns the files instructions from the absolute instructions list.""""""\r\n    # For each split, return the files instruction (skip/take)\r\n    file_instructions = []\r\n    num_examples = 0\r\n    for abs_instr in absolute_instructions:\r\n        length = name2len[abs_instr.splitname]\r\n        if not length:\r\n            raise ValueError(\r\n                    \'Split empty. This might means that dataset hasn\\\'t been generated \'\r\n                    \'yet and info not restored from GCS, or that legacy dataset is used.\')\r\n        filename = filename_for_dataset_split(\r\n                dataset_name=name,\r\n                split=abs_instr.splitname,\r\n                filetype_suffix=\'arrow\')\r\n        from_ = 0 if abs_instr.from_ is None else abs_instr.from_\r\n        to = length if abs_instr.to is None else abs_instr.to\r\n        num_examples += to - from_\r\n        single_file_instructions = [{""filename"": filename, ""skip"": from_, ""take"": to - from_}]\r\n        file_instructions.extend(single_file_instructions)\r\n    return FileInstructions(\r\n            num_examples=num_examples,\r\n            file_instructions=file_instructions,\r\n    )\r\n```\r\nBecomes:\r\n```python\r\ndef _make_file_instructions_from_absolutes(\r\n        name,\r\n        name2len,\r\n        absolute_instructions,\r\n):\r\n    """"""Returns the files instructions from the absolute instructions list.""""""\r\n    # For each split, return the files instruction (skip/take)\r\n    file_instructions = []\r\n    num_examples = 0\r\n    for abs_instr in absolute_instructions:\r\n        length = name2len[abs_instr.splitname]\r\n        ## Delete the if not length and the raise\r\n        filename = filename_for_dataset_split(\r\n                dataset_name=name,\r\n                split=abs_instr.splitname,\r\n                filetype_suffix=\'arrow\')\r\n        from_ = 0 if abs_instr.from_ is None else abs_instr.from_\r\n        to = length if abs_instr.to is None else abs_instr.to\r\n        num_examples += to - from_\r\n        single_file_instructions = [{""filename"": filename, ""skip"": from_, ""take"": to - from_}]\r\n        file_instructions.extend(single_file_instructions)\r\n    return FileInstructions(\r\n            num_examples=num_examples,\r\n            file_instructions=file_instructions,\r\n    )\r\n```\r\n\r\nSecond update the following method:\r\n```python\r\ndef _read_files(files, info):\r\n    """"""Returns Dataset for given file instructions.\r\n\r\n    Args:\r\n        files: List[dict(filename, skip, take)], the files information.\r\n            The filenames contain the absolute path, not relative.\r\n            skip/take indicates which example read in the file: `ds.slice(skip, take)`\r\n    """"""\r\n    pa_batches = []\r\n    for f_dict in files:\r\n        pa_table: pa.Table = _get_dataset_from_filename(f_dict)\r\n        pa_batches.extend(pa_table.to_batches())\r\n    pa_table = pa.Table.from_batches(pa_batches)\r\n    ds = Dataset(arrow_table=pa_table, data_files=files, info=info)\r\n    return ds\r\n```\r\nBecomes:\r\n```python\r\ndef _read_files(files, info):\r\n    """"""Returns Dataset for given file instructions.\r\n\r\n    Args:\r\n        files: List[dict(filename, skip, take)], the files information.\r\n            The filenames contain the absolute path, not relative.\r\n            skip/take indicates which example read in the file: `ds.slice(skip, take)`\r\n    """"""\r\n    pa_batches = []\r\n    for f_dict in files:\r\n        pa_table: pa.Table = _get_dataset_from_filename(f_dict)\r\n        pa_batches.extend(pa_table.to_batches())\r\n    ## we modify the table only if there are some batches\r\n    if pa_batches:\r\n        pa_table = pa.Table.from_batches(pa_batches)\r\n    ds = Dataset(arrow_table=pa_table, data_files=files, info=info)\r\n    return ds\r\n```\r\n\r\nWith these two updates it works now. @thomwolf are you ok with this changes?'
 'Yes sounds good to me!\r\nDo you want to make a PR? or I can do it as well'
 'Fixed.']","When a split is empty either TEST, VALIDATION or TRAIN I get the following error:
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/jplu/dev/jplu/datasets/src/nlp/load.py"", line 295, in load
    ds = dbuilder.as_dataset(**as_dataset_kwargs)
  File ""/home/jplu/dev/jplu/datasets/src/nlp/builder.py"", line 587, in as_dataset
    datasets = utils.map_nested(build_single_dataset, split, map_tuple=True)
  File ""/home/jplu/dev/jplu/datasets/src/nlp/utils/py_utils.py"", line 158, in map_nested
    for k, v in data_struct.items()
  File ""/home/jplu/dev/jplu/datasets/src/nlp/utils/py_utils.py"", line 158, in <dictcomp>
    for k, v in data_struct.items()
  File ""/home/jplu/dev/jplu/datasets/src/nlp/utils/py_utils.py"", line 172, in map_nested
    return function(data_struct)
  File ""/home/jplu/dev/jplu/datasets/src/nlp/builder.py"", line 601, in _build_single_dataset
    split=split,
  File ""/home/jplu/dev/jplu/datasets/src/nlp/builder.py"", line 625, in _as_dataset
    split_infos=self.info.splits.values(),
  File ""/home/jplu/dev/jplu/datasets/src/nlp/arrow_reader.py"", line 200, in read
    return py_utils.map_nested(_read_instruction_to_ds, instructions)
  File ""/home/jplu/dev/jplu/datasets/src/nlp/utils/py_utils.py"", line 172, in map_nested
    return function(data_struct)
  File ""/home/jplu/dev/jplu/datasets/src/nlp/arrow_reader.py"", line 191, in _read_instruction_to_ds
    file_instructions = make_file_instructions(name, split_infos, instruction)
  File ""/home/jplu/dev/jplu/datasets/src/nlp/arrow_reader.py"", line 104, in make_file_instructions
    absolute_instructions=absolute_instructions,
  File ""/home/jplu/dev/jplu/datasets/src/nlp/arrow_reader.py"", line 122, in _make_file_instructions_from_absolutes
    'Split empty. This might means that dataset hasn\'t been generated '
ValueError: Split empty. This might means that dataset hasn't been generated yet and info not restored from GCS, or that legacy dataset is used.
``` 

How to reproduce:
```python
import csv

import nlp


class Bbc(nlp.GeneratorBasedBuilder):
    VERSION = nlp.Version(""1.0.0"")

    def __init__(self, **config):
        self.train = config.pop(""train"", None)
        self.validation = config.pop(""validation"", None)
        super(Bbc, self).__init__(**config)

    def _info(self):
        return nlp.DatasetInfo(builder=self, description=""bla"", features=nlp.features.FeaturesDict({""id"": nlp.int32, ""text"": nlp.string, ""label"": nlp.string}))

    def _split_generators(self, dl_manager):
        return [nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={""filepath"": self.train}),
                nlp.SplitGenerator(name=nlp.Split.VALIDATION, gen_kwargs={""filepath"": self.validation}),
                nlp.SplitGenerator(name=nlp.Split.TEST, gen_kwargs={""filepath"": None})]

    def _generate_examples(self, filepath):
        if not filepath:
            return None, {}

        with open(filepath) as f:
            reader = csv.reader(f, delimiter=',', quotechar=""\"""")
            lines = list(reader)[1:]

            for idx, line in enumerate(lines):
                yield idx, {""id"": idx, ""text"": line[1], ""label"": line[0]}
```

```python
import nlp
dataset = nlp.load(""bbc"", builder_kwargs={""train"": ""bbc/data/train.csv"", ""validation"": ""bbc/data/test.csv""})
```"
https://github.com/huggingface/datasets/issues/4,[Feature] Keep the list of labels of a dataset as metadata,"['Yes! I see mostly two options for this:\r\n- a `Feature` approach like currently (but we might deprecate features)\r\n- wrapping in a smart way the Dictionary arrays of Arrow: https://arrow.apache.org/docs/python/data.html?highlight=dictionary%20encode#dictionary-arrays'
 'I would have a preference for the second bullet point.'
 'This should be accessible now as a feature in dataset.info.features (and even have the mapping methods).'
 'Perfect! Well done!!'
 'Hi,\r\nI hope we could get a better documentation.\r\nIt took me more than 1 hour to found this way to get the label information.'
 'Yes we are working on the doc right now, should be in the next release quite soon.']",It would be useful to keep the list of the labels of a dataset as metadata. Either directly in the `DatasetInfo` or in the Arrow metadata.
https://github.com/huggingface/datasets/issues/3,[Feature] More dataset outputs,"[""Yes!\r\n- pandas will be a one-liner in `arrow_dataset`: https://arrow.apache.org/docs/python/generated/pyarrow.Table.html#pyarrow.Table.to_pandas\r\n- for Spark I have no idea. let's investigate that at some point""
 'For Spark it looks to be pretty straightforward as well https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html but looks to be having a dependency to Spark is necessary, then nevermind we can skip it'
 'Now Pandas is available.']","Add the following dataset outputs:

- Spark
- Pandas"
https://github.com/huggingface/datasets/issues/2,Issue to read a local dataset,"['My first bug report ❤️\r\nLooking into this right now!'
 'Ok, there are some news, most good than bad :laughing: \r\n\r\nThe dataset script now became:\r\n```python\r\nimport csv\r\n\r\nimport nlp\r\n\r\n\r\nclass Bbc(nlp.GeneratorBasedBuilder):\r\n    VERSION = nlp.Version(""1.0.0"")\r\n\r\n    def __init__(self, **config):\r\n        self.train = config.pop(""train"", None)\r\n        self.validation = config.pop(""validation"", None)\r\n        super(Bbc, self).__init__(**config)\r\n\r\n    def _info(self):\r\n        return nlp.DatasetInfo(builder=self, description=""bla"", features=nlp.features.FeaturesDict({""id"": nlp.int32, ""text"": nlp.string, ""label"": nlp.string}))\r\n\r\n    def _split_generators(self, dl_manager):\r\n        return [nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={""filepath"": self.train}),\r\n                nlp.SplitGenerator(name=nlp.Split.VALIDATION, gen_kwargs={""filepath"": self.validation})]\r\n\r\n    def _generate_examples(self, filepath):\r\n        with open(filepath) as f:\r\n            reader = csv.reader(f, delimiter=\',\', quotechar=""\\"""")\r\n            lines = list(reader)[1:]\r\n\r\n            for idx, line in enumerate(lines):\r\n                yield idx, {""id"": idx, ""text"": line[1], ""label"": line[0]}\r\n\r\n```\r\n\r\nAnd the dataset folder becomes:\r\n```\r\n.\r\n├── bbc\r\n│   ├── bbc.py\r\n│   └── data\r\n│       ├── test.csv\r\n│       └── train.csv\r\n```\r\nI can load the dataset by using the keywords arguments like this:\r\n```python\r\nimport nlp\r\ndataset = nlp.load(""bbc"", builder_kwargs={""train"": ""bbc/data/train.csv"", ""validation"": ""bbc/data/test.csv""})\r\n```\r\n\r\nThat was the good part ^^ Because it took me some time to understand that the script itself is put in cache in `datasets/src/nlp/datasets/some-hash/bbc.py` which is very difficult to discover without checking the source code. It means that doesn\'t matter the changes you do to your original script it is taken into account. I think instead of doing a hash on the name (I suppose it is the name), a hash on the content of the script itself should be a better solution.\r\n\r\nThen by diving a bit in the code I found the `force_reload` parameter [here](https://github.com/huggingface/datasets/blob/master/src/nlp/load.py#L50) but the call of this `load_dataset` method is done with the `builder_kwargs` as seen [here](https://github.com/huggingface/datasets/blob/master/src/nlp/load.py#L166) which is ok until the call to the builder is done as the builder do not have this `force_reload` parameter. To show as example, the previous load becomes:\r\n```python\r\nimport nlp\r\ndataset = nlp.load(""bbc"", builder_kwargs={""train"": ""bbc/data/train.csv"", ""validation"": ""bbc/data/test.csv"", ""force_reload"": True})\r\n```\r\nRaises\r\n```\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/jplu/dev/jplu/datasets/src/nlp/load.py"", line 283, in load\r\n    dbuilder: DatasetBuilder = builder(path, name, data_dir=data_dir, **builder_kwargs)\r\n  File ""/home/jplu/dev/jplu/datasets/src/nlp/load.py"", line 170, in builder\r\n    builder_instance = builder_cls(**builder_kwargs)\r\n  File ""/home/jplu/dev/jplu/datasets/src/nlp/datasets/84d638d2a8ca919d1021a554e741766f50679dc6553d5a0612b6094311babd39/bbc.py"", line 12, in __init__\r\n    super(Bbc, self).__init__(**config)\r\nTypeError: __init__() got an unexpected keyword argument \'force_reload\'\r\n```\r\nSo yes the cache is refreshed with the new script but then raises this error.'
 ""Ok great, so as discussed today, let's:\r\n- have a main dataset directory inside the lib with sub-directories hashed by the content of the file\r\n- keep a cache for downloading the scripts from S3 for now\r\n- later: add methods to list and clean the local versions of the datasets (and the distant versions on S3 as well)\r\n\r\nSide question: do you often use `builder_kwargs` for other things than supplying file paths? I was thinking about having a more easy to read and remember `data_files` argument maybe.""
 'Good plan!\r\n\r\nYes I do use `builder_kwargs` for other things such as:\r\n- dataset name\r\n- properties to know how to properly read a CSV file: do I have to skip the first line in a CSV, which delimiter is used, and the columns ids to use.\r\n- properties to know how to properly read a JSON file: which properties in a JSON object to read'
 'Done!']","Hello,

As proposed by @thomwolf, I open an issue to explain what I'm trying to do without success. What I want to do is to create and load a local dataset, the script I have done is the following:
```python
import os
import csv

import nlp


class BbcConfig(nlp.BuilderConfig):
    def __init__(self, **kwargs):
        super(BbcConfig, self).__init__(**kwargs)


class Bbc(nlp.GeneratorBasedBuilder):
    _DIR = ""./data""
    _DEV_FILE = ""test.csv""
    _TRAINING_FILE = ""train.csv""

    BUILDER_CONFIGS = [BbcConfig(name=""bbc"", version=nlp.Version(""1.0.0""))]

    def _info(self):
        return nlp.DatasetInfo(builder=self, features=nlp.features.FeaturesDict({""id"": nlp.string, ""text"": nlp.string, ""label"": nlp.string}))

    def _split_generators(self, dl_manager):
        files = {""train"": os.path.join(self._DIR, self._TRAINING_FILE), ""dev"": os.path.join(self._DIR, self._DEV_FILE)}

        return [nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={""filepath"": files[""train""]}),
                nlp.SplitGenerator(name=nlp.Split.VALIDATION, gen_kwargs={""filepath"": files[""dev""]})]

    def _generate_examples(self, filepath):
        with open(filepath) as f:
            reader = csv.reader(f, delimiter=',', quotechar=""\"""")
            lines = list(reader)[1:]

            for idx, line in enumerate(lines):
                yield idx, {""idx"": idx, ""text"": line[1], ""label"": line[0]}

```

The dataset is attached to this issue as well:
[data.zip](https://github.com/huggingface/datasets/files/4476928/data.zip)

Now the steps to reproduce what I would like to do:
1. unzip data locally (I know the nlp lib can detect and extract archives but I want to reduce and facilitate the reproduction as much as possible)
2. create the `bbc.py` script as above at the same location than the unziped `data` folder.

Now I try to load the dataset in three different ways and none works, the first one with the name of the dataset like I would do with TFDS:
```python
import nlp
from bbc import Bbc
dataset = nlp.load(""bbc"")
```

I get:
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/opt/anaconda3/envs/transformers/lib/python3.7/site-packages/nlp/load.py"", line 280, in load
    dbuilder: DatasetBuilder = builder(path, name, data_dir=data_dir, **builder_kwargs)
  File ""/opt/anaconda3/envs/transformers/lib/python3.7/site-packages/nlp/load.py"", line 166, in builder
    builder_cls = load_dataset(path, name=name, **builder_kwargs)
  File ""/opt/anaconda3/envs/transformers/lib/python3.7/site-packages/nlp/load.py"", line 88, in load_dataset
    local_files_only=local_files_only,
  File ""/opt/anaconda3/envs/transformers/lib/python3.7/site-packages/nlp/utils/file_utils.py"", line 214, in cached_path
    if not is_zipfile(output_path) and not tarfile.is_tarfile(output_path):
  File ""/opt/anaconda3/envs/transformers/lib/python3.7/zipfile.py"", line 203, in is_zipfile
    with open(filename, ""rb"") as fp:
TypeError: expected str, bytes or os.PathLike object, not NoneType
```

But @thomwolf told me that no need to import the script, just put the path of it, then I tried three different way to do:
```python
import nlp
dataset = nlp.load(""bbc.py"")
```
And
```python
import nlp
dataset = nlp.load(""./bbc.py"")
```
And
```python
import nlp
dataset = nlp.load(""/absolute/path/to/bbc.py"")
```

These three ways gives me:
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/opt/anaconda3/envs/transformers/lib/python3.7/site-packages/nlp/load.py"", line 280, in load
    dbuilder: DatasetBuilder = builder(path, name, data_dir=data_dir, **builder_kwargs)
  File ""/opt/anaconda3/envs/transformers/lib/python3.7/site-packages/nlp/load.py"", line 166, in builder
    builder_cls = load_dataset(path, name=name, **builder_kwargs)
  File ""/opt/anaconda3/envs/transformers/lib/python3.7/site-packages/nlp/load.py"", line 124, in load_dataset
    dataset_module = importlib.import_module(module_path)
  File ""/opt/anaconda3/envs/transformers/lib/python3.7/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 965, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'nlp.datasets.2fd72627d92c328b3e9c4a3bf7ec932c48083caca09230cebe4c618da6e93688.bbc'
```
Any idea of what I'm missing? or I might have spot a bug :)"
