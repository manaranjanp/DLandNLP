{"cells":[{"cell_type":"markdown","metadata":{"id":"Xws2p6SgbNF-"},"source":["# IMDB Classification - Bag of Words and Embeddings\n","\n","This tutorial will go through steps for building a deep learning model for sentiment Analysis. We will classify IMDB movie reviews as either positive or negative. This tutorial will be used for teaching during the workshop.\n","\n","The tutorial has taken contents from various places including the tutorial from http://www.hvass-labs.org/ for the purpose of teaching in the deep learning class.\n","\n","The topics addressed in the tutorial:\n","\n","1. Basic exploration of the IMDB movies dataset.\n","2. Tokenization, text to sequences, padding and truncating\n","3. Building NN Model using Bag Of Words\n","4. Building NN Model using Embeddings\n","5. Peeping to Word Embeddings\n","\n","We will be exploring mostly how to use Bag of Words and Word Embeddings vector representation of texts and build plain vanila NN models. In the future tutorials, we will explore RNN, LSTM models in the future."]},{"cell_type":"markdown","metadata":{"id":"ZWhTaM5CbNGF"},"source":["### IMDB Movie Reviews\n","\n","The dataset is available at https://www.kaggle.com/c/word2vec-nlp-tutorial/data\n","\n","The labeled data set consists of 50,000 IMDB movie reviews, specially selected for sentiment analysis. The sentiment of reviews is binary, meaning the IMDB rating < 5 results in a sentiment score of 0, and rating >=7 have a sentiment score of 1. No individual movie has more than 30 reviews.\n","\n","**Data Fields**\n","\n","- id - Unique ID of each review\n","- sentiment - Sentiment of the review; 1 for positive reviews and 0 for negative reviews\n","- review - Text of the review"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"tp8JceL7bTjJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hb91-UQ8bNGG"},"source":["### Loading the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cp3EZNi4bNGH"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RLE_SJJobNGJ"},"outputs":[],"source":["imdb_df = pd.read_csv('/content/drive/MyDrive/AdvancedML/labeledTrainData.tsv', sep = '\\t')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VGRs67fWbNGJ"},"outputs":[],"source":["pd.set_option('display.max_colwidth', 500)\n","imdb_df.head(5)"]},{"cell_type":"markdown","metadata":{"id":"uFXFNPyabNGL"},"source":["### Data Tokenization"]},{"cell_type":"markdown","metadata":{"id":"sChuq5RdbNGM"},"source":["The text data need to be converted into vectors using either bag of words or embeddings model. We will first explore bag of words (BOW) model. In the BOW model, a sentence will be represented as a vector with the words (also called tokens) as dimensions of the vectors. \n","\n","For the purpose of creating vectors, we need to tokenize the sentences first and find out all unique tokens (words) used across all sentences. The corpus of unquie words used could very large, so we can limit the corpus of tokens by using only the most popular (frequently used) words. In this example, we will use 10000 words."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zFabd9jBbNGN"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from keras.preprocessing.text import Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2u7Jwdf9bNGO"},"outputs":[],"source":["all_tokenizer = Tokenizer()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i-jOHMA_bNGP"},"outputs":[],"source":["all_tokenizer.fit_on_texts( imdb_df.review )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QB_H6vyibNGU"},"outputs":[],"source":["max_num_tokens = 10000"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lk7TLTO6bNGU"},"outputs":[],"source":["tokenizer = Tokenizer(num_words = max_num_tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8y7QCVqnbNGV"},"outputs":[],"source":["tokenizer.fit_on_texts( imdb_df.review )"]},{"cell_type":"markdown","metadata":{"id":"NFPJvJXGbNGd"},"source":["### Encode Y Variable"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FkUnnz-3bNGd"},"outputs":[],"source":["y = np.array(imdb_df.sentiment)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mmJrmTSwbNGd"},"outputs":[],"source":["y[0:5]"]},{"cell_type":"markdown","metadata":{"id":"4bAhxbiLbNGe"},"source":["How many classes available?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O_zmFiWZbNGe"},"outputs":[],"source":["imdb_df.sentiment.unique()"]},{"cell_type":"markdown","source":["## Text Vectorization"],"metadata":{"id":"wOUDmordrV_1"}},{"cell_type":"code","source":["from keras.layers import TextVectorization"],"metadata":{"id":"Wht9SNbQsNX_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_review_length = 552"],"metadata":{"id":"vzZteSHDr76_"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z24TwEJ_bNGj"},"outputs":[],"source":["vectorize_layer = TextVectorization(max_tokens = max_num_tokens,\n","                                    output_mode='int',\n","                                    output_sequence_length = max_review_length,\n","                                    standardize='lower_and_strip_punctuation',\n","                                    split='whitespace')"]},{"cell_type":"code","source":["text_dataset = tf.data.Dataset.from_tensor_slices(list(imdb_df.review))"],"metadata":{"id":"VD4KUlEuszDS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vectorize_layer.adapt(text_dataset)"],"metadata":{"id":"Wj15Jrhushl7"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I346GV1YbNGj"},"outputs":[],"source":["vectorize_layer.get_vocabulary()[0:10]"]},{"cell_type":"markdown","source":["### Creating Word Index"],"metadata":{"id":"hFCs-0RNriGW"}},{"cell_type":"code","source":["voc = vectorize_layer.get_vocabulary()\n","word_index = dict(zip(voc, range(len(voc))))"],"metadata":{"id":"UYGTwr5Lrg1K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from itertools import islice\n","\n","first10 = dict(islice(word_index.items(), 10))\n","         \n","for word, i in first10.items():\n","  print(f\"{word} : {i}\")\n"],"metadata":{"id":"C-jUGPX9te9h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NPYtrILXbNGk"},"source":["### Split Datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XFM0pgKpbNGk"},"outputs":[],"source":["from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_4NNXJpbbNGk"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(imdb_df.review, \n","                                                    imdb_df.sentiment, \n","                                                    test_size = 0.2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xe-yzMaPbNGl"},"outputs":[],"source":["X_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0GLjMTx3bNGl"},"outputs":[],"source":["X_test.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ddvUY_jbNGl"},"outputs":[],"source":["input_shape = X_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dnV61_CxbNGl"},"outputs":[],"source":["input_shape"]},{"cell_type":"markdown","metadata":{"id":"gvSXQthsbNG6"},"source":["## Applying Pre trained embeddings"]},{"cell_type":"markdown","metadata":{"id":"1Qj6SugubNG6"},"source":["Word embeddings are generally computed using word-occurrence statistics (observations about what words co-occur in sentences or documents), using a variety of  techniques, some involving neural networks, others not. The idea of a dense, lowdimensional embedding space for words, computed in an unsupervised way, was initially explored by Bengio et al. in the early 2000s,1 but it only started to take off in research and industry applications after the release of one of the most famous and successful word-embedding schemes: the Word2vec algorithm (https://code.google.com/ archive/p/word2vec), developed by Tomas Mikolov at Google in 2013. Word2vec dimensions capture specific semantic properties, such as gender.\n","\n","There are various precomputed databases of word embeddings that you can download and use in a Keras Embedding layer. Word2vec is one of them. Another popular one is called Global Vectors for Word Representation (GloVe, https://nlp.stanford.edu/projects/glove), which was developed by Stanford researchers in 2014. This embedding technique is based on factorizing a matrix of word co-occurrence statistics. Its developers have made available precomputed embeddings for millions of English tokens, obtained from Wikipedia data and Common Crawl data.\n","\n","One of the most widely used pretrained word embeddings is Glove and can be downloaded from https://nlp.stanford.edu/projects/glove/ \n","\n","GloVe is pre-computed embeddings from 2014 English Wikipedia. It's a 822MB zip file named glove.6B.zip, containing 100-dimensional embedding vectors for 400,000 words (or non-word tokens)."]},{"cell_type":"code","source":["!wget https://nlp.stanford.edu/data/glove.6B.zip"],"metadata":{"id":"FA3EGyDf7wHd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!mkdir glove\n","!unzip glove.6B.zip -d glove/"],"metadata":{"id":"uJN8PVIW75jw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!head -20 /content/glove/glove.6B.50d.txt"],"metadata":{"id":"nJuA1AaVAyl3"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rBPK0l5KbNG7"},"outputs":[],"source":["import os\n","\n","glove_dir = '/content/glove'\n","\n","embeddings_index = {}\n","f = open(os.path.join(glove_dir, 'glove.6B.50d.txt'))\n","line_num = 0\n","\n","for line in f:\n","    ## The following code is done for printing the first line \n","    if( line_num == 0):\n","        print( line )\n","        line_num += 1\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","\n","print('Found %s word vectors.' % len(embeddings_index))"]},{"cell_type":"markdown","metadata":{"id":"--jRreb0bNG7"},"source":["Get the word indexes from the our tokenizer, which contains the indexes of the words in our corpus."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jIHAcxz5bNG7"},"outputs":[],"source":["word_index = tokenizer.word_index"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sAJto-V5bNG8"},"outputs":[],"source":["embedding_dim = 50 #This is because we have downloaded GloVec for 100d embeddings\n","max_words = 10000\n","\n","### The embedding matrix will have \n","embedding_matrix = np.zeros((max_words, \n","                             embedding_dim))\n","\n","for word, i in word_index.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if i < max_words:\n","        if embedding_vector is not None:\n","            # Words not found in embedding index will be all-zeros.\n","            embedding_matrix[i] = embedding_vector\n"]},{"cell_type":"code","source":["embedding_matrix.shape"],"metadata":{"id":"wFaBBxnAEFQS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dE-1lYJbbNG8"},"source":["### Embedding Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F_Ci7bAXbNG8"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from keras.models import Sequential\n","from keras.layers import Flatten, Dense, Activation\n","from keras.layers import Embedding\n","from keras.layers import Dropout\n","from keras.optimizers import SGD\n","\n","tf.keras.backend.clear_session()  # clear default graph\n","\n","pre_trained_emb_model = Sequential()\n","pre_trained_emb_model.add(keras.Input(shape=(1,), dtype=tf.string))\n","pre_trained_emb_model.add(vectorize_layer)\n","pre_trained_emb_model.add(Embedding(max_num_tokens, \n","                                    embedding_dim, \n","                                    input_length=max_review_length,\n","                                    name='layer_embedding'))\n","\n","pre_trained_emb_model.add(Flatten())\n","pre_trained_emb_model.add(Dense(32, activation='relu'))\n","pre_trained_emb_model.add(Dense(1, activation='sigmoid'))\n","pre_trained_emb_model.summary()"]},{"cell_type":"markdown","metadata":{"id":"EkVy6IvabNG9"},"source":["The Embedding layer has a single weight matrix: a 2D float matrix where each entry *i* is the word vector meant to be associated with index i. Simple enough. Let's just load the GloVe matrix we prepared into our Embedding layer, the first layer in our model:\n","\n","Additionally, we freeze the embedding layer (we set its trainable attribute to False), following the same rationale as what you are already familiar with in the context of pre-trained convnet features: when parts of a model are pre-trained (like our Embedding layer), and parts are randomly initialized (like our classifier), the pre-trained parts should not be updated during training to avoid forgetting what they already know. The large gradient update triggered by the randomly initialized layers would be very disruptive to the already learned features."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oY3iUJr-bNG9"},"outputs":[],"source":["pre_trained_emb_model.get_layer('layer_embedding').set_weights([embedding_matrix])\n","pre_trained_emb_model.get_layer('layer_embedding').trainable = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oerg97gBbNG-"},"outputs":[],"source":["#sgd = SGD(learning_rate=0.01, momentum=0.8)\n","pre_trained_emb_model.compile(optimizer='adam',\n","                              loss='binary_crossentropy',\n","                              metrics=['accuracy'])"]},{"cell_type":"code","source":["pre_trained_emb_history = pre_trained_emb_model.fit(X_train, \n","                                                    y_train,\n","                                                    epochs=20,\n","                                                    batch_size=128,\n","                                                    validation_split=0.1)"],"metadata":{"id":"E3GlC__OzYd6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ivXdsbwhbNG-"},"source":["### Embedding Layer with Dropouts"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aItSACvtbNG-"},"outputs":[],"source":["tf.keras.backend.clear_session()  # clear default graph\n","\n","pre_trained_emb_model = Sequential()\n","pre_trained_emb_model.add(keras.Input(shape=(1,), dtype=tf.string))\n","pre_trained_emb_model.add(vectorize_layer)\n","pre_trained_emb_model.add(Embedding(max_num_tokens, \n","                                    embedding_dim, \n","                                    input_length=max_review_length,\n","                                    name='layer_embedding'))\n","pre_trained_emb_model.add(Flatten())\n","pre_trained_emb_model.add(Dense(64))\n","pre_trained_emb_model.add(Activation('sigmoid'))\n","pre_trained_emb_model.add(Dropout(0.4))\n","\n","pre_trained_emb_model.add(Dense(1))\n","pre_trained_emb_model.add(Activation('sigmoid'))\n","pre_trained_emb_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iaskvYBobNG_"},"outputs":[],"source":["pre_trained_emb_model.get_layer('layer_embedding').set_weights([embedding_matrix])\n","pre_trained_emb_model.get_layer('layer_embedding').trainable = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_BiFnv04bNG_"},"outputs":[],"source":["pre_trained_emb_model.compile(optimizer='rmsprop',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","pre_trained_emb_history = pre_trained_emb_model.fit(X_train, \n","                                                    y_train,\n","                                                    epochs=10,\n","                                                    batch_size=64,\n","                                                    validation_split=0.2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rYgjdxxwbNG_"},"outputs":[],"source":["plot_accuracy(pre_trained_emb_history.history)"]},{"cell_type":"markdown","source":["### Exploring the embeddings"],"metadata":{"id":"YqeTLMGTHKs9"}},{"cell_type":"code","source":["from gensim.test.utils import datapath, get_tmpfile\n","from gensim.models import KeyedVectors\n","\n","word2vec_output_file = \"/content/glove/glove.6B.50d.txt\""],"metadata":{"id":"EQEBccoWG_ZE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pretrained_w2v_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False, no_header=True)"],"metadata":{"id":"CDSlnOYqHFXH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pretrained_w2v_model.most_similar('bangalore')"],"metadata":{"id":"CzodvN1LHbo8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pretrained_w2v_model.most_similar('dhoni')"],"metadata":{"id":"c61PbcV0H1sY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pretrained_w2v_model.most_similar('google')"],"metadata":{"id":"0Ri2LbuZH7vW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pretrained_w2v_model.most_similar('hp')"],"metadata":{"id":"oL_867btJDIK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pretrained_w2v_model.most_similar('wikipedia')"],"metadata":{"id":"7CiiUxAnJGaa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def analogy(a, b, c):\n","    result = pretrained_w2v_model.most_similar([c, b], [a])\n","    return result[0][0]"],"metadata":{"id":"Re5m3yCpIDvW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["analogy('india', 'indian', 'japan')"],"metadata":{"id":"HN3-w5sMIIiu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["analogy('india', 'delhi', 'canada')"],"metadata":{"id":"Vck-uW0WIVKL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["analogy('india', 'dhoni', 'england')"],"metadata":{"id":"pYVkNww-IdZp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"ZUywYW9tbNHA"},"source":["## Excellent References\n","\n","For further exploration and better understanding, you can use the following references.\n","\n","- Glossary of Deep Learning: Word Embedding\n","\n","    https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca\n","\n","\n","- wevi: word embedding visual inspector\n","\n","    https://ronxin.github.io/wevi/  \n","    \n","    \n","- Learning Word Embedding    \n","\n","    https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html\n","\n","\n","- On the contribution of neural networks and word embeddings in Natural Language Processing\n","\n","    https://medium.com/@josecamachocollados/on-the-contribution-of-neural-networks-and-word-embeddings-in-natural-language-processing-c8bb1b85c61c"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"provenance":[]},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}